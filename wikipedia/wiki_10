<doc id="1777" url="http://en.wikipedia.org/wiki?curid=1777" title="April 2">
April 2

April 2 is the day of the year in the Gregorian calendar.

</doc>
<doc id="1778" url="http://en.wikipedia.org/wiki?curid=1778" title="Acetylene">
Acetylene

Acetylene (systematic name: ethyne) is the chemical compound with the formula C2H2. It is a hydrocarbon and the simplest alkyne. This colourless gas is widely used as a fuel and a chemical building block. It is unstable in pure form and thus is usually handled as a solution. Pure acetylene is odourless, but commercial grades usually have a marked odour due to impurities.
As an alkyne, acetylene is unsaturated because its two carbon atoms are bonded together in a triple bond. The carbon–carbon triple bond places all four atoms in the same straight line, with CCH bond angles of 180°. Since acetylene is a linear symmetrical molecule, it possesses the D∞h point group.
Discovery.
Acetylene was discovered in 1836 by Edmund Davy, who identified it as a "new carburet of hydrogen". It was rediscovered in 1860 by French chemist Marcellin Berthelot, who coined the name "acetylene". Berthelot was able to prepare this gas by passing vapours of organic compounds (methanol, ethanol, etc.) through a red-hot tube and collecting the effluent. He also found acetylene was formed by sparking electricity through mixed cyanogen and hydrogen gases. Berthelot later obtained acetylene directly by passing hydrogen between the poles of a carbon arc. Commercially available acetylene gas could smell foul due to the common impurities hydrogen sulphide and phosphine. However, acetylene gas with high purity would generate a light and sweet smell.
Preparation.
Today acetylene is mainly manufactured by the partial combustion of methane or appears as a side product in the ethylene stream from cracking of hydrocarbons. Approximately 400,000 tonnes are produced by this method annually. Its presence in ethylene is usually undesirable because of its explosive character and its ability to poison Ziegler-Natta catalysts. It is selectively hydrogenated into ethylene, usually using Pd–Ag catalysts.
Until the 1950s, when oil supplanted coal as the chief source of reduced carbon, acetylene (and the aromatic fraction from coal tar) was the main source of organic chemicals in the chemical industry. It was prepared by the hydrolysis of calcium carbide, a reaction discovered by Friedrich Wöhler in 1862 and still familiar to students:
Calcium carbide production requires extremely high temperatures, ~2000 °C, necessitating the use of an electric arc furnace. In the US, this process was an important part of the late-19th century revolution in chemistry enabled by the massive hydroelectric power project at Niagara Falls.
Bonding.
In terms of valence bond theory, in each carbon atom the 2s orbital hybridizes with one 2p orbital thus forming an sp hybrid. The other two 2p orbitals remain unhybridized. The two ends of the two sp hybrid orbital overlap to form a strong σ valence bond between the carbons, while on each of the other two ends hydrogen atoms attach also by σ bonds. The two unchanged 2p orbitals form a pair of weaker π bonds.
Physical properties.
Changes of state.
At atmospheric pressure, acetylene cannot exist as a liquid and does not have a melting point. The triple point on the phase diagram corresponds to the melting point (−80.8 °C) at the minimum pressure at which liquid acetylene can exist (1.27 atm). At temperatures below the triple point, solid acetylene can change directly to the vapour (gas) by sublimation. The sublimation point at atmospheric pressure is −84 °C.
Other.
The adiabatic flame temperature in air at atmospheric pressure is 2534 °C.
Acetylene gas can be dissolved in acetone or dimethylformamide in room temperature and 1 atm.
Reactions.
One new application is the conversion of acetylene to ethylene for use in making a variety of polyethylene plastics. An important reaction of acetylene is its combustion, the basis of the acetylene welding technologies. Otherwise, its major applications involve its conversion to acrylic acid derivatives.
Compared to most hydrocarbons, acetylene is relatively acidic, though it is still much less acidic than water or ethanol. Thus it reacts with strong bases to form acetylide salts. For example, acetylene reacts with sodium amide in liquid ammonia to form sodium acetylide, and with butyllithium in cold THF to give lithium acetylide.
Acetylides of heavy metals are easily formed by reaction of acetylene with the metal ions. Several, e.g., silver acetylide (Ag2C2) and copper acetylide (Cu2C2), are powerful and very dangerous explosives.
Reppe chemistry.
Walter Reppe discovered that in the presence of metal catalysts, acetylene can react to give a wide range of industrially significant chemicals.
Applications.
Welding.
Approximately 20 percent of acetylene is supplied by the industrial gases industry for oxyacetylene gas welding and cutting due to the high temperature of the flame; combustion of acetylene with oxygen produces a flame of over 3,600 K (3,300 °C, 6,000 °F), releasing 11.8 kJ/g. Oxyacetylene is the hottest burning common fuel gas. Acetylene is the third hottest natural chemical flame after dicyanoacetylene's 5260 K (4990 °C, 9010 °F) and cyanogen at 4798 K (4525 °C, 8180 °F). Oxy-acetylene welding was a very popular welding process in previous decades; however, the development and advantages of arc-based welding processes have made oxy-fuel welding nearly extinct for many applications. Acetylene usage for welding has dropped significantly. On the other hand, oxy-acetylene welding "equipment" is quite versatile – not only because the torch is preferred for some sorts of iron or steel welding (as in certain artistic applications), but also because it lends itself easily to brazing, braze-welding, metal heating (for annealing or tempering, bending or forming), the loosening of corroded nuts and bolts, and other applications. Bell Canada cable repair technicians still use portable acetylene fuelled torch kits as a soldering tool for sealing lead sleeve splices in manholes and in some aerial locations. Oxyacetylene welding may also be used in areas where electricity is not readily accessible. As well, oxy-fuel cutting is still very popular and oxy-acetylene cutting is utilized in nearly every metal fabrication shop. For use in welding and cutting, the working pressures must be controlled by a regulator, since above 15 psi, if subjected to a shockwave (caused for example by a flashback), acetylene will decompose explosively into hydrogen and carbon. 
Portable lighting.
Calcium carbide was used to generate acetylene used in the lamps for portable or remote applications. It was used for miners and cavers before the widespread use of incandescent lighting; or many years later low-power/high-lumen LED lighting; and is still used by mining industries in some nations without workplace safety laws. It was also used as an early light source for lighthouses.
Niche applications.
In 1881, the Russian chemist Mikhail Kucherov described the hydration of acetylene to acetaldehyde using catalysts such as mercury(II) bromide. Before the advent of the Wacker process, this reaction was conducted on an industrial scale.
The polymerization of acetylene with Ziegler-Natta catalysts produces polyacetylene films. Polyacetylene, a chain of CH centres with alternating single and double bonds, was the one of first discovered organic semiconductors. Its reaction with iodine produces a highly electrically conducting material. Although such materials are not useful, these discoveries led to the developments of organic semiconductors, as recognized by the Nobel Prize in Chemistry in 2000 to Alan J. Heeger, Alan G MacDiarmid, and Hideki Shirakawa.
In the early 20th Century acetylene was widely used for illumination, including street lighting in some towns. Most early automobiles used carbide lamps before the adoption of electric headlights.
Acetylene is sometimes used for carburization (that is, hardening) of steel when the object is too large to fit into a furnace.
Acetylene is used to volatilize carbon in radiocarbon dating. The carbonaceous material in an archeological sample is treated with lithium metal in a small specialized research furnace to form lithium carbide (also known as lithium acetylide). The carbide can then be reacted with water, as usual, to form acetylene gas to be fed into mass spectrometer to measure the isotopic ratio of carbon-14 to carbon-12.
Natural occurrence.
The energy richness of the C≡C triple bond and the rather high solubility of acetylene in water make it a suitable substrate for bacteria, provided an adequate source is available. A number of bacteria living on acetylene have been identified. The enzyme acetylene hydratase catalyzes the hydration of acetylene to give acetaldehyde.
Acetylene is a moderately common chemical in the universe, often associated with the atmospheres of gas giants. One curious discovery of acetylene is on Enceladus, a moon of Saturn. Natural acetylene is believed to form from catalytic decomposition of long chain hydrocarbons at temperatures of 1,770 K and above. Since such temperatures are highly unlikely on such a small distant body, this discovery is potentially suggestive of catalytic reactions within that moon, making it a promising site to search for prebiotic chemistry.
Safety and handling.
Acetylene is not especially toxic but, when generated from calcium carbide, it can contain toxic impurities such as traces of phosphine and arsine, which give it a distinct garlic-like smell. It is also highly flammable, as most light hydrocarbons, hence its use in welding. Its most singular hazard is associated with its intrinsic instability, especially when it is pressurized: under certain conditions acetylene can react in an exothermic addition-type reaction to form a number of products, typically benzene and/or vinylacetylene, possibly in addition to carbon and hydrogen. Consequently, acetylene, if initiated by intense heat or a shockwave, can decompose explosively if the absolute pressure of the gas exceeds about 200 kPa (29 psi). Most regulators and pressure gauges on equipment report gauge pressure and the safe limit for acetylene therefore is 101 kPagage or 15 psig. It is therefore supplied and stored dissolved in acetone or dimethylformamide (DMF), contained in a gas cylinder with a porous filling (Agamassan), which renders it safe to transport and use, given proper handling.
Copper catalyses the decomposition of acetylene and as a result acetylene should not be transported in copper pipes. Brass pipe fittings should also be avoided.

</doc>
<doc id="1779" url="http://en.wikipedia.org/wiki?curid=1779" title="Alfred">
Alfred

The name Alfred may refer to:

</doc>
<doc id="1781" url="http://en.wikipedia.org/wiki?curid=1781" title="August 28">
August 28

August 28 is the day of the year in the Gregorian calendar.

</doc>
<doc id="1784" url="http://en.wikipedia.org/wiki?curid=1784" title="Athenian democracy">
Athenian democracy

Athenian democracy developed around the fifth century BC in the Greek city-state (known as a polis) of Athens, comprising the city of Athens and the surrounding territory of Attica. Athenian democracy is the first known democracy in the world. Other Greek cities set up democracies, most following the Athenian model, but none are as well documented as Athens.
It was a system of direct democracy, in which participating citizens voted directly on legislation and executive bills. Participation was not open to all residents: to vote one had to be an adult, male citizen, and the number of these "varied between 30,000 and 50,000 out of a total population of around 250,000 to 300,000."
At times, the opinion of voters could be strongly influenced by the political satire of the comic poets at the theatres.
Solon (594 BC), Cleisthenes (508/7 BC), an aristocrat, and Ephialtes (462 BC) contributed to the development of Athenian democracy.
The longest-lasting democratic leader was Pericles. After his death, Athenian democracy was twice briefly interrupted by oligarchic revolutions towards the end of the Peloponnesian War. It was modified somewhat after it was restored under Eucleides; and the most detailed accounts of the system are of this fourth-century modification rather than the Periclean system. Democracy was suppressed by the Macedonians in 322 BC. The Athenian institutions were later revived, but how close they were to a real democracy is debatable.
Etymology.
The word "democracy" (Greek: δημοκρατία) combines the elements "dêmos" (δῆμος, which means "people") and "krátos" (κράτος, which means "force" or "power"). In the words "monarchy" and "oligarchy", the second element "arche" (ἀρχή) means "rule", "leading" or "being first". It is unlikely that the term "democracy" was coined by its detractors who rejected the possibility of a valid "demarchy", as the word "demarchy" already existed and had the meaning of mayor or municipal. One could assume the new term was coined and adopted by Athenian democrats.
The word is attested in Herodotus, who wrote some of the first surviving Greek prose, but this might not have been before 440 or 430 BC. We are not certain that the word "democracy" was extant when systems that came to be called democratic were first instituted, but around 460 BC an individual is known with the name of 'Democrates', a name possibly coined as a gesture of democratic loyalty; the name can also be found in Aeolian Temnus.
History and development.
Athens was not the only polis in Ancient Greece that instituted a democratic regime. Aristotle cites many other cities as well. "Yet, it is only with reference to Athens that we can attempt to trace some of specific sixth century events that led to the institution of democracy at the end of the century."
Before the first attempt at democratic government, Athens was ruled by a series of archons or chief magistrates, and the Areopagus, made up of ex-archons. The members of these institutions were generally aristocrats, who ruled the polis for their own advantage. In 621 BC Draco codified a set of "notoriously harsh" laws that were "a clear expression of the power of the aristocracy over everybody else." This did not stop the aristocratic families feuding amongst themselves to obtain as much power as possible.
Therefore, by the 6th century BC, the majority of Athenians "had been 'enslaved' to the rich", and they called upon Plato's ancestor Solon, premier archon at the time, to liberate them and halt the feuding of the aristocracy. However, the "enfranchisement of the local laboring classes was succeeded by the development of chattel slavery, the enslavement of, in large part, foreigners."
Solon, the mediator, reshaped the city "by absorbing the traditional aristocracy in a definition of citizenship which allotted a political function to every free resident of Attica. Athenians were not slaves but citizens, with the right, at the very least, to participate in the meetings of the assembly." Under these reforms, the position of archon "was opened to all with certain property qualifications, and a Boule, a rival council of 400, was set up. The Areopagus, nevertheless, retained 'guardianship of the laws'". A major contribution to democracy was Solon's setting up of an "Ecclesia" or Assembly, which was open to all male citizens. However, "one must bear in mind that its agenda was apparently set entirely by the Council of 400", "consisting of 100 members from each of the four tribes", that had taken "over many of the powers which the Areopagos had previously exercised."
Not long afterwards, the nascent democracy was overthrown by the tyrant Peisistratos, but was reinstated after the expulsion of the son of Peisistratos in 510. This sort of aristocratic takeover "was ended by the appeal by one contender, Cleisthenes, for the support of the populace." The reforms of Cleisthenes in 508/7 undermined the domination of the aristocratic families and connected every Athenian to the city's rule. "Cleisthenes fixed the boundaries of the polis as a political rather than a geographical entity – boundaries which Solon had left permeable – by formally identifying the free inhabitants of Attica at that time as Athenian citizens." He did this by making the traditional tribes politically irrelevant and instituting ten new tribes, each made up of about three "trytties", each consisting of several "demes". "Every male citizen on reaching the age of 18 was now to be registered in his deme. It was this registration which confirmed his citizenship."
A third set of reforms was instigated by Ephialtes in 462/1. While his opponents were away attempting to assist the Spartans, Ephialtes persuaded the Assembly to reduce the powers of the Areopagus: "in effect stripping it of all its controlling and supervisory powers and leaving it only as a court for cases of homicide and certain offences of sacrilege." At the same time or soon afterwards, the membership of the Areopagus was extended to the lower level of the propertied citizenship.
Participation and exclusion.
Size and make-up of the Athenian population.
Estimates of the population of ancient Athens vary. During the 4th century BC, there might well have been some 250,000–300,000 people in Attica. Citizen families could have amounted to 100,000 people and out of these some 30,000 would have been the adult male citizens entitled to vote in the assembly. In the mid-5th century the number of adult male citizens was perhaps as high as 60,000, but this number fell precipitously during the Peloponnesian War. This slump was permanent, due to the introduction of a stricter definition of citizen described below. From a modern perspective these figures may seem small, but among Greek city-states Athens was huge: most of the thousand or so Greek cities could only muster 1000–1500 adult male citizens each; and Corinth, a major power, had at most 15,000.
The non-citizen component of the population was made up of resident foreigners (metics) and slaves, with the latter perhaps somewhat more numerous. Around 338 BC the orator Hyperides (fragment 13) claimed that there were 150,000 slaves in Attica, but this figure is probably no more than an impression: slaves outnumbered those of citizen stock but did not swamp them.
Citizenship in Athens.
Only adult male Athenian citizens who had completed their military training as ephebes had the right to vote in Athens. The percentage of the population that actually participated in the government was 10 to 20% of the total number of inhabitants, but this varied from the fifth to the fourth century BC. This excluded a majority of the population: slaves, freed slaves, children, women and metics (foreigners
resident in Athens). The women had limited rights and privileges, had restricted movement in public, and were very segregated from the men.
Also excluded from voting were citizens whose rights were under suspension (typically for failure to pay a debt to the city: see atimia); for some Athenians this amounted to permanent (and in fact inheritable) disqualification. Given the exclusive and ancestral concept of citizenship held by Greek city-states, a relatively large portion of the population took part in the government of Athens and of other radical democracies like it, compared to oligarchies and aristocracies.
At Athens some citizens were far more active than others, but the vast numbers required for the system to work testify to a breadth of direct participation among those eligible that greatly surpassed any present-day democracy. Athenian citizens had to be descended from citizens—after the reforms of Pericles and Cimon in 450 BC, they "would be confined to those whose parents were both
Athenian". Although the legislation was not retrospective, five years later, when a free gift of grain had arrived from the Egyptian king, to be distributed among all citizens, "many 'illegitimates' were discovered" and removed from the registers.
Citizenship, "commonly applied not only to the individuals themselves but to their descendants as well", could be granted by the assembly, and was sometimes given to large groups (e.g. Plateans in 427 BC and Samians in 405 BC) but, by the 4th century, only to individuals and by a special vote with a quorum of 6000. This was generally done as a reward for some service to the state. In the course of a century, the number of citizenships so granted was in the hundreds rather than thousands.
Main bodies of governance.
There were three political bodies where citizens gathered in numbers running into the hundreds or thousands. These are the assembly (in some cases with a quorum of 6000), the council of 500 ("boule") and the courts (a minimum of 200 people, but running at least on some occasions up to 6000). Of these three bodies it is the assembly and the courts that were the true sites of power — although courts, unlike the assembly, were never simply called the "demos" (the People) as they were manned by a subset of the citizen body, those over thirty. But crucially citizens voting in both were not subject to review and prosecution as were council members and all other officeholders.
In the 5th century BC we often hear of the assembly sitting as a court of judgment itself for trials of political importance and it is not a coincidence that 6000 is the number both for the full quorum for the assembly and for the annual pool from which jurors were picked for particular trials. By the mid-4th century however the assembly's judicial functions were largely curtailed, though it always kept a role in the initiation of various kinds of political trial.
Assembly/Ekklesia.
The central events of the Athenian democracy were the meetings of the assembly (ἐκκλησία, "ekklêsia"). Unlike a parliament, the assembly's members were not elected, but attended by right when they chose. Greek democracy created at Athens was direct, rather than representative: any adult male citizen over the age of 20 could take part, and it was a duty to do so. The officials of the democracy were in part elected by the Assembly and in large part chosen by lottery.
The assembly had four main functions: it made executive pronouncements (decrees, such as deciding to go to war or granting citizenship to a foreigner); it elected some officials; it legislated; and it tried political crimes. As the system evolved, the last function was shifted to the law courts. The standard format was that of speakers making speeches for and against a position followed by a general vote (usually by show of hands) of yes or no.
Though there might be blocs of opinion, sometimes enduring, on important matters, there were no political parties and likewise no government or opposition (as in the Westminster system). Voting was by simple majority. In the 5th century at least there were scarcely any limits on the power exercised by the assembly. If the assembly broke the law, the only thing that might happen is that it would punish those who had made the proposal that it had agreed to. If a mistake had been made, from the assembly's viewpoint it could only be because it had been misled.
As usual in ancient democracies, one had to physically attend a gathering in order to vote. Military service or simple distance prevented the exercise of citizenship. Voting was usually by show of hands (χειροτονία, "kheirotonia", "arm stretching") with officials judging the outcome by sight. This could cause problems when it became too dark to see properly. However, "any member of the Assembly could demand a recount". For a small category of votes a quorum of 6000 was required, principally grants of citizenship, and here small coloured stones were used, white for yes and black for no. At the end of the session, each voter tossed one of these into a large clay jar which was afterwards cracked open for the counting of the ballots. Ostracism required the voters to scratch names onto pieces of broken pottery (ὄστρακα, "ostraka"), though this did not occur within the assembly as such.
In the 5th century BC, there were 10 fixed assembly meetings per year, one in each of the ten state months, with other meetings called as needed. In the following century the meetings were set to forty a year, with four in each state month. One of these was now called the main meeting, "kyria ekklesia". Additional meetings might still be called, especially as up until 355 BC there were still political trials that were conducted in the assembly rather than in court. The assembly meetings did not occur at fixed intervals, as they had to avoid clashing with the annual festivals that followed the lunar calendar. There was also a tendency for the four meetings to be aggregated toward the end of each state month.
Attendance at the assembly was not always voluntary. In the 5th century public slaves forming a cordon with a red-stained rope herded citizens from the agora into the assembly meeting place (Pnyx), with a fine being imposed on those who got the red on their clothes. After the restoration of the democracy in 403 BC, pay for assembly attendance was introduced. This promoted a new enthusiasm for assembly meetings. Only the first 6000 to arrive were admitted and paid, with the red rope now used to keep latecomers at bay.
The Council/The Boule.
In 594 BC Solon is said to have created a boule of 400 to guide the work of the assembly. After the reforms of Cleisthenes, the Athenian Boule was elected by lot every year. Each of Cleisthenes's 10 tribes provided 50 councillors who were at least 30 years old.
The most important task of the Athenian Boule was to draft the deliberations ("probouleumata") for discussion and approval in the Ecclesia. The Boule also directed finances, controlled the maintenance of the fleet and of the cavalry, judged the fitness of the magistrates-elect, received foreign ambassadors, advised the "stratēgoi" [(generals)] in military matters, and could be given special powers by the Ecclesia in an emergency.
According to John Thorley, its membership was very carefully vetted. Cleisthenes restricted its membership, "to those of zeugitai status and above, probably arguing that these classes had a financial interest in good government". A member had to be approved by his deme, "and one can well imagine that demes were careful to select only those of known good sense who also had experience of local politics, and who were actually available to do the time-consuming job which demanded frequent attendance in Athens; and they probably favoured those who were well past 30".
The members from each of the ten tribes in the Boule took it in turns to act as a standing committee (the "prytaneis") of the Boule for a period of thirty-six days. All fifty members of the prytaneis on duty were housed and fed in the tholos of the "Prytaneion", a building adjacent to the "bouleuterion", where the boule met. "Each day one of their number was chosen by lot as chairman, and he was required to stay in the tholos for the twenty-four hour period of his office. The chairman for the day presided over any meeting of the Boule held that day, and if there was a meeting of the Assembly that day ... he also presided over that".
The boule also served as an executive committee for the assembly, and oversaw the activities of certain other magistrates. The boule coordinated the activities of the various boards and magistrates that carried out the administrative functions of Athens and provided from its own membership randomly selected boards of ten responsible for areas ranging from naval affairs to religious observances. Altogether, the boule was responsible for a great portion of the administration of the state, but was granted relatively little latitude for initiative; the boule's control over policy was executed in its probouleutic, rather than its executive function; in the former, it prepared measures for deliberation by the assembly, in the latter, it merely executed the wishes of the assembly.
Courts.
Athens had an elaborate legal system centered on full citizen rights (see "atimia"). The age limit of 30 or older, the same as that for office holders but ten years older than that required for participation in the assembly, gave the courts a certain standing in relation to the assembly. Jurors were required to be under oath, which was not required for attendance at the assembly. The authority exercised by the courts had the same basis as that of the assembly: both were regarded as expressing the direct will of the people. Unlike office holders (magistrates), who could be impeached and prosecuted for misconduct, the jurors could not be censured, for they, in effect, were the people and no authority could be higher than that. A corollary of this was that, at least acclaimed by defendants, if a court had made an unjust decision, it must have been because it had been misled by a litigant.
Essentially there were two grades of suit, a smaller kind known as "dike" (δίκη) or private suit, and a larger kind known as "graphe" or public suit. For private suits the minimum jury size was 200 (increased to 401 if a sum of over 1000 drachmas was at issue), for public suits 501. Under Cleisthenes' reforms, juries were selected by lot from a panel of 600 jurors, there being 600 jurors from each of the ten tribes of Athens, making a jury pool of 6000 in total. For particularly important public suits the jury could be increased by adding in extra allotments of 500. 1000 and 1500 are regularly encountered as jury sizes and on at least one occasion, the first time a new kind of case was brought to court (see "graphē paranómōn"), all 6,000 members of the jury pool may have attended to one case.
The cases were put by the litigants themselves in the form of an exchange of single speeches timed by a water clock or "clepsydra", first prosecutor then defendant. In a public suit the litigants each had three hours to speak, much less in private suits (though here it was in proportion to the amount of money at stake). Decisions were made by voting without any time set aside for deliberation. Jurors did talk informally amongst themselves during the voting procedure and juries could be rowdy, shouting out their disapproval or disbelief of things said by the litigants. This may have had some role in building a consensus. The jury could only cast a 'yes' or 'no' vote as to the guilt and sentence of the defendant. For private suits only the victims or their families could prosecute, while for public suits anyone ("ho boulomenos", 'whoever wants to' i.e. any citizen with full citizen rights) could bring a case since the issues in these major suits were regarded as affecting the community as a whole.
Justice was rapid: a case could last no longer than one day and "completed by sunset". Some convictions triggered an automatic penalty, but where this was not the case the two litigants each proposed a penalty for the convicted defendant and the jury chose between them in a further vote. No appeal was possible. There was however a mechanism for prosecuting the witnesses of a successful prosecutor, which it appears could lead to the undoing of the earlier verdict.
Payment for jurors was introduced around 462 BC and is ascribed to Pericles, a feature described by Aristotle as fundamental to radical democracy ("Politics" 1294a37). Pay was raised from 2 to 3 obols by Cleon early in the Peloponnesian war and there it stayed; the original amount is not known. Notably, this was introduced more than fifty years before payment for attendance at assembly meetings. Running the courts was one of the major expenses of the Athenian state and there were moments of financial crisis in the 4th century when the courts, at least for private suits, had to be suspended.
The system showed a marked anti-professionalism. No judges presided over the courts nor did anyone give legal direction to the jurors; magistrates had only an administrative function and were laymen. Most of the annual magistracies at Athens could only be held once in a lifetime. There were no lawyers as such; litigants acted solely in their capacity as citizens. Whatever professionalism there was tended to disguise itself; it was possible to pay for the services of a speechwriter or logographer ("logographos"), but this may not have been advertised in court. Probably jurors would be more impressed if it seemed as though the litigant were speaking for themselves.
Shifting balance between assembly and courts.
As the system evolved, the courts (that is, citizens under another guise) intruded upon the power of the assembly. From 355 BC political trials were no longer held in the assembly, but only in a court. In 416 BC the "graphē paranómōn" ("indictment against measures contrary to the laws") was introduced. Under this, anything passed by the assembly or even proposed but not yet voted on, could be put on hold for review before a jury — which might annul it and perhaps punish the proposer as well.
Remarkably, it seems that a measure blocked before the assembly voted on it did not need to go back to the assembly if it survived the court challenge: the court was enough to validate it. Once again it is important to bear in mind the lack of 'neutral' state intervention. To give a schematic scenario by way of illustration: two men have clashed in the assembly about a proposal put by one of them; it passed, and now the two of them go to court with the loser in the assembly prosecuting both the law and its proposer. The quantity of these suits was enormous: in effect the courts became a kind of upper house.
In the 5th century there was in effect no procedural difference between an executive decree and a law: they were both simply passed by the assembly. But from 403 BC they were set sharply apart. Henceforth laws were made not in the assembly, but by special panels of citizens drawn from the annual jury pool of 6000. They were known as the "nomothetai" (νομοθέται), the lawmakers.
Citizen-initiator.
The institutions sketched above — assembly, officeholders, council, courts — are incomplete without the figure that drove the whole system, "Ho boulomenos", he who wishes, or anyone who wishes. This expression encapsulated the right of citizens to take the initiative: to stand to speak in the assembly, to initiate a public lawsuit (that is, one held to affect the political community as a whole), to propose a law before the lawmakers or to approach the council with suggestions. Unlike officeholders, the citizen initiator was not voted before taking up office or automatically reviewed after stepping down — it had after all no set tenure and might be an action lasting only a moment. But any stepping forward into the democratic limelight was risky and if someone chose (another citizen initiator) they could be called to account for their actions and punished. There were also other terms used for "the persons who pleaded in public actions and those who had initiated private suits. Although the expression "ho diokon" (literally 'the one who pursues') was applied to the initiators of both public and private actions, the designations "kategoros" ('accuser') ... were used only of prosecutors in public actions and in the actions for homicide heard by the Areiopagos and other homicide courts."
Archons and the Areopagus.
Just before the reforms of Solon in the 7th century BC, Athens was governed by a few archons (three rising to nine) and the council of the Areopagus "(appointed by the powerful noble families from their own members)". There also seems to have been a type of citizen assembly, presumably of the hoplite class. However, "There seems little doubt that it was the arkhons, with the advice of the Areopagos, who really ran the state." The mass of people had no say in government at all.
Solon's reforms allowed the archons to come from some of the higher propertied classes and not only from the aristocratic families. Since the Areopagus was made up of ex-archons, this would eventually mean the weakening of the hold of the nobles there as well. However, even with Solon's creation of the citizen's assembly, the Archons and Areopagus still wielded a great deal of power.
The reforms of Cleisthenes meant that the archons were elected by the Assembly, but were still selected from the upper classes. The Areopagus kept its power as 'Guardian of the Laws', "which probably gave the Areopagos the power to intervene and to apply a veto if the Council of 500 or the Assembly or any magistrate acted or proposed to act 'unconstitutionally'", however this worked in practice.
When Ephialtes, and later Pericles, reduced the power of the Areopagus dramatically. The Assembly "passed a measure to limit the powers of the Areopagos, in effect stripping it of all its controlling and supervisory powers." In the play "The Eumenides", performed in 458, Aeschylus, himself a noble, portrays the Areopagus as a court established by Athena herself. It appears that Aeschylus "is trying to preserve the dignity of a severely battered institution."
Officeholders.
Approximately 1100 citizens (including the members of the council of 500) held office each year. They were mostly chosen by lot, with a much smaller (and more prestigious) group of about 100 elected. Neither was compulsory; individuals had to nominate themselves for both selection methods. In particular, those chosen by lot were citizens acting without particular expertise. This was almost inevitable since, with the notable exception of the generals (strategoi), each office could be held by the same person only once. For example "The same person could not be a member of the Boule in two consecutive
years, and could only be a member twice in a lifetime."
Part of the ethos of democracy, however, was the building of general competence by ongoing involvement. In the 5th century version of the democracy, the ten annually elected generals were often very prominent, but for those who had power, it lay primarily in their frequent speeches and in the respect accorded them in the assembly, rather than their vested powers.
While citizens voting in the assembly "were" the people and so were free of review or punishment, those same citizens when holding an office "served" the people and could be punished very severely. All of them were subject to a review beforehand that might disqualify them for office and an examination after stepping down. Officeholders were the agents of the people, not their representatives. Citizens active as office holders served in a quite different capacity from when they voted in the assembly or served as jurors.
The assembly and the courts were regarded as the instantiation of the people of Athens: they were the people, no power was above them and they could not be reviewed, impeached or punished. However, when an Athenian took up an office, he was regarded as 'serving' the people. As such, he could be regarded as failing in his duty and be punished for it.
There were in fact some limitations on who could hold office. Age restrictions were in place with thirty years as a minimum, rendering about a third of the adult citizen body ineligible at any one time. An unknown proportion of citizens were also subject to disenfranchisement (atimia), excluding some of them permanently and others temporarily (depending on the type). Furthermore, all citizens selected were reviewed before taking up office ("dokimasia") at which they might be disqualified.
Competence does not seem to have been the main issue, but rather, at least in the 4th century BC, whether they were loyal democrats or had oligarchic tendencies. However, magistrates, after leaving office were subject to a scrutiny ("euthunai", literally 'straightenings' or 'submission of accounts') to review their performance. Both of these processes were in most cases brief and formulaic, but they opened up in the possibility, if some citizen wanted to take some matter up, of a contest before a jury court.
In the case of a scrutiny going to trial, there was the risk for the former officeholder of suffering severe penalties. Finally, even during his period of office, any officeholder could be impeached and removed from office by the assembly. In each of the ten "main meetings" ("kuriai ekklesiai") a year, the question was explicitly raised in the assembly agenda: were the office holders carrying out their duties correctly?
By and large the power exercised by these officials was routine administration and quite limited. The powers of officials were precisely defined and their capacity for initiative limited. They administered rather than governed. When it came to penal sanctions, no officeholder could impose a fine over fifty drachmas. Anything higher had to go before a court.
Selection by lot (allotment).
The use of a lottery to select officeholders was regarded as the most democratic means: elections would favour those who were rich, noble, eloquent and well-known, while allotment spread the work of administration throughout the whole citizen body, engaging them in the crucial democratic experience of, to use Aristotle's words, "ruling and being ruled in turn" (Politics 1317b28–30). The allotment of an individual was based on citizenship rather than merit or any form of personal popularity which could be bought. Allotment therefore was seen as a means to prevent the corrupt purchase of votes and it gave citizens a unique form of political equality as all had an equal chance of obtaining government office. Samons writes that "the system of selection by lottery for members of the Council of 500 and other officials (like the treasurers of the sacred funds) provided a potentially significant check on the dangers of demagoguery." However, this may not have been completely successful, as some "increasingly pandered to the electorate and ... often told the
people only what they wanted to hear."
The random assignment of responsibility to individuals who may or may not be competent has obvious risks, but the system included features meant to obviate possible problems. Athenians selected for office served as teams (boards, panels). In a group someone will know the right way to do things and those that do not may learn from those that do. During the period of holding a particular office everyone on the team is observing everybody else. There were however officials such as the nine archons, who while seemingly a board carried out very different functions from each other.
No office appointed by lot could be held twice by the same individual. The only exception was the boule or council of 500. In this case, simply by demographic necessity, an individual could serve twice in a lifetime. This principle extended down to the secretaries and undersecretaries who served as assistants to magistrates such as the archons. To the Athenians it seems what had to be guarded against was not incompetence but any tendency to use office as a way of accumulating ongoing power.
Election.
During an Athenian election, approximately one hundred officials out of a thousand were elected rather than chosen by lot. There were two main categories in this group: those required to handle large sums of money, and the 10 generals, the "strategoi". One reason that financial officials were elected was that any money embezzled could be recovered from their estates; election in general strongly favoured the rich, but in this case wealth was virtually a prerequisite.
Generals were elected not only because their role required expert knowledge but also because they needed to be people with experience and contacts in the wider Greek world where wars were fought. In the 5th century BC, principally as seen through the figure of Pericles, the generals could be among the most powerful people in the polis. Yet in the case of Pericles, it is wrong to see his power as coming from his long series of annual generalships (each year along with nine others). His office holding was rather an expression and a result of the influence he wielded. That influence was based on his relation with the assembly, a relation that in the first instance lay simply in the right of any citizen to stand and speak before the people. Under the 4th century version of democracy the roles of general and of key political speaker in the assembly tended to be filled by different persons. In part this was a consequence of the increasingly specialised forms of warfare practiced in the later period.
Elected officials too were subject to review before holding office and scrutiny after office. And they too could be removed from office at any time that the assembly met. There was also a death penalty for "inadequate performance" while in office.
Individualism in Athenian democracy.
A good example of the contempt the first democrats felt for those who did not participate in politics can be found in the modern word 'idiot', which finds its origins in the ancient Greek word ἰδιώτης, "idiōtēs", meaning a private person, a person who is not actively interested in politics; such characters were talked about with contempt, and the word eventually acquired its modern meaning. According to Thucydides, Pericles may have declared in a funeral oration:
We do not say that a man who takes no interest in politics is a man who minds his own business; we say that he has no business here at all.
Criticism of the democracy.
Athenian democracy had many critics, both ancient and modern. Ancient Greek critics of the democracy include Thucydides the general and historian, Aristophanes the playwright, Plato the pupil of Socrates, Aristotle the pupil of Plato, and a writer known as the Old Oligarch. Modern critics are more likely to find fault with the narrow definition of the citizen body, but in the ancient world the complaint, if anything, went in the opposite direction. For them, the common people were not necessarily the right people to rule and had made huge mistakes. According to Samons:The modern desire to look to Athens for lessons or encouragement for modern thought, government, or society must confront this strange paradox: the people that gave rise to and practiced ancient democracy left us almost nothing but criticism of this form of regime (on a philosophical or theoretical level). And what is more, the actual history of Athens in the period of its democratic government is marked by numerous failures, mistakes, and misdeeds—most infamously, the execution of Socrates—that would seem to discredit the ubiquitous modern idea that democracy leads to good government.
Thucydides, from his Aristocratic and historical viewpoint, reasoned that the common people were often much too credulous about even contemporary facts to rule justly. Josiah Ober notes that "Thucydides cites examples of two errors regarding Sparta: the beliefs that the two Spartan kings each had two votes in council and that there was a Spartan battalion called the 'Pitanate "lochos".' Thucydides sums up: 'Such is the degree of carelessness among the many ("hoi polloi") in the search for truth ("aletheia") and their preference for ready-made accounts'." He contrasted his own critical-historical approach to history with the way the demos decided upon the truth. So "Thucydides has established for his reader the existence of a potentially fatal structural flaw in the edifice of democratic ways of knowing and doing. The identification of this "flaw" is a key to his criticism of Athenian popular rule."
Also, Donald Kagan writes that "In the fourth century, Plato and Aristotle must have been repeating old complaints when they pointed out the unfairness of democracy: 'it distributes a sort of equality to equal and unequal alike'." Instead of seeing it as a fair system under which 'everyone' has equal rights, the critics saw it as the numerically preponderant poor tyrannizing the rich. They regarded this as manifestly unjust. In Aristotle this is categorized as the difference between 'arithmetic' and 'geometric' (i.e. proportional) equality.
To its ancient detractors rule by the "demos" was also reckless and arbitrary. Two examples demonstrate this:
While Plato blamed democracy for killing Socrates, his criticisms of the rule of the demos were much more extensive. Much of his writings were about his alternatives to democracy. His "The Republic", "The Statesman" and "Laws" contained many arguments against democratic rule and in favour of a much narrower form of government: "The organization of the city must be confided to those who possess knowledge, who alone can enable their fellow-citizens to attain virtue, and therefore excellence, by means of education."
Whether the democratic failures should be seen as systemic, or as a product of the extreme conditions of the Peloponnesian war, there does seem to have been a move toward correction. A new version of democracy was established from 403 BC, but it can be linked with both earlier and subsequent reforms (graphē paranómōn 416 BC; end of assembly trials 355 BC). For instance, the system of "nomothesia" was introduced. In this:
A new law might be proposed by any citizen. Any proposal to modify an existing law had to be accompanied by a proposed replacement law. The citizen making the proposal had to publish it [in] advance: publication consisted of writing the proposal on a whitened board located next to the statues of the Eponymous Heroes in the agora. The proposal would be considered by the Council, and would be placed on the agenda of the Assembly in the form of a motion. If the Assembly voted in favor of the proposed change, the proposal would be referred for further consideration by a group of citizens called nomothetai (literally "establishers of the law").
Increasingly, responsibility was shifted from the assembly to the courts, with laws being made by jurors and all assembly decisions becoming reviewable by courts. That is to say, the mass meeting of all citizens lost some ground to gatherings of a thousand or so which were under oath, and with more time to focus on just one matter (though never more than a day). One downside was that the new democracy was less capable of rapid response.
Another tack of criticism is to notice the disquieting links between democracy and a number of less than appealing features of Athenian life. Although democracy predated Athenian imperialism by over thirty years, they are sometimes associated with each other. For much of the 5th century at least democracy fed off an empire of subject states. Thucydides the son of Milesias (not the historian), an aristocrat, stood in opposition to these policies, for which he was ostracised in 443 BC.
At times the imperialist democracy acted with extreme brutality, as in the decision to execute the entire male population of Melos and sell off its women and children simply for refusing to became subjects of Athens. The common people were numerically dominant in the navy, which they used to pursue their own interests in the form of work as rowers and in the hundreds of overseas administrative positions. Further they used the income from empire to fund payment for officeholding. This is the position set out by the anti-democratic pamphlet known whose anonymous author is often called the Old Oligarch. This writer (also called pseudo-Xenophon) produced several comments critical of democracy, such as:
1. Democracy is not the rule of the demos qua citizenship in the interest of the entire polis, but the self-interested rule of a sociological faction.
2. The collectivization of political responsibility for decisions and agreements in a democracy leads to dishonesty and the tendency to scapegoat individual speakers or magistrates.
3. Because it is an integrated system, democracy seems incapable of internal amelioration, yet because of its inclusivist tendencies, especially in regard to citizenship, it coopts its natural enemies and so generates few active opponents.
4. There is a strong relationship between a democracy's domestic and foreign policies; a rational imperial democracy will be likely to foment democracy among its subjects.
5. Democracy depends on naval power; naval power in turn depends on the control of capital resources; ergo a democracy will tend to be aggressively acquisitive. 
6. Democracy's core values of freedom and equality are not exclusive to the citizen population; noncitizens are also treated more equitably than is seemly.
7. Democracy tends to blur the distinction between nature and political culture, thereby blinding elites to their own best interests and luring them into immorality.
Aristotle also wrote about what he considered to be a better form of government than democracy. Rather than any citizen partaking with equal share in the rule, he thought that "Virtue understood as embracing courage and temperance and prudence as well as justice turns out to be the chief determinant for shares in rule. Those who are superior in virtue should receive greater shares in rule."
A case can be made that discriminatory lines came to be drawn more sharply under Athenian democracy than before or elsewhere, in particular in relation to women and slaves, as well as in the line between citizens and non-citizens. By so strongly validating one role, that of the male citizen, it has been argued that democracy compromised the status of those who did not share it.
Since the 19th century, the Athenian version of democracy has been seen by one group as a goal yet to be achieved by modern societies. They want representative democracy to be added to or even replaced by direct democracy in the Athenian way, perhaps by utilizing electronic democracy. Another group, on the other hand, considers that, since many Athenians were not allowed to participate in its government, Athenian democracy was not a democracy at all. "[C]omparisons with Athens will continue to be made as long as societies keep striving to realize democracy under modern conditions and their successes and failures are discussed."
Aftermath.
Alexander the Great had led a coalition of the Greek states to war with Persia in 336 BC, but his Greek soldiers were hostages for the behavior of their states as much as allies. His relations with Athens were already strained when he returned to Babylon in 324 BC; after his death, Athens and Sparta led several Greek states to war with Macedon and lost.
This led to the Hellenistic control of Athens, when the Macedonian king appointed a local agent as political governor in Athens. However, the governors, like Demetrius of Phalerum, appointed by Cassander, kept some of the traditional institutions in formal existence, although the Athenian public would consider them to be nothing more than Macedonian puppet dictators. Once Demetrius Poliorcetes ended Cassander's rule over Athens, Demetrius of Phalerum went into exile and the democracy was restored in 307 BC. However, by now Athens had become "politically impotent". An example of this was that, in 307, in order to curry favour with Macedonia and Egypt, three new tribes were created, two in honour of the Macedonian king and his son, and the other in honour of the Egyptian king.
However, when Rome fought Macedonia in 200, the Athenians abolished the first two new tribes and created a twelfth tribe in honour of the Pergamene king. The Athenians declared for Rome, and in 146 B.C. Athens became an autonomous "civitas foederata". "Her independence was however little more than municipal, and, though the forms of the democracy survived, Rome ... strengthened the aristocratic elements in the constitution."
Under Roman rule, the archons ranked as the highest officials. They were elected, and even foreigners such as Domitian and Hadrian held the office as a mark of honour. Four presided over the judicial administration. The Council (whose numbers varied at different times from three hundred to seven hundred and fifty) was appointed by lot. It was superseded in importance by the Areopagus, which, recruited from the elected archons, had an aristocratic character and was entrusted with wide powers. From the time Of Hadrian an imperial curator superintended the finances. The shadow of the old constitution lingered on and Archons and Areopagus survived the fall of the Roman Empire.
In 88 BC, there was a revolution under the philosopher Athenion, who, as tyrant, forced the Assembly to agree to elect whomever he might ask to office. Athenion allied with Mithridates of Pontus, and went to war with Rome; he was killed during the war, and was replaced by Aristion. The victorious Roman general, Publius Cornelius Sulla, left the Athenians their lives and did not sell them into slavery; he also restored the previous government, in 86 BC.
After Rome became an Empire under Augustus, the nominal independence of Athens dissolved and its government converged to the normal type for a Roman municipality, with a Senate of "decuriones".
Legacy.
Since the middle of the 20th century, every country has claimed to be a democracy, regardless of the actual makeup of its government. Yet, after the demise of Athenian democracy, few looked upon it as a good form of government. This was because no legitimation of that rule was formulated to counter the negative accounts of Plato and Aristotle. They saw it as the rule of the poor that plundered the rich, and so democracy was viewed as a sort of "collective tyranny". "Well into the 18th century democracy was consistently condemned." Sometimes, mixed constitutions evolved with a democratic element, but "it definitely did not mean self-rule by citizens."
In the age of Cicero and Caesar Rome was a republic, but not a democracy. Furthermore,
it would be misleading to say that the tradition of Athenian democracy was an important part of the 18th-century revolutionaries' intellectual background. The classical example that inspired the American and French revolutionaries as well as the English radicals was Rome rather than Greece. Thus, the Founding Fathers who met in Philadelphia in 1787, did not set up a Council of the Areopagos, but a Senate, that, eventually, met on the Capitol.
Following Rousseau (1712–1778), "democracy came to be associated with popular sovereignty instead of popular participation in the exercise of power."
Several German philosophers and poets took delight in the fullness of life in Athens, and not long afterwards "the English liberals put forward a new argument in favor of the Athenians". In opposition, thinkers such as Samuel Johnson were worried about the ignorance of a democratic decision-making body. However, "Macaulay and John Stuart Mill and George Grote saw the great strength of the Athenian democracy in the high level of cultivation that citizens enjoyed and called for improvements in the educational system of Britain that would make possible a shared civic consciousness parallel to that achieved by the ancient Athenians."
Therefore, it was George Grote, in his "History of Greece" (1846–1856), who would claim that "Athenian democracy was neither the tyranny of the poor, nor the rule of the mob." He argued that only by giving every citizen the vote would people ensure that the state would be run in the general interest. Later,
to the end of World War Il, democracy became dissociated from its ancient frame of reference It was not anymore only one of the many possible ways in which political rule could be organised in a polity: it became the only possible political system in an egalitarian society.

</doc>
<doc id="1786" url="http://en.wikipedia.org/wiki?curid=1786" title="Arabic numerals">
Arabic numerals

Arabic numerals or Hindu-Arabic numerals or Indo-Arabic numerals are the ten digits: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9. They are the most common symbolic representation of numbers in the world today.
The first positional numerical system was developed in Babylon in the 2nd millennium BC. While it used a zero-like placeholder, the first true zero was developed by ancient mathematicians in the Indian Subcontinent. Arabic numerals are used to represent this Hindu-Arabic numeral system, in which a sequence of digits such as "975" is read as a single number. This system is traditionally thought to have been adopted by the Persian and Arab mathematicians in India, and passed on to the Arabs further west. There is some evidence which suggests that the numerals in their current form developed from Arabic letters in the western regions of the Arab World. The current form of the numerals developed in North Africa, distinct in form from the Indian and eastern Arabic numerals. It was in the North African city of Bejaia that the Italian scholar Fibonacci first encountered the numerals; his work was crucial in making them known throughout Europe, and then further to the Europeans who spread it worldwide. The use of Arabic numerals spread around the world through European trade, books and colonialism.
In English, the term "Arabic numerals" can be ambiguous. It most commonly refers to the numeral system widely used in Europe and the Americas; to avoid confusion, Unicode calls these "European digits". "Arabic numerals" is also the conventional name for the entire family of related systems of Arabic and Indian numerals. It may also be intended to mean the numerals used by Arabs, in which case it generally refers to the Eastern Arabic numerals.
Although the phrase "Arabic numeral" is frequently capitalized, it is sometimes written in lower case: for instance, in its entry in the Oxford English dictionary. This helps distinguish it from "Arabic numerals" as the East Arabic numerals specific to the Arabs.
History.
Origins.
By the middle of the 2nd millennium BC, Babylonian mathematics had a sophisticated sexagesimal positional numeral system. The lack of a positional value (or zero) was indicated by a "space" between sexagesimal numerals. By 300 BC, a punctuation symbol (two slanted wedges) was co-opted as a placeholder in the same Babylonian system. In a tablet unearthed at Kish (dating from about 700 BC), the scribe Bêl-bân-aplu wrote his zeros with three hooks, rather than two slanted wedges.
The Babylonian placeholder was not a true zero because it was not used alone. Nor was it used at the end of a number. Thus numbers like 2 and 120 (2×60), 3 and 180 (3×60), 4 and 240 (4×60) looked the same because the larger numbers lacked a final sexagesimal placeholder. Only context could differentiate them.
The decimal Hindu-Arabic numeral system was invented in India around AD 500. The system was revolutionary by including a zero and positional notation. It is considered an important milestone in the development of mathematics. One may distinguish between this positional "system", which is identical throughout the family, and the precise glyphs used to write the numerals, which vary regionally. The glyphs most commonly used in conjunction with the Latin script since early modern times are 0 1 2 3 4 5 6 7 8 9.
The first universally accepted inscription containing the use of the 0 glyph is first recorded in the 9th century, in an inscription at Gwalior in Central India dated to 870. By this time, the use of the glyph had already reached Persia, and was mentioned in Al-Khwarizmi's descriptions of Indian numerals. Numerous Indian documents on copper plates exist, with the same symbol for zero in them, dated back as far as the 6th century AD.
The numeral system came to be known to both the Persian mathematician Al-Khwarizmi, whose book "On the Calculation with Hindu Numerals" written about 825 in Arabic, and the Arab mathematician Al-Kindi, who wrote four volumes, "On the Use of the Indian Numerals" ("Ketab fi Isti'mal al-'Adad al-Hindi") about 830. Their work was principally responsible for the diffusion of the Indian system of numeration in the Middle East and the West. In the 10th century, Middle-Eastern mathematicians extended the decimal numeral system to include fractions, as recorded in a treatise by Syrian mathematician Abu'l-Hasan al-Uqlidisi in 952–953. The decimal point notation was introduced by Sind ibn Ali, he also wrote the earliest treatise on Arabic numerals.
A distinctive West Arabic variant of the symbols begins to emerge around the 10th century in the Maghreb and Al-Andalus, called "ghubar" ("sand-table" or "dust-table") numerals, which are the direct ancestor of the modern Western Arabic numerals used throughout the world. Ghubar numerals themselves are probably of Roman origin.
Folk etymologies.
Some folk etymologies have argued that the original forms of these symbols indicated their value through the number of angles they contained, but no evidence exists of any such origin.
Adoption in Europe.
In 825 Al-Khwārizmī wrote a treatise in Arabic, "On the Calculation with Hindu Numerals", which survives only as the 12th-century Latin translation, "Algoritmi de numero Indorum". "Algoritmi", the translator's rendition of the author's name, gave rise to the word "algorithm" (Latin "algorithmus", "calculation method").
The first mentions of the numerals in the West are found in the "Codex Vigilanus" of 976.
From the 980s, Gerbert of Aurillac (later, Pope Sylvester II) used his position to spread knowledge of the numerals in Europe. Gerbert studied in Barcelona in his youth. He was known to have requested mathematical treatises concerning the astrolabe from Lupitus of Barcelona after he had returned to France.
Leonardo Fibonacci (Leonardo of Pisa), a mathematician born in the Republic of Pisa who had studied in Béjaïa (Bougie), Algeria, promoted the Indian numeral system in Europe with his 1202 book "Liber Abaci":
The numerals are arranged with their lowest value digit to the right, with higher value positions added to the left. This arrangement was adopted identically into the numerals as used in Europe. Languages written in the Latin alphabet run from left-to-right, unlike languages written in the Arabic alphabet. Hence, from the point of view of the reader, numerals in Western texts are written with the highest power of the base first whereas numerals in Arabic texts are written with the lowest power of the base first.
The reason the digits are more commonly known as "Arabic numerals" in Europe and the Americas is that they were introduced to Europe in the 10th century by Arabic-speakers of North Africa, who were then using the digits from Libya to Morocco. Arabs, on the other hand, call the system "Hindu numerals", referring to their origin in India. This is not to be confused with what the Arabs call the "Hindi numerals", namely the Eastern Arabic numerals (٠‎ - ١‎ - ٢‎ - ٣‎ -٤‎ - ٥‎ - ٦‎ - ٧‎ - ٨‎ - ٩‎) used in the Middle East, or any of the numerals currently used in Indian languages (e.g. Devanagari: ०.१.२.३.४.५.६.७.८.९).
The European acceptance of the numerals was accelerated by the invention of the printing press, and they became widely known during the 15th century. Early evidence of their use in Britain includes: an equal hour horary quadrant from 1396, in England, a 1445 inscription on the tower of Heathfield Church, Sussex; a 1448 inscription on a wooden lych-gate of Bray Church, Berkshire; and a 1487 inscription on the belfry door at Piddletrenthide church, Dorset; and in Scotland a 1470 inscription on the tomb of the first Earl of Huntly in Elgin Cathedral. (See G.F. Hill, "The Development of Arabic Numerals in Europe" for more examples.) In central Europe, the King of Hungary Ladislaus the Posthumous, started the use of Arabic numerals, which appear for the first time in a royal document of 1456. By the mid-16th century, they were in common use in most of Europe. Roman numerals remained in use mostly for the notation of Anno Domini years, and for numbers on clockfaces. Sometimes, Roman numerals are still used for enumeration of lists (as an alternative to alphabetical enumeration), for sequential volumes, to differentiate monarchs or family members with the same first names, and (in lower case) to number pages in prefatory material in books.
Adoption in Russia.
Cyrillic numerals were a numbering system derived from the Cyrillic alphabet, used by South and East Slavic peoples. The system was used in Russia as late as the early 18th century when Peter the Great replaced it with Arabic numerals.
Adoption in China.
Arabic numerals were introduced to China during the Yuan Dynasty (1271–1368) by the Muslim Hui people. In the early 17th century, European-style Arabic numerals were introduced by Spanish and Portuguese Jesuits.
Evolution of symbols.
The numeral system employed, known as algorism, is positional decimal notation. Various symbol sets are used to represent numbers in the Hindu-Arabic numeral system, which may have evolved from the Brahmi numerals, or developed independently from it. The symbols used to represent the system have split into various typographical variants since the Middle Ages:
The evolution of the numerals in early Europe is shown on a table created by the French scholar Jean-Étienne Montucla in his "Histoire de la Mathematique", which was published in 1757:
The Arabic numeral glyphs 0-9 are encoded in ASCII and Unicode at positions 0x30 to 0x39, matching up with the second hexadecimal digit for convenience:

</doc>
<doc id="1787" url="http://en.wikipedia.org/wiki?curid=1787" title="April 9">
April 9

April 9 is the day of the year in the Gregorian calendar.

</doc>
<doc id="1788" url="http://en.wikipedia.org/wiki?curid=1788" title="ABM">
ABM

ABM or Abm may refer to:

</doc>
<doc id="1789" url="http://en.wikipedia.org/wiki?curid=1789" title="Apuleius">
Apuleius

Apuleius (; also called Lucius Apuleius Madaurensis; c. 124 – c. 170 CE) was a Latin-language prose writer. He was a Numidian Berber and lived under the Roman Empire. He was from Madaurus (now M'Daourouch, Algeria). He studied Platonist philosophy in Athens, travelled to Italy, Asia Minor, and Egypt and was an initiate in several cults or mysteries. The most famous incident in his life was when he was accused of using magic to gain the attentions (and fortune) of a wealthy widow. He declaimed and then distributed a witty "tour de force" in his own defense before the proconsul and a court of magistrates convened in Sabratha, near ancient Tripoli, Libya. This is known as the "Apologia".
His most famous work is his bawdy picaresque novel, the "Metamorphoses", otherwise known as "The Golden Ass". It is the only Latin novel that has survived in its entirety. It relates the ludicrous adventures of one Lucius, who experiments with magic and is accidentally turned into a donkey.
Life.
Apuleius was born in Madaurus (now M'Daourouch, Algeria), a Roman colony in Numidia on the North African coast, bordering Gaetulia, and he described himself as "half-Numidian half-Gaetulian." Madaurus was the same "colonia" where Saint Augustine later received part of his early education, and, though located well away from the Romanized coast, is today the site of some pristine Roman ruins. As to his first name, no "praenomen" is given in any ancient source; late-medieval manuscripts began the tradition of calling him "Lucius" from the name of the hero of his novel. Details regarding his life come mostly from his defense speech ("Apology") and his work "Florida", which consists of snippets taken from some of his best speeches.
His father was a provincial magistrate ("duumvir") who bequeathed at his death the sum of nearly two million sesterces to his two sons. Apuleius studied with a master at Carthage (where he later settled) and later at Athens, where he studied Platonist philosophy among other subjects. He subsequently went to Rome to study Latin rhetoric and, most likely, to declaim in the law courts for a time before returning to his native North Africa. He also travelled extensively in Asia Minor and Egypt, studying philosophy and religion, burning up his inheritance while doing so.
Apuleius was an initiate in several cults or mysteries, including the Dionysian mysteries. He was a priest of Aesculapius and, according to Augustine, "sacerdos provinciae Africae" (ie., priest of the province of Carthage).
Not long after his return home he set out upon a new journey to Alexandria. On his way there he was taken ill at the town of Oea (modern-day Tripoli) and was hospitably received into the house of Sicinius Pontianus, with whom he had been friends when he had studied in Athens. The mother of Pontianus, Pudentilla, was a very rich widow. With her son's consent – indeed encouragement – Apuleius agreed to marry her. Meanwhile, Pontianus himself married the daughter of one Herennius Rufinus; he, indignant that Pudentilla's wealth should pass out of the family, instigated his son-in-law, together with a younger brother, Sicinius Pudens, a mere boy, and their paternal uncle, Sicinius Aemilianus, to join him in impeaching Apuleius upon the charge that he had gained the affections of Pudentilla by charms and magic spells. The case was heard at Sabratha, near Tripoli, c. 158 CE, before Claudius Maximus, proconsul of Africa. The accusation itself seems to have been ridiculous, and the spirited and triumphant defence spoken by Apuleius is still extant. This is known as the "Apologia (A Discourse on Magic)".
Apuleius accused a profligate personal enemy of turning his house into a brothel and prostituting his own wife.
Of his subsequent career we know little. Judging from the many works of which he was author, he must have devoted himself assiduously to literature. He occasionally gave speeches in public with great applause; he had the charge of exhibiting gladiatorial shows and wild beast events in the province, and statues were erected in his honour by the senate of Carthage and of other senates.
Works.
The Golden Ass.
"The Golden Ass" ("Asinus Aureus") or "Metamorphoses" is the only Latin novel that has survived in its entirety. It is an imaginative, irreverent, and amusing work that relates the ludicrous adventures of one Lucius, who experiments with magic and is accidentally turned into an ass. In this guise he hears and sees many unusual things, until escaping from his predicament in a rather unexpected way. Within this frame story are found many digressions, the longest among them being the well-known tale of Cupid and Psyche.
The "Metamorphoses" ends with the (once again human) hero, Lucius, eager to be initiated into the mystery cult of Isis; he abstains from forbidden foods, bathes, and purifies himself. He is introduced to the "Navigium Isidis". Then the secrets of the cult's books are explained to him, and further secrets are revealed before he goes through the process of initiation, which involves a trial by the elements in a journey to the underworld. Lucius is then asked to seek initiation into the cult of Osiris in Rome, and eventually is initiated into the "pastophoroi" – a group of priests that serves Isis and Osiris.
Other works.
His other works are:
Apuleius wrote many other works which have not survived. He wrote works of poetry and fiction, as well as technical treatises on politics, dendrology, agriculture, medicine, natural history, astronomy, music, and arithmetic, and he translated Plato's "Phaedo".
Spurious works.
The extant works wrongly attributed to Apuleius are:
Apuleian Sphere.
The Apuleian Sphere, also known as 'Columcille's Circle' or 'Petosiris's Circle', is a magical prognosticating device for predicting the survival of a patient.

</doc>
<doc id="1790" url="http://en.wikipedia.org/wiki?curid=1790" title="Alexander Selkirk">
Alexander Selkirk

Alexander Selkirk (1676 – 13 December 1721), also known as Alexander Selcraig, was a Scottish sailor who spent more than four years as a castaway after being marooned on an uninhabited island in the South Pacific Ocean (also known as the South Sea).
An unruly youth, Selkirk joined buccaneering expeditions to the South Sea, including one commanded by William Dampier, which called in for provisions at the Juan Fernández Islands off Chile. Selkirk judged correctly that his craft, the "Cinque Ports", was unseaworthy, and asked to be left there.
By the time he was rescued, he had become adept at hunting and making use of the resources he found on the island. His story of survival was widely publicised when he returned home and became a likely source of inspiration for the writer Daniel Defoe's fictional character Robinson Crusoe.
Early life and privateering.
The son of a shoemaker and tanner in Lower Largo, Fife, Scotland, Alexander Selkirk was born in 1676. In his youth he displayed a quarrelsome and unruly disposition. Summoned in August 1693 before the Kirk Session for his "indecent conduct in church", he "did not appear, being gone to sea." He was back at Largo in 1701, when he again came to the attention of church authorities for beating up his brothers.
Early on he was engaged in buccaneering. In 1703, he joined an expedition of the English privateer and explorer William Dampier to the South Sea, setting sail from Kinsale in Ireland on 11 September. They carried "letters of marque" from the Lord High Admiral authorising their armed merchant ships to attack foreign enemies, as the War of the Spanish Succession was then going on between England and France. While Dampier was captain of the "St George," Selkirk served on the "Cinque Ports", "St George"‍ '​s companion, as sailing master under Captain Thomas Stradling. By this time Selkirk must have had considerable experience at sea. 
In February 1704, following a stormy passage round Cape Horn, the privateers fought a long battle with a well-armed French vessel, the "St Joseph", only to have it escape to warn the Spanish of the buccaneers' arrival in the Pacific. A raid on the Panamanian gold mining town, Santa María, failed when the buccaneers' landing party was ambushed. The easy capture of the "Asunción", a heavily-laden merchantman, revived the men's hopes of plunder, and Selkirk was put in charge of the prize ship. After taking off some much needed provisions of wine, brandy, sugar, and flour, Dampier abruptly set the ship free, believing the gain was not worth the effort. In May 1704, Stradling decided to abandon Dampier and strike out on his own.
Castaway.
In September 1704, after parting ways with Dampier, Captain Stradling brought the "Cinque Ports" to an island known to the Spanish as Más a Tierra, located in the uninhabited Juan Fernández archipelago, 670 km off the coast of Chile, for a mid-expedition restocking of fresh water and supplies.
Selkirk had grave concerns about the seaworthiness of their vessel and probably wanted to make the needed repairs before going any further. Selkirk declared that he would rather be left on Juan Fernández than continue in a dangerously leaky ship. Stradling granted his request and landed Selkirk and his personal effects on the island. Selkirk regretted his rashness, but Stradling refused to let him back on board.
"Cinque Ports" did indeed later founder off the coast of what is now Colombia. Stradling and some of his crew survived the loss of their ship but were forced to surrender to the Spanish. The survivors were taken to Lima, Peru, where they endured a harsh imprisonment.
Life on the island.
At first, Selkirk remained along the shoreline of Juan Fernández. During this time he ate jasus (shellfish) and scanned the ocean daily for rescue, suffering all the while from loneliness, misery, and remorse. Hordes of raucous sea lions, gathering on the beach for the mating season, eventually drove him to the island's interior. Once inland, his way of life took a turn for the better. More foods were available there: feral goats—introduced by earlier sailors—provided him with meat and milk, while wild turnips, cabbage leaves, and dried pepper berries offered him variety and spice. Although rats would attack him at night, he was able, by domesticating and living near feral cats, to sleep soundly and in safety.
Selkirk proved resourceful in using materials he found on the island: he forged a new knife out of barrel hoops left on the beach, he built two huts out of pepper trees (one of which he used for cooking and the other for sleeping), and he employed his musket to hunt goats and his knife to clean their carcasses. As his gunpowder dwindled, he had to chase prey on foot. During one such chase he was badly injured when he tumbled from a cliff, lying helpless and unable to move for about a day. His prey had cushioned his fall, likely sparing him a broken back.
The lessons he had learned as a child from his father, a tanner, now served him well. For example, when his clothes wore out, he made new ones from hair-covered goatskins using a nail for sewing. 
As his shoes became unusable, he had no need to replace them, since his toughened, callused feet made protection unnecessary. He sang psalms and read from the Bible, finding it a comfort in his situation and a prop for his English.
During his sojourn on the island, two vessels came to anchor. Unfortunately for Selkirk, both were Spanish. As a Scotsman and a privateer, he risked a grim fate if captured and, therefore, tried to hide himself. On one occasion he was spotted and chased by a group of sailors from one of the ships. His pursuers urinated beneath the tree in which he was hiding, but failed to discover him. Frustrated, his would-be captors gave up and sailed away.
Rescue.
Selkirk's long-awaited deliverance came on 2 February 1709 by way of the "Duke", a privateering ship piloted by William Dampier, and its sailing companion, the "Duchess". Thomas Dover led the landing party that met Selkirk. After four years and four months without human company, Selkirk was almost incoherent with joy. The "Duke"‍‍ '​‍s captain and leader of the expedition, Woodes Rogers, mischievously referred to him as the governor of the island. The agile castaway, catching two or three goats a day, helped restore the health of Rogers' men, who were suffering from scurvy.
Captain Rogers was impressed not only by Selkirk's physical vigour, but by the peace of mind he had attained while living on the island, as well, observing: "One may see that solitude and retirement from the world is not such an insufferable state of life as most men imagine, especially when people are fairly called or thrown into it unavoidably, as this man was." He made Selkirk the "Duke"‍ '​s second mate, later giving him command of one of their prize ships, the "Increase", before it was ransomed by the Spanish.
Selkirk returned to privateering with a vengeance. At Guayaquil, in present-day Ecuador, he led a boat crew up the Guayas River, where a number of wealthy Spanish ladies had fled, and relieved them of the gold and jewels they had hidden inside their clothing. His part in the hunt for treasure galleons along the coast of Mexico resulted in the capture of the "Nuestra Señora de la Encarnación y Desengaño", renamed "Batchelor", on which he served as sailing master under Captain Dover to the Dutch East Indies. Selkirk completed the round-the-world voyage by the Cape of Good Hope as the sailing master of the "Duke", arriving at the Downs off the English coast on 1 October 1711. He had been away for eight years.
Later life and influence.
Selkirk's experience as a castaway aroused a great deal of attention in England. Rogers included an account of Selkirk's ordeal in a book chronicling their privateering expedition, titled "A Cruising Voyage Round the World" (1712). The following year, the prominent essayist Richard Steele wrote an article about him for "The Englishman" newspaper. Claiming his share of the "Duke"‍‍ '​‍s plundered wealth—about £800 (equivalent to £ today)—Selkirk appeared set to enjoy a life of ease and celebrity. However, legal disputes made the amount of any payment uncertain.
After a few months in London, he began to seem more like his former self again. In September 1713, he was charged with assaulting a shipwright in Bristol and may have been kept in confinement for two years. He returned to Lower Largo, where he met Sophia Bruce, a young dairymaid. They eloped to London early in 1717 but apparently did not marry. He was soon off to sea again, having enlisted in the Royal Navy. While on a visit to Plymouth in 1720, he married a widowed innkeeper named Frances Candis. He was serving as master's mate on board HMS "Weymouth", engaged in an anti-piracy patrol off the west coast of Africa, when he died on 13 December 1721, succumbing to the yellow fever that plagued the voyage. He was buried at sea.
When Daniel Defoe published "The Life and Surprising Adventures of Robinson Crusoe" (1719), few readers could have missed the resemblance to Selkirk. An illustration on the first page of the novel shows, in the words of modern explorer Tim Severin, "a rather melancholy-looking man standing on the shore of an island, gazing inland." He is dressed in the familiar hirsute goatskins, his feet and shins bare. Yet Crusoe's island is located not in the mid-latitudes of the South Pacific but 4300 km away in the Caribbean, where the furry attire would hardly be comfortable in the tropical heat. This incongruity supports the popular belief that Selkirk was a model for the fictional character.
In other literary works.
<poem>I am monarch of all I survey,
My right there is none to dispute;
From the centre all round to the sea,
I am lord of the fowl and the brute.</poem>
<poem>Oh, Alexander Selkirk knew the plight
Of being king and government and nation.
A road, a mile of kingdom, I am king
Of banks and stones and every blooming thing.</poem>
<poem>These passengers, by reason of their clinging to a mast,
Upon a desert island were eventually cast.
They hunted for their meals, as Alexander Selkirk used,
But they couldn’t chat together—they had not been introduced.</poem>
In film.
"Selkirk, the Real Robinson Crusoe" is a stop motion film by Walter Tournier, based on Selkirk's life. It premièred simultaneously in Argentina, Chile, and Uruguay on 2 February 2012. Distributed by The Walt Disney Company, it was the first full-length animated feature to be produced in Uruguay.
Commemoration.
The Scotsman is remembered in his former island home. In 1863, the crew of HMS "Topaze" placed a bronze tablet at a spot called Selkirk's Lookout on a hill of Más a Tierra, Juan Fernández Islands, to mark his stay.
Selkirk has also been memorialised in his Scottish birthplace. On 11 December 1885, after a speech by Lord Aberdeen, the Lord's wife, Lady Aberdeen, unveiled a bronze statue and plaque in memory of Selkirk outside a house on the site of his original home on the Main Street of Lower Largo, Fife, Scotland. David Gillies of Cardy House, Lower Largo, a descendant of the Selkirks, donated the statue created by Thomas Stuart Burnett ARSA.
On 1 January 1966, Chilean president Eduardo Frei Montalva renamed Más a Tierra Robinson Crusoe Island after Defoe's fictional character, in order to attract tourists. At the same time, the largest of the Juan Fernández Islands, known as Más Afuera, became Alejandro Selkirk Island, although Selkirk probably never saw that island, since it is located 180 km to the west.
Archaeological findings.
An archaeological expedition to the Juan Fernández Islands in February 2005 found part of a nautical instrument that could have belonged to Selkirk. It was "a fragment of copper alloy identified as being from a pair of navigational dividers" dating from the early 18th (or late 17th) century. Selkirk is the only person known to have been on the island at that time who is likely to have had dividers and was even said by Rogers to have had such instruments in his possession. The artefact was discovered while excavating a site not far from Selkirk's Lookout, where the famous castaway is believed to have lived.
References.
Sources
</dl>

</doc>
<doc id="1791" url="http://en.wikipedia.org/wiki?curid=1791" title="Anti-ballistic missile">
Anti-ballistic missile

An anti-ballistic missile (ABM) is a surface-to-air missile designed to counter ballistic missiles (see missile defense). Ballistic missiles are used to deliver nuclear, chemical, biological or conventional warheads in a ballistic flight trajectory. The term "anti-ballistic missile" is commonly used for systems designed to counter intercontinental ballistic missiles (ICBMs).
Current counter-ICBM systems.
There are only two systems in the world that can intercept ICBMs. Besides them, many smaller systems exist (tactical ABMs), that generally cannot intercept intercontinental strategic missiles, even if within range—an incoming ICBM simply moves too fast for these systems.
The Russian A-35 anti-ballistic missile system, used for the defence of Moscow, whose development started in 1971. The currently active system is called A-135. The system uses Gorgon and Gazelle missiles with nuclear warheads to intercept incoming ICBMs.
The U.S. Ground-Based Midcourse Defense (GMD; previously known as National Missile Defense – NMD) system has reached initial operational capability. Instead of using an explosive charge, it launches a kinetic projectile. The George W. Bush administration accelerated development and deployment of a system proposed in 1998 by the Clinton administration. The system is a dual purpose test and interception facility in Alaska, and in 2006 was operational with a few interceptor missiles. The Alaska site provides more protection against the Nuclear threat from North Korean missiles or launches from Russia or China, but is likely less effective against missiles launched from the Middle East. President Bush referenced the September 11 attacks in 2001 and the proliferation of ballistic missiles as reasons for missile defense. The current GMD system has the more limited goal of shielding against a limited attack by a rogue state such as North Korea.
American plans for Central European site.
During 1993, a symposium was held by western European nations to discuss potential future ballistic missile defence programs. In the end, the council recommended deployment of early warning and surveillance systems as well as regionally controlled defence systems.
During Spring 2006 reports about negotiations between the United States and Poland as well as the Czech Republic were published. The plans propose the installation of a latest generation ABM system with a radar site in the Czech Republic and the launch site in Poland. The system was announced to be aimed against ICBMs from Iran and North Korea. This caused harsh comments by Russia's then-President Vladimir Putin at the Organization for Security and Co-operation in Europe (OSCE) security conference during spring 2007 in Munich. Other European ministers commented that any change of strategic weapons should be negotiated on NATO level and not 'unilaterally' between the U.S. and other states (although most strategic arms reduction treaties were between the Soviet Union and U.S., not NATO). German foreign minister Frank-Walter Steinmeier expressed severe concerns about the way in which the U.S. had conveyed its plans to its European partners and criticised the U.S. administration for not having consulted Russia prior to announcing its endeavours to deploy a new missile defence system in Central Europe. As of July 2007, a majority of Poles were opposed to hosting a component of the system in Poland.
Current tactical systems.
United States of America.
In several tests, the U.S. military have demonstrated the feasibility of destroying long and short range ballistic missiles. Combat effectiveness of newer systems against 1950s tactical ballistic missiles seems very high, as the Patriot Advanced Capability 3 (PAC-3) had a 100% success rate in Operation Iraqi Freedom.
U.S. Navy Aegis combat system uses RIM-161 Standard Missile 3.
These systems, as opposed to U.S. GMD system, are not capable of intercepting an ICBM, even if it is in range.
A new system, scheduled for deployment during 2009, is U.S. Terminal High Altitude Area Defense (THAAD) system. It has a longer range, but it is not known if it will be able to intercept ICBMs.
Russian Federation.
The Moscow ABM defense system is able to intercept the ICBM warheads, and is based on:
Apart from the main Moscow deployment, Russia has striven actively for intrinsic ABM capabilities of its SAM systems.
India.
India has an active ABM development effort using indigenously developed and integrated radars, and indigenous missiles. In November 2006, India successfully conducted the PADE (Prithvi Air Defence Exercise) in which an anti-ballistic missile, called the Prithvi Air Defense (PAD), an "exoatmospheric" (outside the atmosphere) interceptor system, intercepted a Prithvi-II ballistic missile. The PAD missile has the secondary stage of the Prithvi missile and can reach altitude of 80 km. During the test, the target missile was intercepted at a 50 km altitude. India became the fourth nation in the world after United States, Russia, and Israel to acquire such a capability and the third nation to acquire it using in-house research and development. On 6 December 2007, the Advanced Air Defence (AAD) missile system was tested successfully. This missile is an Endoatmospheric interceptor with an altitude of 30 km. In 2009, reports emerged of a new missile named the PDV. The DRDO is developing a new Prithvi interceptor missile codenamed PDV. The PDV is designed to take out the target missile at altitudes above 150 km. The first PDV was successfully test fired on 27 April 2014. According to scientist V K Saraswat of DRDO, the missiles will work in tandem to ensure a hit probability of 99.8 percent.
People's Republic of China (P.R.C.).
Historical Project 640.
Project 640 had been the PRC's indigenous effort to develop ABM capability. The Academy of Anti-Ballistic Missile & Anti-Satellite was established from 1969 for the purpose of developing Project 640. The project was to involve at least three elements, including the necessary sensors and guidance/command systems, the Fan Ji (FJ) missile interceptor, and the XianFeng missile-intercepting cannon. The FJ-1 had completed two successful flight tests during 1979, while the low-altitude interceptor FJ-2 completed some successful flight tests using scaled prototypes. A high altitude FJ-3 interceptor was also proposed. Despite the development of missiles, the programme was slowed down due to financial and political reasons. It was finally closed down during 1980 under a new leadership of Deng Xiaoping as it was seemingly deemed unnecessary after the 1972 Anti-Ballistic Missile Treaty between the Soviet Union and the United States and the closure of the US Safeguard ABM system.
Operational Chinese system.
In March 2006, China tested an interceptor system comparable to the U.S. Patriot missiles.
China has acquired and is license-producing the S-300PMU-2/S-300PMU-1 series of terminal ABM-capable SAMs. China-produced HQ-9 SAM system may possess terminal ABM capabilities. PRC Navy's operating modern air-defense destroyers known as the Type 052C Destroyer and Type 051C Destroyer are armed with naval HHQ-9 missiles.
Surface-to-air missiles that supposedly have some terminal ABM capability (as opposed to midcourse capability):
Development of midcourse ABM in China.
The technology and experience from the successful anti-satellite test using a ground-launched interceptor during January 2007 was immediately applied to current ABM efforts and development.
China carried out a land-based anti-ballistic missile test on 11 January 2010. The test was exoatmospheric and done in midcourse phase and with a kinetic kill vehicle. China is the second country after US that demonstrated intercepting ballistic missile with a kinetic kill vehicle, the interceptor missile was a SC-19. The sources suggest the system is not operationally deployed as of 2010.
On 27 January 2013, China did another anti ballistic missile test. According to the Chinese Defence Ministry, the missile launch is defensive in character and is not aimed against any countries. Experts hailed China's technological breakthrough because it is difficult to intercept ballistic missiles that have reached the highest point and speed in the middle of their course. Only 2 countries, including the US, have successfully conducted such a test in the past decade.
Rumored midcourse missiles:
France, United Kingdom and Italy.
Italy and France have developed a missile family called Aster (Aster 15 and Aster 30). Aster 30 is capable of ballistic missile defense. On 18 October 2010, France announced a successful tactical ABM test of the Aster 30 missile and on 1 December 2011 a successful interception of a Black Sparrow ballistic target missile. Royal Navy Type 45 destroyers and French Navy and Italian Navy "Horizon" -class frigates are armed with PAAMS, using Aster 15 and 30 missiles.
Also France is developing another version, Aster 30 block II which can destroy ballistic missiles with a maximum range of 3000 km. It will have a Kill Vehicle warhead.
Japan.
Since 1998, when North Korea launched a Taepodong-1 missile over northern Japan, the Japanese have been jointly developing a new Surface-to-air interceptor known as the Patriot Advanced Capability 3 (PAC-3) with the US. So far tests have been successful, and there are planned 11 locations that the PAC-3 will be installed. A military spokesman said that tests had been done on two sites, one of them a business park in central Tokyo, and Ichigaya – a site not far from the Imperial Palace.
Along with the PAC-3, Japan has installed a US-developed ship-based anti-ballistic missile system, which was tested successfully on 18 December 2007. The missile was launched from a Japanese warship, in partnership with the U.S. Missile Defense Agency and destroyed a mock target launched from the coast.
Israel.
Arrow.
The Arrow project was begun after the U.S. and Israel agreed to co-fund it on 6 May 1986.
The Arrow ABM system was designed and constructed in Israel with financial support by the United States by a multi-billion dollar development program called "Minhelet Homa" with the participation of companies like Israel Military Industries, Tadiran and Israel Aerospace Industries.
During 1998 the Israeli military conducted a successful test of their Arrow missile. Designed to intercept incoming missiles travelling at up to 2 mile/s (3 km/s), the Arrow is expected to perform much better than the Patriot did in the Gulf War. On 29 July 2004 Israel and the United States carried out joint experiment in the USA, in which the Arrow was launched against a real Scud missile. The experiment was a success, as the Arrow destroyed the Scud with a direct hit. During December 2005 the system was deployed successfully in a test against a replicated Shahab-3 missile. This feat was repeated on 11 February 2007.
The Arrow 3 system is being developed, and, if successful, will be capable of exo-atmosphere interception of ballistic missiles, including of ICBMs.
David’s sling.
David's Sling (Hebrew: קלע דוד), also sometimes called Magic Wand (Hebrew: שרביט קסמים), is an Israel Defense Forces military system being jointly developed by the Israeli defense contractor Rafael Advanced Defense Systems and the American defense contractor Raytheon, designed to intercept medium- to long-range rockets and slower-flying cruise missiles, such as those possessed by Hezbollah, fired at ranges from 40 km to 300 km.
History of ABMs.
1940s and 1950s.
The idea of destroying rockets before they can hit their target dates from the first use of modern missiles in warfare, the German V-1 and V-2 program of World War II.
British fighters attempted to destroy V-1 "buzz bombs" in flight prior to impact, with some success, although concentrated barrages of heavy anti-aircraft artillery had greater success. Under the lend-lease program, 200 US 90 mm AA guns with SCR-584 radars and Western Electric/Bell Labs computers were sent to the UK. These demonstrated a 95% success rate against V-1s that flew into their range.
The V-2, the first true ballistic missile, was impossible to destroy in the air. SCR-584's could be used to plot the trajectories of the missiles and provide some warning, but were more useful in backtracking their ballistic trajectory and determining the rough launch locations. The Allies launched Operation Crossbow to find and destroy V-2s before launch, but these operations were largely ineffective. In one instance a Spitfire happened upon a V-2 rising through the trees, and fired on it with no effect. This led to allied efforts to advance over their launching sites in Belgium and the Netherlands.
A wartime study by Bell Labs into the task of shooting down ballistic missiles in flight concluded it was not possible. In order to intercept a missile, one needs to be able to steer the attack onto the missile before it hits. At the speeds the V-2 flew at, this required guns of effectively infinite reaction time, or some sort of weapon with ranges on the order of dozens of miles, neither of which appeared possible. This was, however, just prior to the emergence of high-speed computing systems in the 1950s. By the mid-1950s things had changed considerably, and many forces worldwide were considering ABM systems.
The American armed forces began experimenting with anti-missile missiles soon after World War II, as the extent of German research into rocketry became clear. But defences against Soviet long-range bombers took priority until 1957, when the Soviet Union demonstrated its advances in intercontinental ballistic missile technology with the launch of Sputnik, the Earth's first artificial satellite. The US Army accelerated development of their LIM-49 Nike Zeus system in response. Zeus was subject to criticism throughout its development program, especially from those within the US Air Force and nuclear weapons establishments who suggested it would be much simpler to build more nuclear warheads and guarantee mutually assured destruction. Zeus was eventually cancelled in 1963.
In 1958, a topic of research by the U.S. was the test explosions of several low yield nuclear weapons at very high altitudes over the southern Atlantic ocean, launched from ships. The devices used were the 1.7 kt boosted fission W25 warhead. When such an explosion takes place a burst of X-rays are released that strike the Earth's atmosphere, causing secondary showers of charged particles over an area hundreds of miles across. These can become trapped in the Earth' magnetic field, creating an artificial radiation belt. It was believed that this might be strong enough to damage warheads travelling through the layer. This proved not to be the case, but Argus returned key data about a related effect, the Nuclear electromagnetic pulse (NEMP).
Canada.
Other countries were also involved in early ABM research. A more advanced project was at CARDE in Canada, which researched the main problems of ABM systems. A key problem with any radar system is that the signal is in the form of a cone, which spreads with distance from the transmitter. For long-distance interceptions like ABM systems, the inherent inaccuracy of the radar makes an interception difficult. CARDE considered using a terminal guidance system to address the accuracy concerns, and developed several advanced infrared detectors for this role. They also studied a number of missile airframe designs, a new and much more powerful solid rocket fuel, and numerous systems for testing it all. After a series of drastic budget reductions during the late 1950s the research ended. One offshoot of the project was Gerald Bull's system for inexpensive high-speed testing, consisting of missile airframes shot from a sabot round, which would later be the basis of Project HARP. Another was the CRV7 and Black Brant rockets, which used the new solid rocket fuel.
Soviet Union.
The Soviet military had requested funding for ABM research as early as 1953, but were only given the go-ahead to begin deployment of such a system on 17 August 1956. Their test system, known simply as System A, was based on the V-1000 missile, which was similar to the early US efforts. The first successful test interception was carried out on 24 November 1960, and the first with a live warhead on 4 March 1961. In this test, a dummy warhead was released by a R-12 ballistic missile launched from the Kapustin Yar, and intercepted by a V-1000 launched from Sary-Shagan. The dummy warhead was destroyed by the impact of 16,000 tungsten-carbide spherical impactors 140 seconds after launch, at an altitude of 25 km.
The V-1000 missile system was nonetheless considered not reliable enough and abandoned in favour of nuclear-armed ABMs. A much larger missile, the Fakel 5V61 (known in the west as Galosh), was developed to carry the larger warhead and carry it much further from the launch site. Further development continued, and the A-35 anti-ballistic missile system, designed to protect Moscow, became operational in 1971. A-35 was designed for exoatmospheric interceptions, and would have been highly susceptible to a well-arranged attack using multiple warheads and radar black-out techniques.
A-35 was upgraded during the 1980s to a two-layer system, the A-135. The Gorgon (SH-11/ABM-4) long-range missile was designed to handle intercepts outside the atmosphere, and the Gazelle (SH-08/ABM-3) short-range missile endoatmospheric intercepts that eluded Gorgon. The A-135 system is considered to be technologically equivalent to the United States Safeguard system of 1975.
American Nike-X and Sentinel.
Nike Zeus failed to be a credible defence in an era of rapidly increasing ICBM counts due to its ability to attack only one target at a time. Additionally, significant concerns about its ability to successfully intercept warheads in the presence of high-altitude nuclear explosions, including its own, lead to the conclusion that the system would simply be too costly for the very low amount of protection it could provide.
By the time it was cancelled in 1963, potential upgrades had been explored for some time. Among these were radars capable of scanning much greater volumes of space and able to track many warheads and launch several missiles at once. These, however, did not address the problems identified with radar blackouts caused by high-altitude explosions. To address this need, a new missile with extreme performance was designed to attack incoming warheads at much lower altitudes, as low as 20 km. The new project encompassing all of these upgrades was launched as Nike-X.
The main missile was LIM-49 Spartan—a Nike Zeus upgraded for longer range and a much larger 5 megatonne warhead intended to destroy enemy's warheads with a burst of x-rays outside the atmosphere. A second shorter-range missile called Sprint with very high acceleration was added to handle warheads that evaded longer-ranged Spartan. Sprint was a very fast missile (some sources claimed it accelerated to 8,000 mph (13 000 km/h) within 4 seconds of flight—an average acceleration of "90 g") and had a smaller W66 enhanced radiation warhead in the 1–3 kiloton range for in-atmosphere interceptions.
The experimental success of Nike X persuaded the Lyndon B. Johnson administration to propose a thin ABM defense, that could provide almost complete coverage of the United States. In a September 1967 speech, Defense Secretary Robert McNamara referred to it as "Sentinel". McNamara, a private ABM opponent because of cost and feasibility (see cost-exchange ratio), claimed that Sentinel would be directed not against the Soviet Union's missiles (since the USSR had more than enough missiles to overwhelm any American defense), but rather against the potential nuclear threat of the People's Republic of China.
In the meantime, a public debate over the merit of ABMs began. Difficulties that had already made an ABM system questionable for defending against an all-out attack. One problem was the Fractional Orbital Bombardment System (FOBS) that would give little warning to the defense. Another problem was high altitude EMP (whether from offensive or defensive nuclear warheads) which could degrade defensive radar systems.
When this proved infeasible for economic reasons, a much smaller deployment using the same systems was proposed, namely Safeguard (described later).
The problem of defense against MIRVs.
ABM systems were developed initially to counter single warheads launched from large Intercontinental ballistic missiles (ICBMs). The economics seemed simple enough; since rocket costs increase rapidly with size, the price of the ICBM launching a large warhead should always be greater than the much smaller interceptor missile needed to destroy it. In an arms race the defense would always win.
In practice, the price of the interceptor missile was considerable, due to its sophistication. The system had to be guided all the way to an interception, which demanded guidance and control systems that worked within and outside the atmosphere. The Nike Zeus was expected to cost about $1 million, about the same as an ICBM. However, due to their relatively short ranges, an ABM missile would be needed to counter an ICBM wherever it might be aimed. That implies that dozens of interceptors are needed for every ICBM. This led to intense debates about the "cost-exchange ratio" between interceptors and warheads.
Conditions changed dramatically in 1970 with the introduction of Multiple independently targetable reentry vehicle (MIRV) warheads. Suddenly, each launcher was throwing not one warhead, but several. These would spread out in space, ensuring that a single interceptor would be needed for each warhead. This simply added to the need to have several interceptors for each warhead in order to provide geographical coverage. Now it was clear that an ABM system would always be many times more expensive than the ICBMs they defended against.
The Anti-Ballistic Missile Treaty of 1972.
Technical, economic and political problems described resulted in the ABM treaty of 1972, which restricted the deployment of strategic (not tactical) anti-ballistic missiles.
By the ABM treaty and a 1974 revision, each country was allowed to deploy a mere 100 ABMs to protect a single, small area. The Soviets retained their Moscow defences. The U.S. designated their ICBM sites near Grand Forks Air Force Base, North Dakota, where Safeguard was already under advanced development. The radar systems and anti-ballistic missiles were approximately 90 miles north/northwest of Grand Forks AFB, near Concrete, North Dakota. The missiles were deactivated in 1975. The main radar site (PARCS) is still used as an early warning ICBM radar, facing relative north. It is located at Cavalier Air Force Station, North Dakota.
Brief use of Safeguard in 1975/1976.
The U.S. Safeguard system, which utilized the nuclear-tipped LIM-49A Spartan and Sprint missiles, in the short operational period of 1975/1976, was a second counter-ICBMs system in the world. Safeguard protected only the main fields of US ICBMs from attack, theoretically ensuring that an attack could be responded to with a US launch, enforcing the mutually assured destruction principle.
SDI experiments in the 1980s.
The Reagan-era Strategic Defense Initiative (often referred to as "Star Wars"), along with research into various energy-beam weaponry, brought new interest in the area of ABM technologies.
SDI was an extremely ambitious program to provide a total shield against a massive Soviet ICBM attack. The initial concept envisioned large sophisticated orbiting laser battle stations, space-based relay mirrors, and nuclear-pumped X-ray laser satellites. Later research indicated that some planned technologies such as X-ray lasers were not feasible with then-current technology. As research continued, SDI evolved through various concepts as designers struggled with the difficulty of such a large complex defense system. SDI remained a research program and was never deployed. Several post-SDI technologies are used by the present Missile Defense Agency (MDA).
Lasers originally developed for the SDI plan are in use for astronomical observations. Used to ionize gas in the upper atmosphere, they provide telescope operators with a target to calibrate their instruments.
Tactical ABMs deployed in 1990s.
The Israeli Arrow missile system was tested initially during 1990, before the first Gulf War. The Arrow was supported by the United States throughout the 1990s.
The Patriot was the first deployed tactical ABM system, although it was not designed from the outset for that task and consequently had limitations. It was used during the 1991 Gulf War to attempt to intercept Iraqi Scud missiles. Post-war analyses show that the Patriot was much less effective than initially thought because of its radar and control system's inability to discriminate warheads from other objects when the Scud missiles broke up during reentry.
Testing ABM technology continued during the 1990s with mixed success. After the Gulf War, improvements were made to several U.S. air defense systems. A new Patriot, PAC-3, was developed and tested—a complete redesign of the PAC-2 deployed during the war, including a totally new missile.
The improved guidance, radar and missile performance improves the probability of kill over the earlier PAC-2. During Operation Iraqi Freedom, Patriot PAC-3s had a nearly 100% success rate against Iraqi TBMs fired. However, since no longer range Iraqi Scud missiles were used, PAC-3 effectiveness against those was untested. Patriot was involved in three friendly fire incidents: two incidents of Patriot shootings at coalition aircraft and one of U.S. aircraft shooting at a Patriot battery.
A new version of the Hawk missile was tested during the early to mid-1990s and by the end of 1998 the majority of US Marine Corps Hawk systems were modified to support basic theater anti-ballistic missile capabilities. MIM-23 Hawk missile is not operational in the U.S. service since 2002, but is used by many other countries.
Soon after the Gulf war, the Aegis combat system was expanded to include ABM capabilities. The Standard missile system was also enhanced and tested for ballistic missile interception. During the late 1990s, SM-2 block IVA missiles were tested in a theater ballistic missile defense function. Standard Missile 3 (SM-3) systems have also been tested for an ABM role. In 2008, an SM-3 missile launched from a "Ticonderoga"-class cruiser, the USS Lake Erie, successfully intercepted a non-functioning satellite.
From 1992 to 2000, a demonstration system for the US Army Terminal High Altitude Area Defense was deployed at White Sands Missile Range. Tests were conducted on a regular basis and resulted in early failures, but successful intercepts occurred from 1999 onward. The US Army is in the process of fielding THAAD line batteries.
Brilliant Pebbles concept.
Approved for acquisition by the Pentagon during 1991 but never realized, Brilliant Pebbles was a proposed space-based anti-ballistic system that was meant to avoid some of the problems of the earlier SDI concepts. Rather than use sophisticated large laser battle stations and nuclear-pumped X-ray laser satellites, Brilliant Pebbles consisted of a thousand very small, intelligent orbiting satellites with kinetic warheads. The system relied on improvements of computer technology, avoided problems with overly centralized command and control and risky, expensive development of large, complicated space defense satellites.
It promised to be much less expensive to develop and have less technical development risk.
The name Brilliant Pebbles comes from the small size of the satellite interceptors and great computational power enabling more autonomous targeting. Rather than rely exclusively on ground-based control, the many small interceptors would cooperatively communicate among themselves and target a large swarm of ICBM warheads in space or in the late boost phase. Development was discontinued later in favor of a limited ground-based defense.
Transformation of SDI into MDA, development of NMD/GMD.
While the Reagan era Strategic Defense Initiative was intended to shield against a massive Soviet attack, during the early 1990s, President George H. W. Bush called for a more limited version using rocket-launched interceptors based on the ground at a single site. Such system was developed since 1992, was expected to become operational in 2010 and capable of intercepting small number of incoming ICBMs. First called the National Missile Defense (NMD), since 2002 it was renamed Ground-Based Midcourse Defense (GMD). It was planned to protect all 50 states from a rogue missile attack. The Alaska site provides more protection against North Korean missiles or accidental launches from Russia or China, but is likely less effective against missiles launched from the Middle East. The Alaska interceptors may be augmented later by the naval Aegis Ballistic Missile Defense System, by ground-based missiles in other locations, or by the Boeing Airborne Laser.
During 1998, Defense secretary William Cohen proposed spending an additional $6.6 billion on intercontinental ballistic missile defense programs to build a system to protect against attacks from North Korea or accidental launches from Russia or China.
In terms of organization, during 1993 SDI was reorganized as the Ballistic Missile Defense Organization (BMDO). In 2002, it was renamed to Missile Defense Agency (MDA).
U.S withdrawal from Anti-Ballistic Missile Treaty in 2002.
On 13 June 2002, the United States withdrew from the Anti-Ballistic Missile Treaty and recommenced developing missile defense systems that would have formerly been prohibited by the bilateral treaty. The action was stated as needed to defend against the possibility of a missile attack conducted by a rogue state.
The next day, the Russian Federation promptly dropped the START II agreement, intended to completely ban MIRVs.

</doc>
<doc id="1793" url="http://en.wikipedia.org/wiki?curid=1793" title="August 29">
August 29

August 29 is the day of the year in the Gregorian calendar.

</doc>
<doc id="1794" url="http://en.wikipedia.org/wiki?curid=1794" title="August 30">
August 30

August 30 is the day of the year in the Gregorian calendar.

</doc>
<doc id="1797" url="http://en.wikipedia.org/wiki?curid=1797" title="Acre">
Acre

The acre is a unit of land area used in the imperial and U.S. customary systems. It is defined as the area of 1 chain by 1 furlong which is equal to 4840 sqyd, 1⁄640 of a square mile or about 40% of a hectare.
The acre is commonly used in Antigua and Barbuda American Samoa, The Bahamas, Belize, the British Virgin Islands, the Cayman Islands, Dominica, the Falkland Islands, Grenada, Guam, the Northern Mariana Islands, India, Jamaica, Montserrat, Myanmar, Pakistan, Samoa, St. Lucia, St. Helena, St. Kitts and Nevis, St. Vincent and the Grenadines, Turks and Caicos, the United Kingdom, the United States and the US Virgin Islands.
The international symbol of the acre is ac. The most commonly used acre today is the international acre. In the United States both the international acre and the slightly different US survey acre are in use. The most common use of the acre is to measure tracts of land. One international acre is defined as exactly 4046.8564224 square metres.
An acre was defined in the Middle Ages, being the amount of land that could be ploughed in one day with a yoke of oxen.
Description.
One acre equals 0.0015625 square miles, 4,840 square yards, 43,560 square feet or about 4047 m2 (see below). While all modern variants of the acre contain 4,840 square yards, there are alternative definitions of a yard, so the exact size of an acre depends on which yard it is based. Originally, an acre was understood as a selion of land sized at forty perches (660 ft or 1 furlong) long and four perches (66 ft or 1 chain) wide; this may have also been understood as an approximation of the amount of land a yoke of oxen could plough in one day. A square enclosing one acre is approximately 69.57 yards, or 208 ft on a side. As a unit of measure, an acre has no prescribed shape; any area of 43,560 square feet is an acre.
Differences between international and U.S. survey acres.
In the international yard and pound agreement of 1959 the United States and five countries of the Commonwealth of Nations defined the length of the international yard to be exactly 0.9144 metres. Consequently, the international acre is exactly 4,046.8564224 square metres.
Both the international acre and the U.S. survey acre contain 1/640 of a square mile or 4,840 square yards, but there are alternative definitions of a yard (see survey foot and survey yard), so the exact size of an acre depends on which yard it is based. The U.S. survey acre is about 4,046.872609874252 square metres; its exact value (4046 13,525,426/15,499,969 m2) is based on an inch defined by 1 metre = 39.37 inches exactly, as established by the Mendenhall Order. Surveyors in the United States use both international and survey feet, and consequently, both varieties of acre.
Since the difference between the U.S. survey acre and international acre is only about a quarter of the size of an A4 sheet of paper (0.016 square metres, 160 square centimetres or 24.8 square inches), it is usually not important which one is being discussed. Areas are seldom measured with sufficient accuracy for the different definitions to be detectable.
South Asia.
In India, residential plots are measured in cents or decimel, which is one hundredth of an acre, or 435.60 ft2. In Sri Lanka the division of an acre into 160 perches or 4 roods is common.
Equivalence to other units of area.
1 international acre is equal to the following metric units:
1 United States survey acre is equal to:
1 acre (both variants) is equal to the following customary units:
Perhaps the easiest way for U.S residents to envisage an acre is as a rectangle measuring 88 yards by 55 yards (1⁄10 of 880 yards by 1⁄16 of 880 yards), about 9⁄10 the size of a standard American football field. To be more exact, one acre is 90.75 percent of a 100 yd long by 53.33 yd wide American football field (without the end zones). The full field, including the end zones, covers approximately 1.32 acres.
For residents of other countries, the acre might be envisaged as approximately 56.68 percent of a 105 m long by 68 m wide association football (soccer) pitch.
It may also be remembered as 44,000 square feet, less 1%.
Historical origin.
The word "acre" is derived from Old English "æcer" originally meaning "open field", cognate to west coast Norwegian "ækre" and Swedish "åker", German "Acker", Dutch "akker", Latin "ager", Sanskrit "ajr", and Greek αγρός ("agros"). In English it was historically spelled "aker".
The acre was approximately the amount of land tillable by a yoke of oxen in one day. This explains one definition as the area of a rectangle with sides of length one chain and one furlong. A long narrow strip of land is more efficient to plough than a square plot, since the plough does not have to be turned so often. The word "furlong" itself derives from the fact that it is "one furrow long".
Before the enactment of the metric system, many countries in Europe used their own official acres. These were differently sized in different countries, for instance, the historical French acre was 4,221 square metres, whereas in Germany as many variants of "acre" existed as there were German states.
Statutory values for the acre were enacted in England, and, subsequently, the United Kingdom, by acts of:
Historically, the size of farms and landed estates in the United Kingdom was usually expressed in acres (or acres, roods, and perches), even if the number of acres was so large that it might conveniently have been expressed in square miles. For example, a certain landowner might have been said to own 32,000 acres of land, not 50 square miles of land.
The acre is related to the square mile, with 640 acres making up one square mile. One mile is 5280 feet (1760 yards). In western Canada and the western United States, divisions of land area were typically based on the square mile, and fractions thereof. If the square mile is divided into quarters, each quarter has a side length of 1⁄2 mile (880 yards) and is 1⁄4 square mile in area, or 160 acres. These subunits would typically then again be divided into quarters, with each side being 1⁄4 mile long, and being 1⁄16 of a square mile in area, or 40 acres. In the United States, farmland was typically divided as such, and the phrase "the back 40" would refer to the 40 acre parcel to the back of the farm. Most of the Canadian Prairie Provinces and the US midwest are on square mile grids for surveying purposes.

</doc>
<doc id="1799" url="http://en.wikipedia.org/wiki?curid=1799" title="ATP">
ATP

ATP may refer to:

</doc>
<doc id="1800" url="http://en.wikipedia.org/wiki?curid=1800" title="Adenosine triphosphate">
Adenosine triphosphate

Adenosine triphosphate (ATP) is a nucleoside triphosphate used in cells as a coenzyme, often called the "molecular unit of currency" of intracellular energy transfer.
ATP transports chemical energy within cells for metabolism. It is one of the end products of photophosphorylation, cellular respiration, and fermentation and used by enzymes and structural proteins in many cellular processes, including biosynthetic reactions, motility, and cell division. One molecule of ATP contains three phosphate groups, and it is produced by a wide variety of enzymes, including ATP synthase, from adenosine diphosphate (ADP) or adenosine monophosphate (AMP) and various phosphate group donors. Substrate-level phosphorylation, oxidative phosphorylation in cellular respiration, and photophosphorylation in photosynthesis are three major mechanisms of ATP biosynthesis.
Metabolic processes that use ATP as an energy source convert it back into its precursors. ATP is therefore continuously recycled in organisms: the human body, which on average contains only 250 g of ATP, turns over its own body weight equivalent in ATP each day.
ATP is used as a substrate in signal transduction pathways by kinases that phosphorylate proteins and lipids. It is also used by adenylate cyclase, which uses ATP to produce the second messenger molecule cyclic AMP. The ratio between ATP and AMP is used as a way for a cell to sense how much energy is available and control the metabolic pathways that produce and consume ATP. Apart from its roles in signaling and energy metabolism, ATP is also incorporated into nucleic acids by polymerases in the process of transcription. ATP is the neurotransmitter believed to signal the sense of taste.
The structure of this molecule consists of a purine base (adenine) attached to the 1' carbon atom of a pentose sugar (ribose). Three phosphate groups are attached at the 5' carbon atom of the pentose sugar. It is the addition and removal of these phosphate groups that inter-convert ATP, ADP and AMP. When ATP is used in DNA synthesis, the ribose sugar is first converted to deoxyribose by ribonucleotide reductase.
ATP was discovered in 1929 by Karl Lohmann, and independently by Cyrus Fiske and Yellapragada Subbarow of Harvard Medical School, but its correct structure was not determined until some years later. It was proposed to be the intermediary molecule between energy-yielding and energy-requiring reactions in cells by Fritz Albert Lipmann in 1941. It was first artificially synthesized by Alexander Todd in 1948.
Physical and chemical properties.
ATP consists of adenosine — composed of an adenine ring and a ribose sugar — and three phosphate groups (triphosphate). The phosphoryl groups, starting with the group closest to the ribose, are referred to as the alpha (α), beta (β), and gamma (γ) phosphates. Consequently, it is closely related to the adenosine nucleotide, a monomer of RNA. ATP is highly soluble in water and is quite stable in solutions between pH 6.8 and 7.4, but is rapidly hydrolysed at extreme pH. Consequently, ATP is best stored as an anhydrous salt.
ATP is an unstable molecule in unbuffered water, in which it hydrolyses to ADP and phosphate. This is because the strength of the bonds between the phosphate groups in ATP is less than the strength of the hydrogen bonds (hydration bonds), between its products (ADP + phosphate), and water. Thus, if ATP and ADP are in chemical equilibrium in water, almost all of the ATP will eventually be converted to ADP. A system that is far from equilibrium contains Gibbs free energy, and is capable of doing work. Living cells maintain the ratio of ATP to ADP at a point ten orders of magnitude from equilibrium, with ATP concentrations fivefold higher than the concentration of ADP. This displacement from equilibrium means that the hydrolysis of ATP in the cell releases a large amount of free energy.
Two phosphoanhydride bonds (those that connect adjacent phosphates) in an ATP molecule are responsible for the high energy content of this molecule. In the context of biochemical reactions, these anhydride bonds are frequently—and sometimes controversially—referred to as "high-energy bonds" (despite the fact it takes energy to break bonds). Energy stored in ATP may be released upon hydrolysis of the anhydride bonds. The primary phosphate group on the ATP molecule that is hydrolyzed when energy is needed to drive anabolic reactions is the γ-phosphate group. Located the farthest from the ribose sugar, it has a higher energy of hydrolysis than either the α- or β-phosphate. The bonds formed after hydrolysis—or the phosphorylation of a residue by ATP—are lower in energy than the phosphoanhydride bonds of ATP. During enzyme-catalyzed hydrolysis of ATP or phosphorylation by ATP, the available free energy can be harnessed by a living system to do work.
Any unstable system of potentially reactive molecules could potentially serve as a way of storing free energy, if the cell maintained their concentration far from the equilibrium point of the reaction. However, as is the case with most polymeric biomolecules, the breakdown of RNA, DNA, and ATP into simpler monomers is driven by both energy-release and entropy-increase considerations, in both standard concentrations, and also those concentrations encountered within the cell.
The standard amount of energy released from hydrolysis of ATP can be calculated from the changes in energy under non-natural (standard) conditions, then correcting to biological concentrations. The net change in heat energy (enthalpy) at standard temperature and pressure of the decomposition of ATP into hydrated ADP and hydrated inorganic phosphate is −30.5 kJ/mol, with a change in free energy of 3.4 kJ/mol. The energy released by cleaving either a phosphate (Pi) or pyrophosphate (PPi) unit from ATP at standard state of 1 M are:
These values can be used to calculate the change in energy under physiological conditions and the cellular ATP/ADP ratio. However, a more representative value (which takes AMP into consideration) called the Energy charge is increasingly being employed. The values given for the Gibbs free energy for this reaction are dependent on a number of factors, including overall ionic strength and the presence of alkaline earth metal ions such as Mg2+ and Ca2+. Under typical cellular conditions, ΔG is approximately −57 kJ/mol (−14 kcal/mol).
Ionization in biological systems.
ATP (adenosine triphosphate) has multiple groups with different acid dissociation constants. In neutral solution, ATP is ionized exists mostly as ATP4−, with a small proportion of ATP3−. As ATP has several negatively charged groups in neutral solution, it can chelate metals with very high affinity. The binding constant for various metal ions are (given as per mole) as Mg2+ (9 554), Na+ (13), Ca2+ (3 722), K+ (8), Sr2+ (1 381) and Li+ (25). Due to the strength of these interactions, ATP exists in the cell mostly in a complex with Mg2+.
Biosynthesis.
The ATP concentration inside the cell is typically 1–10 mM. ATP can be produced by redox reactions using simple and complex sugars (carbohydrates) or lipids as an energy source. For complex fuels to be synthesized into ATP, they first need to be broken down into smaller, more simple molecules. Carbohydrates are hydrolysed into simple sugars, such as glucose and fructose. Fats (triglycerides) are metabolised to give fatty acids and glycerol.
The overall process of oxidizing glucose to carbon dioxide is known as cellular respiration and can produce about 30 molecules of ATP from a single molecule of glucose. ATP can be produced by a number of distinct cellular processes; the three main pathways used to generate energy in eukaryotic organisms are glycolysis and the citric acid cycle/oxidative phosphorylation, both components of cellular respiration; and beta-oxidation. The majority of this ATP production by a non-photosynthetic aerobic eukaryote takes place in the mitochondria, which can make up nearly 25% of the total volume of a typical cell.
Glycolysis.
In glycolysis, glucose and glycerol are metabolized to pyruvate via the glycolytic pathway. In most organisms, this process occurs in the cytosol, but, in some protozoa such as the kinetoplastids, this is carried out in a specialized organelle called the glycosome. Glycolysis generates a net two molecules of ATP through substrate phosphorylation catalyzed by two enzymes: PGK and pyruvate kinase. Two molecules of NADH are also produced, which can be oxidized via the electron transport chain and result in the generation of additional ATP by ATP synthase. The pyruvate generated as an end-product of glycolysis is a substrate for the Krebs Cycle.
Glucose.
In the mitochondrion, pyruvate is oxidized by the pyruvate dehydrogenase complex to Acetyl group, which is fully oxidized to carbon dioxide by the citric acid cycle (also known as the Krebs Cycle). Every "turn" of the citric acid cycle produces two molecules of carbon dioxide, one molecule of the ATP equivalent guanosine triphosphate (GTP) through substrate-level phosphorylation catalyzed by succinyl-CoA synthetase, three molecules of the reduced coenzyme NADH, and one molecule of the reduced coenzyme FADH2. Both of these latter molecules are recycled to their oxidized states (NAD+ and FAD, respectively) via the electron transport chain, which generates additional ATP by oxidative phosphorylation. The oxidation of an NADH molecule results in the synthesis of 2–3 ATP molecules, and the oxidation of one FADH2 yields between 1–2 ATP molecules. The majority of cellular ATP is generated by this process. Although the citric acid cycle itself does not involve molecular oxygen, it is an obligately aerobic process because O2 is needed to recycle the reduced NADH and FADH2 to their oxidized states. In the absence of oxygen the citric acid cycle will cease to function due to the lack of available NAD+ and FAD.
The generation of ATP by the mitochondrion from cytosolic NADH relies on the malate-aspartate shuttle (and to a lesser extent, the glycerol-phosphate shuttle) because the inner mitochondrial membrane is impermeable to NADH and NAD+. Instead of transferring the generated NADH, a malate dehydrogenase enzyme converts oxaloacetate to malate, which is translocated to the mitochondrial matrix. Another malate dehydrogenase-catalyzed reaction occurs in the opposite direction, producing oxaloacetate and NADH from the newly transported malate and the mitochondrion's interior store of NAD+. A transaminase converts the oxaloacetate to aspartate for transport back across the membrane and into the intermembrane space.
In oxidative phosphorylation, the passage of electrons from NADH and FADH2 through the electron transport chain powers the pumping of protons out of the mitochondrial matrix and into the intermembrane space. This creates a proton motive force that is the net effect of a pH gradient and an electric potential gradient across the inner mitochondrial membrane. Flow of protons down this potential gradient — that is, from the intermembrane space to the matrix — provides the driving force for ATP synthesis by ATP synthase. This enzyme contains a rotor subunit that physically rotates relative to the static portions of the protein during ATP synthesis.
Most of the ATP synthesized in the mitochondria will be used for cellular processes in the cytosol; thus it must be exported from its site of synthesis in the mitochondrial matrix. The inner membrane contains an antiporter, the ADP/ATP translocase, which is an integral membrane protein used to exchange newly synthesized ATP in the matrix for ADP in the intermembrane space. This translocase is driven by the membrane potential, as it results in the movement of about 4 negative charges out of the mitochondrial membrane in exchange for 3 negative charges moved inside. However, it is also necessary to transport phosphate into the mitochondrion; the phosphate carrier moves a proton in with each phosphate, partially dissipating the proton gradient.
Beta oxidation.
Fatty acids can also be broken down to acetyl-CoA by beta-oxidation. Each round of this cycle reduces the length of the acyl chain by two carbon atoms and produces one NADH and one FADH2 molecule, which are used to generate ATP by oxidative phosphorylation. Because NADH and FADH2 are energy-rich molecules, dozens of ATP molecules can be generated by the beta-oxidation of a single long acyl chain. The high energy yield of this process and the compact storage of fat explain why it is the most dense source of dietary calories.
Fermentation.
Fermentation entails the generation of energy via the process of substrate-level phosphorylation in the absence of a respiratory electron transport chain. In most eukaryotes, glucose is used as both an energy store and an electron donor. The equation for the oxidation of glucose to lactic acid is:
Anaerobic respiration.
Anaerobic respiration is the process of respiration using an electron acceptor other than O2. In prokaryotes, multiple electron acceptors can be used in anaerobic respiration. These include nitrate, sulfate or carbon dioxide. These processes lead to the ecologically important processes of denitrification, sulfate reduction and acetogenesis, respectively.
ATP replenishment by nucleoside diphosphate kinases.
ATP can also be synthesized through several so-called "replenishment" reactions catalyzed by the enzyme families of nucleoside diphosphate kinases (NDKs), which use other nucleoside triphosphates as a high-energy phosphate donor, and the family,
ATP production during photosynthesis.
In plants, ATP is synthesized in thylakoid membrane of the chloroplast during the light-dependent reactions of photosynthesis in a process called photophosphorylation. Here, light energy is used to pump protons across the chloroplast membrane. This produces a proton-motive force and this drives the ATP synthase, exactly as in oxidative phosphorylation. Some of the ATP produced in the chloroplasts is consumed in the Calvin cycle, which produces triose sugars.
ATP recycling.
The total quantity of ATP in the human body is about 0.2 mole. The majority of ATP is not usually synthesised "de novo", but is generated from ADP by the aforementioned processes. Thus, at any given time, the total amount of ATP + ADP remains fairly constant.
The energy used by human cells requires the hydrolysis of 100 to 150 moles of ATP daily, which is around 50 to 75 kg. A human will typically use up his or her body weight of ATP over the course of the day. This means that each ATP molecule is recycled 500 to 750 times during a single day (100 / 0.2 = 500). ATP cannot be stored, hence its consumption closely follows its synthesis. However a total of around 5g of ATP is used by cell processes at any time in the body.
Regulation of biosynthesis.
ATP production in an aerobic eukaryotic cell is tightly regulated by allosteric mechanisms, by feedback effects, and by the substrate concentration dependence of individual enzymes within the glycolysis and oxidative phosphorylation pathways. Key control points occur in enzymatic reactions that are so energetically favorable that they are effectively irreversible under physiological conditions.
In glycolysis, hexokinase is directly inhibited by its product, glucose-6-phosphate, and pyruvate kinase is inhibited by ATP itself. The main control point for the glycolytic pathway is phosphofructokinase (PFK), which is allosterically inhibited by high concentrations of ATP and activated by high concentrations of AMP. The inhibition of PFK by ATP is unusual, since ATP is also a substrate in the reaction catalyzed by PFK; the biologically active form of the enzyme is a tetramer that exists in two possible conformations, only one of which binds the second substrate fructose-6-phosphate (F6P). The protein has two binding sites for ATP — the active site is accessible in either protein conformation, but ATP binding to the inhibitor site stabilizes the conformation that binds F6P poorly. A number of other small molecules can compensate for the ATP-induced shift in equilibrium conformation and reactivate PFK, including cyclic AMP, ammonium ions, inorganic phosphate, and fructose 1,6 and 2,6 biphosphate.
The citric acid cycle is regulated mainly by the availability of key substrates, particularly the ratio of NAD+ to NADH and the concentrations of calcium, inorganic phosphate, ATP, ADP, and AMP. Citrate — the molecule that gives its name to the cycle — is a feedback inhibitor of citrate synthase and also inhibits PFK, providing a direct link between the regulation of the citric acid cycle and glycolysis.
In oxidative phosphorylation, the key control point is the reaction catalyzed by cytochrome c oxidase, which is regulated by the availability of its substrate—the reduced form of cytochrome c. The amount of reduced cytochrome c available is directly related to the amounts of other substrates:
which directly implies this equation:
Thus, a high ratio of [NADH] to [NAD+] or a high ratio of [ADP] [Pi] to [ATP] imply a high amount of reduced cytochrome c and a high level of cytochrome c oxidase activity. An additional level of regulation is introduced by the transport rates of ATP and NADH between the mitochondrial matrix and the cytoplasm.
Functions in cells.
Metabolism, synthesis, and active transport.
ATP is consumed in the cell by energy-requiring (endothermic) processes and can be generated by energy-releasing (exothermic) processes. In this way ATP transfers energy between spatially separate metabolic reactions. ATP is the main energy source for the majority of cellular functions. This includes the synthesis of macromolecules, including DNA and RNA (see below), and proteins. ATP also plays a critical role in the transport of macromolecules across cell membranes, e.g. exocytosis and endocytosis.
Roles in cell structure and locomotion.
ATP is critically involved in maintaining cell structure by facilitating assembly and disassembly of elements of the cytoskeleton. In a related process, ATP is required for the shortening of actin and myosin filament crossbridges required for muscle contraction. This latter process is one of the main energy requirements of animals and is essential for locomotion and respiration.
Cell signalling.
Extracellular signalling.
ATP is also a signalling molecule. ATP, ADP, or adenosine are recognised by purinergic receptors. Purinoreceptors might be the most abundant receptors in mammalian tissues.
In humans, this signalling role is important in both the central and peripheral nervous system. Activity-dependent release of ATP from synapses, axons and glia activates purinergic membrane receptors known as P2. The "P2Y" receptors are "metabotropic", i.e. G protein-coupled and modulate mainly intracellular calcium and sometimes cyclic AMP levels. Though named between P2Y1 and P2Y15, only nine members of the P2Y family have been cloned, and some are only related through weak homology and several (P2Y5, P2Y7, P2Y9, P2Y10) do not function as receptors that raise cytosolic calcium. The "P2X ionotropic" receptor subgroup comprises seven members (P2X1–P2X7), which are ligand-gated Ca2+-permeable ion channels that open when bound to an extracellular purine nucleotide. In contrast to P2 receptors (agonist order ATP > ADP > AMP > ADO), purinergic nucleoside triphosphates like ATP are not strong agonists of P1 receptors, which are strongly activated by adenosine and other nucleosides (ADO > AMP > ADP > ATP). P1 receptors have A1, A2a, A2b, and A3 subtypes ("A" as a remnant of old nomenclature of "adenosine receptor"), all of which are G protein-coupled receptors, A1 and A3 being coupled to Gi, and A2a and A2b being coupled to Gs.
All adenosine receptors were shown to activate at least one subfamily of mitogen-activated protein kinases. The actions of adenosine are often antagonistic or synergistic to the actions of ATP. In the CNS, adenosine has multiple functions, such as modulation of neural development, neuron and glial signalling and the control of innate and adaptive immune systems.
Intracellular signaling.
ATP is critical in signal transduction processes. It is used by kinases as the source of phosphate groups in their phosphate transfer reactions. Kinase activity on substrates such as proteins or membrane lipids are a common form of signal transduction. Phosphorylation of a protein by a kinase can activate this cascade such as the mitogen-activated protein kinase cascade.
ATP is also used by adenylate cyclase and is transformed to the second messenger molecule cyclic AMP, which is involved in triggering calcium signals by the release of calcium from intracellular stores. This form of signal transduction is particularly important in brain function, although it is involved in the regulation of a multitude of other cellular processes.
DNA and RNA synthesis.
In all known organisms, the Deoxyribonucleotides that make up DNA are synthesized by the action of ribonucleotide reductase (RNR) enzymes on their corresponding ribonucleotides. These enzymes reduce the sugar residue from ribose to deoxyribose by removing oxygen from the 2' hydroxyl group; the substrates are ribonucleoside diphosphates and the products deoxyribonucleoside diphosphates (the latter are denoted dADP, dCDP, dGDP, and dUDP respectively.) All ribonucleotide reductase enzymes use a common sulfhydryl radical mechanism reliant on reactive cysteine residues that oxidize to form disulfide bonds in the course of the reaction. RNR enzymes are recycled by reaction with thioredoxin or glutaredoxin.
The regulation of RNR and related enzymes maintains a balance of dNTPs relative to each other and relative to NTPs in the cell. Very low dNTP concentration inhibits DNA synthesis and DNA repair and is lethal to the cell, while an abnormal ratio of dNTPs is mutagenic due to the increased likelihood of the DNA polymerase incorporating the wrong dNTP during DNA synthesis. Regulation of or differential specificity of RNR has been proposed as a mechanism for alterations in the relative sizes of intracellular dNTP pools under cellular stress such as hypoxia.
In the synthesis of the nucleic acid RNA, adenosine derived from ATP is one of the four nucleotides incorporated directly into RNA molecules by RNA polymerases. The energy driving this polymerization comes from cleaving off a pyrophosphate (two phosphate groups). The process is similar in DNA biosynthesis, except that ATP is reduced to the deoxyribonucleotide dATP, before incorporation into DNA.
Amino acid activation in protein synthesis.
Aminoacyl-tRNA synthetase enzymes utilise ATP as an energy source to attach a tRNA molecule to its specific amino acid, forming an aminoacyl-tRNA complex, ready for translation at ribosomes. The energy is made available by ATP hydrolysis to adenosine monophosphate (AMP) as two phosphate groups are removed.
Amino acid activation refers to the attachment of an amino acid to its Transfer RNA (tRNA).
 Aminoacyl transferase binds Adenosine triphosphate (ATP) to amino acid, PP is released.
 Aminoacyl transferase binds AMP-amino acid to tRNA. The AMP is used in this step.
Amino Acid Activation.
During amino acid activation the amino acids (aa) are attached to their corresponding tRNA. The coupling reactions are catalysed by a group of enzymes called aminoacyl-tRNA synthetases (named after the reaction product aminoacyl-tRNA or aa-tRNA). The coupling reaction proceeds in two steps:
1. aa + ATP aa-AMP + PP, (pyrophosphate) 2. aa-AMP + tRNA aa-tRNA + AMP
The amino acid is coupled to the penultimate nucleotide at the 3’-end of the tRNA (the A in the sequence CCA) via an ester bond (roll over in illustration). The formation of the ester bond conserves a considerable part of the energy from the activation reaction. This stored energy provides the majority of the energy needed for peptide bond formation during translation.
Each of the 20 amino acids are recognized by its specific aminoacyl-tRNA synthetase. The synthetases are usually composed of one to four protein subunits. The enzymes vary considerably in structure although they all perform the same type of reaction by binding ATP, one specific amino acid and its corresponding tRNA.
The specificity of the amino acid activation is as critical for the translational accuracy as the correct matching of the codon with the anticodon. The reason is that the ribosome only sees the anticodon of the tRNA during translation. Thus, the ribosome will not be able to discriminate between tRNAs with the same anticodon but linked to different amino acids.
The error frequency of the amino acid activation reaction is approximately 1 in 10 000 despite the small structural differences between some of the amino acids.
Binding to proteins.
Some proteins that bind ATP do so in a characteristic protein fold known as the Rossmann fold, which is a general nucleotide-binding structural domain that can also bind the coenzyme NAD. The most common ATP-binding proteins, known as kinases, share a small number of common folds; the protein kinases, the largest kinase superfamily, all share common structural features specialized for ATP binding and phosphate transfer.
ATP in complexes with proteins, in general, requires the presence of a divalent cation, almost always magnesium, which binds to the ATP phosphate groups. The presence of magnesium greatly decreases the dissociation constant of ATP from its protein binding partner without affecting the ability of the enzyme to catalyze its reaction once the ATP has bound. The presence of magnesium ions can serve as a mechanism for kinase regulation.
ATP analogues.
Biochemistry laboratories often use "in vitro" studies to explore ATP-dependent molecular processes. Enzyme inhibitors of ATP-dependent enzymes such as kinases are needed to examine the binding sites and transition states involved in ATP-dependent reactions. ATP analogs are also used in X-ray crystallography to determine a protein structure in complex with ATP, often together with other substrates.
Most useful ATP analogs cannot be hydrolyzed as ATP would be; instead they trap the enzyme in a structure closely related to the ATP-bound state. Adenosine 5'-(gamma-thiotriphosphate) is an extremely common ATP analog in which one of the gamma-phosphate oxygens is replaced by a sulfur atom; this molecule is hydrolyzed at a dramatically slower rate than ATP itself and functions as an inhibitor of ATP-dependent processes. In crystallographic studies, hydrolysis transition states are modeled by the bound vanadate ion. However, caution is warranted in interpreting the results of experiments using ATP analogs, since some enzymes can hydrolyze them at appreciable rates at high concentration.

</doc>
<doc id="1802" url="http://en.wikipedia.org/wiki?curid=1802" title="Ægir">
Ægir

In Norse mythology, Ægir (Old Norse "sea") is a sea jötunn associated with the ocean. He is also known for hosting elaborate parties for the gods.
Ægir's servants are Fimafeng (killed by Loki) and Eldir.
Description.
The "Nafnaþulur" attached to the "Prose Edda" list Ægir as a giant. Richard Cleasby and Guðbrandur Vigfússon saw his name as pre-Norse, derived from an ancient Indo-European root.
Attestations.
Both "Fundinn Noregr" and Snorri Sturluson in "Skáldskaparmál" state that Ægir is the same as the sea-giant Hlér, who lives on the isle of Hlésey, and this is borne out by kennings. Snorri uses his visiting the Æsir as the frame of that section of the Prose Edda.
In "Lokasenna", Ægir hosts a party for the gods where he provides the ale brewed in an enormous pot or cauldron provided by Thor and Týr. The story of their obtaining the pot from the giant Hymir is told in "Hymiskviða".
The prose introduction to "Lokasenna" and Snorri's list of kennings state that Ægir is also known as Gymir, who is Gerðr's father, but this is evidently an erroneous interpretation of kennings in which different giant-names are used interchangeably.
Family.
According to "Fundinn Noregr", Ægir is a son of the giant Fornjótr, the king of Finland, Kvenland and Gotland, and brother of Logi ("fire") and Kári ("wind").
Ægir's wife is Rán. She is by Ægir mother of nine billow maidens, whose names are:

</doc>
<doc id="1805" url="http://en.wikipedia.org/wiki?curid=1805" title="Antibiotics">
Antibiotics

Antibiotics or antibacterials are a type of antimicrobial used in the treatment and prevention of bacterial infection. They may either kill or inhibit the growth of bacteria. Several antibiotics are also effective against fungi and protozoans, and some are toxic to humans and animals, even when given in therapeutic dosage. Antibiotics are not effective against viruses such as the common cold or influenza, and may be harmful when taken inappropriately.
Antibiotics revolutionized medicine in the 20th century, and have together with vaccination led to the near eradication of diseases such as tuberculosis in the developed world. Their effectiveness and easy access led to overuse, especially in live-stock raising, prompting bacteria to develop resistance. This has led to widespread problems with antimicrobial and antibiotic resistance, so much as to prompt the World Health Organization to classify antimicrobial resistance as a "serious threat [that] is no longer a prediction for the future, it is happening right now in every region of the world and has the potential to affect anyone, of any age, in any country".
The era of antibacterial chemotherapy began with the discovery of arsphenamine, first synthesized by Alfred Bertheim and Paul Ehrlich in 1907, used to treat syphilis. The first systemically active antibiotic, prontosil was discovered in 1933 by Gerhard Domagk, for which he was awarded the 1939 Nobel Prize.
Sometimes the term antibiotic is used to refer to any substance used against microbes, synonymous to antimicrobial. Some sources distinguish between antibacterial and antibiotic; antibacterials used in soaps and cleaners etc., but not as medicine. This article treats the terms as synonymous and according to the most widespread definition of antibiotics being a substance used against bacteria.
Pharmacodynamics.
The successful outcome of antimicrobial therapy with antibacterial compounds depends on several factors. These include host defense mechanisms, the location of infection, and the pharmacokinetic and pharmacodynamic properties of the antibacterial. A bactericidal activity of antibacterials may depend on the bacterial growth phase, and it often requires ongoing metabolic activity and division of bacterial cells. These findings are based on laboratory studies, and in clinical settings have also been shown to eliminate bacterial infection. Since the activity of antibacterials depends frequently on its concentration, "in vitro" characterization of antibacterial activity commonly includes the determination of the minimum inhibitory concentration and minimum bactericidal concentration of an antibacterial.
To predict clinical outcome, the antimicrobial activity of an antibacterial is usually combined with its pharmacokinetic profile, and several pharmacological parameters are used as markers of drug efficacy.
Classes.
Antibacterial antibiotics are commonly classified based on their mechanism of action, chemical structure, or spectrum of activity. Most target bacterial functions or growth processes. Those that target the bacterial cell wall (penicillins and cephalosporins) or the cell membrane (polymyxins), or interfere with essential bacterial enzymes (rifamycins, lipiarmycins, quinolones, and sulfonamides) have bactericidal activities. Those that target protein synthesis (macrolides, lincosamides and tetracyclines) are usually bacteriostatic (with the exception of bactericidal aminoglycosides). Further categorization is based on their target specificity. "Narrow-spectrum" antibacterial antibiotics target specific types of bacteria, such as Gram-negative or Gram-positive bacteria, whereas broad-spectrum antibiotics affect a wide range of bacteria. Following a 40-year hiatus in discovering new classes of antibacterial compounds, four new classes of antibacterial antibiotics have been brought into clinical use: cyclic lipopeptides (such as daptomycin), glycylcyclines (such as tigecycline), oxazolidinones (such as linezolid), and lipiarmycins (such as fidaxomicin).
Production.
With advances in medicinal chemistry, most modern antibacterials are semisynthetic modifications of various natural compounds. These include, for example, the beta-lactam antibiotics, which include the penicillins (produced by fungi in the genus "Penicillium"), the cephalosporins, and the carbapenems. Compounds that are still isolated from living organisms are the aminoglycosides, whereas other antibacterials—for example, the sulfonamides, the quinolones, and the oxazolidinones—are produced solely by chemical synthesis. In accordance with this, many antibacterial compounds are classified on the basis of chemical/biosynthetic origin into natural, semisynthetic, and synthetic. Another classification system is based on biological activity; in this classification, antibacterials are divided into two broad groups according to their biological effect on microorganisms: Bactericidal agents kill bacteria, and bacteriostatic agents slow down or stall bacterial growth.
Many antibacterial compounds are relatively small molecules with a molecular weight of less than 2000 atomic mass units.
Since the first pioneering efforts of Florey and Chain in 1939, the importance of antibiotics, including antibacterials, to medicine has led to intense research into producing antibacterials at large scales. Following screening of antibacterials against a wide range of bacteria, production of the active compounds is carried out using fermentation, usually in strongly aerobic conditions.
Administration.
Oral antibiotics are taken by mouth, whereas intravenous administration may be used in more serious cases, such as deep-seated systemic infections. Antibiotics may also sometimes be administered topically, as with eye drops or ointments.
The topical antibiotics are:
While topical medications that act as Comedolytics as well as antibiotics are:
Side-effects.
Antibiotics are screened for any negative effects on humans or other mammals before approval for clinical use, and are usually considered safe and most are well-tolerated. However, some antibiotics have been associated with a range of adverse side effects.
Side-effects range from mild to very serious depending on the antibiotics used, the microbial organisms targeted, and the individual patient. Safety profiles of newer drugs are often not as well-established as for those that have a long history of use. Adverse effects range from fever and nausea to major allergic reactions, including photodermatitis and anaphylaxis. Common side-effects include diarrhea, resulting from disruption of the species composition in the intestinal flora, resulting, for example, in overgrowth of pathogenic bacteria, such as "Clostridium difficile". Antibacterials can also affect the vaginal flora, and may lead to overgrowth of yeast species of the genus "Candida" in the vulvo-vaginal area. Additional side-effects can result from interaction with other drugs, such as elevated risk of tendon damage from administration of a quinolone antibiotic with a systemic corticosteroid. Some scientists have hypothesized that the indiscriminate use of antibiotics alter the host microbiota and this has been associated with chronic disease. Moreover, several organizations (e.g., The American Society for Microbiology (ASM), American Public Health Association (APHA) and the American Medical Association (AMA)) have called for restrictions on antibiotic use in food animal production and an end to all nontherapeutic uses. However, commonly there are delays in regulatory and legislative actions to limit the use of antibiotics, attributable partly to resistance against such regulation by industries using or selling antibiotics, and to the time required for research to test causal links between their use and resistance to them. Two federal bills (S.742 and H.R. 2562) aimed at phasing out nontherapeutic use of antibiotics in US food animals were proposed, but have not passed. These bills were endorsed by public health and medical organizations, including the American Holistic Nurses' Association, the American Medical Association, and the American Public Health Association (APHA).
There has been extensive use of antibiotics in animal husbandry. In the United States, the question of emergence of antibiotic-resistant bacterial strains due to use of antibiotics in livestock was raised by the U.S. Food and Drug Administration (FDA) in 1977. In March 2012, the United States District Court for the Southern District of New York, ruling in an action brought by the Natural Resources Defense Council and others, ordered the FDA to revoke approvals for the use of antibiotics in livestock, which violated FDA regulations.
Alternatives.
The increase in bacterial strains that are resistant to conventional antibacterial therapies has prompted the development of bacterial disease treatment strategies that are alternatives to conventional antibacterials.
Resistance-modifying agents.
One strategy to address bacterial drug resistance is the discovery and application of compounds that modify resistance to common antibacterials. For example, some resistance-modifying agents may inhibit multidrug resistance mechanisms, such as drug efflux from the cell, thus increasing the susceptibility of bacteria to an antibacterial. Targets include:
Metabolic stimuli such as sugar can help eradicate a certain type of antibiotic-tolerant bacteria by keeping their metabolism active.
Vaccines.
Vaccines rely on immune modulation or augmentation. Vaccination either excites or reinforces the immune competence of a host to ward off infection, leading to the activation of macrophages, the production of antibodies, inflammation, and other classic immune reactions. Antibacterial vaccines have been responsible for a drastic reduction in global bacterial diseases. Vaccines made from attenuated whole cells or lysates have been replaced largely by less reactogenic, cell-free vaccines consisting of purified components, including capsular polysaccharides and their conjugates, to protein carriers, as well as inactivated toxins (toxoids) and proteins.
Phage therapy.
Phage therapy is another option that is being looked into for treating resistant strains of bacteria. The way that researchers are doing this is by infecting pathogenic bacteria with their own viruses, more specifically, bacteriophages. Bacteriophages, also known simply as phages, are precisely bacterial viruses that infect bacteria by disrupting pathogenic bacterium lytic cycles. By disrupting the lytic cycles of bacterium, phages destroy their metabolism, which eventually results in the cell's death. Phages will insert their DNA into the bacterium, allowing their DNA to be transcribed. Once their DNA is transcribed the cell will proceed to make new phages and as soon as they are ready to be released, the cell will lyse. One of the worries about using phages to fight pathogens is that the phages will infect "good" bacteria, or the bacteria that are important in the everyday function of human beings. However, studies have proven that phages are very specific when they target bacteria, which makes researchers confident that bacteriophage therapy is the definite route to defeating antibiotic resistant bacteria.
Supplements.
Some over-the-counter antioxidant supplements containing polyphenols, such as grape seed extract, demonstrate "in vitro" anti-bacterial properties.
Status of new antibiotics development.
In a policy report released by the Infectious Disease Society of America (IDSA) on April 2013, IDSA expressed grave concern over the weak pipeline of antibiotics to combat the growing ability of bacteria, especially the Gram-negative bacilli (GNB), to develop resistance to antibiotics. Since 2009, only 2 new antibiotics were approved in United States, and the number of new antibiotics annually approved for marketing continues to decline. The report could identify only seven antibiotics currently in phase 2 or phase 3 clinical trials to treat the GNB, which includes "E. coli", "Salmonella", "Shigella", and the "Enterobacteriaceae" bacteria, and these drugs do not address the entire spectrum of the resistance developed by those bacteria.
Some of these seven new antibiotics are combination of existent antibiotics, including:
Many new antibiotics are still to come from research into "Streptomyces", including new pharmaceuticals able to treat MRSA and other infections resistant to commonly-used medication. Investments into this sector of research have made a profound impact on the UK economy and human health. "Streptomyces" research supported by BBSRC at the John Innes Centre and universities in the UK has resulted in the creation of a number of spin-out companies. One of them, , has designed the type-b lantibiotic-based compound NVB302 (in phase 1) to treat "Clostridium difficile" infections.
The IDSA's prognosis for sustainable R&D infrastructure for antibiotics development will depend upon clarification of FDA regulatory clinical trial guidance that would facilitate the speedy approval of new drugs, and the appropriate economic incentives for the pharmaceuticals companies to invest in this endeavor. On 12 December 2013, the Antibiotic Development to Advance Patient Treatment (ADAPT) Act of 2013 was introduced in the U.S. Congress. The ADAPT Act aims to fast track the drug development in order to combat the growing public health threat of 'superbugs'. Under this Act, FDA can approve antibiotics and antifungals needed for life-threatening infections based on data from smaller clinical trials. The CDC will reinforce the monitoring of the use of antibiotics that treat serious and life-threatening infections and the emerging resistance, and make the data publicly available. The FDA antibiotics labeling process, 'Susceptibility Test Interpretive Criteria for Microbial Organisms' or 'breakpoints' is also streamlined to allow the most up-to-date and cutting-edge data available to healthcare professionals under the new Act. Congress has been urged in 2014 from several parties to aid the development of new drugs via bills such as ADAPT. Allan Coukell, director of drugs and medical devices at The Pew Charitable Trusts, testified in from of the House Committee, in a statement published by Reuters, that "By allowing drug developers to rely on smaller datasets, and clarifying FDA's authority to tolerate a higher level of uncertainty for these drugs when making a risk/benefit calculation, ADAPT would make the clinical trials more feasible."
Antibiotics antagonism.
Chloramphenicol and tetracyclines are antagonists to penicillins and aminoglycosides. This means the combined effect of two antibiotics from separate groups can be less than a single antibiotic. However, this can vary depending on the species of bacteria.
History.
Before the early 20th century, treatments for infections were based primarily on medicinal folklore. Mixtures with antimicrobial properties that were used in treatments of infections were described over 2000 years ago. Many ancient cultures, including the ancient Egyptians and ancient Greeks, used specially selected mold and plant materials and extracts to treat infections. More recent observations made in the laboratory of antibiosis between microorganisms led to the discovery of natural antibacterials produced by microorganisms. Louis Pasteur observed, "if we could intervene in the antagonism observed between some bacteria, it would offer perhaps the greatest hopes for therapeutics".
The term 'antibiosis', meaning "against life", was introduced by the French bacteriologist Jean Paul Vuillemin as a descriptive name of the phenomenon exhibited by these early antibacterial drugs. Antibiosis was first described in 1877 in bacteria when Louis Pasteur and Robert Koch observed that an airborne bacillus could inhibit the growth of "Bacillus anthracis". These drugs were later renamed antibiotics by Selman Waksman, an American microbiologist, in 1942. Synthetic antibiotic chemotherapy as a science and development of antibacterials began in Germany with Paul Ehrlich in the late 1880s. Ehrlich noted certain dyes would color human, animal, or bacterial cells, whereas others did not. He then proposed the idea that it might be possible to create chemicals that would act as a selective drug that would bind to and kill bacteria without harming the human host. After screening hundreds of dyes against various organisms, in 1907, he discovered a medicinally useful drug, the synthetic antibacterial salvarsan now called arsphenamine.
The effects of some types of mold on infection had been noticed many times over the course of history (see: History of penicillin). In 1928, Alexander Fleming noticed the same effect in a Petri dish, where a number of disease-causing bacteria were killed by a fungus of the genus "Penicillium." Fleming postulated that the effect is mediated by an antibacterial compound he named penicillin, and that its antibacterial properties could be exploited for chemotherapy. He initially characterized some of its biological properties, and attempted to use a crude preparation to treat some infections, but he was unable to pursue its further development without the aid of trained chemists.
The first sulfonamide and first commercially available antibacterial, Prontosil, was developed by a research team led by Gerhard Domagk in 1932 at the Bayer Laboratories of the IG Farben conglomerate in Germany. Domagk received the 1939 Nobel Prize for Medicine for his efforts. Prontosil had a relatively broad effect against Gram-positive cocci, but not against enterobacteria. Research was stimulated apace by its success. The discovery and development of this sulfonamide drug opened the era of antibacterials.
In 1939, coinciding with the start of World War II, Rene Dubos reported the discovery of the first naturally derived antibiotic, tyrothricin, a compound of 20% gramicidin and 80% tyrocidine, from "B. brevis." It was one of the first commercially manufactured antibiotics universally and was very effective in treating wounds and ulcers during World War II. Gramicidin, however, could not be used systemically because of toxicity. Tyrocidine also proved too toxic for systemic usage. Research results obtained during that period were not shared between the Axis and the Allied powers during the war.
Florey and Chain succeeded in purifying the first penicillin, penicillin G, in 1942, but it did not become widely available outside the Allied military before 1945. The chemical structure of penicillin was determined by Dorothy Crowfoot Hodgkin in 1945. Purified penicillin displayed potent antibacterial activity against a wide range of bacteria and had low toxicity in humans. Furthermore, its activity was not inhibited by biological constituents such as pus, unlike the synthetic sulfonamides. The discovery of such a powerful antibiotic was unprecedented, and the development of penicillin led to renewed interest in the search for antibiotic compounds with similar efficacy and safety. For their successful development of penicillin, which Fleming had accidentally discovered but could not develop himself, as a therapeutic drug, Ernst Chain and Howard Florey shared the 1945 Nobel Prize in Medicine with Fleming. Florey credited Dubos with pioneering the approach of deliberately and systematically searching for antibacterial compounds, which had led to the discovery of gramicidin and had revived Florey's research in penicillin.
Etymology.
The term "antibiotic" was first used in 1942 by Selman Waksman and his collaborators in journal articles to describe any substance produced by a microorganism that is antagonistic to the growth of other microorganisms in high dilution. This definition excluded substances that kill bacteria but that are not produced by microorganisms (such as gastric juices and hydrogen peroxide). It also excluded synthetic antibacterial compounds such as the sulfonamides.
The term "antibiotic" derives from "anti" + βιωτικός ("biōtikos"), "fit for life, lively", which comes from βίωσις ("biōsis"), "way of life", and that from βίος ("bios"), "life".
The term "antibacterial" derives from Greek ἀντί ("anti"), "against" + βακτήριον ("baktērion"), diminutive of βακτηρία ("baktēria"), "staff, cane", because the first ones to be discovered were rod-shaped.

</doc>
<doc id="1806" url="http://en.wikipedia.org/wiki?curid=1806" title="Arnold Schwarzenegger">
Arnold Schwarzenegger

Arnold Alois Schwarzenegger (; ]; born July 30, 1947) is an Austrian-born American actor, model, producer, director, activist, businessman, investor, writer, philanthropist, former professional bodybuilder, and politician. Schwarzenegger served two terms as the 38th Governor of California from 2003 until 2011.
Schwarzenegger began weight training at the age of 15. He won the Mr. Universe title at age 20 and went on to win the Mr. Olympia contest seven times. Schwarzenegger has remained a prominent presence in bodybuilding and has written many books and articles on the sport. Schwarzenegger gained worldwide fame as a Hollywood action film icon. Schwarzenegger's breakthrough film was the sword-and-sorcery epic "Conan the Barbarian" in 1982, which was a box-office hit and resulted in a sequel. In 1984, he appeared in James Cameron's science-fiction thriller film "The Terminator", which was a massive critical and box-office success. Schwarzenegger subsequently reprised the Terminator character in the franchise's later installments in , , and 2015. He appeared in a number of successful films, such as "Commando" (1985), "The Running Man" (1987), "Predator" (1987), "Twins" (1988), "Total Recall" (1990), "Kindergarten Cop" (1990) and "True Lies" (1994). He was nicknamed the "Austrian Oak" and the "Styrian Oak" in his bodybuilding days, "Arnie" during his acting career, and "The Governator" (a portmanteau of "Governor" and "The Terminator" – one of his best-known movie roles) since the start of his political career.
As a Republican, he was first elected on October 7, 2003, in a special recall election to replace then-Governor Gray Davis. Schwarzenegger was sworn in on November 17, 2003, to serve the remainder of Davis's term. Schwarzenegger was then re-elected on November 7, 2006, in California's 2006 gubernatorial election, to serve a full term as governor, defeating Democrat Phil Angelides, who was California State Treasurer at the time. Schwarzenegger was sworn in for his second term on January 5, 2007. In 2011, Schwarzenegger completed his second term as governor, and it was announced that he had separated from Maria Shriver, his wife for the previous 25 years; she is a member of the influential Kennedy family, as a niece of the late Democratic US President John F. Kennedy. On January 26, 2015, he was announced as the second inductee into the WWE Hall of Fame class of 2015.
Early life.
Schwarzenegger was born in Thal, Austria, a village bordering the city of Graz, he was christened Arnold Alois Schwarzenegger. His parents were the local police chief, Gustav Schwarzenegger (17 August 1907 – 13 December 1972), and Aurelia (née Jadrny; 29 July 1922 – 2 August 1998). Gustav served in World War II, after he voluntarily applied to join the Nazi Party in 1938. Gustav served with the German Army as a "Hauptfeldwebel" of the "Feldgendarmerie" and was discharged in 1943 after contracting malaria. They were married on October 20, 1945 – Gustav was 38, and Aurelia was 23-years-old. According to Schwarzenegger, both of his parents were very strict: "Back then in Austria it was a very different world, if we did something bad or we disobeyed our parents, the rod was not spared." He grew up in a Roman Catholic family who attended Mass every Sunday.
Gustav had a preference for his older son, Meinhard (17 July 1946 – 20 May 1971), over Arnold. His favoritism was "strong and blatant," which stemmed from unfounded suspicion that Arnold was not his biological child. Schwarzenegger has said his father had "no patience for listening or understanding your problems." Schwarzenegger had a good relationship with his mother and kept in touch with her until her death. In later life, Schwarzenegger commissioned the Simon Wiesenthal Center to research his father's wartime record, which came up with no evidence of Gustav's being involved in atrocities, despite Gustav's membership in the Nazi Party and SA. Schwarzenegger's father's background received wide press attention during the 2003 California recall campaign. At school, Schwarzenegger was apparently in the middle but stood out for his "cheerful, good-humored and exuberant" character. Money was a problem in their household; Schwarzenegger recalled that one of the highlights of his youth was when the family bought a refrigerator.
As a boy, Schwarzenegger played several sports, heavily influenced by his father. He picked up his first barbell in 1960, when his soccer coach took his team to a local gym. At the age of 14, he chose bodybuilding over soccer as a career. Schwarzenegger has responded to a question asking if he was 13 when he started weightlifting: "I actually started weight training when I was 15, but I'd been participating in sports, like soccer, for years, so I felt that although I was slim, I was well-developed, at least enough so that I could start going to the gym and start Olympic lifting." However, his official website biography claims: "At 14, he started an intensive training program with Dan Farmer, studied psychology at 15 (to learn more about the power of mind over body) and at 17, officially started his competitive career." During a speech in 2001, he said, "My own plan formed when I was 14 years old. My father had wanted me to be a police officer like he was. My mother wanted me to go to trade school." Schwarzenegger took to visiting a gym in Graz, where he also frequented the local movie theaters to see bodybuilding idols such as Reg Park, Steve Reeves, and Johnny Weissmuller on the big screen. When Reeves died in 2000, Schwarzenegger fondly remembered him: "As a teenager, I grew up with Steve Reeves. His remarkable accomplishments allowed me a sense of what was possible, when others around me didn't always understand my dreams. Steve Reeves has been part of everything I've ever been fortunate enough to achieve." In 1961, Schwarzenegger met former Mr. Austria Kurt Marnul, who invited him to train at the gym in Graz. He was so dedicated as a youngster that he broke into the local gym on weekends, when it was usually closed, so that he could train. "It would make me sick to miss a workout... I knew I couldn't look at myself in the mirror the next morning if I didn't do it." When Schwarzenegger was asked about his first movie experience as a boy, he replied: "I was very young, but I remember my father taking me to the Austrian theaters and seeing some newsreels. The first real movie I saw, that I distinctly remember, was a John Wayne movie."
On 20 May 1971, his brother, Meinhard, died in a car accident. Meinhard had been drinking and was killed instantly. Schwarzenegger did not attend his funeral. Meinhard was due to marry Erika Knapp, and the couple had a three-year-old son, Patrick. Schwarzenegger would pay for Patrick's education and help him to immigrate to the United States. Gustav died the following year from a stroke. In "Pumping Iron", Schwarzenegger claimed that he did not attend his father's funeral because he was training for a bodybuilding contest. Later, he and the film's producer said this story was taken from another bodybuilder for the purpose of showing the extremes that some would go to for their sport and to make Schwarzenegger's image more cold and machine-like in order to fan controversy for the film. Barbara Baker, his first serious girlfriend, has said he informed her of his father's death without emotion and that he never spoke of his brother. Over time, he has given at least three versions of why he was absent from his father's funeral.
In an interview with "Fortune" in 2004, Schwarzenegger told how he suffered what "would now be called child abuse" at the hands of his father: "My hair was pulled. I was hit with belts. So was the kid next door. It was just the way it was. Many of the children I've seen were broken by their parents, which was the German-Austrian mentality. They didn't want to create an individual. It was all about conforming. I was one who did not conform, and whose will could not be broken. Therefore, I became a rebel. Every time I got hit, and every time someone said, 'you can't do this,' I said, 'this is not going to be for much longer, because I'm going to move out of here. I want to be rich. I want to be somebody.'"
Early adulthood.
Schwarzenegger served in the Austrian Army in 1965 to fulfill the one year of service required at the time of all 18-year-old Austrian males. During his army service, he won the Junior Mr. Europe contest. He went AWOL during basic training so he could take part in the competition and spent a week in military prison: "Participating in the competition meant so much to me that I didn't carefully think through the consequences." He won another bodybuilding contest in Graz, at Steirer Hof Hotel (where he had placed second). He was voted best built man of Europe, which made him famous. "The Mr. Universe title was my ticket to America – the land of opportunity, where I could become a star and get rich." Schwarzenegger made his first plane trip in 1966, attending the NABBA Mr. Universe competition in London. He would come in second in the Mr. Universe competition, not having the muscle definition of American winner Chester Yorton.
Charles "Wag" Bennett, one of the judges at the 1966 competition, was impressed with Schwarzenegger and he offered to coach him. As Schwarzenegger had little money, Bennett invited him to stay in his crowded family home above one of his two gyms in Forest Gate, London, England. Yorton's leg definition had been judged superior, and Schwarzenegger, under a training program devised by Bennett, concentrated on improving the muscle definition and power in his legs. Staying in the East End of London helped Schwarzenegger improve his rudimentary grasp of the English language. Also in 1966, Schwarzenegger had the opportunity to meet childhood idol Reg Park, who became his friend and mentor. The training paid off and, in 1967, Schwarzenegger won the title for the first time, becoming the youngest ever Mr. Universe at the age of 20. He would go on to win the title a further three times. Schwarzenegger then flew back to Munich, training for four to six hours daily, attending business school and working in a health club (Rolf Putzinger's gym where he worked and trained from 1966–1968), returning in 1968 to London to win his next Mr. Universe title. He frequently told Roger C. Field, his English coach and friend in Munich at that time, "I'm going to become the greatest actor!"
Move to the U.S..
Schwarzenegger, who dreamed of moving to the U.S. since the age of 10, and saw bodybuilding as the avenue through which to do so, realized his dream by moving to the United States in September 1968 at the age of 21, speaking little English. There he trained at Gold's Gym in Venice, Los Angeles, California, under Joe Weider. From 1970 to 1974, one of Schwarzenegger's weight training partners was Ric Drasin, a professional wrestler who designed the original Gold's Gym logo in 1973. Schwarzenegger also became good friends with professional wrestler Superstar Billy Graham. In 1970, at age 23, he captured his first Mr. Olympia title in New York, and would go on to win the title a total of seven times.
Immigration law firm Siskind & Susser have stated that Schwarzenegger may have been an illegal immigrant at some point in the late 1960s or early 1970s because of violations in the terms of his visa. "LA Weekly" would later say in 2002 that Schwarzenegger is the most famous immigrant in America, who "overcame a thick Austrian accent and transcended the unlikely background of bodybuilding to become the biggest movie star in the world in the 1990s".
In 1977, Schwarzenegger's autobiography/weight-training guide "Arnold: The Education of a Bodybuilder" was published and became a huge success. After taking English classes at Santa Monica College in California, he earned a BA by correspondence from the University of Wisconsin–Superior, where he graduated with a degree in international marketing of fitness and business administration in 1979.
He tells that during this time he ran into a friend who told him that he was teaching Transcendental Meditation (TM), which prompted Schwarzenegger to reveal he had been struggling with anxiety for the first time in his life: "Even today, I still benefit from [the year of TM] because I don't merge and bring things together and see everything as one big problem."

</doc>
<doc id="1807" url="http://en.wikipedia.org/wiki?curid=1807" title="ASA">
ASA

ASA as an abbreviation or initialism may refer to:

</doc>
<doc id="1810" url="http://en.wikipedia.org/wiki?curid=1810" title="Actium">
Actium

Actium (Greek: Ἄκτιον) was the ancient name of a promontory of western Greece in northwestern Acarnania, at the mouth of the Sinus Ambracius (Gulf of Arta) opposite Nicopolis, built by Augustus on the north side of the strait.
On the promontory was an ancient temple of Apollo Actius, which was enlarged by Augustus, who, to memorialize the Battle of Actium, instituted or renewed the quinquennial games known as Actia or Ludi Actiaci. Actiaca Aera was a computation of time from the battle. On the promontory there was a small town, or rather a village, also called Actium.
History.
Actium belonged originally to the Corinthian colonists of Anactorium, who probably founded the worship of Apollo Actius and the Actia games; in the 3rd century BCE it fell to the Acarnanians, who subsequently held their synods there. Actium is chiefly famous as the site of the naval Battle of Actium, in which Octavian won a decisive victory over Mark Antony on September 2, 31 BCE. This battle ended a long series of ineffectual operations. The final conflict was provoked by Antony, who is said to have been persuaded by his lover, the queen Cleopatra of Egypt, to retire to her land and give battle to mask his retreat; but lack of provisions and the growing demoralization of his army would eventually account for this decision. An ancient Roman festival, Actia, was named after Actium, in Nicopolis, the new city (today Preveza, Greece). Since 2002, Actium is linked with Preveza on the north shore of the Ambracian Gulf by the Aktio-Preveza Undersea Tunnel, or Aktio-Preveza Immersed Tunnel (traffic labels). Also during Summer of 2009 archaeologists discovered in Actium the ruins of the Temple of Apollo (in Greek Ναός του Ακτίου Απόλλωνος) and found two statues' heads, one of Apollo, one of Artemis (Diana).

</doc>
<doc id="1812" url="http://en.wikipedia.org/wiki?curid=1812" title="Amway">
Amway

Amway (short for American Way) is an American company using a multi-level marketing model to sell a variety of products, primarily in the health, beauty, and home care markets. Amway was founded in 1959 by Jay Van Andel and Richard DeVos. Based in Ada, Michigan, the company and family of companies under Alticor reported sales of $11.3 billion for 2012, the seventh consecutive year of growth for the company. Its product lines include home care products, personal care products, jewelry, electronics, Nutrilite dietary supplements, water purifiers, air purifiers, insurance, and cosmetics. Amway conducts business through a number of affiliated companies in more than a hundred countries and territories around the world. Amway was ranked No.114 among the largest global retailers by Deloitte in 2006, and No.25 among the largest private companies in the United States by "Forbes" in 2012, but has been frequently subject to investigation as a pyramid scheme or fraud.
History.
Founding.
Jay Van Andel and Rich DeVos, friends since school days, had been business partners in various endeavors including a hamburger stand, air charter service, and a sailing business. In 1949 they were introduced by Neil Maaskant (Van Andel's second cousin) to the "Nutrilite Products" Corporation. Nutrilite was a California-based direct sales company founded by Carl Rehnborg, developer of the first multivitamin marketed in the United States. In August 1949, after a night-long talk, DeVos and Van Andel signed up to become distributors for Nutrilite food supplements. They sold their first box the next day for $19.50, but lost interest for the next two weeks. Shortly thereafter, at the urging of Maaskant, who had become their sponsor, they traveled to Chicago to attend a Nutrilite seminar. The meeting was at a downtown hotel, with over a hundred people in attendance. After seeing promotional filmstrips and listening to talks by company representatives and successful distributors, they decided to pursue the Nutrilite business opportunity with enthusiasm. They sold their second box of supplements on their return trip to Michigan, and rapidly proceeded to develop their new business further.
In 1949, DeVos and Van Andel had formed Ja-Ri Corporation (abbreviated from their respective first names) for importing wooden goods from South American countries. After their trip to the Nutrilite seminar, they dropped this business and Ja-Ri became their Nutrilite distributorship. In addition to profits on each product sold, Nutrilite also offered commission on the sales of products by new distributors introduced to the company by existing distributors—a system today known as multi-level marketing or network marketing. By 1958, DeVos and Van Andel had built an organization of over 5,000 distributors. However, following concerns about the stability of Nutrilite, in April 1959 they and some of their top distributors formed "The American Way Association" to represent the distributors and look for additional products to market.
Their first product was called Frisk, a concentrated organic cleaner developed by a scientist in Ohio. DeVos and Van Andel bought the rights to manufacture and distribute Frisk, and later changed the name to LOC (Liquid Organic Cleaner). They subsequently formed Amway Sales Corporation to procure and inventory products and to handle the sales and marketing plan, and Amway Services Corporation to handle insurance and other benefits for distributors (Amway being an abbreviation of "American Way"). In 1960 they purchased a 50% share in Atco Manufacturing Company in Detroit, the original manufacturers of LOC, and changed its name to Amway Manufacturing Corporation. In 1964 the Amway Sales Corporation, Amway Services Corporation, and Amway Manufacturing Corporation merged to form a single entity, Amway Corporation. Amway bought control of Nutrilite in 1972 and full ownership in 1994.
International expansion.
Amway expanded overseas to Australia in 1971, to Europe in 1973, to parts of Asia in 1974, to Japan in 1979, to Latin America in 1985, to Thailand in 1987, to China in 1995, to Africa in 1997, to India and Scandinavia in 1998, to Ukraine in 2003, to Russia in 2005, and to Vietnam in 2008.
Quixtar.
In 1999 the founders of the Amway corporation established a new holding company, named Alticor, and launched three new companies: a sister (and separate) Internet-focused company named Quixtar, Access Business Group, and Pyxis Innovations. Pyxis, later replaced by Fulton Innovation, pursued research and development and Access Business Group handled manufacturing and logistics for Amway, Quixtar, and third-party clients.
The main difference was that all "Independent Business Owners" (IBO) could order directly from Amway on the Internet, rather than from their upline "direct distributor", and have products shipped directly to their home. The Amway name continued being used in the rest of the world. After virtually all Amway distributors in North America switched to Quixtar, Alticor elected to close Amway North America after 2001. In June 2007 it was announced that the Quixtar brand would be phased out over an 18- to 24-month period in favor of a unified Amway brand (Amway Global) worldwide.
In 2006, Quixtar published "The Quixtar Independent Business Owner Compensation Plan", in which the company reported that the average monthly gross income for "Active" IBOs was $115.
Global markets.
According to the Amway website, as of 2011[ [update]] the company operates in over 100 countries and territories, organized into regional markets: the Americas, Europe, greater China, Japan and Korea, and SE Asia/Australia.
In 2008, Alticor announced that two-thirds of the company's 58 markets reported sales increases, including strong growth in the China, Russia, Ukraine and India markets.
Amway China.
Amway grew quickly in China from its market launch in 1995. In 1998, after abuses of illegal pyramid schemes led to riots, the Chinese government enacted a ban on all direct selling companies, including Amway. After the negotiations, some companies like Amway, Avon, and Mary Kay continued to operate through a network of retail stores promoted by an independent sales force. China introduced new direct selling laws in December 2005, and in December 2006 Amway was one of the first companies to receive a license to resume direct sales. However, the law forbids teachers, doctors, and civil servants from becoming direct sales agents for the company and, unlike in the United States, salespeople in China are ineligible to receive commissions from sales made by the distributors they recruit.
In 2006, Amway China had a reported 180,000 sales representatives, 140 stores, and $2 billion in annual sales. In 2007 Amway Greater China and South-east Asia Chief Executive Eva Cheng was ranked No.88 by "Forbes" magazine in its list of the World's Most Powerful Women. In 2008, China was Amway's largest market, reporting 28% growth and sales of ¥17 billion (US$2.5 billion). According to a report in Bloomberg Businessweek in April 2010, Amway had 237 retail shops in China, 160,000 direct sales agents, and $3 billion in revenue.
Amway China holds large incentive trips each year as rewards for its top achievers to different destinations. As the trips involve thousands of participants, events often result in a boost for the local economy of the destination.
Brands.
Amway's product line grew from LOC, with the laundry detergent SA8 added in 1960, and later the hair care product Satinique (1965) and the cosmetics line Artistry (1968). Today Amway manufactures over 450 products, with manufacturing facilities in China, India and the United States, as well as Nutrilite organic farms in Brazil, Mexico and the United States (California and Washington State). Amway brands include: Artistry, Atmosphere, Body Blends, Bodykey, Body Works, Clear Now, eSpring, Glister (brand)|Glister, iCook, Legacy of Clean, Nutrilite, Peter Island, Perfect Empowered Drinking Water, Personal Accents, Ribbon, Satinique, Artistry Men and XS.
Household cleaners.
Amway is best known in North America for its original multi-purpose cleaning product LOC, SA8 laundry detergent, and Dish Drops dishwashing liquid. In the January 2007 issue of "Consumer Reports", SA8 with Bioquest was rated the best-performing laundry detergent, scoring 99 out of a possible 100 points. "Consumer Reports" did, however, criticize SA8's pricing, a situation which was disputed by Amway. Consumer Reports conducted blind testing of detergents in 2010 and ranked versions of Amway's Legacy of Clean detergents 9th and 18th of 20 detergents tested. "Consumer Reports" program manager Pat Slaven recommended against buying the products because consumers can "go to the grocery store and get something that performs a whole lot better for a whole lot less money".
Health and beauty.
Amway's health and beauty brands include Artistry, Beautycycle, Time Defiance, Pure White, Satinique, Tolsom, Body Series, Glister, "Moiskin" (South America), Nutrilite, "Nutriway" (Scandinavia and Australia/New Zealand), eSpring, "Attitude" (India), "Atmosphere" and "iCook" as well as "XL" and XS Energy drinks.
Artistry.
Amway's Artistry products include skin care, cosmetics, and anti-aging creams and serums.
Nutrilite.
Amway's largest selling brand is the Nutrilite range of health supplements (marketed as Nutriway in some countries), and in 2008 Nutrilite sales exceeded $3 billion globally. In 2001, five Nutrilite products were the first dietary supplements to be certified by NSF International. In 2006, 2007, 2008, and 2009 in the nutrient and health food category, Nutrilite won "Platinum" and "Gold" awards in Malaysia, China, Taiwan, Thailand, and Asia overall in the "Reader's Digest" "Trusted Brands of Asia" survey. In 2008 Nutrilite scientists, in partnership with Alticor subsidiary Interleukin Genetics won the 12th John M. Kinney Award for Nutrition and Metabolism for their research into the interaction between nutrition and genetics. In January 2009, Amway announced a voluntary recall of Nutrilite and XS Energy Bars after learning that they had possibly been manufactured with Salmonella-contaminated ingredients from Peanut Corporation of America. The company indicated that it had not received any reports of illness in connection with the products.
eSpring.
Amway's eSpring water filter, introduced in 2000, was the first home water treatment system to incorporate a carbon block filter and ultraviolet disinfection unit, becoming the first home system to achieve certification for ANSI/NSF Standards 42, 53, and 55. The unit was also the first commercial product to include sister company Fulton Innovations eCoupled wireless power induction technology. Fulton Innovation introduced the technology in other consumer electronic products at the 2007 Consumer Electronics Show. Companies licensing this technology include General Motors, Motorola, and Visteon. In 2006 eSpring was named "Product of the Year" by Polish nonprofit World Foundation of Health, Heart and Mind. eSpring has won numerous "Gold" and "Platinum" awards in the "Reader's Digest" Most Trusted Brand Asia surveys.
Atmosphere.
In 2009 Amway's Atmosphere Air Purifier became the first air cleaner certified Asthma and Allergy Friendly by the Asthma and Allergy Foundation of America.
Ditto Delivery.
Amway owns a patent on the online shopping method of Ditto Delivery, which allows consumers to specify an automatic monthly delivery of each product. In May 2001, Ditto Delivery accounted for 30% of Quixtar's North American sales.
Business model.
Amway combines direct selling with a multi-level marketing strategy. "Independent Business Owners" (IBOs) may market products directly to potential customers and may also sponsor and mentor other people to become IBOs. IBOs may earn income both from the retail markup on any products they sell personally, plus a performance bonus based on the sales volume they and their downline (IBOs they have sponsored) have generated. People may also register as IBOs to buy products at discounted prices.
Commercial sponsorships.
In December 2006, Alticor secured the naming rights for the Orlando Magic's home basketball arena in Orlando, Florida. The Orlando Magic are owned by the DeVos family. The arena, formerly known as the TD Waterhouse Centre, was renamed the Amway Arena. Its successor, the Amway Center, was opened in 2010, and the older arena was demolished in 2012.
In 2009, Amway Global signed a three-year deal with the San Jose Earthquakes Major League Soccer team to become the jersey sponsor.
In March 2009, Amway Global signed a multi-year deal to become the presenting partner of the Los Angeles Sol of Women's Professional Soccer. The deal, however, would last only one year, as the Sol folded the next year.
In 2011 Amway signed a three-year deal to be the presenting sponsor of the National Hockey League's Detroit Red Wings.
Since 2012, Amway has been the title sponsor of the Canadian Championship, an annual soccer tournament.
Politics and culture.
Political causes.
In the 1990s, the Amway organization was a major contributor to the Republican Party (GOP) and to the election campaigns of various GOP candidates. Amway and its sales force contributed a substantial amount (up to half) of the total funds ($669,525) for the 1994 political campaign of Republican congresswoman and Amway distributor Sue Myrick (N.C.). According to two reports by "Mother Jones" magazine, Amway distributor Dexter Yager "used the company’s extensive voice-mail system to rally hundreds of Amway distributors into giving a total of $295,871" to Myrick’s campaign. According to a campaign staffer quoted by the magazine, Myrick had appeared regularly on the Amway circuit, speaking at hundreds of rallies and selling $5 and $10 audiotapes. Following the 1994 election, Myrick maintained "close ties to Amway and Yager", and raised $100,000 from Amway sources, "most notably through fundraisers at the homes of big distributors", in the 1997–98 election cycle.
In October 1994, Amway gave the biggest corporate contribution recorded to that date to a political party for a single election, $2.5 million to the Republican National Committee, and was the number one corporate political donor in the United States. In the 2004 election cycle, the organization contributed a total of $4 million to a conservative 527 group, Progress for America.
In July 1996, Amway co-founder Richard DeVos was honored at a $3 million fundraiser for the Republican Party, and a week later, it was reported that Amway had tried to donate $1.3 million to pay for Republican "infomercials" and televising of the GOP convention on Pat Robertson's Family Channel, but backed off when Democrats criticized the donation as a ploy to avoid campaign-finance restrictions.
In April 1997 Richard DeVos and his wife, Helen, gave $1 million to the Republican National Committee (RNC), which at the time was the second-largest soft-money donation ever, behind Amway's 1994 gift of $2.5 million to the RNC. In July 1997, Senate Majority leader Trent Lott and House Speaker Newt Gingrich slipped a last-minute provision into a hotly contested compromise tax bill that granted Amway a tax break on its Asian branches, saving it $19 million.
In a column published in the "Fort Worth Star-Telegram" newspaper in August 1997, reporter Molly Ivins wrote that Amway had "its own caucus in Congress...Five Republican House members are also Amway distributors: Reps. Sue Myrick of North Carolina, Jon Christensen of Nebraska, Dick Chrysler of Michigan, Richard Pombo of California, and John Ensign of Nevada. Their informal caucus meets several times a year with Amway bigwigs to discuss policy matters affecting the company, including China's trade status."
A 1998 analysis of campaign contributions conducted by "Businessweek" found that Amway, along with the founding families and some top distributors, had donated at least $7 million to GOP causes in the preceding decade. Political candidates who received campaign funding from Amway in 1998 included Representatives Bill Redmond (R–N.M.), Heather Wilson (R–N.M.), and Jon Christensen (R–Neb).
According to a report by the Center for Public Integrity, in the 2004 election cycle, members of the Van Andel and DeVos families were the second, third and fifth largest donors to the Republican party.
Dick DeVos, son of Amway founder Richard DeVos and past president of the company, served as Finance Chairman of the Republican National Committee, and his wife Betsy DeVos served as chair of the Michigan Republican Party from 1996 to 2000 and 2003 to 2005.
In May 2005, Dick DeVos ran against incumbent Governor Jennifer Granholm in Michigan's 2006 gubernatorial election. DeVos was defeated by Granholm, who won 56% of the popular vote to DeVos' 42%.
In August 2012, gay rights activist Fred Karger began a movement to boycott Amway in protest of the contribution from a private foundation of Amway President Doug DeVos to the National Organization for Marriage, a political organization which opposes legalization of same-sex marriage in the United States.
Religion.
Several sources have commented on the promotion of Christian conservative ideology within the Amway organization. "Mother Jones" magazine described the Amway distributor force as "heavily influenced by the company's dual themes of Christian morality and free enterprise" and operating "like a private political army". In "The Cult of Free Enterprise", author (and former Amway distributor) Stephen Butterfield wrote "[Amway] sells a marketing and motivational system, a cause, a way of life, in a fervid emotional atmosphere of rallies and political religious revivalism." "Philadelphia City Paper" correspondent Maryam Henein stated that, "the language used in motivational tools for Amway frequently echoes or directly quotes the Bible, with the unstated assumption of a shared Christian perspective."
"Businessweek" correspondents Bill Vlasic and Beth Regan characterized the founding families of Amway as "fervently conservative, fervently Christian, and hugely influential in the Republican Party", noting that "Rich DeVos charged up the troops with a message of Christian beliefs and rock-ribbed conservatism."
High-ranking Amway leaders such as Richard DeVos and Dexter Yager were owners and members of the board of Gospel Films, a producer of movies and books geared towards conservative Christians, as well as co-owners (along with Salem Communications) of a right-wing, Christian nonprofit called Gospel Communications International.
"Rolling Stone's" Bob Moser reported that former Amway CEO and co-founder Richard DeVos is connected with the Dominionist political movement in the United States. Moser states that DeVos was a supporter of the late D. James Kennedy, giving more than $5 million to Kennedy's Coral Ridge Ministries. DeVos was also a founding member and two-time president of the Council for National Policy, a right-wing Christian organization.
Sociologist David G. Bromley calls Amway a "quasi-religious corporation" having sectarian characteristics. Bromley and Anson Shupe view Amway as preaching the Gospel of Prosperity. Patralekha Bhattacharya and Krishna Kumar Mehta, of the consulting firm Thinkalytics, LLC, reasoned that although some critics have referred to organizations such as Amway as "cults" and have speculated that they engage in "mind control", there are other explanations that could account for the behavior of distributors. Namely, continued involvement of distributors despite minimal economic return may result from social satisfaction compensating for diminished economic satisfaction.
Chamber of Commerce.
Amway co-founder, Jay Van Andel (in 1980), and later his son Steve Van Andel (in 2001) were elected by the board of directors of the United States Chamber of Commerce as chairman of the private American lobbying organization.
Pyramid scheme accusations.
Amway utilizes a tiered distribution and remuneration model (the Amway "Sales and Marketing Plan") that promises to reward participants who grow Amway's market share through a combination of sales and recruitment. This tiered distribution model relies on Independent Business Owners (IBOs) acquiring and training further Independent Business Owners, which is the principal characteristic of a pyramid scheme.
Harvard Business School of Leadership, which described Amway as "one of the most profitable direct selling companies in the world", noted that Amway founders Van Andel and DeVos "accomplished their success through the use of an elaborate pyramid-like distribution system in which independent distributors of Amway products received a percentage of the merchandise they sold and also a percentage of the merchandise sold by recruited distributors".
The "pyramid-like distribution system" of the Amway business model led to Amway being accused of being a pyramid scheme. A 1979 US Federal Trade Commission ruling established that the Amway business model is not illegal in the United States.
Robert Carroll, of the "Skeptic's Dictionary", has described Amway as a "legal pyramid scheme", and has said that the religion-like devotion of its affiliates is used by the company to conceal poor performance rates by distributors.
FTC investigation.
In a 1979 ruling, the Federal Trade Commission found that Amway does not qualify as a pyramid scheme because distributors were not paid to recruit people and had to sell products to get bonus checks, and the company was committed to buying back its distributors' excess inventory.
The FTC did, however, find Amway "guilty of price-fixing and making exaggerated income claims"; the company was ordered to stop retail price fixing and allocating customers among distributors and was prohibited from misrepresenting the amount of profit, earnings or sales its distributors are likely to achieve with the business. Amway was ordered to accompany any such statements with the actual averages per distributor, pointing out that more than half of the distributors do not make any money, with the average distributor making less than $100 per month. The order was violated with a 1986 ad campaign, resulting in a $100,000 fine.
Amway India (Andhra Pradesh and Kerala).
In September 2006, following a public complaint, Andhra Pradesh state police (CID) initiated raids and seizures against Amway distributors in the state, and submitted a petition against them, claiming the company violated the Prize Chits and Money Circulation Schemes (banning) Act. They shut down all corporate offices associated with the Amway organization including the offices of some Amway distributors. The enforcement said that the business model of the company is illegal. The Reserve Bank of India (RBI) had notified the police that Amway in India may be violating certain laws regarding a "money circulation scheme" and the "IB Times" article writes that "some say ... Amway is really more about making money from recruiting people to become distributors, as opposed to selling products". The complaint was initiated following a dowry dispute between a local man and his wife, an Amway distributor.
Following a petition by Amway, the state High Court issued an injunction against the CID and stated the Act did not "prima facie" apply, however after Amway requested the CID petition be dismissed the High Court declared that if police allegations were true, Amway's Indian subsidiary would be in violation of the act and the investigation should continue. On August 14, 2007, the Supreme Court of India ordered the state police to complete the investigation against Amway in six months. In 2008, citing the High Court decision, the Andhra Pradesh state government enacted a ban on Amway media advertisements. Amway challenged the ban and in July 2009 the AP High Court refused a petition the ban should be enforced. As of June 2009 the original 2006 CID case was still pending at the Chief Metropolitan Magistrate Court in Hyderabad.
On August 6, 2011, Kerala Police sealed the offices of Amway at Kozhikode, Kannur, Kochi, Kottayam, Thrissur, Kollam and Thiruvananthapuram following complaints. Amway stated that "it had been receiving complaints from distributors over the past one month that they were being called to police stations and being 'harassed' to give complaints against the company". The distributors faced a tough time because of the lack of proactive steps and slow actions from the company management.
The Kerala High Court on November 9 directed the DGP, Kerala Police to file a statement regarding the status of the investigation initiated against the multi-level marketing companies. A Division Bench comprising Acting Chief Justice Manjula Chellur and Justice C N Ramachandran Nair issued the directive while considering a petition challenging the government order regarding the Kerala direct selling regulations.
On November 2012, The Economic Offences Wing of Kerala Police, conducted searches at the offices of Amway at Kozhikode, Thrissur and Kannur as part of its crackdown on money chain activities and closed down the firm's godowns at these centres. Products valued at Rs.2.14 crore were also seized. Later, Area manager of Amway, P. M. Rajkumar, who was arrested following searches was remanded in judicial custody for 14 days by the Judicial First Class Magistrate, Thamarassery.
On 27 May 2013, Crime Branch officials of Kerala Police arrested William S. Pinckney, Managing Director & CEO of Amway India Enterprises along with two other directors of the company from Kozhikode. The three were arrested on charges of running a pyramid scheme. They were granted bail the next day and the business was unaffected. On June 8, 2013, Kozhikode Court lifted the freeze on Amway offices in Kerala.
Class action settlement.
On November 3, 2010, Amway announced that it had agreed to pay $56 million – $34 million in cash and $22 million in products – to settle a class action that had been filed in Federal District Court in California in 2007. The class action, which had been brought against Quixtar and several of its top-level distributors, alleged fraud, racketeering, and that the defendants operated as an illegal pyramid scheme.
While noting that the settlement is not an admission of wrongdoing or liability, Amway acknowledged that it had made changes to its business operations as a result of the lawsuit. The settlement is subject to approval by the court, which was expected in early 2011. The economic value of the settlement, including the changes Amway made to its business model, totals $100 million.
Other legal actions.
Canadian tax case.
In 1983, Amway pleaded guilty to criminal tax evasion and customs fraud in Canada, resulting in a fine of $25 million CAD, the largest fine ever imposed in Canada at the time. In 1989 the company settled the outstanding customs duties for $45 million CAD.
In a 1994 interview, Amway co-founder Rich DeVos stated that this incident had been his greatest "moral or spiritual challenge", first in "soul searching as to whether they had done anything wrong" and then for pleading guilty for technical reasons, despite believing they were innocent of the charges. DeVos stated he believed that the case had been motivated by "political reasons".
RIAA lawsuit.
The Recording Industry Association of America (RIAA), as part of its anti-piracy efforts, sued Amway and several distributors in 1996, alleging that copyrighted music was used on "highly profitable" training videotapes. Amway denied wrongdoing, blaming the case on a misunderstanding by distributors, and settled the case out of court for $9 million. In a related lawsuit initiated by the distributors involved, the Court established that Mahaleel Lee Luster, who had been contracted to make the videotapes, had violated copyright without the knowledge of three of the five of those distributors.
Amway UK.
In May 2007, the UK Department of Trade and Industry (DTI) accused Amway and distributor organizations Britt WorldWide and Network TwentyOne UK of "objectionable practices" and petitioned to wind up the companies. The case against Amway was dismissed in 2008 on the condition that a full earnings disclosure is published publicly, no registration or renewal fees are charged and that the sale of business support materials is prohibited. The case against Network 21 was dismissed in 2009.
"Welcome to Life" (Poland).
In 1997, Amway Poland and Network TwentyOne separately sued the makers of a Polish film "Welcome to Life" for defamation and copyright violations. The director and producer were later acquitted on the charge of disseminating false information. The film, banned for 12 years, was one of the highly anticipated movies of 2009's Warsaw Film Festival and was dubbed by the promoters as a "scary movie about brainwashing" that depicts hard-sell "pep rallies" and distributors stating meetings were operated similar to the Communist Party and methods of recruitment that confusingly resembled those of a sect. A bestseller on the local video black market, the film was banned while the suit proceeded.
In 2001 a regional court ruled in favor of Network 21; however, in 2004 the Warsaw Regional Court dismissed Amway's civil lawsuit. On appeal Amway won the case and the producers were ordered to pay a fine to a children's charity and publish a public apology. s of 2009[ [update]] the film was still banned due to an ongoing case brought by "private individuals" ridiculed in the film.
On December 18, 2012, the court ruled that film can be screened, but the makers have to remove "untrue informations", as the screen near the end of the movie stated that 30% of company income is generated by sales of training materials and that the vast majority of its profits are shared only by the tiny fraction of top distributors. This is not the only court case, so the film is still banned on other grounds.
Dr. Phil and Shape Up.
In March 2004, TV personality Phil McGraw (a.k.a. Dr. Phil) pulled his "Shape Up" line of supplements off the market in the face of an investigation by the U.S. Federal Trade Commission (FTC). The supplements were manufactured by CSA Nutraceuticals, a subsidiary of Alticor’s Access Business Group. The FTC later dropped the probe, but in October 2005 a class-action lawsuit was filed against McGraw by several people who used the products and claimed that the supplements, which cost $120 per month, did not stimulate weight loss. In September 2006, a $10.5 million settlement was reached, in which Alticor agreed to provide $4.5 million in cash and $6 million in Nutrilite products to disgruntled users of Shape Up.
Dateline NBC.
In 2004, "Dateline NBC" featured a critical report based on a yearlong undercover investigation of business practices within the Amway organization. The report noted that the average distributor makes only about $1,400 per year and that many of the “high level distributors singing the praises of Quixtar [Amway]” are actually “making most of their money by selling motivational books, tapes and seminars; not Quixtar’s cosmetics, soaps, and electronics”.
It was also revealed that an Amway recruiter featured in the report (Greg Fredericks) made misleading and inconsistent statements about Amway earnings during a recruitment meeting and had an outstanding arrest warrant for drug dealing.
Other issues.
Some Amway distributor groups have been accused of using "cult-like" tactics to attract new distributors and keep them involved and committed. Allegations include resemblance to a Big Brother organization with paranoid attitude to insiders critical of the organization, seminars and rallies resembling religious revival meetings and enormous involvement of distributors despite minimal incomes. An examination of the 1979–1980 tax records in the state of Wisconsin showed that the Direct Distributors reported a net loss of $918 on average.
Procter & Gamble.
Some Amway distributors were involved with an urban legend that the (old) Procter & Gamble service mark was in fact a Satanic symbol or that the CEO of P&G is himself a practicing Satanist. (In some variants of the urban legend, it is also claimed that the CEO of Procter & Gamble donated "satanic tithes" to the Church of Satan.) Procter & Gamble alleged that several Amway distributors were behind a resurgence of the urban legend in the 1990s and sued several independent Amway distributors and the company for defamation and slander. The distributors had used Amway's Amvox voice messaging service to send the rumor to their downline distributors in April 1995. After more than a decade of lawsuits in multiple states, by 2003 all allegations against Amway and Amway distributors had been dismissed. In October 2005 a Utah appeals court reversed part of the decision dismissing the case against the four Amway distributors, and remanded it to the lower court for further proceedings. On March 20, 2007, Procter & Gamble was awarded $19.25 million by a U.S. District Court jury in Salt Lake City, in the lawsuit against the four former Amway distributors. On November 24, 2008, the case was officially settled.

</doc>
<doc id="1814" url="http://en.wikipedia.org/wiki?curid=1814" title="Adam Smith">
Adam Smith

Adam Smith (16 June 1723 NS – 17 July 1790) was a Scottish moral philosopher, pioneer of political economy, and key Scottish Enlightenment figure.
Smith is best known for two classic works: "The Theory of Moral Sentiments" (1759), and "An Inquiry into the Nature and Causes of the Wealth of Nations" (1776). The latter, usually abbreviated as "The Wealth of Nations", is considered his "magnum opus" and the first modern work of economics. Smith is cited as the "father of modern economics" and is still among the most influential thinkers in the field of economics today.
Smith studied social philosophy at the University of Glasgow and at Balliol College, Oxford, where he was one of the first students to benefit from scholarships set up by fellow Scot, John Snell. After graduating, he delivered a successful series of public lectures at Edinburgh, leading him to collaborate with David Hume during the Scottish Enlightenment. Smith obtained a professorship at Glasgow teaching moral philosophy, and during this time he wrote and published "The Theory of Moral Sentiments". In his later life, he took a tutoring position that allowed him to travel throughout Europe, where he met other intellectual leaders of his day.
Smith laid the foundations of classical free market economic theory. "The Wealth of Nations" was a precursor to the modern academic discipline of economics. In this and other works, he expounded upon how rational self-interest and competition can lead to economic prosperity. Smith was controversial in his own day and his general approach and writing style were often satirised by Tory writers in the moralising tradition of William Hogarth and Jonathan Swift. In 2005, "The Wealth of Nations" was named among the 100 Best Scottish Books of all time. It is said former UK Prime Minister Margaret Thatcher carried a copy of the book in her handbag.
Biography.
Early life.
Smith was born in Kirkcaldy, in the County of Fife, in Scotland. His father, also of the same name, was a Scottish "Writer to the Signet" (senior solicitor), advocate, and prosecutor (Judge Advocate) and also served as comptroller of the Customs in Kirkcaldy. In 1720 he married Margaret Douglas, daughter of the landed Robert Douglas of Strathendry, also in Fife. His father died two months after he was born, leaving his mother a widow. The date of Smith's baptism into the Church of Scotland at Kirkcaldy was 5 June 1723, and this has often been treated as if it were also his date of birth, which is unknown. Although few events in Smith's early childhood are known, the Scottish journalist John Rae, Smith's biographer, recorded that Smith was abducted by gypsies at the age of three and released when others went to rescue him. Smith was close to his mother, who probably encouraged him to pursue his scholarly ambitions. He attended the Burgh School of Kirkcaldy—characterised by Rae as "one of the best secondary schools of Scotland at that period"—from 1729 to 1737, he learned Latin, mathematics, history, and writing.
Formal education.
Smith entered the University of Glasgow when he was fourteen and studied moral philosophy under Francis Hutcheson. Here, Smith developed his passion for liberty, reason, and free speech. In 1740 Smith was the graduate scholar presented to undertake postgraduate studies at Balliol College, Oxford, under the Snell Exhibition.
Adam Smith considered the teaching at Glasgow to be far superior to that at Oxford, which he found intellectually stifling. In Book V, Chapter II of "The Wealth of Nations", Smith wrote: "In the University of Oxford, the greater part of the public professors have, for these many years, given up altogether even the pretence of teaching."
Smith is also reported to have complained to friends that Oxford officials once discovered him reading a copy of David Hume's "Treatise on Human Nature", and they subsequently confiscated his book and punished him severely for reading it. According to William Robert Scott, "The Oxford of [Smith's] time gave little if any help towards what was to be his lifework." Nevertheless, Smith took the opportunity while at Oxford to teach himself several subjects by reading many books from the shelves of the large Bodleian Library. When Smith was not studying on his own, his time at Oxford was not a happy one, according to his letters. Near the end of his time there, Smith began suffering from shaking fits, probably the symptoms of a nervous breakdown. He left Oxford University in 1746, before his scholarship ended.
In Book V of "The Wealth of Nations", Smith comments on the low quality of instruction and the meager intellectual activity at English universities, when compared to their Scottish counterparts. He attributes this both to the rich endowments of the colleges at Oxford and Cambridge, which made the income of professors independent of their ability to attract students, and to the fact that distinguished men of letters could make an even more comfortable living as ministers of the Church of England.
Adam Smith's discontent at Oxford might be in part due to the absence of his beloved teacher in Glasgow, Francis Hutcheson. Hutcheson was well regarded as one of the most prominent lecturers at the University of Glasgow in his day and earned the approbation of students, colleagues, and even ordinary residents with the fervor and earnestness of his orations (which he sometimes opened to the public). His lectures endeavoured not merely to teach philosophy but to make his students embody that philosophy in their lives, appropriately acquiring the epithet, the preacher of philosophy. Unlike Smith, Hutcheson was not a system builder; rather it was his magnetic personality and method of lecturing that so influenced his students and caused the greatest of those to reverentially refer to him as "the never to be forgotten Hutcheson" – a title that Smith in all his correspondence used to describe only two people, his good friend David Hume and influential mentor Francis Hutcheson.
Teaching career.
Smith began delivering public lectures in 1748 in Edinburgh, sponsored by the Philosophical Society of Edinburgh under the patronage of Lord Kames. His lecture topics included rhetoric and "belles-lettres", and later the subject of "the progress of opulence". On this latter topic he first expounded his economic philosophy of "the obvious and simple system of natural liberty". While Smith was not adept at public speaking, his lectures met with success.
In 1750, he met the philosopher David Hume, who was his senior by more than a decade. In their writings covering history, politics, philosophy, economics, and religion, Smith and Hume shared closer intellectual and personal bonds than with other important figures of the Scottish Enlightenment.
In 1751, Smith earned a professorship at Glasgow University teaching logic courses, and in 1752 he was elected a member of the Philosophical Society of Edinburgh, having been introduced to the society by Lord Kames. When the head of Moral Philosophy died the next year, Smith took over the position. He worked as an academic for the next thirteen years, which he characterised as "by far the most useful and therefore by far the happiest and most honorable period [of his life]".
Smith published "The Theory of Moral Sentiments" in 1759, embodying some of his Glasgow lectures. This work was concerned with how human morality depends on sympathy between agent and spectator, or the individual and other members of society. Smith defined "mutual sympathy" as the basis of moral sentiments. He based his explanation, not on a special "moral sense" as the Third Lord Shaftesbury and Hutcheson had done, nor on utility as Hume did, but on mutual sympathy, a term best captured in modern parlance by the twentieth-century concept of empathy, the capacity to recognise feelings that are being experienced by another being.
Following the publication of "The Theory of Moral Sentiments", Smith became so popular that many wealthy students left their schools in other countries to enroll at Glasgow to learn under Smith. After the publication of "The Theory of Moral Sentiments", Smith began to give more attention to jurisprudence and economics in his lectures and less to his theories of morals. For example, Smith lectured that the cause of increase in national wealth is labour, rather than the nation's quantity of gold or silver, which is the basis for mercantilism, the economic theory that dominated Western European economic policies at the time.
In 1762, the University of Glasgow conferred on Smith the title of Doctor of Laws (LL.D.). At the end of 1763, he obtained an offer from Charles Townshend – who had been introduced to Smith by David Hume – to tutor his stepson, Henry Scott, the young Duke of Buccleuch. Smith then resigned from his professorship to take the tutoring position. He subsequently attempted to return the fees he had collected from his students because he resigned in the middle of the term, but his students refused.
Tutoring and travels.
Smith's tutoring job entailed touring Europe with Scott, during which time he educated Scott on a variety of subjects – such as proper Polish. He was paid £300 per year (plus expenses) along with a £300 per year pension; roughly twice his former income as a teacher. Smith first travelled as a tutor to Toulouse, France, where he stayed for one and a half years. According to his own account, he found Toulouse to be somewhat boring, having written to Hume that he "had begun to write a book to pass away the time". After touring the south of France, the group moved to Geneva, where Smith met with the philosopher Voltaire.
From Geneva, the party moved to Paris. Here Smith came to know several great intellectual leaders of the time; invariably having an effect on his future works. This list included: Benjamin Franklin, Turgot, Jean D'Alembert, André Morellet, Helvétius, and, notably, François Quesnay, the head of the Physiocratic school. So impressed with his ideas Smith considered dedicating "The Wealth of Nations" to him – had Quesnay not died beforehand. Physiocrats were opposed to mercantilism, the dominating economic theory of the time. Illustrated in their motto Laissez faire et laissez passer, le monde va de lui même! (Let do and let pass, the world goes on by itself!). They were also known to have declared that only agricultural activity produced real wealth; merchants and industrialists (manufacturers) did not. This however, did not represent their true school of thought, but was a mere "smoke screen" manufactured to hide their actual criticisms of the nobility and church; arguing that they made up the only real clients of merchants.
The wealth of France was virtually destroyed by Louis XIV and Louis XV in ruinous wars, by aiding the American insurgents against the British, and perhaps most destructive (in terms of public perceptions) was what was seen as the excessive consumption of goods and services deemed to have no economic contribution – unproductive labour. Assuming that nobility and church are essentially detractors from economic growth, the feudal system of agriculture in France was the only sector important to maintain the wealth of the nation. Given that the English economy of the day yielded an income distribution that stood in contrast to that which existed in France, Smith concluded that the teachings and beliefs of Physiocrats were, "with all [their] imperfections [perhaps], the nearest approximation to the truth that has yet been published upon the subject of political economy". The distinction between productive versus unproductive labour – the physiocratic "classe steril" – was a predominant issue in the development and understanding of what would become classical economic theory.
Later years.
In 1766, Henry Scott's younger brother died in Paris, and Smith's tour as a tutor ended shortly thereafter. Smith returned home that year to Kirkcaldy, and he devoted much of the next ten years to his magnum opus. There he befriended Henry Moyes, a young blind man who showed precocious aptitude. As well as teaching Moyes, Smith secured the patronage of David Hume and Thomas Reid in the young man's education. In May 1773, Smith was elected fellow of the Royal Society of London, and was elected a member of the Literary Club in 1775. "The Wealth of Nations" was published in 1776 and was an instant success, selling out its first edition in only six months.
In 1778, Smith was appointed to a post as commissioner of customs in Scotland and went to live with his mother in Panmure House in Edinburgh's Canongate. Five years later, as a member of the Philosophical Society of Edinburgh when it received its royal charter, he automatically became one of the founding members of the Royal Society of Edinburgh, and from 1787 to 1789 he occupied the honorary position of Lord Rector of the University of Glasgow. He died in the northern wing of Panmure House in Edinburgh on 17 July 1790 after a painful illness and was buried in the Canongate Kirkyard. On his death bed, Smith expressed disappointment that he had not achieved more.
Smith's literary executors were two friends from the Scottish academic world: the physicist and chemist Joseph Black, and the pioneering geologist James Hutton. Smith left behind many notes and some unpublished material, but gave instructions to destroy anything that was not fit for publication. He mentioned an early unpublished "History of Astronomy" as probably suitable, and it duly appeared in 1795, along with other material such as "Essays on Philosophical Subjects".
Smith's library went by his will to David Douglas, Lord Reston (son of his cousin Colonel Robert Douglas of Strathendry, Fife), who lived with Smith. It was eventually divided between his two surviving children, Cecilia Margaret (Mrs. Cunningham) and David Anne (Mrs. Bannerman). On the death of her husband, the Rev. W. B. Cunningham of Prestonpans in 1878, Mrs. Cunningham sold some of the books. The remainder passed to her son, Professor Robert Oliver Cunningham of Queen's College, Belfast, who presented a part to the library of Queen's College. After his death the remaining books were sold. On the death of Mrs. Bannerman in 1879 her portion of the library went intact to the New College (of the Free Church), Edinburgh.
Personality and beliefs.
Character.
Not much is known about Smith's personal views beyond what can be deduced from his published articles. His personal papers were destroyed after his death at his request. He never married, and seems to have maintained a close relationship with his mother, with whom he lived after his return from France and who died six years before his own death.
Smith was described by several of his contemporaries and biographers as comically absent-minded, with peculiar habits of speech and gait, and a smile of "inexpressible benignity". He was known to talk to himself, a habit that began during his childhood when he would smile in rapt conversation with invisible companions. He also had occasional spells of imaginary illness, and he is reported to have had books and papers placed in tall stacks in his study. According to one story, Smith took Charles Townshend on a tour of a tanning factory, and while discussing free trade, Smith walked into a huge tanning pit from which he needed help to escape. He is also said to have put bread and butter into a teapot, drunk the concoction, and declared it to be the worst cup of tea he ever had. According to another account, Smith distractedly went out walking in his nightgown and ended up 15 mi outside of town, before nearby church bells brought him back to reality.
James Boswell who was a student of Smith's at Glasgow University, and later knew him at the Literary Club, says that Smith thought that speaking about his ideas in conversation might reduce the sale of his books, and so his conversation was unimpressive. According to Boswell, he once told Sir Joshua Reynolds that 'he made it a rule when in company never to talk of what he understood'.
Smith, who is reported to have been an odd-looking fellow, has been described as someone who "had a large nose, bulging eyes, a protruding lower lip, a nervous twitch, and a speech impediment". Smith is said to have acknowledged his looks at one point, saying, "I am a beau in nothing but my books." Smith rarely sat for portraits, so almost all depictions of him created during his lifetime were drawn from memory. The best-known portraits of Smith are the profile by James Tassie and two etchings by John Kay. The line engravings produced for the covers of 19th century reprints of "The Wealth of Nations" were based largely on Tassie's medallion.
Religious views.
There has been considerable scholarly debate about the nature of Smith's religious views. Smith's father had shown a strong interest in Christianity and belonged to the moderate wing of the Church of Scotland. The fact that Adam Smith received the Snell Exhibition suggests that he may have gone to Oxford with the intention of pursuing a career in the Church of England.
Anglo-American economist Ronald Coase has challenged the view that Smith was a deist, based on the fact that Smith's writings never explicitly invoke God as an explanation of the harmonies of the natural or the human worlds. According to Coase, though Smith does sometimes refer to the "Great Architect of the Universe", later scholars such as Jacob Viner have "very much exaggerated the extent to which Adam Smith was committed to a belief in a personal God", a belief for which Coase finds little evidence in passages such as the one in the "Wealth of Nations" in which Smith writes that the curiosity of mankind about the "great phenomena of nature", such as "the generation, the life, growth and dissolution of plants and animals", has led men to "enquire into their causes", and that "superstition first attempted to satisfy this curiosity, by referring all those wonderful appearances to the immediate agency of the gods. Philosophy afterwards endeavoured to account for them, from more familiar causes, or from such as mankind were better acquainted with than the agency of the gods".
Some other authors argue that Smith's social and economic philosophy is inherently theological and that his entire model of social order is logically dependent on the notion of God's action in nature.
Smith was also a close friend and later the executor of David Hume, who was commonly characterised in his own time as an atheist. The publication in 1777 of Smith's letter to William Strahan, in which he described Hume's courage in the face of death in spite his irreligiosity, attracted considerable controversy.
Published works.
"The Theory of Moral Sentiments".
In 1759, Smith published his first work, "The Theory of Moral Sentiments". He continued making extensive revisions to the book, up until his death. Although "The Wealth of Nations" is widely regarded as Smith's most influential work, it is believed that Smith himself considered "The Theory of Moral Sentiments" to be a superior work.
In the work, Smith critically examines the moral thinking of his time, and suggests that conscience arises from social relationships. His goal in writing the work was to explain the source of mankind's ability to form moral judgements, in spite of man's natural inclinations towards self-interest. Smith proposes a theory of sympathy, in which the act of observing others makes people aware of themselves and the morality of their own behaviour.
Scholars have traditionally perceived a conflict between "The Theory of Moral Sentiments" and "The Wealth of Nations"; the former emphasises sympathy for others, while the latter focuses on the role of self-interest. In recent years, however, some scholars of Smith's work have argued that no contradiction exists. They claim that in "The Theory of Moral Sentiments", Smith develops a theory of psychology in which individuals seek the approval of the "impartial spectator" as a result of a natural desire to have outside observers sympathise with them. Rather than viewing "The Theory of Moral Sentiments" and "The Wealth of Nations" as presenting incompatible views of human nature, some Smith scholars regard the works as emphasising different aspects of human nature that vary depending on the situation. Ekelund and Hebert offer a differing view, observing that self-interest is present in both works and that "in the former, sympathy is the moral faculty that holds self-interest in check, whereas in the latter, competition is the economic faculty that restrains self-interest."
"The Wealth of Nations".
There is a fundamental disagreement between classical and neoclassical economists about the central message of Smith's most influential work: "An Inquiry into the Nature and Causes of the Wealth of Nations". Neoclassical economists emphasise Smith's invisible hand, a concept mentioned in the middle of his work – Book IV, Chapter II – and classical economists believe that Smith stated his programme for promoting the "wealth of nations" in the first sentences.
Smith used the term "the invisible hand" in "History of Astronomy" referring to "the invisible hand of Jupiter" and twice – each time with a different meaning – the term "an invisible hand": in "The Theory of Moral Sentiments" (1759) and in "The Wealth of Nations" (1776). This last statement about "an invisible hand" has been interpreted as "the invisible hand" in numerous ways. It is therefore important to read the original:
As every individual, therefore, endeavours as much as he can both to employ his capital in the support of domestic industry, and so to direct that industry that its produce may be of the greatest value; every individual necessarily labours to render the annual revenue of the society as great as he can. He generally, indeed, neither intends to promote the public interest, nor knows how much he is promoting it. By preferring the support of domestic to that of foreign industry, he intends only his own security; and by directing that industry in such a manner as its produce may be of the greatest value, he intends only his own gain, and he is in this, as in many other cases, led by an invisible hand to promote an end which was no part of his intention. Nor is it always the worse for the society that it was no part of it. By pursuing his own interest he frequently promotes that of the society more effectually than when he really intends to promote it. I have never known much good done by those who affected to trade for the public good. It is an affectation, indeed, not very common among merchants, and very few words need be employed in dissuading them from it.
Those who regard that statement as Smith's central message also quote frequently Smith's dictum:
It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner, but from their regard to their own interest. We address ourselves, not to their humanity but to their self-love, and never talk to them of our own necessities but of their advantages.
Smith's statement about the benefits of "an invisible hand" is certainly meant to answer Mandeville's contention that "Private Vices ... may be turned into Public Benefits". It shows Smith's belief that when an individual pursues his self-interest, he indirectly promotes the good of society. Self-interested competition in the free market, he argued, would tend to benefit society as a whole by keeping prices low, while still building in an incentive for a wide variety of goods and services. Nevertheless, he was wary of businessmen and warned of their "conspiracy against the public or in some other contrivance to raise prices". Again and again, Smith warned of the collusive nature of business interests, which may form cabals or monopolies, fixing the highest price "which can be squeezed out of the buyers". Smith also warned that a business-dominated political system would allow a conspiracy of businesses and industry against consumers, with the former scheming to influence politics and legislation. Smith states that the interest of manufacturers and merchants "...in any particular branch of trade or manufactures, is always in some respects different from, and even opposite to, that of the public...The proposal of any new law or regulation of commerce which comes from this order, ought always to be listened to with great precaution, and ought never be adopted till after having been long and carefully examined, not only with the most scrupulous, but with the most suspicious attention."
The neoclassical interest in Smith's statement about "an invisible hand" originates in the possibility to see it as a precursor of neoclassical economics and its General Equilibrium concept. Samuelson's "Economics" refers 6 times to Smith's "invisible hand". To emphasise this relation, Samuelson quotes Smith's "invisible hand" statement putting "general interest" where Smith wrote "public interest". Samuelson concluded: "Smith was unable to prove the essence of his invisible-hand doctrine. Indeed, until the 1940s no one knew how to prove, even to state properly, the kernel of truth in this proposition about perfectly competitive market."
Very differently, classical economists see in Smith's first sentences his programme to promote "The Wealth of Nations". Taking up the physiocratical concept of the economy as a circular process means that to have growth the inputs of period2 must excel the inputs of period1. Therefore the outputs of period1 not used or usable as input of period2 are regarded as unproductive labour as they do not contribute to growth. This is what Smith had learned in France with Quesnay. To this French insight that unproductive labour should be pushed back to use more labour productively, Smith added his own proposal, that productive labour should be made even more productive by deepening the division of labour. Deepening the division of labour means under competition lower prices and thereby extended markets. Extended markets and increased production lead to a new step of reorganising production and inventing new ways of producing which again lower prices, etc., etc.. Smith's central message is therefore that under dynamic competition a growth machine secures "The Wealth of Nations". It predicted England's evolution as the workshop of the World, underselling all its competitors. The opening sentences of the "Wealth of Nations" summarise this policy:
The annual labour of every nation is the fund which originally supplies it with all the necessaries and conveniences of life which it annually consumes ... . [T]his produce ... bears a greater or smaller proportion to the number of those who are to consume it ... .[B]ut this proportion must in every nation be regulated by two different circumstances;
Criticism and dissent.
Alfred Marshall criticised Smith's definition of economy on several points. He argued that man should be equally important as money, services are as important as goods, and that there must be an emphasis on human welfare, instead of just wealth.
Nobel Prize-winning economist Joseph E. Stiglitz says, on the topic of one of Smith's better known ideas: "the reason that the invisible hand often seems invisible is that it is often not there."
Other works.
Shortly before his death, Smith had nearly all his manuscripts destroyed. In his last years, he seemed to have been planning two major treatises, one on the theory and history of law and one on the sciences and arts. The posthumously published "Essays on Philosophical Subjects", a history of astronomy down to Smith's own era, plus some thoughts on ancient physics and metaphysics, probably contain parts of what would have been the latter treatise. "Lectures on Jurisprudence" were notes taken from Smith's early lectures, plus an early draft of "The Wealth of Nations", published as part of the 1976 Glasgow Edition of the works and correspondence of Smith. Other works, including some published posthumously, include "Lectures on Justice, Police, Revenue, and Arms" (1763) (first published in 1896); and "Essays on Philosophical Subjects" (1795).
Legacy.
In economics and moral philosophy.
"The Wealth of Nations" was a precursor to the modern academic discipline of economics. In this and other works, Smith expounded how rational self-interest and competition can lead to economic prosperity. Smith was controversial in his own day and his general approach and writing style were often satirised by Tory writers in the moralising tradition of Hogarth and Swift, as a discussion at the University of Winchester suggests. In 2005, "The Wealth of Nations" was named among the 100 Best Scottish Books of all time. Former UK Prime Minister Margaret Thatcher, it is said, used to carry a copy of the book in her handbag.
In light of the arguments put forward by Smith and other economic theorists in Britain, academic belief in mercantalism began to decline in England in the late 18th century. During the Industrial Revolution, Britain embraced free trade and Smith's laissez-faire economics, and via the British Empire, used its power to spread a broadly liberal economic model around the world, characterised by open markets, and relatively barrier free domestic and international trade.
George Stigler attributes to Smith "the most important substantive proposition in all of economics". It is that, under competition, owners of resources (for example labour, land, and capital) will use them most profitably, resulting in an equal rate of return in equilibrium for all uses, adjusted for apparent differences arising from such factors as training, trust, hardship, and unemployment.
Paul Samuelson finds in Smith's pluralist use of supply and demand as applied to wages, rents, profit a valid and valuable anticipation of the general equilibrium modelling of Walras a century later. Smith's allowance for wage increases in the short and intermediate term from capital accumulation and invention contrasted with Malthus, Ricardo, and Karl Marx in their propounding a rigid subsistence–wage theory of labour supply.
On the other hand, Joseph Schumpeter dismissed Smith's contributions as unoriginal, saying "His very limitation made for success. Had he been more brilliant, he would not have been taken so seriously. Had he dug more deeply, had he unearthed more recondite truth, had he used more difficult and ingenious methods, he would not have been understood. But he had no such ambitions; in fact he disliked whatever went beyond plain common sense. He never moved above the heads of even the dullest readers. He led them on gently, encouraging them by trivialities and homely observations, making them feel comfortable all along."
Classical economists presented competing theories of those of Smith, termed the "labour theory of value". Later Marxian economics descending from classical economics also use Smith's labour theories, in part. The first volume of Karl Marx's major work, "Capital", was published in German in 1867. In it, Marx focused on the labour theory of value and what he considered to be the exploitation of labour by capital. The labour theory of value held that the value of a thing was determined by the labour that went into its production. This contrasts with the modern contention of neoclassical economics, that the value of a thing is determined by what one is willing to give up to obtain the thing.
The body of theory later termed "neoclassical economics" or "marginalism" formed from about 1870 to 1910. The term "economics" was popularised by such neoclassical economists as Alfred Marshall as a concise synonym for "economic science" and a substitute for the earlier, broader term "political economy" used by Smith. This corresponded to the influence on the subject of mathematical methods used in the natural sciences. Neoclassical economics systematised supply and demand as joint determinants of price and quantity in market equilibrium, affecting both the allocation of output and the distribution of income. It dispensed with the labour theory of value of which Smith was most famously identified with in classical economics, in favour of a marginal utility theory of value on the demand side and a more general theory of costs on the supply side.
The bicentennial anniversary of the publication of "The Wealth of Nations" was celebrated in 1976, resulting in increased interest for "The Theory of Moral Sentiments" and his other works throughout academia. After 1976, Smith was more likely to be represented as the author of both "The Wealth of Nations" and "The Theory of Moral Sentiments", and thereby as the founder of a moral philosophy and the science of economics. His "homo economicus" or "economic man" was also more often represented as a moral person. Additionally, economists David Levy and Sandra Peart in "The Secret History of the Dismal Science" point to his opposition to hierarchy and beliefs in inequality, including racial inequality, and provide additional support for those who point to Smith's opposition to slavery, colonialism, and empire. They show the caricatures of Smith drawn by the opponents of views on hierarchy and inequality in this online article. Emphasized also are Smith's statements of the need for high wages for the poor, and the efforts to keep wages low. In The "Vanity of the Philosopher": From Equality to Hierarchy in Postclassical Economics Peart and Levy also cite Smith's view that a common street porter was not intellectually inferior to a philosopher, and point to the need for greater appreciation of the public views in discussions of science and other subjects now considered to be technical. They also cite Smith's opposition to the often expressed view that science is superior to common sense.
Smith also explained the relationship between growth of private property and civil government:
"Men may live together in society with some tolerable degree of security, though there is no civil magistrate to protect them from the injustice of those passions. But avarice and ambition in the rich, in the poor the hatred of labour and the love of present ease and enjoyment, are the passions which prompt to invade property, passions much more steady in their operation, and much more universal in their influence. Wherever there is great property there is great inequality. For one very rich man there must be at least five hundred poor, and the affluence of the few supposes the indigence of the many. The affluence of the rich excites the indignation of the poor, who are often both driven by want, and prompted by envy, to invade his possessions. It is only under the shelter of the civil magistrate that the owner of that valuable property, which is acquired by the labour of many years, or perhaps of many successive generations, can sleep a single night in security. He is at all times surrounded by unknown enemies, whom, though he never provoked, he can never appease, and from whose injustice he can be protected only by the powerful arm of the civil magistrate continually held up to chastise it. The acquisition of valuable and extensive property, therefore, necessarily requires the establishment of civil government. Where there is no property, or at least none that exceeds the value of two or three days' labour, civil government is not so necessary. Civil government supposes a certain subordination. But as the necessity of civil government gradually grows up with the acquisition of valuable property, so the principal causes which naturally introduce subordination gradually grow up with the growth of that valuable property. (...) Men of inferior wealth combine to defend those of superior wealth in the possession of their property, in order that men of superior wealth may combine to defend them in the possession of theirs. All the inferior shepherds and herdsmen feel that the security of their own herds and flocks depends upon the security of those of the great shepherd or herdsman; that the maintenance of their lesser authority depends upon that of his greater authority, and that upon their subordination to him depends his power of keeping their inferiors in subordination to them. They constitute a sort of little nobility, who feel themselves interested to defend the property and to support the authority of their own little sovereign in order that he may be able to defend their property and to support their authority. Civil government, so far as it is instituted for the security of property, is in reality instituted for the defence of the rich against the poor, or of those who have some property against those who have none at all." (Source: The Wealth of Nations, Book 5, Chapter 1, Part 2)
In British Imperial debates.
Smith's in turn would help shape British imperial debates from the mid-nineteenth century onward. "The Wealth of Nations" would become an ambiguous text regarding the imperial question. In his chapter on colonies, Smith pondered how to solve the crisis developing across the Atlantic among the empire's thirteen American colonies. He offered two different proposals for easing tensions. The first proposal called for giving the colonies their independence and, by thus parting on a friendly basis, Britain would be able to develop and maintain a free-trade relationship with them, and possibly even an informal military alliance. Smith's second proposal called for a theoretical imperial federation that would bring the colonies and the metropole closer together through an imperial parliamentary system and imperial free trade. 
Smith's most prominent disciple in nineteenth-century England, peace advocate Richard Cobden, preferred the first proposal. Cobden would lead the Anti-Corn Law League in overturning the Corn Laws in 1846, shifting England to a policy of free trade and empire "on the cheap" for decades to come. This hands-off approach toward the British Empire would become known as Cobdenism or the Manchester School. By the turn of the century, however, advocates of Smith's second proposal such as Joseph Shield Nicholson would become ever more vocal in opposing Cobdenism, calling instead for imperial federation. As Marc-William Palen notes: "On the one hand, Adam Smith’s late nineteenth and early twentieth-century Cobdenite adherents used his theories to argue for gradual imperial devolution and empire ‘on the cheap’. On the other, various proponents of imperial federation throughout the British World sought to use Smith’s theories to overturn the predominant Cobdenite hands-off imperial approach and instead, with a firm grip, bring the empire closer than ever before." Smith's ideas thus played an important part in subsequent debates over the British Empire.
Portraits, monuments, and banknotes.
Smith has been commemorated in the UK on banknotes printed by two different banks; his portrait has appeared since 1981 on the £50 notes issued by the Clydesdale Bank in Scotland, and in March 2007 Smith's image also appeared on the new series of £20 notes issued by the Bank of England, making him the first Scotsman to feature on an English banknote.
A large-scale memorial of Smith by Alexander Stoddart was unveiled on 4 July 2008 in Edinburgh. It is a 10 ft-tall bronze sculpture and it stands above the Royal Mile outside St Giles' Cathedral in Parliament Square, near the Mercat cross. 20th-century sculptor Jim Sanborn (best known for the "Kryptos" sculpture at the United States Central Intelligence Agency) has created multiple pieces which feature Smith's work. At Central Connecticut State University is "Circulating Capital", a tall cylinder which features an extract from "The Wealth of Nations" on the lower half, and on the upper half, some of the same text but represented in binary code. At the University of North Carolina at Charlotte, outside the Belk College of Business Administration, is "Adam Smith's Spinning Top". Another Smith sculpture is at Cleveland State University. He also appears as the narrator in the 2013 play "The Low Road", centred on a proponent on laissez-faire economics in the late eighteenth century but dealing obliquely with the financial crisis of 2007–2008 and the recession which followed – in the premiere production, he was portrayed by Bill Paterson.
Residence.
Adam Smith resided at Panmure house from 1778–90. This residence has now been purchased by the Edinburgh Business School at Heriot Watt University and fundraising has begun to restore it. Part of the Northern end of the original building appears to have been demolished in the 19th century to make way for an iron foundry.
As a symbol of free market economics.
Smith has been celebrated by advocates of free market policies as the founder of free market economics, a view reflected in the naming of bodies such as the Adam Smith Institute in London, the Adam Smith Society and the Australian Adam Smith Club, and in terms such as the Adam Smith necktie.
Alan Greenspan argues that, while Smith did not coin the term "laissez-faire", "it was left to Adam Smith to identify the more-general set of principles that brought conceptual clarity to the seeming chaos of market transactions". Greenspan continues that "The Wealth of Nations" was "one of the great achievements in human intellectual history". P. J. O'Rourke describes Smith as the "founder of free market economics".
However, other writers have argued that Smith's support for "laissez-faire" (which in French means leave alone) has been overstated. Herbert Stein wrote that the people who "wear an Adam Smith necktie" do it to "make a statement of their devotion to the idea of free markets and limited government", and that this misrepresents Smith's ideas. Stein writes that Smith "was not pure or doctrinaire about this idea. He viewed government intervention in the market with great skepticism...yet he was prepared to accept or propose qualifications to that policy in the specific cases where he judged that their net effect would be beneficial and would not undermine the basically free character of the system. He did not wear the Adam Smith necktie." In Stein's reading, "The Wealth of Nations" could justify the Food and Drug Administration, the Consumer Product Safety Commission, mandatory employer health benefits, environmentalism, and "discriminatory taxation to deter improper or luxurious behavior".
Similarly, Vivienne Brown stated in "The Economic Journal" that in the 20th century United States, Reaganomics supporters, the "Wall Street Journal", and other similar sources have spread among the general public a partial and misleading vision of Smith, portraying him as an "extreme dogmatic defender of "laissez-faire" capitalism and supply-side economics". In fact, "The Wealth of Nations" includes the following statement on the payment of taxes:
"The subjects of every state ought to contribute towards the support of the government, as nearly as possible, in proportion to their respective abilities; that is, in proportion to the revenue which they respectively enjoy under the protection of the state."
Some commentators have argued that Smith's works show support for a progressive, not flat, income tax and that he specifically named taxes that he thought should be required by the state, among them luxury goods taxes and tax on rent.
Additionally, Smith outlined the proper expenses of the government in "The Wealth of Nations, Book V, Ch. I". Included in his requirements of a government is to enforce contracts and provide justice system, grant patents and copy rights, provide public goods such as infrastructure, provide national defence and regulate banking. It was the role of the government to provide goods "of such a nature that the profit could never repay the expense to any individual" such as roads, bridges, canals, and harbours. He also encouraged invention and new ideas through his patent enforcement and support of infant industry monopolies. He supported public education and religious institutions as providing general benefit to the society. Finally he outlined how the government should support the dignity of the monarch or chief magistrate, such that they are equal or above the public in fashion. He even states that monarchs should be provided for in a greater fashion than magistrates of a republic because "we naturally expect more splendor in the court of a king than in the mansion-house of a doge". In addition, he was in favour of retaliatory tariffs and believed that they would eventually bring down the price of goods. He even stated in Wealth of Nations:
"The recovery of a great foreign market will generally more than compensate the transitory inconvenience of paying dearer during a short time for some sorts of goods."
Economic historians such as Jacob Viner regard Smith as a strong advocate of free markets and limited government (what Smith called "natural liberty") but not as a dogmatic supporter of "laissez-faire".
Economist Daniel Klein believes using the term "free market economics" or "free market economist" to identify the ideas of Smith is too general and slightly misleading. Klein offers six characteristics central to the identity of Smith's economic thought and argues that a new name is needed to give a more accurate depiction of the "Smithian" identity. Economist David Ricardo set straight some of the misunderstandings about Smith's thoughts on free market. Most people still fall victim to the thinking that Smith was a free market economist without exception, though he was not. Ricardo pointed out that Smith was in support of helping infant industries. Smith believed that the government should subsidise newly formed industry, but he did fear that when the infant industry grew into adulthood it would be unwilling to surrender the government help. Smith also supported tariffs on imported goods to counteract an internal tax on the same good. Smith also fell to pressure in supporting some tariffs in support for national defence. Some have also claimed, Emma Rothschild among them, that Smith supported a minimum wage.
Though, Smith had written in his book "The Wealth of Nations":
"The price of labour, it must be observed, cannot be ascertained very accurately anywhere, different prices being often paid at the same place and for the same sort of labour, not only according to the different abilities of the workmen, but according to the easiness or hardness of the masters. Where wages are not regulated by law, all that we can pretend to determine is what are the most usual; and experience seems to show that law can never regulate them properly, though it has often pretended to do so." (Source: The Wealth of Nations, Book 1, Chapter 8)
Smith also noted the inequality of bargaining power:
A landlord, a farmer, a master manufacturer, a merchant, though they did not employ a single workman, could generally live a year or two upon the stocks which they have already acquired. Many workmen could not subsist a week, few could subsist a month, and scarce any a year without employment. In the long run the workman may be as necessary to his master as his master is to him; but the necessity is not so immediate.

</doc>
<doc id="1822" url="http://en.wikipedia.org/wiki?curid=1822" title="Antoine Lavoisier">
Antoine Lavoisier

Antoine-Laurent de Lavoisier (also Antoine Lavoisier after the French Revolution; 26 August 1743 – 8 May 1794; ]) was a French nobleman and chemist central to the 18th-century chemical revolution and a large influence on both the history of chemistry and the history of biology. He is widely considered in popular literature as the "father of modern chemistry". This label, however, is more a product of Lavoisier's eminent skill as a self-promoter and underplays his dependence on the instruments, experiments, and ideas of other chemists.
It is generally accepted that Lavoisier's great accomplishments in chemistry largely stem from his changing the science from a qualitative to a quantitative one. Lavoisier is most noted for his discovery of the role oxygen plays in combustion. He recognized and named oxygen (1778) and hydrogen (1783) and opposed the phlogiston theory. Lavoisier helped construct the metric system, wrote the first extensive list of elements, and helped to reform chemical nomenclature. He predicted the existence of silicon (1787) and was also the first to establish that sulfur was an element (1777) rather than a compound. He discovered that, although matter may change its form or shape, its mass always remains the same.
Lavoisier was a powerful member of a number of aristocratic councils, and an administrator of the "Ferme Générale". The "Ferme générale" was one of the most hated components of the "Ancien Régime" because of the profits it took at the expense of the state, the secrecy of the terms of its contracts, and the violence of its armed agents. All of these political and economic activities enabled him to fund his scientific research. At the height of the French Revolution, he was accused by Jean-Paul Marat of selling adulterated tobacco and of other crimes, and was eventually guillotined a year after Marat's death.
Biography.
Early life and education.
Antoine-Laurent Lavoisier was born to a wealthy family in Paris on 26 August 1743. The son of an attorney at the Parliament of Paris, he inherited a large fortune at the age of five with the passing of his mother. Lavoisier began his schooling at the Collège des Quatre-Nations (known as the Collège Mazarin) in Paris in 1754 at the age of 11. In his last two years (1760–1761) at the school, his scientific interests were aroused, and he studied chemistry, botany, astronomy, and mathematics. In the philosophy class he came under the tutelage of Abbé Nicolas Louis de Lacaille, a distinguished mathematician and observational astronomer who imbued the young Lavoisier with an interest in meteorological observation, an enthusiasm which never left him. Lavoisier entered the school of law, where he received a bachelor's degree in 1763 and a licentiate in 1764. Lavoisier received a law degree and was admitted to the bar, but never practiced as a lawyer. However, he continued his scientific education in his spare time.
Early scientific work.
Lavoisier's education was filled with the ideals of the French Enlightenment of the time, and he was fascinated by Pierre Macquer's dictionary of chemistry. He attended lectures in the natural sciences. Lavoisier's devotion and passion for chemistry were largely influenced by Étienne Condillac, a prominent French scholar of the 18th century. His first chemical publication appeared in 1764. From 1763 to 1767, he studied geology under Jean-Étienne Guettard. In collaboration with Guettard, Lavoisier worked on a geological survey of Alsace-Lorraine in June 1767. In 1764 he read his first paper to the French Academy of Sciences, France's most elite scientific society, on the chemical and physical properties of gypsum (hydrated calcium sulfate), and in 1766 he was awarded a gold medal by the King for an essay on the problems of urban street lighting. In 1768 Lavoisier received a provisional appointment to the Academy of Sciences. In 1769, he worked on the first geological map of France.
Ferme générale and marriage.
At the age of 26, around the time he was elected to the Academy of Sciences, Lavoisier bought a share in the "Ferme générale", a tax farming financial company which advanced the estimated tax revenue to the royal government in return for the right to collect the taxes. Lavoisier attempted to introduce reforms in the French monetary and taxation system to help the peasants. While in government work, he helped develop the metric system to secure uniformity of weights and measures throughout France. Lavoisier consolidated his social and economic position when, in 1771 at age 28, he married Marie-Anne Pierrette Paulze, the 13-year-old daughter of a senior member of the "Ferme générale". She was to play an important part in Lavoisier's scientific career—notably, she translated English documents for him, including Richard Kirwan's "Essay on Phlogiston" and Joseph Priestley's research. In addition, she assisted him in the laboratory and created many sketches and carved engravings of the laboratory instruments used by Lavoisier and his colleagues for their scientific works.
Madame Lavoisier edited and published Antoine's memoirs (whether any English translations of those memoirs have survived is unknown as of today) and hosted parties at which eminent scientists discussed ideas and problems related to chemistry. For 3 years following his entry into the "Ferme générale", Lavoisier's scientific activity diminished somewhat, for much of his time was taken up with official "Ferme générale" business. He did, however, present one important memoir to the Academy of Sciences during this period, on the supposed conversion of water into earth by evaporation. By a very precise quantitative experiment Lavoisier showed that the "earthy" sediment produced after long-continued reflux heating of water in a glass vessel was not due to a conversion of the water into earth but rather to the gradual disintegration of the inside of the glass vessel produced by the boiling water.
Oxygen theory of combustion.
During late 1772 Lavoisier turned his attention to the phenomenon of combustion, the topic on which he was to make his most significant contribution to science. He reported the results of his first experiments on combustion in a note to the Academy on 20 October, in which he reported that when phosphorus burned, it combined with a large quantity of air to produce acid spirit of phosphorus, and that the phosphorus increased in weight on burning. In a second sealed note deposited with the Academy a few weeks later (1 November) Lavoisier extended his observations and conclusions to the burning of sulfur and went on to add that "what is observed in the combustion of sulfur and phosphorus may well take place in the case of all substances that gain in weight by combustion and calcination: and I am persuaded that the increase in weight of metallic calces is due to the same cause."
Joseph Black's "fixed air".
During 1773 Lavoisier determined to review thoroughly the literature on air, particularly "fixed air," and to repeat many of the experiments of other workers in the field. He published an account of this review in 1774 in a book entitled "Opuscules physiques et chimiques" (Physical and Chemical Essays). In the course of this review he made his first full study of the work of Joseph Black, the Scottish chemist who had carried out a series of classic quantitative experiments on the mild and caustic alkalies. Black had shown that the difference between a mild alkali, for example, chalk (CaCO3), and the caustic form, for example, quicklime (CaO), lay in the fact that the former contained "fixed air," not common air fixed in the chalk, but a distinct chemical species, now understood to be carbon dioxide (CO2), which was a constituent of the atmosphere. Lavoisier recognized that Black's fixed air was identical with the air evolved when metal calces were reduced with the charcoal and even suggested that the air which combined with metals on calcination and increased the weight might be Black's fixed air, that is, CO2.
Joseph Priestley.
In the spring of 1774 Lavoisier carried out experiments on the calcination of tin and lead in sealed vessels which conclusively confirmed that the increase in weight of metals on calcination was due to combination with air. But the question remained about whether it was combination with common atmospheric air or with only a part of atmospheric air. In October the English chemist Joseph Priestley visited Paris, where he met Lavoisier and told him of the air which he had produced by heating the red calx of mercury with a burning glass and which had supported combustion with extreme vigor. Priestley at this time was unsure of the nature of this gas, but he felt that it was an especially pure form of common air. Lavoisier carried out his own researches on this peculiar substance. The result was his famous memoir "On the Nature of the Principle Which Combines with Metals during Their Calcination and Increases Their Weight", read to the Academy on 26 April 1775 (commonly referred to as the Easter Memoir). In the original memoir Lavoisier showed that the mercury calx was a true metallic calx in that it could be reduced with charcoal, giving off Black's fixed air in the process. When reduced without charcoal, it gave off an air which supported respiration and combustion in an enhanced way. He concluded that this was just a pure form of common air, and that it was the air itself "undivided, without alteration, without decomposition" which combined with metals on calcination.
After returning from Paris, Priestley took up once again his investigation of the air from mercury calx. His results now showed that this air was not just an especially pure form of common air but was "five or six times better than common air, for the purpose of respiration, inflammation, and ... every other use of common air." He called the air dephlogisticated air, as he thought it was common air deprived of its phlogiston. Since it was therefore in a state to absorb a much greater quantity of phlogiston given off by burning bodies and respiring animals, the greatly enhanced combustion of substances and the greater ease of breathing in this air were explained.
Easter memoir.
The "official" version of Lavoisier's Easter Memoir appeared in 1778. In the intervening period Lavoisier had ample time to repeat some of Priestley's latest experiments and perform some new ones of his own. In addition to studying Priestley's dephlogisticated air, he studied more thoroughly the residual air after metals had been calcined. He showed that this residual air supported neither combustion nor respiration and that approximately five volumes of this air added to one volume of the dephlogisticated air gave common atmospheric air. Common air was then a mixture of two distinct chemical species with quite different properties. Thus when the revised version of the Easter Memoir was published in 1778, Lavoisier no longer stated that the principle which combined with metals on calcination was just common air but "nothing else than the healthiest and purest part of the air" or the "eminently respirable part of the air." In the following year Lavoisier coined the name oxygen for this constituent of the air, from the Greek words meaning "acid former." and "Considérations générales sur la nature des acides" ("General Considerations on the Nature of Acids," 1778), He was struck by the fact that the combustion products of such nonmetals as sulfur, phosphorus, charcoal, and nitrogen were acidic. He held that all acids contained oxygen and that oxygen was therefore the acidifying principle.
Dismantling phlogiston theory.
Lavoisier's chemical research between 1772 and 1778 was largely concerned with developing his own new theory of combustion. In 1783 he read to the academy his famous paper entitled "Réflexions sur le phlogistique" (Reflections on Phlogiston), a full-scale attack on the current phlogiston theory of combustion. That year Lavoisier also began a series of experiments on the composition of water which were to prove an important capstone to his combustion theory and win many converts to it. Many investigators had been experimenting with the combination of Henry Cavendish's inflammable air, which Lavoisier termed hydrogen (Greek for "water-former"), with dephlogisticated air (oxygen) by electrically sparking mixtures of the gases. All of the researchers noted the production of water, but all interpreted the reaction in varying ways within the framework of the phlogiston theory. In cooperation with mathematician Pierre Simon de Laplace, Lavoisier synthesized water by burning jets of hydrogen and oxygen in a bell jar over mercury. The quantitative results were good enough to support the contention that water was not an element, as had been thought for over 2,000 years, but a compound of two gases, hydrogen and oxygen.
Gunpowder Commission.
Lavoisier's researches on combustion were carried out in the midst of a very busy schedule of public and private duties, especially in connection with the "Ferme générale". There were also innumerable reports for and committees of the Academy of Sciences to investigate specific problems on order of the royal government. Lavoisier, whose organizing skills were outstanding, frequently landed the task of writing up such official reports. In 1775 he was made one of four commissioners of gunpowder appointed to replace a private company, similar to the Ferme générale, which had proved unsatisfactory in supplying France with its munitions requirements. As a result of his efforts, both the quantity and quality of French gunpowder greatly improved, and it became a source of revenue for the government. His appointment to the Gunpowder Commission brought one great benefit to Lavoisier's scientific career as well. As a commissioner, he enjoyed both a house and a laboratory in the Royal Arsenal. Here he lived and worked between 1775 and 1792.
Pioneer of stoichiometry.
Lavoisier's researches included some of the first truly quantitative chemical experiments. He carefully weighed the reactants and products of a chemical reaction in a sealed glass vessel, which was a crucial step in the advancement of chemistry. In 1774, he showed that, although matter can change its state in a chemical reaction, the total mass of matter is the same at the end as at the beginning of every chemical change. Thus, for instance, if a piece of wood is burned to ashes, the total mass remains unchanged. Lavoisier's experiments supported the law of conservation of mass. In France it is taught as Lavoisier's Law and is paraphrased from a statement in his "Traité Élémentaire de Chimie" to "Rien ne se perd, rien ne se crée, tout se transforme." ("Nothing is lost, nothing is created, everything is transformed."). Mikhail Lomonosov (1711–1765) had previously expressed similar ideas in 1748 and proved them in experiments; others whose ideas pre-date the work of Lavoisier include Jean Rey (1583–1645), Joseph Black (1728–1799), and Henry Cavendish (1731–1810). (See )
Chemical nomenclature.
Lavoisier, together with L. B. Guyton de Morveau, Claude-Louis Berthollet, and Antoine François de Fourcroy, submitted a new program for the reforms of chemical nomenclature to the Academy in 1787, for there was virtually no rational system of chemical nomenclature at this time. The new system was tied inextricably to Lavoisier's new oxygen theory of chemistry. The Classical elements of earth, air, fire, and water were discarded, and instead some 55 substances which could not be decomposed into simpler substances by any known chemical means were provisionally listed as elements. The elements included light; caloric (matter of heat); the principles of oxygen, hydrogen, and azote (nitrogen); carbon; sulfur; phosphorus; the yet unknown "radicals" of muriatic acid (hydrochloric acid), boracic acid, and "fluoric" acid; 17 metals; 5 earths (mainly oxides of yet unknown metals such as magnesia, barite, and strontia); three alkalies (potash, soda, and ammonia); and the "radicals" of 19 organic acids. The acids, regarded in the new system as compounds of various elements with oxygen, were given names which indicated the element involved together with the degree of oxygenation of that element, for example sulfuric and sulfurous acids, phosphoric and phosphorus acids, nitric and nitrous acids, the "ic" termination indicating acids with a higher proportion of oxygen than those with the "ous" ending. Similarly, salts of the "ic" acids were given the terminal letters "ate," as in copper sulfate, whereas the salts of the "ous" acids terminated with the suffix "ite," as in copper sulfite. The total effect of the new nomenclature can be gauged by comparing the new name "copper sulfate" with the old term "vitriol of Venus." Lavoisier described this system of nomenclature in "Méthode de nomenclature chimique" ("Method of Chemical Nomenclature", 1787).
Elementary Treatise of Chemistry.
Lavoisier employed the new nomenclature in his "Traité Élémentaire de Chimie" ("Elementary Treatise on Chemistry"), published in 1789. This work represents the synthesis of Lavoisier's contribution to chemistry and can be considered the first modern textbook on the subject. The core of the work was the oxygen theory, and the work became a most effective vehicle for the transmission of the new doctrines. It presented a unified view of new theories of chemistry, contained a clear statement of the law of conservation of mass, and denied the existence of phlogiston. This text clarified the concept of an element as a substance that could not be broken down by any known method of chemical analysis, and presented Lavoisier's theory of the formation of chemical compounds from elements. It remains a classic in the history of science. While many leading chemists of the time refused to accept Lavoisier's new ideas, demand for "Traité élémentaire" as a textbook in Edinburgh was sufficient to merit translation into English within about a year of its French publication. In any event, the "Traité élémentaire" was sufficiently sound to convince the next generation.
Physiological work.
The relationship between combustion and respiration had long been recognized from the essential role which air played in both processes. Lavoisier was almost obliged, therefore, to extend his new theory of combustion to include the area of respiration physiology. His first memoirs on this topic were read to the Academy of Sciences in 1777, but his most significant contribution to this field was made in the winter of 1782/1783 in association with Laplace. The result of this work was published in a famous memoir, "On Heat." Lavoisier and Laplace designed an ice calorimeter apparatus for measuring the amount of heat given off during combustion or respiration. The outer shell of the calorimeter was packed with snow, which melted to maintain a constant temperature of 0 °C around an inner shell filled with ice. By measuring the quantity of carbon dioxide and heat produced by confining a live guinea pig in this apparatus, and by comparing the amount of heat produced when sufficient carbon was burned in the ice calorimeter to produce the same amount of carbon dioxide as that which the guinea pig exhaled, they concluded that respiration was in fact a slow combustion process. Lavoisier stated, "la respiration est donc une combustion," that is, respiratory gas exchange is a combustion, like that of a candle burning.
This continuous slow combustion, which they supposed took place in the lungs, enabled the living animal to maintain its body temperature above that of its surroundings, thus accounting for the puzzling phenomenon of animal heat. Lavoisier continued these respiration experiments in 1789–1790 in cooperation with Armand Seguin. They designed an ambitious set of experiments to study the whole process of body metabolism and respiration using Seguin as a human guinea pig in the experiments. Their work was only partially completed and published because of the disruption of the Revolution; but Lavoisier's pioneering work in this field served to inspire similar research on physiological processes for generations to come.
Final days and execution.
As the French Revolution gained momentum from 1789 on, Lavoisier's world inexorably collapsed around him. Attacks mounted on the deeply unpopular "Ferme Générale", and it was eventually suppressed in 1791. In 1792 Lavoisier was forced to resign from his post on the Gunpowder Commission and to move from his house and laboratory at the Royal Arsenal. On 8 August 1793, all the learned societies, including the Academy of Sciences, were suppressed.
It is difficult to assess Lavoisier's own attitude to the political turmoil. Like so many intellectual liberals, he felt that the Ancien Régime could be reformed from the inside if only reason and moderation prevailed. Characteristically, one of his last major works was a proposal to the National Convention for the reform of French education. He tried to remain aloof from the political cockpit, no doubt fearful and uncomprehending of the violence he saw therein. However, on 24 November 1793, the arrest of all the former tax gatherers was ordered. He was branded a traitor by the Convention under Maximilien de Robespierre during the Reign of Terror, in 1794. He had also intervened on behalf of a number of foreign-born scientists including mathematician Joseph Louis Lagrange, which helped to exempt them from a mandate stripping all foreigners of possessions and freedom. Lavoisier was tried, convicted, and guillotined on 8 May 1794 in Paris, at the age of 50, along with his 27 co-defendants.
According to a (probably apocryphal) story, the appeal to spare his life so that he could continue his experiments was cut short by the judge: "La République n'a pas besoin de savants ni de chimistes ; le cours de la justice ne peut être suspendu." ("The Republic needs neither scientists nor chemists; the course of justice cannot be delayed.") Lavoisier was convicted with summary justice of having plundered the people and the treasury of France, of having adulterated the nation's tobacco with water, and of having supplied the enemies of France with huge sums of money from the national treasury.
Lavoisier's importance to science was expressed by Joseph Louis Lagrange who lamented the beheading by saying: "Il ne leur a fallu qu’un moment pour faire tomber cette tête, et cent années peut-être ne suffiront pas pour en reproduire une semblable." ("It took them only an instant to cut off this head, and one hundred years might not suffice to reproduce its like.")
Post-mortem.
A year and a half after his death, Lavoisier was exonerated by the French government. When his private belongings were delivered to his widow, a brief note was included, reading "To the widow of Lavoisier, who was falsely convicted".
About a century after his death, a statue of Lavoisier was erected in Paris. It was later discovered that the sculptor had not actually copied Lavoisier's head for the statue, but used a spare head of the Marquis de Condorcet, the Secretary of the Academy of Sciences during Lavoisier's last years. Lack of money prevented alterations from being made. The statue was melted down during the Second World War and has not since been replaced. However, one of the main "lycées" (high schools) in Paris and a street in the 8th arrondissement are named after Lavoisier, and statues of him are found on the Hôtel de Ville and on the façade of the "Cour Napoléon" of the Louvre. His name is one of the 72 names of eminent French scientists, engineers and mathematicians inscribed on the Eiffel Tower as well as on buildings around Killian Court at MIT in Cambridge, MA US.
Lavoisier is listed among eminent Roman Catholic scientists, and as such he defended his faith against those who attempted to use science to attack it. Louis Edouard Grimaux, author of the standard French biography of Lavoisier, and the first biographer to obtain access to Lavoisier's papers, writes the following: Raised in a pious family which had given many priests to the Church, he had held to his beliefs. To Edward King, an English author who had sent him a controversial work, he wrote, "You have done a noble thing in upholding revelation and the authenticity of the Holy Scripture, and it is remarkable that you are using for the defense precisely the same weapons which were once used for the attack".
Legacy.
Lavoisier's fundamental contributions to chemistry were a result of a conscious effort to fit all experiments into the framework of a single theory. He established the consistent use of the chemical balance, used oxygen to overthrow the phlogiston theory, and developed a new system of chemical nomenclature which held that oxygen was an essential constituent of all acids (which later turned out to be erroneous). Lavoisier also did early research in physical chemistry and thermodynamics in joint experiments with Laplace. They used a calorimeter to estimate the heat evolved per unit of carbon dioxide produced, eventually finding the same ratio for a flame and animals, indicating that animals produced energy by a type of combustion reaction.
Lavoisier also contributed to early ideas on composition and chemical changes by stating the radical theory, believing that radicals, which function as a single group in a chemical process, combine with oxygen in reactions. He also introduced the possibility of allotropy in chemical elements when he discovered that diamond is a crystalline form of carbon.
He was essentially a theorist, and his great merit lay in his capacity to take over experimental work that others had carried out—without always adequately recognizing their claims—and by a rigorous logical procedure, reinforced his own quantitative experiments, expounding the true explanation of the results. He completed the work of Black, Priestley and Cavendish, and gave a correct explanation of their experiments.
Overall, his contributions are considered the most important in advancing chemistry to the level reached in physics and mathematics during the 18th century. Lavoisier's work was recognized as an International Historic Chemical Landmark by the American Chemical Society, Académie des sciences de L'institut de France and the Société Chimique de France in 1999.

</doc>
<doc id="1825" url="http://en.wikipedia.org/wiki?curid=1825" title="Hermann Kolbe">
Hermann Kolbe

Hermann Kolbe ("Adolph Wilhelm Hermann Kolbe", 27 Sept. 1818–25 Nov. 1884), was a seminal contributor in the birth of modern organic chemistry as Professor at Marburg and Leipzig. Kolbe coined the term synthesis, and contributed to the philosophical demise of vitalism through synthesis of the biologic natural product acetic acid from carbon disulfide, to structural theory via modifications to the idea of "radicals" and accurate prediction of the existence of secondary and tertiary alcohols, and to the emerging array of organic reactions through his Kolbe electrolysis of carboxylate salts, the Kolbe-Schmitt reaction in the preparation of aspirin, and the Kolbe nitrile synthesis. After studies with Wöhler and Bunsen, Kolbe was involved with the early internationalization of chemistry through overseas work in London (with Frankland), and rose through the ranks of his field to edit the , to be elected to the Royal Swedish Academy of Sciences, and to win the Royal Society of London's Davy Medal in the year of his death. Despite these accomplishments and his training a storied next generation of chemists (including Zaitsev, Curtius, Beckmann, Graebe, Markovnikov, etc.), Kolbe is remembered for editing the "Journal" for more than a decade, where his rejection of Kekulé's structure of benzene, van't Hoff's theory on the origin of chirality, and von Baeyer's reforms of nomenclature were personally critical and linguistically violent. Kolbe died of a heart attack in Leipzig at age 68, six years after the death of his wife, Charlotte. He was survived by four children.
Life.
Kolbe was born in Elliehausen, near Göttingen, Kingdom of Hanover (Germany) as the eldest son of a Protestant pastor. At the age of 13 he entered the Göttingen Gymnasium, residing at the home of one of the professors. He obtained the leaving certificate (the Abitur) six years later. He had become passionate about the study of chemistry, matriculating at the University of Göttingen in the spring of 1838 in order to study with the famous chemist Friedrich Wöhler.
In 1842 he became an assistant to Robert Bunsen at the Philipps-Universität Marburg; he took his doctoral degree there in 1843. A new opportunity arose in 1845, when he became assistant to Lyon Playfair at the new "Museum of Economic Geology" in London, where he became a close friend of Edward Frankland. From 1847 he was engaged in editing the "Handwörterbuch der reinen und angewandten Chemie" ("Dictionary of Pure and Applied Chemistry") edited by Justus von Liebig, Wöhler, and Johann Christian Poggendorff, and he also wrote an important textbook. In 1851 Kolbe succeeded Bunsen as professor of chemistry at Marburg, and in 1865 he was called to the Universität Leipzig. In 1864, he was elected a foreign member of the Royal Swedish Academy of Sciences.
In 1853 he married Charlotte, the daughter of General-Major Wilhelm von Bardeleben. His wife died in 1876 after 23 years of happy marriage. They had four children.
Work in chemical research.
As late as the 1840s, and despite Friedrich Wöhler's synthesis of urea in 1828, some chemists still believed in the doctrine of vitalism, according to which a special life-force was necessary to create "organic" (i.e., in its original meaning, biologically derived) compounds. Kolbe promoted the idea that organic compounds could be derived from substances clearly sourced from outside this "organic" context, directly or indirectly, by substitution processes. (Hence, while by modern definitions, he was converting one organic molecule to another, by the parlance of his era, he was converting "inorganic"—"anorganisch"—substances into "organic" ones only thought accessible through vital processes.) He validated his theory by converting carbon disulfide to acetic acid in several steps (1843–45). Kolbe also introduced a modified idea of structural radicals, so contributing to the development of structural theory. A dramatic success came when his theoretical prediction of the existence of secondary and tertiary alcohols was confirmed by the synthesis of the first of these classes of organic molecules. Kolbe was the first person to use the word synthesis in its present day meaning, and contributed a number of new chemical reactions.
In particular, Kolbe developed procedures for the electrolysis of the salts of fatty and other carboxylic acids (Kolbe electrolysis) and prepared salicylic acid, a building block of aspirin in a process called Kolbe synthesis or Kolbe-Schmitt reaction. His method for the synthesis of nitriles is called the Kolbe nitrile synthesis, and with Edward Frankland he found that nitriles can be hydrolyzed to the corresponding acids.
In addition to his own bench research and scholarly and editorial work, Kolbe oversaw student research at Leipzig and especially at Marburg; students spending time under his tutelage included Peter Griess, Aleksandr Mikhailovich Zaitsev (known for Zaitsev's rule predicting the product composition of elimination reactions), Theodor Curtius (discoverer of diazo compounds, hydrazines, and the Curtius rearrangement), Ernst Otto Beckmann (discoverer of the Beckmann rearrangement), Carl Graebe (discoverer of alizarin), Oscar Loew, Constantin Fahlberg, Nikolai Menshutkin, Vladimir Markovnikov (first to describe carbocycles smaller and larger than cyclohexane, and known for Markovnikov's rule describing addition reactions to alkenes), Jacob Volhard, Ludwig Mond, Alexander Crum Brown (first to describe the double bond of ethylene), Maxwell Simpson, and Frederick Guthrie.
Work as journal editor.
Kolbe served for more than a decade as what, in modern terms, would be understood the senior editor of the ("Journal of practical chemistry", from 1870 to 1884), Kolbe was sometimes so severely critical of the work of others, especially after about 1874, that some wondered whether he might have been suffering a mental illness. He was intolerant of what he regarded as loose speculation parading as theory, and sought through his writings to save his beloved science of chemistry from what he regarded as the scourge of modern structural theory.
His rejection of structural chemistry, especially the theories of the structure of benzene by August Kekulé, the theory of the asymmetric carbon atom by J.H. van't Hoff, and the reform of chemical nomenclature by Adolf von Baeyer, resulted in vituperative articles in the "Journal für Praktische Chemie". Some translated quotes illustrate his manner of articulating the deep conflict between his interpretation of chemistry and that of the structural chemists: 
"...Baeyer is an excellent experimentor, but he is only an empiricist, lacking sense and capability, and his interpretations of his experiments show particular deficiency in his familiarity with the principles of true science..."
The violence of his language worked unfairly to limit his posthumous reputation. He died of a heart attack, in Leipzig.

</doc>
<doc id="1826" url="http://en.wikipedia.org/wiki?curid=1826" title="April 18">
April 18

April 18 is the day of the year in the Gregorian calendar.

</doc>
<doc id="1827" url="http://en.wikipedia.org/wiki?curid=1827" title="April 23">
April 23

April 23 is the day of the year in the Gregorian calendar.

</doc>
<doc id="1828" url="http://en.wikipedia.org/wiki?curid=1828" title="Amitabh Bachchan">
Amitabh Bachchan

Amitabh Harivansh Bachchan (]; born 11 October 1942) is an Indian film actor. He first gained popularity in the early 1970s for movies like "Deewar" and "Zanjeer", and was dubbed India's first "angry young man" for his on-screen roles in Bollywood, and has since appeared in over 180 Indian films in a career spanning more than four decades. Bachchan is widely regarded as one of the greatest and most influential actors in the history of Indian cinema. So total was his dominance of the movie scene in the 1970s and 1980s that the French director François Truffaut called him a "one-man industry."
Bachchan has won many major awards in his career, including three National Film Awards as Best Actor, a number of awards at international film festivals and award ceremonies and fourteen Filmfare Awards. He is the most-nominated performer in any major acting category at Filmfare, with 39 nominations overall. In addition to acting, Bachchan has worked as a playback singer, film producer and television presenter. He also had a stint in politics in the 1980s.
The Government of India honoured him with the Padma Shri in 1984, the Padma Bhushan in 2001 and the Padma Vibhushan in 2015 for his contributions to the arts. The Government of France honoured him with its highest civilian honour Knight of the Legion of Honour in 2007 for his exceptional career in the world of cinema and beyond.
Bachchan made his Hollywood debut in 2013 with "The Great Gatsby", in which he played a non-Indian Jewish character, Meyer Wolfsheim.
Early and personal life.
Bachchan was born in Allahabad, Uttar Pradesh, in north central India. His father Harivansh Rai Bachchan was a Hindi poet and his mother Teji Bachchan was a Punjabi Sikh from Faisalabad, Punjab. Bachchan was initially named "Inquilaab," inspired by the phrase "Inquilab Zindabad" popularly used during the Indian independence struggle. In English, "Inquilab Zindabad" means "long live the revolution." However at the suggestion of fellow poet Sumitranandan Pant, Harivansh Rai changed the boy's name to Amitabh, which means "the light that will never die."
Although his surname was Shrivastava, Amitabh's father had adopted the pen name Bachchan ("child-like" in colloquial Hindi), under which he published all of his works. It is with this last name that Amitabh debuted in films and for all other practical purposes, Bachchan has become the surname for all of his immediate family. Bachchan's father died in 2003, and his mother in 2007.
Bachchan is an alumnus of Sherwood College, Nainital. He later attended Kirori Mal College, Delhi University. He has a younger brother, Ajitabh. His mother had a keen interest in theatre and was offered a feature film role, but she preferred her domestic duties. Teji had some influence in Amitabh Bachchan's choice of career because she always insisted that he should "take the centre stage."
Bachchan is married to actress Jaya Bhaduri. The couple have two children, Shweta Nanda (wife of businessman Nikhil Nanda) and Abhishek Bachchan (actor and husband of actress Aishwarya Rai).
Career.
Early work: 1969–1972.
Bachchan made his film debut in 1969 as a voice narrator in Mrinal Sen's National Award winning film "Bhuvan Shome". His first acting role was as one of the seven protagonists in the film "Saat Hindustani" directed by Khwaja Ahmad Abbas and featuring Utpal Dutt, Anwar Ali (brother of comedian Mehmood), Madhu and Jalal Agha.
"Anand" (1971) followed, in which Bachchan starred alongside Rajesh Khanna. His role as a doctor with a cynical view of life garnered Bachchan his first Filmfare "Best Supporting Actor" award. He then played his first antagonist role as an infatuated lover-turned-murderer in "Parwaana" (1971). Following Parwaana were several films including "Reshma Aur Shera" (1971). During this time, he made a guest appearance in the film "Guddi" which starred his future wife Jaya Bhaduri. He narrated part of the film "Bawarchi." In 1972 he made an appearance in the road action comedy "Bombay to Goa" directed by S. Ramanathan. Many of Bachchan's films during this early period did not do well, but that was about to change.
Rise to stardom: 1973–1983.
Director Prakash Mehra cast him in the leading role for the film "Zanjeer" (1973) as Inspector Vijay Khanna. The film was a sharp contrast to the romantically themed films that had generally preceded it and established Amitabh in a new persona—the "angry young man" of Bollywood cinema. Filmfare considers this one of the most iconic performances of Bollywood history. The film was a huge success and one of the highest grossing films of that year, breaking Bachchan's dry spell at the box office and making him a star. From then onwards, Bachchan became one of the most successful leading men of the film industry. He earned his first Filmfare nomination for Best Actor for "Zanjeer". The year 1973 was also when he married Jaya, and around this time they appeared in several films together; not only in "Zanjeer" but in films such as "Abhimaan" which followed and was released only a month after their marriage and was also successful at the box office. Later, Bachchan played the role of Vikram, once again along with Rajesh Khanna, in the film "Namak Haraam", a social drama directed by Hrishikesh Mukherjee and scripted by Biresh Chatterjee addressing themes of friendship. His supporting role won him his second Filmfare "Best Supporting Actor" award.
In 1974, Bachchan made several guest appearances in films such as "Kunwara Baap" and "Dost", before playing a supporting role in "Roti Kapda Aur Makaan". The film, directed and written by Manoj Kumar, addressed themes of honesty in the face of oppression and financial and emotional hardship and was the top earning film of 1974. Bachchan then played the leading role in film "Majboor", released on 6 December 1974, which was a remake of the Hollywood film "Zig Zag". The film was a success at the box office. In 1975, he starred in a variety of film genres from the comedy "Chupke Chupke," the crime drama "Faraar" to the romantic drama "Mili." 1975 was also the year when Bachchan appeared in two films regarded as important in Hindi cinema history. He starred in the Yash Chopra directed film "Deewaar" along with Shashi Kapoor, Nirupa Roy, and Neetu Singh, earning him a Filmfare nomination for Best Actor. The film became a major hit at the box office in 1975, ranking in at number 4. "Indiatimes Movies" ranks "Deewaar" amongst the "Top 25 Must See Bollywood Films". Released on 15 August 1975 was "Sholay", which became the highest grossing film of 1975 and also of all time in India, earning INR equivalent to US$60 million, after adjusting for inflation. in which Bachchan played the role of Jaidev. In 1999, BBC India declared it the "Film of the Millennium" and like "Deewar", has been cited by "Indiatimes movies" as amongst the "Top 25 Must See Bollywood Films". In that same year, the judges of the 50th annual Filmfare Awards awarded it with the special distinction award called Filmfare Best Film of 50 Years.
In 1976 he was cast by Yash Chopra in the romantic family drama "Kabhie Kabhie". Bachchan starred as a young poet named Amit Malhotra who falls deeply in love with a beautiful young girl named Pooja (Raakhee) who ends up marrying someone else (Shashi Kapoor). The film was notable for portraying Bachchan as a romantic hero, a far cry from his "angry young man" roles like "Zanjeer" and "Deewar". The film evoked a favourable response from critics and audiences alike. Bachchan was again nominated for the Filmfare Best Actor Award for his role in the film. That same year he played a double role in "Adalat" as father and son. In 1977, he won his first Filmfare Best Actor Award for his performance in "Amar Akbar Anthony" where he played the third lead opposite Vinod Khanna and Rishi Kapoor as Anthony Gonsalves. The film was the highest grossing film of that year. His other successes that year include "Parvarish" and "Khoon Pasina". He once again resumed double roles in films such as "Kasme Vaade" (1978) as Amit and Shankar and "Don" (1978) playing the characters of Don, a leader of an underworld gang and his look alike Vijay. His performance won him his second Filmfare Best Actor Award. He also gave towering performances in Yash Chopra's "Trishul" and Prakash Mehra's "Muqaddar Ka Sikandar" both of which earned him further Filmfare Best Actor nominations.
In 1979, Bachchan starred in "Suhaag" which was the highest earning film of that year. In the same year he also enjoyed critical acclaim and commercial success with films like "Mr. Natwarlal," "Kaala Patthar" and "The Great Gambler". Amitabh was required to use his singing voice for the first time in a song from the film "Mr. Natwarlal" in which he starred with Rekha. Bachchan's performance in the film saw him nominated for both the Filmfare Best Actor Award and the Filmfare "Best Male Playback Singer" award. He also received Best Actor nomination for "Kaala Patthar" and then went on to be nominated again in 1980 for the Raj Khosla directed film "Dostana", in which he starred opposite Shatrughan Sinha and Zeenat Aman. "Dostana" proved to be the top grossing film of 1980. In 1981, he starred in Yash Chopra's melodrama film "Silsila", where he starred alongside his wife Jaya and Rekha. Other films of this period like "Shaan" (1980), "Shakti" (1982) which pitted him against the veteran actor Dilip Kumar were not successful at the box office but "Ram Balram" (1980), "Naseeb" (1981) and "Lawaaris" (1981) were successful.
In 1982 he played double roles in the films "Satte Pe Satta" and "Desh Premee" which succeeded at the box office. In 1983 he played a triple role in "Mahaan" and starred in the top grossing film of that year "Coolie".
1982 injury while filming "Coolie".
On 26 July 1982, while filming "Coolie" in the University Campus in Bangalore, Bachchan suffered a near fatal intestinal injury during the filming of a fight scene with co-actor Puneet Issar. Bachchan was performing his own stunts in the film and one scene required him to fall onto a table and then on the ground. However, as he jumped towards the table, the corner of the table struck his abdomen, resulting in a splenic rupture from which he lost a significant amount of blood. He required an emergency splenectomy and remained critically ill in hospital for many months, at times close to death. The public response included prayers in temples and offers to sacrifice limbs to save him, while later, there were long queues of well-wishing fans outside the hospital where he was recuperating.
Nevertheless, he resumed filming later that year after a long period of recuperation. The film was released in 1983, and partly due to the huge publicity of Bachchan's accident, the film was a box office success and the top grossing film that year.
The director, Manmohan Desai, altered the ending of "Coolie" after Bachchan's accident. Bachchan's character was originally intended to have been killed off but after the change of script, the character lived in the end. It would have been inappropriate, said Desai, for the man who had just fended off death in real life to be killed on screen. Also, in the released film the footage of the fight scene is frozen at the critical moment, and a caption appears onscreen marking this as the instant of the actor's injury and the ensuing publicity of the accident.
Later, he was diagnosed with Myasthenia gravis. His illness made him feel weak both mentally and physically and he decided to quit films and venture into politics. At this time he became pessimistic, expressing concern with how a new film would be received and stated before every release, "Yeh film to flop hogi!" ("This film will flop").
Politics: 1984–87.
In 1984, Bachchan took a break from acting and briefly entered politics in support of long-time family friend, Rajiv Gandhi. He contested Allahabad's seat of 8th Lok Sabha against H. N. Bahuguna, former Chief Minister of Uttar Pradesh and won by one of the highest victory margins in general election history (68.2% of the vote). His political career, however, was short-lived: he resigned after three years, calling politics a cesspool. The resignation followed the implication of Bachchan and his brother in the "Bofors scandal" by a newspaper, which he vowed to take to court. Bachchan was eventually found not guilty of involvement in the ordeal.
His old friend, Amar Singh, helped him during a financial crisis due to the failure of his company ABCL. Therefore Bachchan started to support Amar Singh's political party, the Samajwadi Party. Jaya Bachchan joined the Samajwadi party and became a Rajya Sabha member. Bachchan has continued to do favours for the Samajwadi party, including advertisements and political campaigns. These activities have recently gotten him into trouble in the Indian courts for false claims after a previous incident of submission of legal papers by him, stating that he is a farmer.
A 15-year press ban against Bachchan was imposed during his peak acting years by "Stardust" and some of the other film magazines. In his defence, Bachchan claimed to have banned the press from entering his sets until late 1989.
Slump and retirement: 1988–1992.
In 1988, Bachchan returned to films, playing the title role in "Shahenshah", which was a box office success. After the success of his comeback film however, his star power began to wane as all of his subsequent films like "Jaadugar", "Toofan" and "Main Azaad Hoon" (all released in 1989) failed at the box office. The 1991 hit film, "Hum", for which he won his third Filmfare Best Actor award, looked like it might reverse the trend, but this momentum was short-lived and his string of box office failures continued. Notably, despite the lack of hits, it was during this era that Bachchan won his first National Film Award for Best Actor for his performance as a Mafia don in the 1990 film "Agneepath." These years would see his last on-screen appearances for some time. After the release of "Khuda Gawah" in 1992, Bachchan went into semi-retirement for five years. With the exception of the delayed release of "Insaniyat" (1994), which was also a box office failure, Bachchan did not appear in any new releases for five years.
Producer and acting comeback 1996–99.
Bachchan turned producer during his temporary retirement period, setting up Amitabh Bachchan Corporation, Ltd. (ABCL) in 1996, with a vision of becoming a 10 billion rupees (approx. U.S. $250 M) premier entertainment company by the year 2000. ABCL's strategy was to introduce products and services covering an entire cross-section of India's entertainment industry. ABCL's operations were mainstream commercial film production and distribution, audio cassettes and video discs, production and marketing of television software, and celebrity and event management. Soon after the company was launched in 1996, the first film it produced was "Tere Mere Sapne," which did not fare well at the boxoffice but launched the careers of actors like Arshad Warsi and South films star Simran. ABCL produced a few other films, none of which did well.
In 1997, Bachchan attempted to make his acting comeback with the film "Mrityudata", produced by ABCL. Though "Mrityudaata" attempted to reprise Bachchan's earlier success as an action hero, the film was a failure both financially and critically. ABCL was the main sponsor of the "1996 Miss World beauty pageant", Bangalore but lost millions. The fiasco and the consequent legal battles surrounding ABCL and various entities after the event, coupled with the fact that ABCL was reported to have overpaid most of its top level managers, eventually led to its financial and operational collapse in 1997. The company went into administration and was later declared a failed company by Indian Industries board. The Bombay high court, in April 1999, restrained Bachchan from selling off his Bombay bungalow 'Prateeksha' and two flats till the pending loan recovery cases of Canara Bank were disposed of. Bachchan had, however, pleaded that he had mortgaged his bungalow to raise funds for his company.
Bachchan attempted to revive his acting career and had average success with "Bade Miyan Chote Miyan" (1998), and received positive reviews for "Sooryavansham" (1999) but other films such as "Lal Baadshah" (1999) and "Hindustan Ki Kasam" (1999) were box office failures.
Return to prominence: 2000–present.
In 2000, Amitabh Bachchan appeared in Yash Chopra's box-office hit, "Mohabbatein", directed by Aditya Chopra. He played a stern, older figure that rivalled the character of Shahrukh Khan. His role won him his third Filmfare Best Supporting Actor Award. Other hits followed, with Bachchan appearing as an older family patriarch in "" (2001), "Kabhi Khushi Kabhie Gham..." (2001) and "Baghban" (2003). As an actor, he continued to perform in a range of characters, receiving critical praise for his performances in "Aks" (2001), "Aankhen" (2002), "Khakee" (2004) and "Dev" (2004). One project that did particularly well for Bachchan was Sanjay Leela Bhansali's "Black" (2005). The film starred Bachchan as an ageing teacher of a deaf-blind girl and followed their relationship. His performance was unanimously praised by critics and audiences and won him his second National Film Award for Best Actor and fourth Filmfare Best Actor Award. Taking advantage of this resurgence, Amitabh began endorsing a variety of products and services, appearing in many television and billboard advertisements. In 2005 and 2006, he starred with his son Abhishek in the hit films "Bunty Aur Babli" (2005), the "Godfather" tribute "Sarkar" (2005), and "Kabhi Alvida Naa Kehna" (2006). All of them were successful at the box office. His later releases in 2006 and early 2007 were "Baabul" (2006), "Ekalavya" and "Nishabd" (2007), which failed to do well at the box office but his performances in each of them were praised by critics.
In May 2007, two of his films "Cheeni Kum" and the multi-starrer "Shootout at Lokhandwala" were released. "Shootout at Lokhandwala" did well at the box office and was declared a semi-hit in India, while "Cheeni Kum" picked up after a slow start and only had average success. A remake of his biggest hit, "Sholay" (1975), entitled "Ram Gopal Varma Ki Aag", released in August of that same year and proved to be a major commercial failure in addition to its poor critical reception. The year also marked Bachchan's first appearance in an English-language film, Rituparno Ghosh's "The Last Lear". The film premiered at the 2007 Toronto International Film Festival on 9 September 2007. He received positive reviews from critics who hailed his performance as his best ever since "Black".
Bachchan was slated to play a supporting role in his first international film, "Shantaram", directed by Mira Nair and starring Hollywood actor Johnny Depp in the lead. The film was due to begin filming in February 2008 but due to the writer's strike, was pushed to September 2008. The film is currently "shelved" indefinitely. Vivek Sharma's "Bhoothnath", in which he plays the title role as a ghost, was released on 9 May 2008. "Sarkar Raj", the sequel of the 2005 film "Sarkar", released in June 2008 and received a positive response at the box-office. "Paa", which released at the end of 2009 was a highly anticipated project as it saw him playing his own son Abhishek's Progeria-affected 13-year-old son, and it opened to favourable reviews, particularly towards Bachchan's performance. It won him his third National Film Award for Best Actor and fifth Filmfare Best Actor Award. In 2010, he debuted in Malayalam film through "Kandahar", directed by Major Ravi and co-starring Mohanlal. The film was based on the hijacking incident of the Indian Airlines Flight 814. Bachchan declined any remuneration for this film. In 2013 he made his Hollywood debut in "The Great Gatsby" playing the role of Meyer Wolfsheim opposite Leonardo DiCaprio and Tobey Maguire.
Television career.
In 2000, Bachchan hosted the first season of "Kaun Banega Crorepati" ("KBC"), the Indian adaptation of the British television game show, "Who Wants to Be a Millionaire?". The show was well received. A second season followed in 2005 but its run was cut short by STAR Plus when Bachchan fell ill in 2006.
In 2009, Bachchan hosted the third season of the reality show "Bigg Boss".
In 2010, Bachchan hosted the fourth season of "KBC". The fifth season started on 15 August 2011 and ended on 17 November 2011. The show became a massive hit with audiences and broke many TRP Records. CNN IBN awarded Indian of the Year- Entertainment to Team KBC and Bachchan. The Show also grabbed all the major Awards for its category.
The sixth season was also hosted by Bachchan, commencing on 7 September 2012, broadcast on Sony TV and received the highest number of viewers thus far.
In 2014, he debuted in the fictional Sony Entertainment Television TV series titled "Yudh" playing the lead role of a businessman battling both his personal and professional life.
Bachchan is also the brand ambassador for Gujarat Tourism since 1 February 2010.
Voice.
Bachchan is known for his deep, baritone voice. He has been a narrator, a playback singer, and presenter for numerous programmes. Renowned film director Satyajit Ray was so impressed with Bachchan's voice that he decided to use Bachchan as the narrator in his 1977 film "Shatranj Ke Khilari" (The Chess Players). Bachchan lent his voice as a narrator to the 2001 movie Lagaan which was a super hit. In 2005, Bachchan lent his voice to the Oscar-winning French documentary March of the Penguins, directed by Luc Jacquet.
He also lent his voice to the following movies.
Humanitarian causes.
Amitabh Bachchan has been involved in many social works. Amitabh donated ₹11 lakh () to clear the debts of nearly 40 beleaguered farmers in Andhra Pradesh. He also donated ₹30 lakh () to clear the debts of some 100 Vidarbha farmers. In 2010, he donated ₹11 lakh () for Resul Pookutty's foundation, for medical centre at Kochi. Amitabh Bachchan donateed ₹2.5 lakh () to Delhi Police constable Subhash Chand Tomar's family,who died after succumbing to injuries during anti-gangrape protest for 2012 Delhi gang rape. He opened a Harivansh Rai Bachchan Memorial Trust, or HRB Memorial Trust in his father's name in 2013. Amitabh Bachchan was made UNICEF goodwill ambassador for polio Eradication Campaign in India in 2002, when 1,556 polio cases were detected that year . On 27 March 2014, World Health Organization (WHO) declared India a polio free country with no case of disease being reported in the previous three years. In 2013, Amitabh and his family donated ₹25 lakh () to charitable trust Plan India, that works for the upliftment of the girl child in India. Amitabh Bachchan donated ₹11 lakh () to Maharashtra Police Welfare Fund in 2013. Amitabh was the face of 'Save Our Tigers' campaign that promoted the importance of tiger conservation in India.
Amitabh also supported PETA India's campaign to free, Sunder, a 14 year old elephant who was chained and tortured in a temple in Kolhapur, Maharashtra. After the elephant was freed and rehabilitated in Bannerghatta National Park in Bangalore, he was reported to tweet, “@PetaIndia has turned elephant Sunder’s home into free-roaming, forested sanctuary... feeling good I contributed to this cause !”
Business investments.
Amitabh Bachchan has invested in many upcoming business ventures. In 2013, he bought a 10% stake in Just Dial from which he made a gain of 4600 percent. He holds a 3.4% equity in Stampede Capital, a financial technology firm specializing in cloud computing for financial markets. The Bachchan family also bought shares worth $252,000 in Meridian Tech, a consulting company in U.S. Recently they made their first overseas investment in Ziddu.com, a cloud based content distribution platform. 
Awards, honours and recognitions.
Apart from National Film Awards, Filmfare Awards and other competitive awards which Bachchan won for his performances throughout the years, he has been awarded several honours for his achievements in the Indian film industry. In 1991, he became the first artist to receive the Filmfare Lifetime Achievement Award, which was established in the name of Raj Kapoor. Bachchan was crowned as Superstar of the Millennium in 2000 at the Filmfare Awards. The Government of India awarded him with the Padma Shri in 1984, the Padma Bhushan in 2001 and the Padma Vibhushan in 2015. France's highest civilian honour, the Knight of the Legion of Honour, was conferred upon him by the French Government in 2007 for his "exceptional career in the world of cinema and beyond". In 2011, actor Dilip Kumar blogged that "Black" should have been nominated for an Oscar. Kumar added: "If any Indian actor, in my personal opinion, deserves the world's most coveted award, it is you."
In 1999, Bachchan was voted the "greatest star of stage or screen" in a BBC "Your Millennium" online poll. The organisation noted that "Many people in the western world will not have heard of [him] ...[but it] is a reflection of the huge popularity of Indian films." In 2001, he was honoured with the Actor of the Century award at the Alexandria International Film Festival in Egypt in recognition of his contribution to the world of cinema. Many other honours for his achievements were conferred upon him at several International Film Festivals, including the Lifetime Achievement Award at the 2010 Asian Film Awards.
In June 2000, he became the first living Asian to have been modelled in wax at London's Madame Tussauds Wax Museum. Another statue was installed in New York in 2009, Hong Kong in 2011, Bangkok in 2011 and Washington, DC in 2012.
In 2003, he was conferred with the Honorary Citizenship of the French town of Deauville.
He was honoured with an Honorary Doctorate by the University of Jhansi, India, in 2004, the University of Delhi in 2006, the De Montfort University in Leicester, UK, in 2006, the Leeds Metropolitan University in Yorkshire, UK, in 2007, the Queensland University of Technology in Brisbane, Australia, in 2011 the Jodhpur National University in 2013. and the Academy of Arts (Egypt) in Cairo, Egypt, in 2015. 
On 27 July 2012, Bachchan carried the Olympic torch during the last leg of its relay in London's Southwark.
Severals books have been written about Bachchan. "Amitabh Bachchan: the Legend" was published in 1999, "To be or not to be: Amitabh Bachchan" in 2004, "AB: The Legend: (A Photographer's Tribute)" in 2006 /, "Amitabh Bachchan: Ek Jeevit Kimvadanti" in 2006, "Amitabh: The Making of a Superstar" in 2006, "Looking for the Big B: Bollywood, Bachchan and Me" in 2007 and "Bachchanalia" in 2009.
Bachchan himself also wrote a book in 2002: "Soul Curry for you and me – An Empowering Philosophy That Can Enrich Your Life". In the early 80s, Bachchan authorised the use of his likeness for the comic book character Supremo in a series titled "The Adventures of Amitabh Bachchan". In May 2014, La Trobe University in Australia named a Scholarship after Bachchan.
He was named "Hottest Vegetarian" by PETA India in 2012. He has also won the title of "Asia's Sexiest Vegetarian" in - a contest poll run by PETA Asia

</doc>
<doc id="1832" url="http://en.wikipedia.org/wiki?curid=1832" title="Allomorph">
Allomorph

In linguistics, an allomorph is a variant form of a morpheme. The concept occurs when a unit of meaning can vary in sound without changing meaning. The term "allomorph" explains the comprehension of phonological variations for specific morphemes.
Allomorphy in English suffixes.
English has several morphemes that vary in sound but not in meaning. Examples include the past tense and the plural morphemes.
For example, in English, a past tense morpheme is "-ed". It occurs in several allomorphs depending on its phonological environment, assimilating voicing of the previous segment or inserting a schwa when following an alveolar stop:
Notice the "other than" restrictions above. This is a common fact about allomorphy: if the allomorphy conditions are ordered from most restrictive (in this case, after an alveolar stop) to least restrictive, then the first matching case usually "wins". Thus, the above conditions could be re-written as follows:
The fact that the /t/ allomorph does not appear after stem-final /t/, despite the fact that the latter is voiceless, is then explained by the fact that /əd/ appears in that environment, together with the fact that the environments are ordered. Likewise, the fact that the /d/ allomorph does not appear after stem-final /d/ is because the earlier clause for the /əd/ allomorph takes priority; and the fact that the /d/ allomorph does not appear after stem-final voiceless phonemes is because the preceding clause for the /t/ takes priority.
Irregular past tense forms, such as "broke" or "was/ were", can be seen as still more specific cases (since they are confined to certain lexical items, such as the verb "break"), which therefore take priority over the general cases listed above.
Stem allomorphy.
Allomorphy can also exist in stems or roots, as in Classical Sanskrit:
There are three allomorphs of the stem: /vaːk/, /vaːt͡ʃ/ and /vaːɡ/. The allomorphs are conditioned by the particular case-marking suffixes.
The form of the stem /vaːk/, found in the nominative singular and locative plural, is the etymological form of the morpheme. Pre-Indic palatalization of velars resulted in the variant form /vaːt͡ʃ/, which was initially phonologically conditioned. This conditioning can still be seen in the Locative Singular form, where the /t͡ʃ/ is followed by the high front vowel /i/.
But subsequent merging of /e/ and /o/ into /a/ made the alternation unpredictable on phonetic grounds in the Genitive case (both Singular and Plural), as well as the Nominative Plural and Instrumental Singular. Hence, this allomorphy was no longer directly relatable to phonological processes.
Phonological conditioning also accounts for the /vaːɡ/ form found in the Instrumental Plural, where the /ɡ/ assimilates in voicing to the following /bʱ/.
History.
The term was originally used to describe variations in chemical structure. It was first applied to language (in writing) in 1948, by Fatih Şat and Sibel Merve in Language XXIV.

</doc>
<doc id="1834" url="http://en.wikipedia.org/wiki?curid=1834" title="Allophone">
Allophone

In phonology, an allophone (; from the Greek: ἄλλος, "állos", "other" and φωνή, "phōnē", "voice, sound") is one of a set of multiple possible spoken sounds (or "phones") used to pronounce a single phoneme in a particular language. For example, [pʰ] (as in "pin") and [p] (as in "spin") are allophones for the phoneme /p/ in the English language. The specific allophone selected in a given situation is often predictable from the phonetic context (such allophones are called positional variants), but sometimes allophones occur in free variation. Replacing a sound by another allophone of the same phoneme will usually not change the meaning of a word, although sometimes the result may sound non-native or even unintelligible. Native speakers of a given language usually perceive one phoneme in that language as a single distinctive sound, and are "both unaware of and even shocked by" the allophone variations used to pronounce single phonemes.
History of concept.
The term "allophone" was coined by Benjamin Lee Whorf in the 1940s. In doing so, he placed a cornerstone in consolidating early phoneme theory. The term was popularized by G. L. Trager and Bernard Bloch in a 1941 paper on English phonology and went on to become part of standard usage within the American structuralist tradition.
Complementary and free-variant allophones.
Every time a speech sound is produced for a given phoneme, it will be slightly different from other utterances, even for the same speaker. This has led to some debate over how real, and how universal, phonemes really are (see phoneme for details). Only some of the variation is significant (i.e., detectable or perceivable) to speakers. There are two types of allophones, based on whether a phoneme must be pronounced using a specific allophone in a specific situation, or whether the speaker has freedom to (unconsciously) choose which allophone to use.
When a specific allophone (from a set of allophones that correspond to a phoneme) "must" be selected in a given context (i.e., using a different allophone for a phoneme will cause confusion or make the speaker sound non-native), the allophones are said to be complementary (i.e., the allophones complement each other, and one is not used in a situation where the usage of another is standard). In the case of complementary allophones, each allophone is used in a specific phonetic context and may be involved in a phonological process.
In other cases, the speaker is able to select freely from free variant allophones, based on personal habit or preference.
Allotone.
A tonic allophone is sometimes called an allotone, for example in the neutral tone of Mandarin.
Examples.
English.
There are many allophonic processes in English, like lack of plosion, nasal plosion, partial devoicing of sonorants, complete devoicing of sonorants, partial devoicing of obstruents, lengthening and shortening vowels, and retraction.
Because the choice of allophone is seldom under conscious control, people may not realize they exist. English speakers may be unaware of the differences among six allophones of the phoneme /t/, namely unreleased [ t̚] as in "cat", aspirated [tʰ] as in "top", glottalized [ʔ] as in "button", flapped [ɾ] as in American English "water", nasalized flapped as in "winter", and none of the above [t] as in "stop". However, they may become aware of the differences if, for example, they contrast the pronunciations of the following words:
If a flame is held before the lips while these words are spoken, it flickers more during aspirated "nitrate" than during unaspirated "night rate." The difference can also be felt by holding the hand in front of the lips. For a Mandarin speaker, to whom /t/ and /tʰ/ are separate phonemes, the English distinction is much more obvious than it is to the English speaker who has learned since childhood to ignore it.
Allophones of English /l/ may be noticed if the 'light' [l] of "leaf" [ˈliːf] is contrasted with the 'dark' [ɫ] of "feel" [ˈfiːɫ]. Again, this difference is much more obvious to a Turkish speaker, for whom /l/ and /ɫ/ are separate phonemes, than to an English speaker, for whom they are allophones of a single phoneme.
Cross-language comparison.
There are many examples for allophones in languages other than English. Typically, languages with a small phoneme inventory allow for quite a lot of allophonic variation. (See e.g. Hawaiian and Toki Pona.) Examples: (Links of language names go to the specific article or subsection on the phenomenon.)
Representing a phoneme with an allophone.
Since phonemes are abstractions of speech sounds, not the sounds themselves, they have no direct phonetic transcription. When they are realized without much allophonic variation, a simple "broad transcription" is used. However, when there are complementary allophones of a phoneme, so that the allophony is significant, things become more complicated. Often, if only one of the allophones is simple to transcribe, in the sense of not requiring diacritics, then that representation is chosen for the phoneme. 
However, there may be several such allophones, or the linguist may prefer greater precision than this allows. In such cases a common convention is to use the "elsewhere condition" to decide which allophone will stand for the phoneme. The "elsewhere" allophone is the one that remains once the conditions for the others are described by phonological rules. For example, English has both oral and nasal allophones of its vowels. The pattern is that vowels are nasal only when preceding a nasal consonant within the same syllable; elsewhere they're oral. Therefore, by the "elsewhere" convention, the oral allophones are considered basic; nasal vowels in English are considered to be allophones of oral phonemes.
In other cases, an allophone may be chosen to represent its phoneme because it is more common in the world's languages than the other allophones, because it reflects the historical origin of the phoneme, or because it gives a more balanced look to a chart of the phonemic inventory. Another alternative, commonly employed for archiphonemes, is the use of a capital letter, such as /N/ for [m], [n], [ŋ]. 
In rare cases a linguist may represent phonemes with abstract symbols, such as dingbats, so as not to privilege any one allophone.

</doc>
<doc id="1835" url="http://en.wikipedia.org/wiki?curid=1835" title="Affix">
Affix

An affix (in modern sense) is a morpheme that is attached to a word stem to form a new word. Affixes may be derivational, like English "-ness" and "pre-", or inflectional, like English plural "-s" and past tense "-ed". They are bound morphemes by definition; prefixes and suffixes may be separable affixes. Affixation is, thus, the linguistic process speakers use to form different words by adding morphemes (affixes) at the beginning (prefixation), the middle (infixation) or the end (suffixation) of words.
Positional categories of affixes.
Affixes are divided into plenty of categories, depending on their position with reference to the stem. "Prefix" and "suffix" are extremely common terms. "Infix" and "circumfix" are less so, as they are not important in European languages. The other terms are uncommon.
"Prefix" and "suffix" may be subsumed under the term "adfix" in contrast to "infix."
When marking text for interlinear glossing, as in the third column in the chart above, simple affixes such as prefixes and suffixes are separated from the stem with hyphens. Affixes which disrupt the stem, or which themselves are discontinuous, are often marked off with angle brackets. Reduplication is often shown with a tilde. Affixes which cannot be segmented are marked with a back slash.
Lexical affixes.
"Lexical affixes" (or "semantic affixes") are bound elements that appear as affixes, but function as incorporated nouns within verbs and as elements of compound nouns. In other words, they are similar to word roots/stems in function but similar to affixes in form. Although similar to incorporated nouns, lexical affixes differ in that they never occur as freestanding nouns, i.e. they always appear as affixes.
Lexical affixes are relatively rare. The Wakashan, Salishan, and Chimakuan languages all have lexical suffixes — the presence of these is an areal feature of the Pacific Northwest of the North America.
The lexical suffixes of these languages often show little to no resemblance to free nouns with similar meanings. Compare the lexical suffixes and free nouns of Northern Straits Saanich written in the Saanich orthography and in Americanist notation:
Lexical suffixes when compared with free nouns often have a more generic or general meaning. For instance, one of these languages may have a lexical suffix that means water in a general sense, but it may not have any noun equivalent referring to water in general and instead have several nouns with a more specific meaning (such "saltwater", "whitewater", etc.). In other cases, the lexical suffixes have become grammaticalized to various degrees.
Some linguists have claimed that these lexical suffixes provide only adverbial or adjectival notions to verbs. Other linguists disagree arguing that they may additionally be syntactic arguments just as free nouns are and thus equating lexical suffixes with incorporated nouns. Gerdts (2003) gives examples of lexical suffixes in the Halkomelem language (the word order here is verb–subject–object):
In sentence (1), the verb "wash" is šak’ʷətəs where šak’ʷ- is the root and -ət and -əs are inflectional suffixes. The subject "the woman" is łə słeniʔ and the object "the baby" is łə qeq. In this sentence, "the baby" is a free noun. (The niʔ here is an auxiliary, which can be ignored for explanatory purposes.)
In sentence (2), "baby" does not appear as a free noun. Instead it appears as the lexical suffix -əyəł which is affixed to the verb root šk’ʷ- (which has changed slightly in pronunciation, but this can also be ignored here). Note how the lexical suffix is neither "the baby" (definite) nor "a baby" (indefinite); such referential changes are routine with incorporated nouns.
Orthographic affixes.
In orthography, the terms for affixes may be used for the smaller elements of conjunct characters. For example, Maya glyphs are generally compounds of a "main sign" and smaller "affixes" joined at its margins. These are called "prefixes, superfixes, postfixes," and "subfixes" according to their position to the left, on top, to the right, or at the bottom of the main glyph. A small glyph placed inside another is called an "infix." Similar terminology is found with the conjunct consonants of the Indic alphabets. For example, the Tibetan alphabet utilizes prefix, suffix, superfix, and subfix consonant letters.
External links.
</dl>

</doc>
<doc id="1837" url="http://en.wikipedia.org/wiki?curid=1837" title="Allegory">
Allegory

As a literary device, an allegory in its most general sense is an extended metaphor. Allegory has been used widely throughout the histories of all forms of art, largely because it readily illustrates complex ideas and concepts in ways that are comprehensible to its viewers, readers, or listeners. Allegories are typically used as literary devices or rhetorical devices that convey hidden meanings through symbolic figures, actions, imagery, and/or events, which together create the moral, spiritual, or political meaning the author wishes to convey.
One of the best known examples is Plato's Allegory of the Cave, a part of his larger work "The Republic." In this allegory, there are a group of people who have lived chained in a cave all of their lives, facing a blank wall (514a-b). The people watch shadows projected on the wall by things passing in front of a fire behind them and begin to ascribe forms to these shadows, using language to identify their world (514c-515a). According to the allegory, the shadows are as close as the prisoners get to viewing reality, until one of them finds his way into the outside world where he sees the actual objects that produced the shadows. He tries to tell the people in the cave of his discovery, but they do not believe him and vehemently resist his efforts to free them so they can see for themselves (516e-518a). This allegory is, on a basic level, about a philosopher who upon finding greater knowledge outside the cave of human understanding, seeks to share it as is his duty, and the foolishness of those who would ignore him because they think themselves educated enough.
Etymology.
First attested in English in 1382, the word "allegory" comes from Latin "allegoria", the latinisation of the Greek ἀλληγορία ("allegoria"), "veiled language, figurative," which in turn comes from both ἄλλος ("allos"), "another, different" and ἀγορεύω ("agoreuo"), "to harangue, to speak in the assembly" which originate from ἀγορά ("agora"), "assembly".
Types.
Northrop Frye discussed what he termed a "continuum of allegory", a spectrum that ranges from what he termed the "naive allegory" of "The Faerie Queene", to the more private allegories of modern paradox literature. In this perspective, the characters in a "naive" allegory are not fully three-dimensional, for each aspect of their individual personalities and the events that befall them embodies some moral quality or other abstraction; the allegory has been selected first, and the details merely flesh it out.
Many ancient religions are based on astrological allegories, that is, allegories of the movement of the sun and the moon as seen from the Earth. Examples include the cult of Horus/Isis.
Classical allegory.
In classical literature two of the best-known allegories are the Cave in Plato's "Republic" (Book VII) and the story of the stomach and its members in the speech of Menenius Agrippa (Livy ii. 32). In Late Antiquity Martianus Capella organized all the information a fifth-century upper-class male needed to know into an allegory of the wedding of Mercury and "Philologia," with the seven liberal arts the young man needed to know as guests.
Other early allegories are found in the Hebrew Bible, such as the extended metaphor in Psalm 80 of the Vine and its impressive spread and growth, representing Israel's conquest and peopling of the Promised Land. Also allegorical is Ezekiel 16 and 17, wherein the capture of that same vine by the mighty Eagle represents Israel's exile to Rome.
Medieval allegory.
Allegory has an ability to freeze the temporality of a story, while infusing it with a spiritual context. Medieval thinking accepted allegory as having a "reality" underlying any rhetorical or fictional uses. The allegory was as true as the facts of surface appearances. Thus, the Papal Bull "Unam Sanctam" (1302) presents themes of the unity of Christendom with the pope as its head in which the allegorical details of the metaphors are adduced as facts on which is based a demonstration with the vocabulary of logic: ""Therefore" of this one and only Church there is one body and one head—not two heads as if it were a monster... If, then, the Greeks or others say that they were not committed to the care of Peter and his successors, they "necessarily" confess that they are not of the sheep of Christ." This text also demonstrates the frequent use of allegory in religious texts during the Medieval Period, following the tradition and example of the Bible.
In the late 15th century, the enigmatic "Hypnerotomachia", with its elaborate woodcut illustrations, shows the influence of themed pageants and masques on contemporary allegorical representation, as humanist dialectic conveyed them.
The denial of medieval allegory as found in the 11th-century works of Hugh of St Victor and Edward Topsell's "Historie of Foure-footed Beastes" (London, 1607, 1653) and its replacement in the study of nature with methods of categorization and mathematics by such figures as naturalist John Ray and the astronomer Galileo is thought to mark the beginnings of early modern science.
Modern allegory.
Since meaningful stories are nearly always applicable to larger issues, allegories may be read into many stories which the author may not have recognized. This is allegoresis, or the act of reading a story as an allegory. For instance, many people have suggested that "The Lord of the Rings" is an allegory for the World Wars, although Tolkien has dismissed this. Other examples of allegory in popular culture that may or may not have been intended include the works of Bertolt Brecht, and even some works of science fiction and fantasy, such as "The Chronicles of Narnia" by C.S. Lewis and "A Kingdom Far and Clear: The Complete Swan Lake Trilogy" by Mark Helprin.
Poetry and fiction.
It is important to note that while allegoresis may make discovery of allegory in any work, not every resonant work of modern fiction is allegorical, and some are clearly not intended to be viewed this way. According to Henry Littlefield's 1964 article, L. Frank Baum's "The Wonderful Wizard of Oz", may be readily understood as a plot-driven fantasy narrative in an extended fable with talking animals and broadly sketched characters, intended to discuss the politics of the time. Yet, George MacDonald emphasized in 1893 that, "A fairy tale is not an allegory," in direct reference to "The Wonderful Wizard of Oz". J.R.R. Tolkien's The Lord of the Rings is another example of a well-known work sometimes perceived as allegorical, yet as the author himself once stated, "...I cordially dislike allegory in all of its manifestations and I have always done so since I grew old and wary enough to detect its presence." While this does not mean his works may not be treated as having allegorical themes, especially when reinterpreted through postmodern sensibilities, it at least suggests that none were conscious in his writings. This further reinforces the idea of forced allegoresis, as allegory is often a matter of interpretation and only sometimes of original artistic intention.
Like allegorical stories, allegorical poetry has two meanings – a literal meaning and a symbolic meaning. Some unique specimens of allegory in poetry can be found in the following works:
Art.
Some elaborate and successful specimens of allegory are to be found in the following works, arranged in approximate chronological order:
Video games.
As video games have become a more legitimate medium for artistic expression, allegorical representations have grown more commonplace.

</doc>
<doc id="1839" url="http://en.wikipedia.org/wiki?curid=1839" title="Allotropy">
Allotropy

Allotropy or allotropism (from " "ἄλλος" (allos)", meaning "other", and " "τρόπος" (tropos)", meaning "manner, form") is the property of some chemical elements to exist in two or more different forms, in the same physical state, known as "allotropes" of these elements. Allotropes are different structural modifications of an element; the atoms of the element are bonded together in a different manner. For example, the allotropes of carbon include diamond (where the carbon atoms are bonded together in a tetrahedral lattice arrangement), graphite (where the carbon atoms are bonded together in sheets of a hexagonal lattice), graphene (single sheets of graphite), and fullerenes (where the carbon atoms are bonded together in spherical, tubular, or ellipsoidal formations). The term "allotropy" is used for elements only, not for compounds. The more general term, used for any crystalline material, is polymorphism. Allotropy refers only to different forms of an element within the same phase (i.e. different solid, liquid or gas forms); these different states are not, themselves, considered to be examples of allotropy.
For some elements, allotropes have different molecular formulae which can persist in different phases – for example, two allotropes of oxygen (dioxygen, O2, and ozone, O3), can both exist in the solid, liquid and gaseous states. Conversely, some elements do not maintain distinct allotropes in different phases – for example phosphorus has numerous solid allotropes, which all revert to the same P4 form when melted to the liquid state.
History.
The concepts of allotropy was originally proposed in 1841 by the Swedish scientist Baron Jöns Jakob Berzelius (1779–1848). The term is derived from the Greek άλλοτροπἱα ("allotropia"; variability, changeableness). After the acceptance of Avogadro's hypothesis in 1860 it was understood that elements could exist as polyatomic molecules, and the two allotropes of oxygen were recognized as O2 and O3. In the early 20th century it was recognized that other cases such as carbon were due to differences in crystal structure.
By 1912, Ostwald noted that the allotropy of elements is just a special case of the phenomenon of polymorphism known for compounds, and proposed that the terms allotrope and allotropy be abandoned and replaced by polymorph and polymorphism. Although many other chemists have repeated this advice, IUPAC and most chemistry texts still favour the usage of allotrope and allotropy for elements only.
Differences in properties of an element's allotropes.
Allotropes are different structural forms of the same element and can exhibit quite different physical properties and chemical behaviours. The change between allotropic forms is triggered by the same forces that affect other structures, i.e. pressure, light, and temperature. Therefore the stability of the particular allotropes depends on particular conditions. For instance, iron changes from a body-centered cubic structure (ferrite) to a face-centered cubic structure (austenite) above 906 °C, and tin undergoes a modification known as tin pest from a metallic form to a semiconductor form below 13.2 °C (55.8 °F). As an example of allotropes having different chemical behaviour, ozone (O3) is a much stronger oxidizing agent than dioxygen (O2).
List of allotropes.
Typically, elements capable of variable coordination number and/or oxidation states tend to exhibit greater numbers of allotropic forms. Another contributing factor is the ability of an element to catenate.
Examples of allotropes include:
Metals.
Among the metallic elements that occur in nature in significant quantities (56 up to U, without Tc and Pm), almost half (27) are allotropic at ambient pressure: Li, Be, Na, Ca, Ti, Mn, Fe, Co, Sr, Y, Zr, Sn, La, Ce, Pr, Nd, Sm, Gd, Tb, Dy, Yb, Hf, Tl, Th, Pa and U. Some phase transitions between allotropic forms of technologically relevant metals are those of Ti at 882 °C, Fe at 912 °C and 1394 °C, Co at 422 °C, Zr at 863 °C, Sn at 13 °C and U at 668 °C and 776 °C.

</doc>
<doc id="1840" url="http://en.wikipedia.org/wiki?curid=1840" title="Agathocles of Syracuse">
Agathocles of Syracuse

Agathocles (Greek: Ἀγαθοκλῆς, "Agathoklḗs"; 361 – 289 BC) was a Greek tyrant of Syracuse (317–289 BC) and king of Sicily (304–289 BC).
Biography.
Agathocles was born at Thermae Himeraeae (modern name Termini Imerese) in Sicily. The son of a potter who had moved to Syracuse in about 343 BC, he learned his father's trade, but afterwards entered the army. In 333 BC he married the widow of his patron Damas, a distinguished and wealthy citizen. He was twice banished for attempting to overthrow the oligarchical party in Syracuse.
In 317 BC he returned with an army of mercenaries under a solemn oath to observe the democratic constitution which was established after they took the city. Having banished or murdered some 10,000 citizens, and thus made himself master of Syracuse, he created a strong army and fleet and subdued the greater part of Sicily.
War with Carthage followed. In 311 BC Agathocles was defeated in the Battle of the Himera River and besieged in Syracuse. In 310 BC he made a desperate effort to break through the blockade and attack the enemy in Africa. In Africa he concluded the treaty with Ophellas, ruler of Cyrenaica. After several victories he was at last completely defeated (307 BC) and fled secretly to Sicily.
After concluding peace with Carthage in 306 BC, Agathocles styled himself king of Sicily in 304 BC, and established his rule over the Greek cities of the island more firmly than ever. A peace treaty with Carthage left him in control of Sicily east of the Halycus River. Even in his old age he displayed the same restless energy, and is said to have been contemplating a fresh attack on Carthage at the time of his death.
His last years were plagued by ill-health and the turbulence of his grandson Archagathus, at whose instigation he is said to have been poisoned; according to others, he died a natural death. He was a born leader of mercenaries, and, although he did not shrink from cruelty to gain his ends, he afterwards showed himself a mild and popular "tyrant." Agathocles restored the Syracusan democracy on his death bed and did not want his sons to succeed him as king.
Agathocles was married three times. His first wife was the widow of his patron Damas by whom he had two sons: Archagathus and Agathocles, who were both murdered in 307 BC. His second wife was Alcia and they had a daughter called Lanassa, who married as the second wife of King Pyrrhus of Epirus. His third wife was the Greek Ptolemaic Princess Theoxena, who was the second daughter of Berenice I from her first husband Philip and was a stepdaughter of Ptolemy I Soter. Theoxena bore Agathocles two children: Archagathus and Theoxena. Theoxena survived Agathocles. He had further descendants from his second and third marriage.
Legacy.
Agathocles was cited as an example "Of Those Who By Their Crimes Come to Be Princes” in Chapter VIII of Niccolò Machiavelli’s treatise on politics, "The Prince" (1513). 
He was described as behaving as a criminal at every stage of his career. 
However, he came to "glory" as much as he did brutality by repelling invading Carthaginians and winning the loyalty of the denizens of his land. However, many later disapproved of his actions, including to an extent Machiavelli, who claimed It cannot be called prowess to kill fellow-citizens, to betray friends, to be treacherous, pitiless, irreligious. ... Still, if the courage of Agathocles in entering into and extricating himself from dangers be considered, together with his greatness of mind in enduring overcoming hardships, it cannot be seen why he should be esteemed less than the most notable captain. Nevertheless, his barbarous cruelty and inhumanity with infinite wickednesses do not permit him to be celebrated among the most excellent men.
Machiavelli goes on to reason that Agathocles' success, in contrast to other criminal tyrants, was due to his ability to mitigate his crimes by limiting them to those that 
are applied at one blow and are necessary to one's security, and that are not persisted in afterwards unless they can be turned to the advantage of the subjects.

</doc>
<doc id="1841" url="http://en.wikipedia.org/wiki?curid=1841" title="Economy of Alberta">
Economy of Alberta

 Alberta's economy is the sum of all economic activity in Alberta, Canada's fourth largest province by population. Although Alberta has a presence in many industries such as agriculture, forestry, education, tourism, finance, and manufacturing, the politics and culture of the province have been closely tied to the production of fossil energy since the 1940s. Revenue from oil and natural gas extraction has fueled a series of economic booms in the province's recent history, and economic spin offs have included petrochemical and pipelines.
Alberta's per capita GDP in 2007 was by far the highest of any province in Canada at C$74,825 (approx. US$75,000). In 2006 Alberta's per capita GDP was higher than all US states, and one of the highest figures in the world. Alberta's per capita GDP in 2007 was 61% higher than the Canadian average of C$46,441 and more than twice that of all the Maritime provinces. In 2006, the deviation from the national average was the largest for any province in Canadian history.
According to the Fraser Institute, Alberta has very high levels of economic freedom and rates Alberta as the most free economy in Canada, and second most free economy amongst U.S. states and Canadian provinces.
Economic geography.
Alberta has a small internal market, and it is relatively distant from major world markets, despite good transportation links to the rest of Canada and to the United States to the south. Alberta is located in the northwestern quadrant of North America, in a region of low population density called the Interior Plains. Alberta is landlocked, and separated by a series of mountain ranges from the nearest outlets to the Pacific Ocean, and by the Canadian Shield from ports on the Lakehead or Hudson Bay. From these ports to major populations centres and markets in Europe or Asia is several thousands of kilometers. The largest population clusters of North America (the Boston – Washington, San Francisco - San Diego, Chicago – Pittsburgh, and Quebec City – Windsor Corridors) are all thousands of kilometers away from Alberta. Partly for this reason, Alberta has never developed a large presence in the industries that have traditionally started industrialization in other places (notably the original Industrial Revolution in Great Britain) but which require large labour forces, and large internal markets or easy transportation to export markets, namely textiles, metallurgy, or transportation-related manufacturing (automotives, ships, or train cars).
Agriculture has the been a key industry since the 1870s. The climate is dry, temperate, and continental, with extreme variations between seasons. Productive soils are found in most of the southern half of the province (excluding the mountains), and in certain parts of the north. Agriculture on a large scale is practiced further north in Alberta than anywhere else in North America, extending into the Peace River country above the 55th parallel north. Generally, however, northern Alberta (and areas along the Alberta Rockies) is forested land and logging is more important than agriculture there. Agriculture is divided into primarily field crops in the east, livestock in the west, and a mixture in between and in the parkland belt in the near north.
Conventional oil and gas fields are found throughout the province on an axis running from the northwest to the southeast. Oil sands are found in the northeast, especially around Fort McMurray (the Athabasca Oil Sands).
Because of its (relatively) economically isolated location, Alberta relies heavily on transportation links with the rest of the world. Alberta's historical development has been largely influenced by the development of new transportation infrastructure, (see "trends" below). Alberta is now served by two major transcontinental railways (CN and CP), by three major highway connections to the Pacific (the Trans-Canada via Kicking Horse Pass, the Yellowhead via Yellowhead Pass and the Crowsnest via Crowsnest Pass), and one to the United States (Interstate 15), as well as two international airports (Calgary and Edmonton). As well Alberta is connected to the TasnCanada pipeline system (natural gas) to Eastern Canada, the Northern Border Pipeline (gas), Alliance Pipeline (gas) and Enbridge Pipeline System (oil) to the Eastern United States, the Gas Transmission Northwest and Northwest Pipeline (gas) to the Western United States, and the McNeill HVDC Back-to-back station (electric power) to Saskatchewan.
Economic regions and cities.
Since the days of early agricultural settlement, the majority of Alberta's population has been concentrated in the parkland belt (mixed forest-grassland), a boomerang-shaped strip of land extending along the North Saskatchewan River from Lloydminster to Edmonton and then along the Rocky Mountain foothills south to Calgary. This area is slightly more humid and treed than the drier prairie (grassland) region called Palliser's Triangle to its south, and large areas of the south (the “Special Areas”) were depopulated during the droughts of the 1920s and 30s. The chernozem (black soil) of the parkland region is more agriculturally productive than the red and grey soils to the south. Urban development has also been most advanced in the parkland belt. Edmonton and Red Deer are parkland cities, while Calgary is on the parkland-prairie fringe. Lethbridge and Medicine Hat are prairie cities. Grande Prairie lies in the Peace River Country a parkland region (with isolated patches of prairie, hence the name) in the northwest isolated from the rest of the parkland by the forested Swan Hills. Fort McMurray is the only urbanized population centre in the boreal forest which covers much of the northern half of the province.
Calgary and Edmonton.
The Calgary and Edmonton regions, by far the province's two largest metropolitan regions, account for the majority of the province’s population. They are relatively close to each other by the standards of Western Canada and distant from other metropolitan regions such as Vancouver or Winnipeg. This has produced a history of political and economical rivalry and comparison but also economic integration that has created an urbanized corridor between the two cities.
The economic profile of the two regions is slightly different. Both cities are mature service economies built on a base of resource extraction in their hinterlands. However Calgary is predominant in hosting the regional and national headquarters of oil and gas exploration and drilling companies. Edmonton skews much more towards governments, universities and hospitals as large employers, while Edmonton’s suburban fringes (e.g. Fort Saskatchewan, Nisku, Strathcona County (Refinery Row), Leduc, Beaumont, Acheson) are home to most of the province’s manufacturing (much of it related to oil and gas).
Calgary-Edmonton Corridor.
The Calgary-Edmonton Corridor is the most urbanized region in the province and one of the densest in Canada. Measured from north to south, the region covers a distance of roughly 400 km. In 2001, the population of the Calgary-Edmonton Corridor was 2.15 million (72% of Alberta's population). It is also one of the fastest growing regions in the country. A 2003 study by TD Bank Financial Group found the corridor was the only Canadian urban centre to amass a U.S. level of wealth while maintaining a Canadian-style quality of life, offering universal health care benefits. The study found GDP per capita in the corridor was 10% above average U.S. metropolitan areas and 40% above other Canadian cities at that time.
Calgary–Edmonton rivalry.
Seeing Calgary and Edmonton are part of a single economic region as the TD study did in 2003 was novel. The more traditional view had been to see the two cities as economic rivals. For example, in the 1980 both cities claimed to be the "Oil Capital of Canada".
Trends.
Alberta has always been an export-oriented economy. In line with Harold Innis' "Staples Thesis", the economy has changed substantially as different export commodities have risen or fallen in importance. In sequence, the most important products have been: fur, wheat and beef, and oil and gas.
The development of transportation in Alberta has been crucial to its historical economic development. The North American fur trade relied on birch-bark canoes, York boats, and Red River carts on buffalo trails to move furs out of, and European trade goods into, the region. Immigration into the province was eased tremendously by the arrival of the Canadian Pacific Railway's transcontinental line in 1880s. Commercial farming became viable in the area once the grain trade had developed technologies to handle the bulk export of grain, especially hopper cars and grain elevators. Oil and gas exports have been possible because of increasing pipeline technology.
Prior to the 1950s, Alberta was a primarily agricultural economy, based on the export of wheat, beef, and a few other commodities. The health of economy was closely bound up with the price of wheat.
In 1947 a major oil field was discovered near Edmonton. It was not the first petroleum find in Alberta, but it was large enough to significantly alter the economy of the province (and coincided with growing American demand for energy). Since that time, Alberta's economic fortunes have largely tracked the price of oil, and increasingly natural gas prices. When oil prices spiked during the 1967 Oil Embargo, 1973 oil crisis, and 1979 energy crisis, Alberta's economy boomed. However, during the 1980s oil glut Alberta's economy suffered. Alberta boomed once again during the 2003-2008 oil price spike. In July 2008 the price of oil peaked and began to decline and Alberta's economy soon followed suit, with unemployment doubling within a year. By 2009 with natural gas prices at a long-term low, Alberta's economy was in poor health compared to before, although still relatively better than many other comparable jurisdictions. By 2012, despite natural gas prices at a ten-year low and a higher Canadian dollar, oil prices had recovered enough to restart economic growth.
The spin-offs from petroleum have allowed Alberta to develop many other industries. Oilpatch-related manufacturing is an obvious example, but financial services and government services have also benefited from oil money.
A comparison of the development of Alberta's less oil and gas-endowed neighbours, Saskatchewan and Manitoba, reveals the role petroleum has played. Alberta was once the smallest of the three Prairie Provinces by population in the early 20th century, but by 2009, Alberta's population was 3,632,483 or approximately three times as much as either Saskatchewan (1,023,810) or Manitoba (1,213,815).
Employment.
Alberta's economy is a highly developed one in which most people work in services such as healthcare, government, or retail. Primary industries are also of great importance, however.
Employment in extraction industries.
As of 2007:
Largest employers.
According to "Alberta Venture" magazine's list of the 50 largest employers in the province, the largest employers are:
Sectors.
Industry.
Alberta is the largest producer of conventional crude oil, synthetic crude, natural gas and gas products in the country. Alberta is the world’s 2nd largest exporter of natural gas and the 4th largest producer. Two of the largest producers of petrochemicals in North America are located in central and north central Alberta. In both Red Deer and Edmonton, world class polyethylene and vinyl manufacturers produce products shipped all over the world, and Edmonton's oil refineries provide the raw materials for a large petrochemical industry to the east of Edmonton.
The Athabasca Oil Sands (sometimes known as the Athabasca Tar sands) have estimated unconventional oil reserves approximately equal to the conventional oil reserves of the rest of the world, estimated to be 1.6 Toilbbl. With the development of new extraction methods such as steam assisted gravity drainage (SAGD), which was developed in Alberta, bitumen and synthetic crude oil can be produced at costs close to those of conventional crude. Many companies employ both conventional strip mining and non-conventional in situ methods to extract the bitumen from the oil sands. With current technology and at current prices, about 315 Goilbbl of bitumen are recoverable. Fort McMurray, one of Canada's fastest growing cities, has grown enormously in recent years because of the large corporations which have taken on the task of oil production. As of late 2006 there were over $100 billion in oil sands projects under construction or in the planning stages in northeastern Alberta.
Another factor determining the viability of oil extraction from the Tar Sands is the price of oil. The oil price increases since 2003 have made it more than profitable to extract this oil, which in the past would give little profit or even a loss.
With concerted effort and support from the provincial government, several high-tech industries have found their birth in Alberta, notably patents related to interactive liquid crystal display systems. With a growing economy, Alberta has several financial institutions dealing with civil and private funds.
Energy.
Oil and gas.
Since the early 1940s, Alberta had supplied oil and gas to the rest of Canada and the United States. The Athabasca River region produces oil for internal and external use. The Athabasca Oil Sands contain the largest proven reserves of oil in the world outside Saudi Arabia. Natural gas has been found at several points, and in 1999, the production of natural gas liquids (ethane, propane, and butanes) totalled 172.8 Moilbbl, valued at $2.27 billion. Alberta also provides 13% of all the natural gas used in the United States.
Notable gas reserves were discovered in the 1883 near Medicine Hat. The town of Medicine Hat began using gas for lighting the town, and supplying light and fuel for the people, and a number of industries using the gas for manufacturing. In fact a large glassworks was established at Redcliff. When Rudyard Kipling visited Medicine Hat he described it as the city "with all hell for a basement".
Coal.
Coal has been mined in Alberta since the late 19th century. Over 1800 mines have operated in Alberta since then.
The coal industry was vital to the early development of several communities, especially those in the foothills and along deep river valleys where coal was close to the surface.
Alberta is still a major coal producer, every two weeks Alberta produces enough coal to fill the Sky Dome in Toronto.
Much of that coal is burned in Alberta for electricity generation. Alberta uses over 25 million tonnes of coal annually to generate electricity.
Alberta has vast coal resources and 70 per cent of Canada's coal reserves are located in Alberta. This amounts to 33.6 Gigatonnes.
Vast beds of coal are found extending for hundreds of miles, a short distance below the surface of the plains. The coal belongs to the Cretaceous beds, and while not so heavy as that of the Coal Measures in England is of excellent quality. In the valley of the Bow River, alongside the Canadian Pacific Railway, valuable beds of anthracite coal are still worked. The usual coal deposits of the area of bituminous or semi-bituminous coal. These are largely worked at Lethbridge in southern Alberta and Edmonton in the centre of the province. Many other parts of the province have pits for private use.
Electricity.
s of 2007[ [update]], Alberta's generating capacity was 11,919 MW, and Alberta has about 21000 km of transmission lines.
Alberta has over 490 megawatts of wind power capacity.
Alberta has added 4400 MW of new supply since 1998 – that is equal to all the power generated in Saskatchewan.
Winter peak for power use in one day was in November 2006 – 9,661 MW.
Summer peak for power use in one day was set on July 18, 2007 – 9,192 MW.
Mineral mining.
Building stones mined in Alberta include Rundle stone, and Paskapoo sandstone.
Diamonds were first found in Alberta in 1958, and many stones have been found since, although to date no large-scale mines have been developed.
Manufacturing.
The Edmonton area, and in particular Nisku is a major centre for manufacturing oil and gas related equipment. As well Edmonton's Refinery Row is home to a petrochemical industry.
Biotechnology.
Several companies and services in the biotech sector are clustered around the University of Alberta, for example ColdFX.
Food processing.
Owing to the strength of agriculture, food processing was once a major part of the economies of Edmonton and Calgary, but this sector has increasingly moved to smaller centres such as Brooks, the home of XL Foods, responsible for one third of Canada's beef processing in 2011.
Transportation.
Edmonton is a major distribution centre for northern communities, hence the nickname "Gateway to the North". Edmonton is one CN Rail's most important hubs. Since 1996, Canadian Pacific Railway has its headquarters in downtown Calgary.
WestJet, Canada's second largest air carrier, is headquartered in Calgary, by Calgary International Airport, which serves as the airline's primary hub. Prior to its dissolution, Canadian Airlines was headquartered in Calgary by the airport. Prior to its dissolution, Air Canada subsidiary Zip was headquartered in Calgary.
Agriculture and forestry.
Agriculture.
In the past, cattle, horses, and sheep were reared in the southern prairie region on ranches or smaller holdings. Currently Alberta produces cattle valued at over $3.3 billion, as well as other livestock in lesser quantities. In this region irrigation is widely used. Wheat, accounting for almost half of the $2 billion agricultural economy, is supplemented by canola, barley, rye, sugar beets, and other mixed farming. In 2011, Alberta producers seeded an estimated total of 17900000000 acres to spring wheat, durum, barley, oats, mixed grains, triticale, canola and dry peas. Of the total seeded area, 94 per cent was harvested as grains and oilseeds and six per cent as greenfeed and silage.
Agriculture has a significant position in the province's economy. Over three million cattle are residents of the province at one time or another, and Albertan beef has a healthy worldwide market. Nearly one half of all Canadian beef is produced in Alberta. Alberta is one of the prime producers of plains buffalo (bison) for the consumer market. Sheep for wool and lamb are also raised.
Wheat and canola are primary farm crops, with Alberta leading the provinces in spring wheat production, with other grains also prominent. Much of the farming is dryland farming, often with fallow seasons interspersed with cultivation. Continuous cropping (in which there is no fallow season) is gradually becoming a more common mode of production because of increased profits and a reduction of soil erosion. Across the province, the once common grain elevator is slowly being lost as rail lines are decreased and farmers now truck the grain to central points.
Alberta is the leading beekeeping province of Canada, with some beekeepers wintering hives indoors in specially designed barns in southern Alberta, then migrating north during the summer into the Peace River valley where the season is short but the working days are long for honeybees to produce honey from clover and fireweed. Hybrid canola also requires bee pollination, and some beekeepers service this need.
Forestry.
The vast northern forest reserves of softwood allow Alberta to produce large quantities of lumber, oriented strand board (OSB) and plywood, and several plants in northern Alberta supply North America and the Pacific Rim nations with bleached wood pulp and newsprint.
In 1999, lumber products from Alberta were valued at $4.1 billion of which 72% were exported around the world. Since forests cover approximately 59% of the province's land area, the government allows about 23.3 e6m3 to be harvested annually from the forests on public lands.
Services.
Despite the high profile of the extractive industries, Alberta has a mature economy and most people work in services.
Finance.
The TSX Venture Exchange is headquartered in Calgary, and Calgary also has a robust service industry relating to the securities market. The city has the second highest number of corporate head offices in Canada after Toronto, and the financial services industry in Calgary has developed to support them.
Edmonton hosts the headquarters of the only major Canadian banks west of Toronto: Canadian Western Bank, and ATB Financial, as well as the only province-wide credit union, Servus.
Government.
Despite Alberta's reputation as a "small government" province, many health care and education professionals are lured to Alberta from other provinces by the higher wages the Alberta government is able to offer because of oil revenues.

</doc>
<doc id="1842" url="http://en.wikipedia.org/wiki?curid=1842" title="Augustin-Louis Cauchy">
Augustin-Louis Cauchy

Baron Augustin-Louis Cauchy (]; 21 August 1789 – 23 May 1857) was a French mathematician reputed as a pioneer of analysis. He was one of the first to state and prove theorems of calculus rigorously, rejecting the heuristic principle of the generality of algebra of earlier authors. He almost singlehandedly founded complex analysis and the study of permutation groups in abstract algebra. A profound mathematician, Cauchy had a great influence over his contemporaries and successors. His writings range widely in mathematics and mathematical physics.
"More concepts and theorems have been named for Cauchy than for any other mathematician (in elasticity alone there are sixteen concepts and theorems named for Cauchy)." Cauchy was a prolific writer; he wrote approximately eight hundred research articles and five complete textbooks. He was a devout Roman Catholic, strict Bourbon royalist, and a close associate of the Jesuit order.
Biography.
Youth and education.
Cauchy was the son of Louis François Cauchy (1760–1848) and Marie-Madeleine Desestre. Cauchy had two brothers, Alexandre Laurent Cauchy (1792–1857), who became a president of a division of the court of appeal in 1847, and a judge of the court of cassation in 1849; and Eugene François Cauchy (1802–1877), a publicist who also wrote several mathematical works.
Cauchy married Aloise de Bure in 1818. She was a close relative of the publisher who published most of Cauchy's works. By her he had two daughters, Marie Françoise Alicia (1819) and Marie Mathilde (1823).
Cauchy's father (Louis François Cauchy) was a high official in the Parisian Police of the New Régime. He lost his position because of the French Revolution (July 14, 1789) that broke out one month before Augustin-Louis was born. The Cauchy family survived the revolution and the following Reign of Terror (1794) by escaping to Arcueil, where Cauchy received his first education, from his father. After the execution of Robespierre (1794), it was safe for the family to return to Paris. There Louis-François Cauchy found himself a new bureaucratic job, and quickly moved up the ranks. When Napoleon Bonaparte came to power (1799), Louis-François Cauchy was further promoted, and became Secretary-General of the Senate, working directly under Laplace (who is now better known for his work on mathematical physics). The famous mathematician Lagrange was also no stranger in the Cauchy family.
On Lagrange's advice, Augustin-Louis was enrolled in the École Centrale du Panthéon, the best secondary school of Paris at that time, in the fall of 1802. Most of the curriculum consisted of classical languages; the young and ambitious Cauchy, being a brilliant student, won many prizes in Latin and Humanities. In spite of these successes, Augustin-Louis chose an engineering career, and prepared himself for the entrance examination to the École Polytechnique.
In 1805 he placed second out of 293 applicants on this exam, and he was admitted. One of the main purposes of this school was to give future civil and military engineers a high-level scientific and mathematical education. The school functioned under military discipline, which caused the young and pious Cauchy some problems in adapting. Nevertheless, he finished the Polytechnique in 1807, at the age of 18, and went on to the École des Ponts et Chaussées (School for Bridges and Roads). He graduated in civil engineering, with the highest honors.
Engineering days.
After finishing school in 1810, Cauchy accepted a job as a junior engineer in Cherbourg, where Napoleon intended to build a naval base. Here Augustin-Louis stayed for three years, and although he had an extremely busy managerial job, he still found time to prepare three mathematical manuscripts, which he submitted to the "Première Classe" (First Class) of the Institut de France. Cauchy's first two manuscripts (on polyhedra) were accepted; the third one (on directrices of conic sections) was rejected.
In September 1812, now 23 years old, after becoming ill from overwork, Cauchy returned to Paris. Another reason for his return to the capital was that he was losing his interest in his engineering job, being more and more attracted to the abstract beauty of mathematics; in Paris, he would have a much better chance to find a mathematics related position. Although he formally kept his engineering position, he was transferred from the payroll of the Ministry of the Marine to the Ministry of the Interior. The next three years Augustin-Louis was mainly on unpaid sick leave, and spent his time quite fruitfully, working on mathematics (on the related topics of symmetric functions, the symmetric group and the theory of higher-order algebraic equations). He attempted admission to the First Class of the Institut de France but failed on three different occasions between 1813 and 1815. In 1815 Napoleon was defeated at Waterloo, and the newly installed Bourbon king Louis XVIII took the restoration in hand. The Académie des Sciences was re-established in March 1816; Lazare Carnot and Gaspard Monge were removed from this Academy for political reasons, and the king appointed Cauchy to take the place of one of them. The reaction by Cauchy's peers was harsh; they considered his acceptance of membership of the Academy an outrage, and Cauchy thereby created many enemies in scientific circles.
Professor at École Polytechnique.
In November 1815, Louis Poinsot, who was an associate professor at the École Polytechnique, asked to be exempted from his teaching duties for health reasons. Cauchy was by then a rising mathematical star, who certainly merited a professorship. One of his great successes at that time was the proof of Fermat's polygonal number theorem. However, the fact that Cauchy was known to be very loyal to the Bourbons, doubtless also helped him in becoming the successor of Poinsot. He finally quit his engineering job, and received a one-year contract for teaching mathematics to second-year students of the École Polytechnique. In 1816, this Bonapartist, non-religious school was reorganized, and several liberal professors were fired; the reactionary Cauchy was promoted to full professor.
When Cauchy was 28 years old, he was still living with his parents. His father found it high time for his son to marry; he found him a suitable bride, Aloïse de Bure, five years his junior. The de Bure family were printers and booksellers, and published most of Cauchy's works. Aloïse and Augustin were married on April 4, 1818, with great Roman Catholic pomp and ceremony, in the Church of Saint-Sulpice. In 1819 the couple's first daughter, Marie Françoise Alicia, was born, and in 1823 the second and last daughter, Marie Mathilde. Cauchy had two brothers: Alexandre Laurent Cauchy, who became a president of a division of the court of appeal in 1847, and a judge of the court of cassation in 1849; and Eugène François Cauchy, a publicist who also wrote several mathematical works.
The conservative political climate that lasted until 1830 suited Cauchy perfectly. In 1824 Louis XVIII died, and was succeeded by his even more reactionary brother Charles X. During these years Cauchy was highly productive, and published one important mathematical treatise after another. He received cross appointments at the Collège de France, and the Faculté des Sciences of the University.
In exile.
In July 1830 France underwent another revolution. Charles X fled the country, and was succeeded by the non-Bourbon king Louis-Philippe (of the House of Orléans). Riots, in which uniformed students of the École Polytechnique took an active part, raged close to Cauchy's home in Paris.
These events marked a turning point in Cauchy's life, and a break in his mathematical productivity. Cauchy, shaken by the fall of the government, and moved by a deep hatred of the liberals who were taking power, left Paris to go abroad, leaving his family behind. He spent a short time at Fribourg in Switzerland, where he had to decide whether he would swear a required oath of allegiance to the new regime. He refused to do this, and consequently lost all his positions in Paris, except his membership of the Academy, for which an oath was not required. In 1831 Cauchy went to the Italian city of Turin, and after some time there, he accepted an offer from the King of Sardinia (who ruled Turin and the surrounding Piedmont region) for a chair of theoretical physics, which was created especially for him. He taught in Turin during 1832–1833. In 1831, he had been elected a foreign member of the Royal Swedish Academy of Sciences.
In August 1833 Cauchy left Turin for Prague, to become the science tutor of the thirteen-year-old Duke of Bordeaux Henri d'Artois (1820–1883), the exiled Crown Prince and grandson of Charles X. As a professor of the École Polytechnique, Cauchy had been a notoriously bad lecturer, assuming levels of understanding that only a few of his best students could reach, and cramming his allotted time with too much material. The young Duke had neither taste nor talent for either mathematics or science, so student and teacher were a perfect mismatch. Although Cauchy took his mission very seriously, he did this with great clumsiness, and with surprising lack of authority over the Duke.
During his civil engineering days, Cauchy once had been briefly in charge of repairing a few of the Parisian sewers, and he made the mistake of telling his pupil this; with great malice, the young Duke went about saying that Mister Cauchy started his career in the sewers of Paris. His role as tutor lasted until the Duke became eighteen years old, in September 1838. Cauchy did hardly any research during those five years, while the Duke acquired a lifelong dislike of mathematics. The only good that came out of this episode was Cauchy's promotion to Baron, a title that Cauchy set great store by. In 1834, his wife and two daughters moved to Prague, and Cauchy was finally reunited with his family, after four years of exile.
Last years.
Cauchy returned to Paris and his position at the Academy of Sciences late in 1838. He could not regain his teaching positions, because he still refused to swear an oath of allegiance. However, he desperately wanted to regain a formal position in Parisian science.
In August 1839 a vacancy appeared in the Bureau des Longitudes. This Bureau had some resemblance to the Academy; for instance, it had the right to co-opt its members. Further, it was believed that members of the Bureau could "forget" about the oath of allegiance, although formally, unlike the Academicians, they were obliged to take it. The Bureau des Longitudes was an organization founded in 1795 to solve the problem of determining position on sea – mainly the longitudinal coordinate, since latitude is easily determined from the position of the sun. Since it was thought that position on sea was best determined by astronomical observations, the Bureau had developed into an organization resembling an academy of astronomical sciences.
In November 1839 Cauchy was elected to the Bureau, and discovered immediately that the matter of the oath was not so easily dispensed with. Without his oath, the king refused to approve his election. For four years Cauchy was in the absurd position of being elected, but not being approved; hence, he was not a formal member of the Bureau, did not receive payment, could not participate in meetings, and could not submit papers. Still Cauchy refused to take any oaths; however, he did feel loyal enough to direct his research to celestial mechanics. In 1840, he presented a dozen papers on this topic to the Academy. He also described and illustrated the signed-digit representation of numbers, an innovation presented in England in 1727 by John Colson. The confounded membership of the Bureau lasted until the end of 1843, when Cauchy was finally replaced by Poinsot.
Throughout the nineteenth century the French educational system struggled over the separation of Church and State. After losing control of the public education system, the Catholic Church sought to establish its own branch of education and found in Cauchy a staunch and illustrious ally. He lent his prestige and knowledge to the École Normale Écclésiastique, a school in Paris run by Jesuits, for training teachers for their colleges. He also took part in the founding of the Institut Catholique. The purpose of this institute was to counter the effects of the absence of Catholic university education in France. These activities did not make Cauchy popular with his colleagues who, on the whole, supported the Enlightenment ideals of the French Revolution. When a chair of mathematics became vacant at the Collège de France in 1843, Cauchy applied for it, but got just three out of 45 votes.
The year 1848 was the year of revolution all over Europe; revolutions broke out in numerous countries, beginning in France. King Louis-Philippe, fearful of sharing the fate of Louis XVI, fled to England. The oath of allegiance was abolished, and the road to an academic appointment was finally clear for Cauchy. On March 1, 1849, he was reinstated at the Faculté de Sciences, as a professor of mathematical astronomy. After political turmoil all through 1848, France chose to become a Republic, under the Presidency of Louis Napoleon Bonaparte, nephew of Napoleon Bonaparte, and son of Napoleon's brother, who had been installed as the first king of Holland. Soon (early 1852) the President made himself Emperor of France, and took the name Napoleon III.
Not unexpectedly, the idea came up in bureaucratic circles that it would be useful to again require a loyalty oath from all state functionaries, including university professors. Not always does history repeat itself, however, because this time a cabinet minister was able to convince the Emperor to exempt Cauchy from the oath. Cauchy remained a professor at the University until his death at the age of 67. He received the Last Rites and died at 4 a.m. on May 23, 1857.
His name is one of the 72 names inscribed on the Eiffel Tower.
Work.
Early work.
The genius of Cauchy was illustrated in his simple solution of the problem of Apollonius—describing a circle touching three given circles—which he discovered in 1805, his generalization of Euler's formula on polyhedra in 1811, and in several other elegant problems. More important is his memoir on wave propagation, which obtained the Grand Prix of the French Academy of Sciences in 1816. Cauchy's writings covered notable topics including: the theory of series, where he developed the notion of convergence and discovered many of the basic formulas for q-series. In the theory of numbers and complex quantities, he was the first to define complex numbers as pairs of real numbers. He also wrote on the theory of groups and substitutions, the theory of functions, differential equations and determinants.
Wave theory, mechanics, elasticity.
In the theory of light he worked on Fresnel's wave theory and on the dispersion and polarization of light. He also contributed significant research in mechanics, substituting the notion of the continuity of geometrical displacements for the principle of the continuity of matter. He wrote on the equilibrium of rods and elastic membranes and on waves in elastic media. He introduced a 3 × 3 symmetric matrix of numbers that is now known as the Cauchy stress tensor. In elasticity, he originated the theory of stress, and his results are nearly as valuable as those of Siméon Poisson.
Number theory.
Other significant contributions include being the first to prove the Fermat polygonal number theorem.
Complex functions.
Cauchy is most famous for his single-handed development of complex function theory. The first pivotal theorem proved by Cauchy, now known as "Cauchy's integral theorem", was the following:
where "f"("z") is a complex-valued function holomorphic on and within the non-self-intersecting closed curve "C" (contour) lying in the complex plane. The "contour integral" is taken along the contour "C". The rudiments of this theorem can already be found in a paper that the 24-year-old Cauchy presented to the Académie des Sciences (then still called "First Class of the Institute") on August 11, 1814. In full form the theorem was given in 1825. The 1825 paper is seen by many as Cauchy's most important contribution to mathematics.
In 1826 Cauchy gave a formal definition of a residue of a function. This concept regards functions that have poles—isolated singularities, i.e., points where a function goes to positive or negative infinity. If the complex-valued function "f"("z") can be expanded in the neighborhood of a singularity "a" as
where φ("z") is analytic (i.e., well-behaved without singularities), then "f" is said to have a pole of order "n" in the point "a". If "n" = 1, the pole is called simple.
The coefficient "B"1 is called by Cauchy the residue of function "f" at "a". If "f" is non-singular at "a" then the residue of "f" is zero at "a". Clearly the residue is in the case of a simple pole equal to,
where we replaced "B"1 by the modern notation of the residue.
In 1831, while in Turin, Cauchy submitted two papers to the Academy of Sciences of Turin. In the first he proposed the formula now known as Cauchy's integral formula,
where "f"("z") is analytic on "C" and within the region bounded by the contour "C" and the complex number "a" is somewhere in this region. The contour integral is taken counter-clockwise. Clearly, the integrand has a simple pole at "z" = "a". In the second paper he presented the residue theorem,
where the sum is over all the "n" poles of "f"("z") on and within the contour "C". These results of Cauchy's still form the core of complex function theory as it is taught today to physicists and electrical engineers. For quite some time, contemporaries of Cauchy ignored his theory, believing it to be too complicated. Only in the 1840s the theory started to get response, with Pierre-Alphonse Laurent being the first mathematician, besides Cauchy, making a substantial contribution (his Laurent series published in 1843).
Cours d'Analyse.
 In his book "Cours d'Analyse" Cauchy stressed the importance of rigor in analysis. "Rigor" in this case meant the rejection of the principle of "Generality of algebra" (of earlier authors such as Euler and Lagrange) and its replacement by geometry and infinitesimals. Judith Grabiner wrote Cauchy was "the man who taught rigorous analysis to all of Europe." The book is frequently noted as being the first place that inequalities, and formula_6 arguments were introduced into Calculus. Here Cauchy defined continuity as follows: "The function f(x) is continuous with respect to x between the given limits if, between these limits, an infinitely small increment in the variable always produces an infinitely small increment in the function itself."
M. Barany claims that the École mandated the inclusion of infinitesimal methods against Cauchy's better judgement . Gilain notes that when the portion of the curriculum devoted to "Analyse Algébrique" was reduced in 1825, Cauchy insisted on placing the topic of continuous functions (and therefore also infinitesimals) at the beginning of the Differential Calculus. Laugwitz (1989) and Benis-Sinaceur (1973) point out that Cauchy continued to use infinitesimals in his own research as late as 1853.
Cauchy gave an explicit definition of an infinitesimal in terms of a sequence tending to zero. There has been a vast body of literature written about Cauchy's notion of "infinitesimally small quantities", arguing they lead from everything from the usual "epsilontic" definitions or to the notions of non-standard analysis. The consensus is that Cauchy omitted or left implicit the important ideas to make clear the precise meaning of the infinitely small quantities he used. 
Taylor's theorem.
He was the first to prove Taylor's theorem rigorously, establishing his well-known form of the remainder. He wrote a textbook (see the illustration) for his students at the École Polytechnique in which he developed the basic theorems of mathematical analysis as rigorously as possible. In this book he gave the necessary and sufficient condition for the existence of a limit in the form that is still taught. Also Cauchy's well-known test for absolute convergence stems from this book: Cauchy condensation test. In 1829 he defined for the first time a complex function of a complex variable in another textbook. In spite of these, Cauchy's own research papers often used intuitive, not rigorous, methods; thus one of his theorems was exposed to a "counter-example" by Abel, later fixed by the introduction of the notion of uniform continuity.
Argument principle, stability.
In a paper published in 1855, two years before Cauchy's death, he discussed some theorems, one of which is similar to the "Argument Principle" in many modern textbooks on complex analysis. In modern control theory textbooks, the Cauchy argument principle is quite frequently used to derive the Nyquist stability criterion, which can be used to predict the stability of negative feedback amplifier and negative feedback control systems. Thus Cauchy's work has a strong impact on both pure mathematics and practical engineering.
Output.
Cauchy was very productive, in number of papers second only to Leonhard Euler. It took almost a century to collect all his writings into 27 large volumes:
His greatest contributions to mathematical science are enveloped in the rigorous methods which he introduced; these are mainly embodied in his three great treatises:
His other works include:
Politics and religious beliefs.
Augustin-Louis Cauchy grew up in the house of a staunch royalist. This made his father flee with the family to Arcueil during the French Revolution. Their life there during that time was apparently hard; Augustin-Louis's father, Louis François, spoke of living on rice, bread, and crackers during the period. A paragraph from an undated letter from Louis François to his mother in Rouen says:
We never had more than a half pound of bread — and sometimes not even that. This we supplement with little supply of hard crackers and rice that we are allotted. Otherwise, we are getting along quite well, which is the important thing and goes to show that human beings can get by with little. I should tell you that for my children's pap I still have a bit of fine flour, made from wheat that I grew on my own land. I had three bushels, and I also have a few pounds of potato starch. It is as white as snow and very good, too, especially for very young children. It, too, was grown on my own land.
In any event, he inherited his father's staunch royalism and hence refused to take oaths to any government after the overthrow of Charles X.
He was an equally staunch Catholic and a member of the Society of Saint Vincent de Paul. He also had links to the Society of Jesus and defended them at the Academy when it was politically unwise to do so. His zeal for his faith may have led to his caring for Charles Hermite during his illness and leading Hermite to become a faithful Catholic. It also inspired Cauchy to plead on behalf of the Irish during the Potato Famine.
His royalism and religious zeal also made him contentious, which caused difficulties with his colleagues. He felt that he was mistreated for his beliefs, but his opponents felt he intentionally provoked people by berating them over religious matters or by defending the Jesuits after they had been suppressed. Niels Henrik Abel called him a "bigoted Catholic" and added he was "mad and there is nothing that can be done about him", but at the same time praised him as a mathematician. Cauchy's views were widely unpopular among mathematicians and when Guglielmo Libri Carucci dalla Sommaja was made chair in mathematics before him he, and many others, felt his views were the cause. When Libri was accused of stealing books he was replaced by Joseph Liouville rather than Cauchy, which caused a rift between Liouville and Cauchy. Another dispute with political overtones concerned Jean Marie Constant Duhamel and a claim on inelastic shocks. Cauchy was later shown, by Jean-Victor Poncelet, to be wrong.

</doc>
<doc id="1844" url="http://en.wikipedia.org/wiki?curid=1844" title="Archimedes">
Archimedes

Archimedes of Syracuse (; Greek: Ἀρχιμήδης;  287 BC –  212 BC) was an Ancient Greek mathematician, physicist, engineer, inventor, and astronomer. Although few details of his life are known, he is regarded as one of the leading scientists in classical antiquity.
Generally considered the greatest mathematician of antiquity and one of the greatest of all time, Archimedes anticipated modern calculus and analysis by applying concepts of infinitesimals and the method of exhaustion to derive and rigorously prove a range of geometrical theorems, including the area of a circle, the surface area and volume of a sphere, and the area under a parabola. Other mathematical achievements include deriving an accurate approximation of pi, defining and investigating the spiral bearing his name, and creating a system using exponentiation for expressing very large numbers. He was also one of the first to apply mathematics to physical phenomena, founding hydrostatics and statics, including an explanation of the principle of the lever. He is credited with designing innovative machines, such as his screw pump, compound pulleys, and defensive war machines to protect his native Syracuse from invasion.
Archimedes died during the Siege of Syracuse when he was killed by a Roman soldier despite orders that he should not be harmed. Cicero describes visiting the tomb of Archimedes, which was surmounted by a sphere and a cylinder, which Archimedes had requested to be placed on his tomb, representing his mathematical discoveries.
Unlike his inventions, the mathematical writings of Archimedes were little known in antiquity. Mathematicians from Alexandria read and quoted him, but the first comprehensive compilation was not made until "c." 530 AD by Isidore of Miletus in Byzantine Constantinople, while commentaries on the works of Archimedes written by Eutocius in the sixth century AD opened them to wider readership for the first time. The relatively few copies of Archimedes' written work that survived through the Middle Ages were an influential source of ideas for scientists during the Renaissance, while the discovery in 1906 of previously unknown works by Archimedes in the Archimedes Palimpsest has provided new insights into how he obtained mathematical results.
Biography.
Archimedes was born "c". 287 BC in the seaport city of Syracuse, Sicily, at that time a self-governing colony in Magna Graecia, located along the coast of Southern Italy. The date of birth is based on a statement by the Byzantine Greek historian John Tzetzes that Archimedes lived for 75 years. In "The Sand Reckoner", Archimedes gives his father's name as Phidias, an astronomer about whom nothing is known. Plutarch wrote in his "Parallel Lives" that Archimedes was related to King Hiero II, the ruler of Syracuse. A biography of Archimedes was written by his friend Heracleides but this work has been lost, leaving the details of his life obscure. It is unknown, for instance, whether he ever married or had children. During his youth, Archimedes may have studied in Alexandria, Egypt, where Conon of Samos and Eratosthenes of Cyrene were contemporaries. He referred to Conon of Samos as his friend, while two of his works ("The Method of Mechanical Theorems" and the "Cattle Problem") have introductions addressed to Eratosthenes.
Archimedes died "c". 212 BC during the Second Punic War, when Roman forces under General Marcus Claudius Marcellus captured the city of Syracuse after a two-year-long siege. According to the popular account given by Plutarch, Archimedes was contemplating a mathematical diagram when the city was captured. A Roman soldier commanded him to come and meet General Marcellus but he declined, saying that he had to finish working on the problem. The soldier was enraged by this, and killed Archimedes with his sword. Plutarch also gives a lesser-known account of the death of Archimedes which suggests that he may have been killed while attempting to surrender to a Roman soldier. According to this story, Archimedes was carrying mathematical instruments, and was killed because the soldier thought that they were valuable items. General Marcellus was reportedly angered by the death of Archimedes, as he considered him a valuable scientific asset and had ordered that he not be harmed. Marcellus called Archimedes "a geometrical Briareus".
The last words attributed to Archimedes are "Do not disturb my circles", a reference to the circles in the mathematical drawing that he was supposedly studying when disturbed by the Roman soldier. This quote is often given in Latin as ""Noli turbare circulos meos"," but there is no reliable evidence that Archimedes uttered these words and they do not appear in the account given by Plutarch. Valerius Maximus, writing in "Memorable Doings and Sayings" in the 1st century AD, gives the phrase as ""...sed protecto manibus puluere 'noli' inquit, 'obsecro, istum disturbare"'" - "... but protecting the dust with his hands, said 'I beg of you, do not disturb this.'" The phrase is also given in Katharevousa Greek as "μὴ μου τοὺς κύκλους τάραττε!" ("Mē mou tous kuklous taratte!").
The tomb of Archimedes carried a sculpture illustrating his favorite mathematical proof, consisting of a sphere and a cylinder of the same height and diameter. Archimedes had proven that the volume and surface area of the sphere are two thirds that of the cylinder including its bases. In 75 BC, 137 years after his death, the Roman orator Cicero was serving as quaestor in Sicily. He had heard stories about the tomb of Archimedes, but none of the locals was able to give him the location. Eventually he found the tomb near the Agrigentine gate in Syracuse, in a neglected condition and overgrown with bushes. Cicero had the tomb cleaned up, and was able to see the carving and read some of the verses that had been added as an inscription. A tomb discovered in a hotel courtyard in Syracuse in the early 1960s was claimed to be that of Archimedes, but its location today is unknown.
The standard versions of the life of Archimedes were written long after his death by the historians of Ancient Rome. The account of the siege of Syracuse given by Polybius in his "Universal History" was written around seventy years after Archimedes' death, and was used subsequently as a source by Plutarch and Livy. It sheds little light on Archimedes as a person, and focuses on the war machines that he is said to have built in order to defend the city.
Discoveries and inventions.
Archimedes' principle.
The most widely known anecdote about Archimedes tells of how he invented a method for determining the volume of an object with an irregular shape. According to Vitruvius, a votive crown for a temple had been made for King Hiero II, who had supplied the pure gold to be used, and Archimedes was asked to determine whether some silver had been substituted by the dishonest goldsmith. Archimedes had to solve the problem without damaging the crown, so he could not melt it down into a regularly shaped body in order to calculate its density.
While taking a bath, he noticed that the level of the water in the tub rose as he got in, and realized that this effect could be used to determine the volume of the crown. For practical purposes water is incompressible, so the submerged crown would displace an amount of water equal to its own volume. By dividing the mass of the crown by the volume of water displaced, the density of the crown could be obtained. This density would be lower than that of gold if cheaper and less dense metals had been added. Archimedes then took to the streets naked, so excited by his discovery that he had forgotten to dress, crying "Eureka!" (Greek: "εὕρηκα,"heúrēka"!", meaning "I have found [it]!"). The test was conducted successfully, proving that silver had indeed been mixed in.
The story of the golden crown does not appear in the known works of Archimedes. Moreover, the practicality of the method it describes has been called into question, due to the extreme accuracy with which one would have to measure the water displacement. Archimedes may have instead sought a solution that applied the principle known in hydrostatics as Archimedes' principle, which he describes in his treatise "On Floating Bodies". This principle states that a body immersed in a fluid experiences a buoyant force equal to the weight of the fluid it displaces. Using this principle, it would have been possible to compare the density of the golden crown to that of solid gold by balancing the crown on a scale with a gold reference sample, then immersing the apparatus in water. The difference in density between the two samples would cause the scale to tip accordingly. Galileo considered it "probable that this method is the same that Archimedes followed, since, besides being very accurate, it is based on demonstrations found by Archimedes himself." In a 12th-century text titled "Mappae clavicula" there are instructions on how to perform the weighings in the water in order to calculate the percentage of silver used, and thus solve the problem. The Latin poem "Carmen de ponderibus et mensuris" of the 4th or 5th century describes the use of a hydrostatic balance to solve the problem of the crown, and attributes the method to Archimedes.
Archimedes' screw.
A large part of Archimedes' work in engineering arose from fulfilling the needs of his home city of Syracuse. The Greek writer Athenaeus of Naucratis described how King Hiero II commissioned Archimedes to design a huge ship, the "Syracusia", which could be used for luxury travel, carrying supplies, and as a naval warship. The "Syracusia" is said to have been the largest ship built in classical antiquity. According to Athenaeus, it was capable of carrying 600 people and included garden decorations, a gymnasium and a temple dedicated to the goddess Aphrodite among its facilities. Since a ship of this size would leak a considerable amount of water through the hull, the Archimedes' screw was purportedly developed in order to remove the bilge water. Archimedes' machine was a device with a revolving screw-shaped blade inside a cylinder. It was turned by hand, and could also be used to transfer water from a low-lying body of water into irrigation canals. The Archimedes' screw is still in use today for pumping liquids and granulated solids such as coal and grain. The Archimedes' screw described in Roman times by Vitruvius may have been an improvement on a screw pump that was used to irrigate the Hanging Gardens of Babylon. The world's first seagoing steamship with a screw propeller was the "SS Archimedes", which was launched in 1839 and named in honor of Archimedes and his work on the screw.
Claw of Archimedes.
The Claw of Archimedes is a weapon that he is said to have designed in order to defend the city of Syracuse. Also known as "the ship shaker," the claw consisted of a crane-like arm from which a large metal grappling hook was suspended. When the claw was dropped onto an attacking ship the arm would swing upwards, lifting the ship out of the water and possibly sinking it. There have been modern experiments to test the feasibility of the claw, and in 2005 a television documentary entitled "Superweapons of the Ancient World" built a version of the claw and concluded that it was a workable device.
Heat ray.
The 2nd century AD author Lucian wrote that during the Siege of Syracuse ("c." 214–212 BC), Archimedes destroyed enemy ships with fire. Centuries later, Anthemius of Tralles mentions burning-glasses as Archimedes' weapon. The device, sometimes called the "Archimedes heat ray", was used to focus sunlight onto approaching ships, causing them to catch fire.
This purported weapon has been the subject of ongoing debate about its credibility since the Renaissance. René Descartes rejected it as false, while modern researchers have attempted to recreate the effect using only the means that would have been available to Archimedes. It has been suggested that a large array of highly polished bronze or copper shields acting as mirrors could have been employed to focus sunlight onto a ship. This would have used the principle of the parabolic reflector in a manner similar to a solar furnace.
A test of the Archimedes heat ray was carried out in 1973 by the Greek scientist Ioannis Sakkas. The experiment took place at the Skaramagas naval base outside Athens. On this occasion 70 mirrors were used, each with a copper coating and a size of around five by three feet (1.5 by 1 m). The mirrors were pointed at a plywood mock-up of a Roman warship at a distance of around 160 feet (50 m). When the mirrors were focused accurately, the ship burst into flames within a few seconds. The plywood ship had a coating of tar paint, which may have aided combustion. A coating of tar would have been commonplace on ships in the classical era.
In October 2005 a group of students from the Massachusetts Institute of Technology carried out an experiment with 127 one-foot (30 cm) square mirror tiles, focused on a mock-up wooden ship at a range of around 100 feet (30 m). Flames broke out on a patch of the ship, but only after the sky had been cloudless and the ship had remained stationary for around ten minutes. It was concluded that the device was a feasible weapon under these conditions. The MIT group repeated the experiment for the television show "MythBusters", using a wooden fishing boat in San Francisco as the target. Again some charring occurred, along with a small amount of flame. In order to catch fire, wood needs to reach its autoignition temperature, which is around 300 °C (570 °F).
When "MythBusters" broadcast the result of the San Francisco experiment in January 2006, the claim was placed in the category of "busted" (or failed) because of the length of time and the ideal weather conditions required for combustion to occur. It was also pointed out that since Syracuse faces the sea towards the east, the Roman fleet would have had to attack during the morning for optimal gathering of light by the mirrors. "MythBusters" also pointed out that conventional weaponry, such as flaming arrows or bolts from a catapult, would have been a far easier way of setting a ship on fire at short distances.
In December 2010, "MythBusters" again looked at the heat ray story in a special edition entitled "President's Challenge". Several experiments were carried out, including a large scale test with 500 schoolchildren aiming mirrors at a mock-up of a Roman sailing ship 400 feet (120 m) away. In all of the experiments, the sail failed to reach the 210 °C (410 °F) required to catch fire, and the verdict was again "busted". The show concluded that a more likely effect of the mirrors would have been blinding, dazzling, or distracting the crew of the ship.
Other discoveries and inventions.
While Archimedes did not invent the lever, he gave an explanation of the principle involved in his work "On the Equilibrium of Planes". Earlier descriptions of the lever are found in the Peripatetic school of the followers of Aristotle, and are sometimes attributed to Archytas. According to Pappus of Alexandria, Archimedes' work on levers caused him to remark: "Give me a place to stand on, and I will move the Earth." (Greek: δῶς μοι πᾶ στῶ καὶ τὰν γᾶν κινάσω) Plutarch describes how Archimedes designed block-and-tackle pulley systems, allowing sailors to use the principle of leverage to lift objects that would otherwise have been too heavy to move. Archimedes has also been credited with improving the power and accuracy of the catapult, and with inventing the odometer during the First Punic War. The odometer was described as a cart with a gear mechanism that dropped a ball into a container after each mile traveled.
Cicero (106–43 BC) mentions Archimedes briefly in his dialogue "De re publica", which portrays a fictional conversation taking place in 129 BC. After the capture of Syracuse "c." 212 BC, General Marcus Claudius Marcellus is said to have taken back to Rome two mechanisms, constructed by Archimedes and used as aids in astronomy, which showed the motion of the Sun, Moon and five planets. Cicero mentions similar mechanisms designed by Thales of Miletus and Eudoxus of Cnidus. The dialogue says that Marcellus kept one of the devices as his only personal loot from Syracuse, and donated the other to the Temple of Virtue in Rome. Marcellus' mechanism was demonstrated, according to Cicero, by Gaius Sulpicius Gallus to Lucius Furius Philus, who described it thus:
Hanc sphaeram Gallus cum moveret, fiebat ut soli luna totidem conversionibus in aere illo quot diebus in ipso caelo succederet, ex quo et in caelo sphaera solis fieret eadem illa defectio, et incideret luna tum in eam metam quae esset umbra terrae, cum sol e regione. — When Gallus moved the globe, it happened that the Moon followed the Sun by as many turns on that bronze contrivance as in the sky itself, from which also in the sky the Sun's globe became to have that same eclipse, and the Moon came then to that position which was its shadow on the Earth, when the Sun was in line.
This is a description of a planetarium or orrery. Pappus of Alexandria stated that Archimedes had written a manuscript (now lost) on the construction of these mechanisms entitled "On Sphere-Making". Modern research in this area has been focused on the Antikythera mechanism, another device built  100 BC that was probably designed for the same purpose. Constructing mechanisms of this kind would have required a sophisticated knowledge of differential gearing. This was once thought to have been beyond the range of the technology available in ancient times, but the discovery of the Antikythera mechanism in 1902 has confirmed that devices of this kind were known to the ancient Greeks.
Mathematics.
While he is often regarded as a designer of mechanical devices, Archimedes also made contributions to the field of mathematics. Plutarch wrote: "He placed his whole affection and ambition in those purer speculations where there can be no reference to the vulgar needs of life."
Archimedes was able to use infinitesimals in a way that is similar to modern integral calculus. Through proof by contradiction (reductio ad absurdum), he could give answers to problems to an arbitrary degree of accuracy, while specifying the limits within which the answer lay. This technique is known as the method of exhaustion, and he employed it to approximate the value of π. In "Measurement of a Circle" he did this by drawing a larger regular hexagon outside a circle and a smaller regular hexagon inside the circle, and progressively doubling the number of sides of each regular polygon, calculating the length of a side of each polygon at each step. As the number of sides increases, it becomes a more accurate approximation of a circle. After four such steps, when the polygons had 96 sides each, he was able to determine that the value of π lay between 31⁄7 (approximately 3.1429) and 310⁄71 (approximately 3.1408), consistent with its actual value of approximately 3.1416. He also proved that the area of a circle was equal to π multiplied by the square of the radius of the circle (πr2). In "On the Sphere and Cylinder", Archimedes postulates that any magnitude when added to itself enough times will exceed any given magnitude. This is the Archimedean property of real numbers.
In "Measurement of a Circle", Archimedes gives the value of the square root of 3 as lying between 265⁄153 (approximately 1.7320261) and 1351⁄780 (approximately 1.7320512). The actual value is approximately 1.7320508, making this a very accurate estimate. He introduced this result without offering any explanation of how he had obtained it. This aspect of the work of Archimedes caused John Wallis to remark that he was: "as it were of set purpose to have covered up the traces of his investigation as if he had grudged posterity the secret of his method of inquiry while he wished to extort from them assent to his results." It is possible that he used an iterative procedure to calculate these values.
In "The Quadrature of the Parabola", Archimedes proved that the area enclosed by a parabola and a straight line is 4⁄3 times the area of a corresponding inscribed triangle as shown in the figure at right. He expressed the solution to the problem as an infinite geometric series with the common ratio 1⁄4:
If the first term in this series is the area of the triangle, then the second is the sum of the areas of two triangles whose bases are the two smaller secant lines, and so on. This proof uses a variation of the series 1/4 + 1/16 + 1/64 + 1/256 + · · · which sums to 1⁄3.
In "The Sand Reckoner", Archimedes set out to calculate the number of grains of sand that the universe could contain. In doing so, he challenged the notion that the number of grains of sand was too large to be counted. He wrote: "There are some, King Gelo (Gelo II, son of Hiero II), who think that the number of the sand is infinite in multitude; and I mean by the sand not only that which exists about Syracuse and the rest of Sicily but also that which is found in every region whether inhabited or uninhabited." To solve the problem, Archimedes devised a system of counting based on the myriad. The word is from the Greek μυριάς "murias", for the number 10,000. He proposed a number system using powers of a myriad of myriads (100 million) and concluded that the number of grains of sand required to fill the universe would be 8 vigintillion, or 8×1063.
Writings.
The works of Archimedes were written in Doric Greek, the dialect of ancient Syracuse. The written work of Archimedes has not survived as well as that of Euclid, and seven of his treatises are known to have existed only through references made to them by other authors. Pappus of Alexandria mentions "On Sphere-Making" and another work on polyhedra, while Theon of Alexandria quotes a remark about refraction from the now-lost "Catoptrica". During his lifetime, Archimedes made his work known through correspondence with the mathematicians in Alexandria. The writings of Archimedes were first collected by the Byzantine Greek architect Isidore of Miletus ("c". 530 AD), while commentaries on the works of Archimedes written by Eutocius in the sixth century AD helped to bring his work a wider audience. Archimedes' work was translated into Arabic by Thābit ibn Qurra (836–901 AD), and Latin by Gerard of Cremona ("c." 1114–1187 AD). During the Renaissance, the "Editio Princeps" (First Edition) was published in Basel in 1544 by Johann Herwagen with the works of Archimedes in Greek and Latin. Around the year 1586 Galileo Galilei invented a hydrostatic balance for weighing metals in air and water after apparently being inspired by the work of Archimedes.
Apocryphal works.
Archimedes' "Book of Lemmas" or "Liber Assumptorum" is a treatise with fifteen propositions on the nature of circles. The earliest known copy of the text is in Arabic. The scholars T. L. Heath and Marshall Clagett argued that it cannot have been written by Archimedes in its current form, since it quotes Archimedes, suggesting modification by another author. The "Lemmas" may be based on an earlier work by Archimedes that is now lost.
It has also been claimed that Heron's formula for calculating the area of a triangle from the length of its sides was known to Archimedes. However, the first reliable reference to the formula is given by Heron of Alexandria in the 1st century AD.
Archimedes Palimpsest.
The foremost document containing the work of Archimedes is the Archimedes Palimpsest. In 1906, the Danish professor Johan Ludvig Heiberg visited Constantinople and examined a 174-page goatskin parchment of prayers written in the 13th century AD. He discovered that it was a palimpsest, a document with text that had been written over an erased older work. Palimpsests were created by scraping the ink from existing works and reusing them, which was a common practice in the Middle Ages as vellum was expensive. The older works in the palimpsest were identified by scholars as 10th century AD copies of previously unknown treatises by Archimedes. The parchment spent hundreds of years in a monastery library in Constantinople before being sold to a private collector in the 1920s. On October 29, 1998 it was sold at auction to an anonymous buyer for $2 million at Christie's in New York. The palimpsest holds seven treatises, including the only surviving copy of "On Floating Bodies" in the original Greek. It is the only known source of "The Method of Mechanical Theorems", referred to by Suidas and thought to have been lost forever. "Stomachion" was also discovered in the palimpsest, with a more complete analysis of the puzzle than had been found in previous texts. The palimpsest is now stored at the Walters Art Museum in Baltimore, Maryland, where it has been subjected to a range of modern tests including the use of ultraviolet and x-ray light to read the overwritten text.
The treatises in the Archimedes Palimpsest are: "On the Equilibrium of Planes, On Spirals, Measurement of a Circle, On the Sphere and the Cylinder, On Floating Bodies, The Method of Mechanical Theorems" and "Stomachion".
Notes.
a. In the preface to "On Spirals" addressed to Dositheus of Pelusium, Archimedes says that "many years have elapsed since Conon's death." Conon of Samos lived "c." 280–220 BC, suggesting that Archimedes may have been an older man when writing some of his works.
b. The treatises by Archimedes known to exist only through references in the works of other authors are: "On Sphere-Making" and a work on polyhedra mentioned by Pappus of Alexandria; "Catoptrica", a work on optics mentioned by Theon of Alexandria; "Principles", addressed to Zeuxippus and explaining the number system used in "The Sand Reckoner"; "On Balances and Levers"; "On Centers of Gravity"; "On the Calendar". Of the surviving works by Archimedes, T. L. Heath offers the following suggestion as to the order in which they were written: "On the Equilibrium of Planes I", "The Quadrature of the Parabola", "On the Equilibrium of Planes II", "On the Sphere and the Cylinder I, II", "On Spirals", "On Conoids and Spheroids", "On Floating Bodies I, II", "On the Measurement of a Circle", "The Sand Reckoner".
c. Boyer, Carl Benjamin "A History of Mathematics" (1991) ISBN 0-471-54397-7 "Arabic scholars inform us that the familiar area formula for a triangle in terms of its three sides, usually known as Heron's formula — "k" = √("s"("s" − "a")("s" − "b")("s" − "c")), where "s" is the semiperimeter — was known to Archimedes several centuries before Heron lived. Arabic scholars also attribute to Archimedes the 'theorem on the broken chord' ... Archimedes is reported by the Arabs to have given several proofs of the theorem."
d. "It was usual to smear the seams or even the whole hull with pitch or with pitch and wax". In Νεκρικοὶ Διάλογοι ("Dialogues of the Dead"), Lucian refers to coating the seams of a skiff with wax, a reference to pitch (tar) or wax.
External links.
Listen to this article ()
This audio file was created from a revision of the "Archimedes" article dated 2009-03-31, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="1845" url="http://en.wikipedia.org/wiki?curid=1845" title="Alternative medicine">
Alternative medicine

Alternative medicine is any practice that is put forward as having the healing effects of medicine, but is not founded on evidence gathered using the scientific method. It consists of a wide range of health care practices, products and therapies. Examples include new and traditional medicine practices such as homeopathy, naturopathy, chiropractic, energy medicine, various forms of acupuncture, traditional Chinese medicine, Ayurvedic medicine, and Christian faith healing. The treatments are those that are not part of the science-based healthcare system, and are not backed by scientific evidence.
Complementary medicine is alternative medicine used together with conventional medical treatment in a belief, not established using the scientific method, that it "complements" (improves the efficacy of) the treatment. CAM is the abbreviation for complementary and alternative medicine. Integrative medicine (or integrative health) is the combination of the practices and methods of alternative medicine with conventional medicine.
 and treatments are not included as science-based treatments that are taught in medical schools, and are not used in medical practice where treatments are based on what is established using the scientific method. Alternative therapies lack such scientific validation, and their effectiveness is either unproved or disproved. Alternative medicine is usually based on religion, tradition, superstition, belief in supernatural energies, pseudoscience, errors in reasoning, propaganda, or fraud. Regulation and licensing of alternative medicine and health care providers varies from country to country, and state to state.
The scientific community has criticized alternative medicine as being based on misleading statements, quackery, pseudoscience, antiscience, fraud, or poor scientific methodology. Promoting alternative medicine has been called dangerous and unethical. Testing alternative medicine has been called a waste of scarce medical research resources. Critics have said "there is really no such thing as alternative medicine, just medicine that works and medicine that doesn't", and "Can there be any reasonable 'alternative' [to medicine based on evidence]?"
Types of alternative medicine.
Alternative medicine consists of a wide range of health care practices, products, and therapies. The shared feature is a claim to heal that is not based on the scientific method. Alternative medicine practices are diverse in their foundations and methodologies. Alternative medicine practices may be classified by their cultural origins or by the types of beliefs upon which they are based. Methods may incorporate or base themselves on traditional medicinal practices of a particular culture, folk knowledge, supersition, spiritual beliefs, belief in supernatural energies (antiscience), pseudoscience, errors in reasoning, propaganda, fraud, new or different concepts of health and disease, and any bases other than being proven by scientific methods. Different cultures may have their own unique traditional or belief based practices developed recently or over thousands of years, and specific practices or entire systems of practices.
Unscientific belief systems.
Alternative medical systems can be based on a common belief systems that are not consistent with facts of science, such as in naturopathy or homeopathy.
Homeopathy.
Homeopathy is a system developed in a belief that a substance that causes the symptoms of a disease in healthy people will cure similar symptoms in sick people. It was developed before knowledge of atoms and molecules, and of basic chemistry, which shows that repeated dilution as practiced in homeopathy produces only water and that homeopathy is scientifically implausible. Homeopathy is considered quackery in the medical community.
Naturopathic medicine.
Naturopathic medicine is based on a belief that the body heals itself using a supernatural vital energy that guides bodily processes, a view in conflict with the paradigm of evidence-based medicine. Many naturopaths have opposed vaccination, and "scientific evidence does not support claims that naturopathic medicine can cure cancer or any other disease".
Traditional ethnic systems.
Alternative medical systems may be based on traditional medicine practices, such as Traditional Chinese medicine, Ayurveda in India, or practices of other cultures around the world.
Traditional Chinese Medicine.
Traditional Chinese Medicine is a combination of traditional practices and beliefs developed over thousands of years in China, together with modifications made by the Communist party. Common practices include herbal medicine, acupuncture (insertion of needles in the body at specified points), massage (Tui na), exercise (qigong), and dietary therapy. The practices are based on belief in a supernatural energy called qi, considerations of Chinese Astrology and Chinese numerology, traditional use of herbs and other substances found in China, a belief that a map of the body is contained on the tongue which reflects changes in the body, and an incorrect model of the anatomy and physiology of internal organs.
The Chinese Communist Party Chairman Mao Zedong, in response to the lack of modern medical practitioners, revived acupuncture and its theory was rewritten to adhere to the political, economic and logistic necessities of providing for the medical needs of China's population. In the 1950s the "history" and theory of Traditional Chinese Medicine was rewritten as communist propaganda, at Mao's insistence, to correct the supposed "bourgeois thought of Western doctors of medicine" (p. 109). Acupuncture gained attention in the United States when President Richard Nixon visited China in 1972, and the delegation was shown a patient undergoing major surgery while fully awake, ostensibly receiving acupuncture rather than anesthesia. Later it was found that the patients selected for the surgery had both a high pain tolerance and received heavy indoctrination before the operation; these demonstration cases were also frequently receiving morphine surreptitiously through an intravenous drip that observers were told contained only fluids and nutrients.
Ayurvedic medicine.
Ayurvedic medicine is a traditional medicine of India. Ayurveda believes in the existence of three elemental substances, the doshas (called Vata, Pitta and Kapha), and states that a balance of the doshas results in health, while imbalance results in disease. Such disease-inducing imbalances can be adjusted and balanced using traditional herbs, minerals and heavy metals. Ayurveda stresses the use of plant-based medicines and treatments, with some animal products, and added minerals, including sulfur, arsenic, lead, copper sulfate. Andrew Weil, an American promoter of alternative medicine, wrote that in Ayurvedic medicine, "being 'healthy' is more than the absence of disease - it is a radiant state of vigor and energy, which is achieved by balance, or moderation, in food intake, sleep, sexual intercourse and other activities of daily life, complemented by various treatments including a wide variety of plant-based medicines".
Safety concerns have been raised about Ayurveda, with two U.S. studies finding about 20 percent of Ayurvedic Indian-manufactured patent medicines contained toxic levels of heavy metals such as lead, mercury and arsenic. Other concerns include the use of herbs containing toxic compounds and the lack of quality control in Ayurvedic facilities. Incidents of heavy metal poisoning have been attributed to the use of these compounds in the United States.
Supernatural energies and misunderstanding of energy in physics.
Bases of belief may include belief in existence of supernatural energies undetected by the science of physics, as in biofields, or in belief in properties of the energies of physics that are inconsistent with the laws of physics, as in energy medicine.
Biofields.
Biofield therapies are intended to influence energy fields that, it is purported, surround and penetrate the body. Writers such as noted astrophysicist and advocate of skeptical thinking (Scientific skepticism) Carl Sagan (1934-1996) have described the lack of empirical evidence to support the existence of the putative energy fields on which these therapies are predicated.
Acupuncture is a component of Traditional Chinese Medicine. In acupuncture, it is believed that a supernatural energy called qi flows through the universe and through the body, and helps propel the blood, blockage of which leads to disease. It is believed that insertion of needles at various parts of the body determined by astrological calculations can restore balance to the blocked flows, and thereby cure disease.
Chiropractic was developed in the belief that manipulating the spine affects the flow of a supernatural vital energy and thereby affects health and disease.
In the western version of Japanese Reiki, the palms are placed on the patient near Chakras, believed to be centers of supernatural energies, in a belief that the supernatural energies can transferred from the palms of the practitioner, to heal the patient.
Energy medicines.
Bioelectromagnetic-based therapies use verifiable electromagnetic fields, such as pulsed fields, alternating-current, or direct-current fields in an unconventional manner. Magnetic healing does not claim existence of supernatural energies, but asserts that magnets can be used to defy the laws of physics to influence health and disease.
Holistic health and mind body medicine.
Mind-body medicine takes a holistic approach to health that explores the interconnection between the mind, body, and spirit. It works under the premise that the mind can affect "bodily functions and symptoms". Mind body medicines includes healing claims made in yoga, meditation, deep-breathing exercises, guided imagery, hypnotherapy, progressive relaxation, qi gong, and tai chi.
Yoga, a method of traditional stretches, exercises, and meditations in Hinduism, may also be classified as an energy medicine insofar as its healing effects are believed to be due to a healing "life energy" that is absorbed into the body through the breath, and is thereby believed to treat a wide variety of illnesses and complaints.
Since the 1990's, tai chi (t'ai chi ch'uan) classes that purely emphasise health have become popular in hospitals, clinics, as well as community and senior centers. This has occurred as the baby boomers generation has aged and the art's reputation as a low-stress training method for seniors has become better known. There has been some divergence between those that say they practice t'ai chi ch'uan primarily for self-defence, those that practice it for its aesthetic appeal (see "wushu" below), and those that are more interested in its benefits to physical and mental health.
Qigong, chi kung, or chi gung, is a practice of aligning body, breath, and mind for health, meditation, and martial arts training. With roots in Chinese Traditional Chinese Medicine, philosophy, and martial arts, qigong is traditionally viewed as a practice to cultivate and balance qi (chi) or what has been translated as "life energy".
Herbal remedies and other substances used.
Substance based practices use substances found in nature such as herbs, foods, non-vitamin supplements and megavitamins, and minerals, and includes traditional herbal remedies with herbs specific to regions in which the cultural practices arose.
Herbalism, herbology, or herbal medicine, is use of plants for medicinal purposes, and the study of such use. Plants have been the basis for medical treatments through much of human history, and such traditional medicine is still widely practiced today.
Nonvitamin supplements include fish oil, Omega-3 fatty acid, glucosamine, echinacea, flaxseed oil or pills, and ginseng, when used under a claim to have healing effects.
Although the practice of herbalism is not strictly based on evidence gathered using the scientific method, modern medicine, does, however, make use of many plant-derived compounds as the basis for evidence-tested pharmaceutical drugs, and phytotherapy works to apply modern standards of effectiveness testing to herbs and medicines that are derived from natural sources.
The scope of herbal medicine is sometimes extended to include fungal and bee products, as well as minerals, shells and certain animal parts. "Herbal" remedies in this case, may include use of nonherbal toxic chemicals from a nonbiological sources, such as use of the poison lead in Traditional Chinese Medicine.
Body manipulation.
Manipulative and body-based practices feature manipulation or movement of body parts, such as is done in bodywork and chiropractic manipulation.
Osteopathic manipulative medicine, also known as osteopathic manipulative treatment, is a core set of techniques of osteopathy and osteopathic medicine distinguishing these fields from mainstream medicine.
Religion, faith healing, and prayer.
Religion based healing practices, such as use of prayer and the laying of hands in Christian faith healing, and shamanism, rely on belief in divine or spiritual intervention for healing.
Shamanism is a practice of many cultures around the world, in which a practitioner reaches an altered states of consciousness in order to encounter and interact with the spirit world or channel supernatural energies in the belief they can heal.
Alternative medicines based on exploitation of ignorance and flawed reasoning.
Some alternative medicine practices may be based on pseudoscience, ignorance, or flawed reasoning. This can lead to fraud.
Practitioners of electricity and magnetism based healing methods may deliberately exploit a patient's ignorance of physics in order to defraud them.
Definitions and terminology.
The science and medical science communities generally define "alternative medicine" as any practice that is put forward as having the healing effects of medicine, but is not founded on evidence gathered with the scientific method. An "alternative medical "system"" is a "set of" practices based on a theory that is different from biomedicine. "Biomedicine" applies principles of biology, physiology, and other natural sciences to clinical practice, and uses scientific methods to establish efficacy. Alternative medicine is a group of diverse medical and health care systems, practices, and products that originate outside of biomedicine, are not considered part of biomedicine, are not widely used by the biomedical healthcare professions, and are not taught as skills practiced in biomedicine.
"Complementary medicine" refers to use of alternative medicine alongside conventional medicine, in the belief that it increases the effectiveness of the science-based medicine. An example of “complementary medicine” is use of acupuncture (sticking needles in the body to influence the flow of a supernatural energy), along with using science-based medicine, in the belief that the acupuncture increases the effectiveness or "complements" the science-based medicine. "CAM" is an abbreviation for "complementary and alternative medicine". The expression "Integrative medicine" (or "integrated medicine") is used in two different ways. One use refers to a belief that medicine based on science can be "integrated" with practices that are not. Another use refers only to a combination of alternative medical treatments with conventional treatments that have some scientific proof of efficacy, in which case it is identical with CAM. "Holistic medicine" (or holistic health) is an alternative medicine practice which claim to treat the "whole person" and not just the illness itself. 
A committee representing salient CAM perspectives adopted a definition with a goal to promote use of CAM therapies. It is based on on a practitioner's or patient's "perception" that the practice is associated with healing effects, not that the efficacy is established using scientific methods: "Complementary and alternative medicine (CAM) is a broad domain of resources that encompasses health systems, modalities, and practices and their accompanying theories and beliefs, other than those intrinsic to the dominant health system of a particular society or culture in a given historical period. CAM includes such resources perceived by their users as associated with positive health outcomes. Boundaries within CAM and between the CAM domain and the domain of the dominant system are not always sharp or fixed.":19
"Traditional medicine" and "folk medicine refer to prescientific practices of a culture, not to what is traditionally practiced in cultures where medical science dominates. "Eastern medicine" typically refers to prescientific traditional medicines of Asia. "Western medicine", when referring to modern practice, typically refers to medical science, and not to alternative medicines practiced in the west (Europe and the Americas). "Western medicine", "biomedicine", "mainstream medicine", “medical science”, “science-based medicine”, “evidence based medicine”, "conventional medicine", "standard medicine", "orthodox medicine", “allopathic medicine”, "dominant health system", and "medicine", are sometimes used interchangeably as having the same meaning (synonyms), when contrasted with alternative medicine, but these terms may have different meanings in some contexts, e.g., some practices in medical science are not supported by rigorous scientific testing, so medical science as practiced is not strictly identical with science-based medicine.
"Alternative medicine", "complementary medicine", "holistic medicine", "natural medicine", "unorthodox medicine", "fringe medicine", and "unconventional medicine" may be used interchangeably as having the same meaning in some contexts, but may have different meanings in other contexts, for example, unorthodox medicine may refer to biomedicine that is different from what is commonly practiced, and fringe medicine may refer to biomedicine that is based on fringe science, which may be valid but is not mainstream. 
The meaning of the term "alternative" in the expression "alternative medicine", is not that it is an actual effective alternative to medical science, although some alternative medicine promoters may use the loose terminology to give the appearance of effectiveness. Loose terminology may also be used to suggest meaning that a dichotomy exists when it does not, e.g., the use of the expressions "western medicine" and "eastern medicine" to suggest that the difference is a cultural difference between the Asiatic east and the European west, rather than that the difference is between evidence based medicine and treatments which don't work.
Problems with defining alternative medicine.
Prominent members of the science and biomedical science community assert that it is not meaningful to define an alternative medicine that is separate from a conventional medicine, that the expressions "conventional medicine", "alternative medicine", "complementary medicine", "integrative medicine", and “holistic medicine” do not refer to anything at all. Their criticisms of trying to make such artificially definitions include "There's no such thing as conventional or alternative or complementary or integrative or holistic medicine. There's only medicine that works and medicine that doesn't", "You know what they call alternative medicine that's been proved to work? Medicine", "There cannot be two kinds of medicine – conventional and alternative. There is only medicine that has been adequately tested and medicine that has not, medicine that works and medicine that may or may not work. Once a treatment has been tested rigorously, it no longer matters whether it was considered alternative at the outset. If it is found to be reasonably safe and effective, it will be accepted", and "There is no alternative medicine. There is only scientifically proven, evidence-based medicine supported by solid data or unproven medicine, for which scientific evidence is lacking." Others in both the biomedical and CAM communities point out that CAM "cannot" be precisely defined because of the diversity of theories and practices it includes, and because the boundaries between CAM and biomedicine overlap, are porous, and change.
The expression "complementary and alternative medicine" (CAM) resists easy definition because the health systems and practices to which it refers are diffuse and its boundaries are poorly defined.
Healthcare practices categorized as alternative may differ in their historical origin, theoretical basis, diagnostic technique, therapeutic practice and in their relationship to the medical mainstream. Some alternative therapies, including traditional Chinese Medicine (TCM) and Ayurveda, have antique origins in East or South Asia and are entirely alternative medical systems; others, such as homeopathy and chiropractic, have origins in Europe or the United States and emerged in the eighteenth and nineteenth centuries. Some, such as osteopathy and chiropractic, employ manipulative physical methods of treatment; others, such as meditation and prayer, are based on mind-body interventions. Treatments considered alternative in one location may be considered conventional in another. Thus, chiropractic is not considered alternative in Denmark and likewise osteopathic medicine is no longer thought of as an alternative therapy in the United States.
One common feature of all definitions of alternative medicine is its designation as "other than" conventional medicine. For example, the widely referenced descriptive definition of complementary and alternative medicine devised by the US National Center for Complementary and Integrative Health (NCCIH) of the National Institutes of Health (NIH), states that it is "a group of diverse medical and health care systems, practices, and products that are not generally considered part of conventional medicine." For conventional medical practitioners, it does not necessarily follow that either it or its practitioners would no longer be considered alternative.
Some definitions seek to specify alternative medicine in terms of its social and political marginality to mainstream healthcare. This can refer to the lack of support that alternative therapies receive from the medical establishment and related bodies regarding access to research funding, sympathetic coverage in the medical press, or inclusion in the standard medical curriculum. In 1993, the British Medical Association (BMA), one among many professional organizations who have attempted to define alternative medicine, stated that it referred to "those forms of treatment which are not widely used by the conventional healthcare professions, and the skills of which are not taught as part of the undergraduate curriculum of conventional medical and paramedical healthcare courses". In a US context, an influential definition coined in 1993 by the Harvard-based physician, David M. Eisenberg, characterized alternative medicine "as interventions neither taught widely in medical schools nor generally available in US hospitals". These descriptive definitions are inadequate in the present-day when some conventional doctors offer alternative medical treatments and CAM introductory courses or modules can be offered as part of standard undergraduate medical training; alternative medicine is taught in more than 50 per cent of US medical schools and increasingly US health insurers are willing to provide reimbursement for CAM therapies. In 1999, 7.7% of US hospitals reported using some form of CAM therapy; this proportion had risen to 37.7% by 2008.
An expert panel at a conference hosted in 1995 by the US Office for Alternative Medicine (OAM), devised a theoretical definition of alternative medicine as "a broad domain of healing resources ... other than those intrinsic to the politically dominant health system of a particular society or culture in a given historical period." This definition has been widely adopted by CAM researchers, cited by official government bodies such as the UK Department of Health, attributed as the definition used by the Cochrane Collaboration, and, with slight modification, was preferred in the 2005 consensus report of the US Institute of Medicine, "Complementary and Alternative Medicine in the United States".
The 1995 OAM conference definition, an expansion of Eisenberg's 1993 formulation, is silent regarding questions of the medical effectiveness of alternative therapies. Its proponents hold that it thus avoids relativism about differing forms of medical knowledge and, while it is an essentially political definition, this should not imply that the dominance of mainstream biomedicine is solely due to political forces. According to this definition, alternative and mainstream medicine can only be differentiated with reference to what is "intrinsic to the politically dominant health system of a particular society of culture". However, there is neither a reliable method to distinguish between cultures and subcultures, nor to attribute them as dominant or subordinate, nor any accepted criteria to determine the dominance of a cultural entity. If the culture of a politically dominant healthcare system is held to be equivalent to the perspectives of those charged with the medical management of leading healthcare institutions and programs, the definition fails to recognize the potential for division either within such an elite or between a healthcare elite and the wider population.
Normative definitions distinguish alternative medicine from the biomedical mainstream in its provision of therapies that are unproven, unvalidated or ineffective and support of theories which have no recognized scientific basis. These definitions characterize practices as constituting alternative medicine when, used independently or in place of evidence-based medicine, they are put forward as having the healing effects of medicine, but which are not based on evidence gathered with the scientific method. Exemplifying this perspective, a 1998 editorial co-authored by Marcia Angell, a former editor of the "New England Journal of Medicine", argued that:
Regional definitions.
Public information websites maintained by the governments of the US and of the UK make a distinction between "alternative medicine" and "complementary medicine", but mention that these two overlap. The National Center for Complementary and Integrative Health (NCCIH) of the National Institutes of Health (NIH) (a part of the US Department of Health and Human Services) states that "alternative medicine" refers to using a non-mainstream approach in place of conventional medicine and that "complementary medicine" generally refers to using a non-mainstream approach together with conventional medicine, and comments that the boundaries between complementary and conventional medicine overlap and change with time.
The National Health Service (NHS) website "NHS Choices" (owned by the UK Department of Health), adopting the terminology of NCCIH, states that when a treatment is used alongside conventional treatments, to help a patient cope with a health condition, and not as an alternative to conventional treatment, this use of treatments can be called "complementary medicine"; but when a treatment is used instead of conventional medicine, with the intention of treating or curing a health condition, the use can be called "alternative medicine".
Similarly, the public information website maintained by the National Health and Medical Research Council (NHMRC) of the Commonwealth of Australia uses the acronym "CAM" for a wide range of health care practices, therapies, procedures and devices not within the domain of conventional medicine. In the Australian context this is stated to include acupuncture; aromatherapy; chiropractic; homeopathy; massage; meditation and relaxation therapies; naturopathy; osteopathy; reflexology, traditional Chinese medicine; and the use of vitamin supplements.
The Danish National Board of Health's "Council for Alternative Medicine" (Sundhedsstyrelsens Råd for Alternativ Behandling (SRAB)), an independent institution under the National Board of Health (Danish: "Sundhedsstyrelsen"), uses the term "alternative medicine" for:
Definitions based on national traditions or dominant practices.
In "General Guidelines for Methodologies on Research and Evaluation of Traditional Medicine", published in 2000 by the World Health Organization (WHO), complementary and alternative medicine were there defined as a broad set of health care practices that are not part of that country's own tradition and are not integrated into the dominant health care system.
Some herbal therapies are mainstream in Europe but are alternative in the US.
Special terminology used by selected individuals.
Two advocates of integrative medicine, writing in 2002 of the American healthcare system, claimed that it also addresses alleged problems with medicine based on science, which are not addressed by CAM; Ralph Snyderman and Andrew Weil stated that "integrative medicine is not synonymous with complementary and alternative medicine. It has a far larger meaning and mission in that it calls for restoration of the focus of medicine on health and healing and emphasizes the centrality of the patient-physician relationship." 
History – 19th century onwards.
"Further information: Rise of modern medicine"
Dating from the 1970s, medical professionals, sociologists, anthropologists and other commentators noted the increasing visibility of a wide variety of health practices that had neither derived directly from nor been verified by biomedical science. Since that time, those who have analyzed this trend have deliberated over the most apt language with which to describe this emergent health field. A variety of terms have been used, including heterodox, irregular, fringe and alternative medicine while others, particularly medical commentators, have been satisfied to label them as instances of quackery. The most persistent term has been alternative medicine but its use is problematic as it assumes a value-laden dichotomy between a medical fringe, implicitly of borderline acceptability at best, and a privileged medical orthodoxy, associated with validated medico-scientific norms. The use of the category of alternative medicine has also been criticized as it cannot be studied as an independent entity but must be understood in terms of a regionally and temporally specific medical orthodoxy. Its use can also be misleading as it may erroneously imply that a real medical alternative exists. As with near-synonymous expressions, such as unorthodox, complementary, marginal, or quackery, these linguistic devices have served, in the context of processes of professionalisation and market competition, to establish the authority of official medicine and police the boundary between it and its unconventional rivals.
An early instance of the influence of this modern, or western, scientific medicine outside Europe and North America is Peking Union Medical College.
From a historical perspective, the emergence of alternative medicine, if not the term itself, is typically dated to the 19th century. This is despite the fact that there are variants of Western non-conventional medicine that arose in the late-eighteenth century or earlier and some non-Western medical traditions, currently considered alternative in the West and elsewhere, which boast extended historical pedigrees. Alternative medical systems, however, can only be said to exist when there is an identifiable, regularized and authoritative standard medical practice, such as arose in the West during the nineteenth-century, to which they can function as an alternative.
During the late eighteenth and nineteenth centuries regular and irregular medical practitioners became more clearly differentiated throughout much of Europe and, as the nineteenth century progressed, most Western states converged in the creation of legally delimited and semi-protected medical markets. It is at this point that an "official" medicine, created in cooperation with the state and employing a scientific rhetoric of legitimacy, emerges as a recognizable entity and that the concept of alternative medicine as a historical category becomes tenable.
As part of this process, professional adherents of mainstream medicine in countries such as Germany, France, and Britain increasingly invoked the scientific basis of their discipline as a means of engendering internal professional unity and of external differentiation in the face of sustained market competition from homeopaths, naturopaths, mesmerists and other nonconventional medical practitioners, finally achieving a degree of imperfect dominance through alliance with the state and the passage of regulatory legislation. In the US the Johns Hopkins University School of Medicine, based in Baltimore, Maryland, opened in 1893,with William H. Welch and William Osler among the founding physicians, and was the first medical school devoted to teaching "German scientific medicine".
Buttressed by the increased authority consequent to the significant advances in the medical sciences of the late 19th century onwards—including the development and application of the germ theory of disease by the chemist Louis Pasteur and the surgeon Joseph Lister, of microbiology co-founded by Robert Koch (in 1885 appointed professor of hygiene at the University of Berlin), and of the use of X-rays (Röntgen rays)—the 1910 Flexner Report called upon American medical schools to follow the model set by the Johns Hopkins School of Medicine and adhere to mainstream science in their teaching and research. This was in a belief, mentioned in the Report's introduction, that the preliminary and professional training then prevailing in medical schools should be reformed in view of the new means for diagnosing and combating disease being made available to physicians and surgeons by the sciences on which medicine depended.
Among putative medical practices available at the time which later became known as "alternative medicine" were homeopathy (founded in Germany in the early 19c.) and chiropractic (founded in North America in the late 19c.). These conflicted in principle with the developments in medical science upon which the Flexner reforms were based, and they have not become compatible with further advances of medical science such as listed in Timeline of medicine and medical technology, 1900–1999 and 2000–present, nor have Ayurveda, acupuncture or other kinds of alternative medicine.
At the same time "Tropical medicine" was being developed as a specialist branch of western medicine in research establishments such as Liverpool School of Tropical Medicine founded in 1898 by Alfred Lewis Jones, London School of Hygiene & Tropical Medicine, founded in 1899 by Patrick Manson and Tulane University School of Public Health and Tropical Medicine, instituted in 1912. A distinction was being made between western scientific medicine and indigenous systems. An example is given by an official report about indigenous systems of medicine in India, including Ayurveda, submitted by Mohammad Usman of Madras and others in 1923. This stated that the first question the Committee considered was "to decide whether the indigenous systems of medicine were scientific or not".
By the later twentieth century the term 'alternative medicine' had come into use for the purposes of public discussion, but it was not always being used with the same meaning by all parties. Arnold S. Relman remarked in 1998 that in the best kind of medical practice, all proposed treatments must be tested objectively, and that in the end there will only be treatments that pass and those that do not, those that are proven worthwhile and those that are not. He asked 'Can there be any reasonable "alternative"?' But also in 1998 the then Surgeon General of the United States, David Satcher, issued public information about eight common alternative treatments (including acupuncture, holistic and massage), together with information about common diseases and conditions, on nutrition, diet, and lifestyle changes, and about helping consumers to decipher fraud and quackery, and to find healthcare centers and doctors who practiced alternative medicine.
By 1990, approximately 60 million Americans had used one or more complementary or alternative therapies to address health issues, according to a nationwide survey in the US published in 1993 by David Eisenberg. A study published in the November 11, 1998 issue of the Journal of the American Medical Association reported that 42% of Americans had used complementary and alternative therapies, up from 34% in 1990. However, despite the growth in patient demand for complementary medicine, most of the early alternative/complementary medical centers failed.
Medical education since 1910.
Mainly as a result of reforms following the Flexner Report of 1910 medical education in established medical schools in the US has generally not included alternative medicine as a teaching topic. Typically, their teaching is based on current practice and scientific knowledge about: anatomy, physiology, histology, embryology, neuroanatomy, pathology, pharmacology, microbiology and immunology. Medical schools' teaching includes such topics as doctor-patient communication, ethics, the art of medicine, and engaging in complex clinical reasoning (medical decision-making). Writing in 2002, Snyderman and Weil remarked that by the early twentieth century the Flexner model had helped to create the 20th-century academic health center in which education, research and practice were inseparable. While this had much improved medical practice by defining with increasing certainty the pathophysiological basis of disease, a single-minded focus on the pathophysiological had diverted much of mainstream American medicine from clinical conditions which were not well understood in mechanistic terms and were not effectively treated by conventional therapies.
By 2001 some form of CAM training was being offered by at least 75 out of 125 medical schools in the US. Exceptionally, the School of Medicine of the University of Maryland, Baltimore includes a research institute for integrative medicine (a member entity of the Cochrane Collaboration). Medical schools are responsible for conferring medical degrees, but a physician typically may not legally practice medicine until licensed by the local government authority. Licensed physicians in the US who have attended one of the established medical schools there have usually graduated Doctor of Medicine (MD). All states require that applicants for MD licensure be graduates of an approved medical school and complete the United States Medical Licensing Exam (USMLE).
The British Medical Association, in its publication "Complementary Medicine, New Approach to Good Practice" (1993), gave as a working definition of non-conventional therapies (including acupuncture, chiropractic and homeopathy): "those forms of treatment which are not widely used by the orthodox health-care professions, and the skills of which are not part of the undergraduate curriculum of orthodox medical and paramedical health-care courses". By 2000 some medical schools in the UK were offering CAM familiarisation courses to undergraduate medical students while some were also offering modules specifically on CAM.
Proponents and opponents.
The Cochrane Collaboration Complementary Medicine Field explains its "Scope and Topics" by giving a broad and general definition for complementary medicine as including practices and ideas which are outside the domain of conventional medicine in several countries and defined by its users as preventing or treating illness, or promoting health and well being, and which complement mainstream medicine in three ways: by contributing to a common whole, by satisfying a demand not met by conventional practices, and by diversifying the conceptual framework of medicine.
Proponents of an evidence-base for medicine such as the Cochrane Collaboration (founded in 1993 and from 2011 providing input for WHO resolutions) take a position that "all" systematic reviews of treatments, whether "mainstream" or "alternative", ought to be held to the current standards of scientific method. In a study titled "Development and classification of an operational definition of complementary and alternative medicine for the Cochrane Collaboration" (2011) it was proposed that indicators that a therapy is accepted include government licensing of practitioners, coverage by health insurance, statements of approval by government agencies, and recommendation as part of a practice guideline; and that if something is currently a standard, accepted therapy, then it is not likely to be widely considered as CAM.
That alternative medicine has been on the rise "in countries where Western science and scientific method generally are accepted as the major foundations for healthcare, and 'evidence-based' practice is the dominant paradigm" was described as an "enigma" in the Medical Journal of Australia.
Critics in the US say the expression is deceptive because it implies there is an effective alternative to science-based medicine, and that "complementary" is deceptive because the word implies that the treatment increases the effectiveness of (complements) science-based medicine, while alternative medicines which have been tested nearly always have no measurable positive effect compared to a placebo.
Some opponents, focused upon health fraud, misinformation, and quackery as public health problems in the US, are highly critical of alternative medicine, notably Wallace Sampson and Paul Kurtz founders of Scientific Review of Alternative Medicine and Stephen Barrett, co-founder of The National Council Against Health Fraud and webmaster of Quackwatch. Grounds for opposing alternative medicine which have been stated in the US and elsewhere are:
Paul Offit has proposed four ways in which "alternative medicine becomes quackery":
The NCCIH classification system.
A United States government agency, the National Center on Complementary and Integrative Health (NCCIH), has created its own classification system for branches of complementary and alternative medicine. It classifies complementary and alternative therapies into five major groups, which have some overlap and two types of energy medicine are distinguished: one, "Veritable" involving scientifically observable energy, including magnet therapy, colorpuncture and light therapy; the other "Putative" which invoke physically undetectable or unverifiable energy.
Alternative medicine practices and beliefs are diverse in their foundations and methodologies. The wide range of treatments and practices referred to as alternative medicine includes some stemming from nineteenth century North America, such as chiropractic and naturopathy, others, mentioned by Jütte, that originated in eighteenth- and nineteenth-century Germany, such as homeopathy and hydropathy, and some that have originated in China or India, while African, Caribbean, Pacific Island, Native American, and other regional cultures have traditional medical systems as diverse as their diversity of cultures.
Examples of CAM as a broader term for unorthodox treatment and diagnosis of illnesses, disease, infections, etc., include yoga, acupuncture, aromatherapy, chiropractic, herbalism, homeopathy, hypnotherapy, massage, osteopathy, reflexology, relaxation therapies, spiritual healing and tai chi. CAM differs from conventional medicine. It is normally private medicine and not covered by health insurance. It is paid out of pocket by the patient and is an expensive treatment. CAM tends to be a treatment for upper class or more educated people.
The NCCIH classification system is - 
Examples classified under the NCCIH system.
Alternative therapies based on electricity or magnetism use verifiable electromagnetic fields, such as pulsed fields, alternating-current, or direct-current fields in an unconventional manner rather than claiming the existence of imponderable or supernatural energies.
Substance based practices use substances found in nature such as herbs, foods, non-vitamin supplements and megavitamins, and minerals, and includes traditional herbal remedies with herbs specific to regions in which the cultural practices arose. Nonvitamin supplements include fish oil, Omega-3 fatty acid, glucosamine, echinacea, flaxseed oil or pills, and ginseng, when used under a claim to have healing effects.
Mind-body interventions, working under the premise that the mind can affect "bodily functions and symptoms", include healing claims made in hypnotherapy, and in guided imagery, meditation, progressive relaxation, qi gong, tai chi and yoga. Meditation practices including mantra meditation, mindfulness meditation, yoga, tai chi, and qi gong have many uncertainties. According to an AHRQ review, the available evidence on meditation practices through September 2005 is of poor methodological quality and definite conclusions on the effects of meditation in healthcare cannot be made using existing research.
Naturopathy is based on a belief in vitalism, which posits that a special energy called vital energy or vital force guides bodily processes such as metabolism, reproduction, growth, and adaptation. The term was coined in 1895 by John Scheel and popularized by Benedict Lust, the "father of U.S. naturopathy". Today, naturopathy is primarily practiced in the United States and Canada. Naturopaths in unregulated jurisdictions may use the Naturopathic Doctor designation or other titles regardless of level of education.
Traditional Chinese medicine is based on a concept of vital energy, or Qi, flowing in the body along specific pathways. These purported pathways consist of 12 primary meridians. TCM has many branches including, acupuncture, massage, feng shui, herbs, as well as Chinese astrology. TCM diagnosis is primarily based on looking at the tongue, which is claimed to show the condition of the organs, as well as feeling the pulse of the radial artery, which is also claimed to show the condition of the organs.
Criticism.
Use of the terms "Complementary and alternative medicine (CAM)" and "alternative medicine" have been criticized.
CAM is not as well researched as conventional medicine which undergoes intense research before being released to the public. Funding for research is also sparse making it difficult to do further research for effectiveness of CAM. Most funding for CAM is funded by government agencies. Proposed research for CAM are rejected by most private funding agencies because the results of research are not reliable. The research for CAM has to meet certain standards from research ethics committees which most CAM researchers find almost impossible to meet. Because the results of CAM are not quantifiable, it is hard to prove its effectiveness and it appears to work in a more holistic sense. CAM is thought to help the patient in a mental or psychological sense since the research for CAM is hit and miss. Even with the little research done on it, CAM has not been proven to be effective. This creates an issue of whether the patient is receiving all the information about the treatment that is necessary for the patient to be well informed.
CAM is not as well regulated as conventional medicine. There are ethical concerns about whether people who perform CAM have the proper knowledge to perform the treatments they give to patients. CAM is often done by non-physicians and does not operate with the same medical licensing laws as conventional medicine. It is an issue of non-maleficence.
In the USA.
A 2002 report on public attitudes and understanding issued by the US National Science Foundation defines the term "alternative medicine" as treatments that had not been proven effective using scientific methods, and described them as giving more weight to ancient traditions and anecdotes over biological science and clinical trials.
Criticisms have come from individuals such as Wallace Sampson in an article in Annals of the New York Academy of Sciences, June 1995. Sampson argued that proponents of alternative medicine often used terminology which was loose or ambiguous to create the appearance that a choice between "alternative" effective treatments existed when it did not, or that there was effectiveness or scientific validity when it did not exist, or to suggest that a dichotomy existed when it did not, or to suggest that consistency with science existed when it might not; that the term "alternative" was to suggest that a patient had a choice between effective treatments when there was not; that use of the word "conventional" or "mainstream" was to suggest that the difference between alternative medicine and science based medicine was the prevalence of use, rather than lack of a scientific basis of alternative medicine as compared to "conventional" or "mainstream" science based medicine; that use of the term "complementary" or "integrative" was to suggest that purported supernatural energies of alternative medicine could complement or be integrated into science based medicine. "Integrative medicine" or "integrated medicine" is used to refer to the belief that medicine based on science would be improved by "integration" with alternative medical treatments practices that are not, and is substantially similar in use to the term "complementary and alternative medicine".
Sampson has also written that CAM is the "propagation of the absurd", and argues that "alternative" and "complementary" have been substituted for "quackery", "dubious", and "implausible".
Another critic, with reference to government funding studies of integrating alternative medicine techniques into the mainstream, Steven Novella, a neurologist at Yale School of Medicine, wrote that it "is used to lend an appearance of legitimacy to treatments that are not legitimate." Another, Marcia Angell, argued that it was "a new name for snake oil." Angell considered that critics felt that healthcare practices should be classified based solely on scientific evidence, and if a treatment had been rigorously tested and found safe and effective, science based medicine will adopt it regardless of whether it was considered "alternative" to begin with. It was thus possible for a method to change categories (proven vs. unproven), based on increased knowledge of its effectiveness or lack thereof. Prominent supporters of this position include George D. Lundberg, former editor of the Journal of the American Medical Association (JAMA).
In an article first published in "CA: A Cancer Journal for Clinicians" in 1999, "Evaluating complementary and alternative therapies for cancer patients.", Barrie R. Cassileth mentioned that a 1997 letter to the US Senate Subcommittee on Public Health and Safety, which had deplored the lack of critical thinking and scientific rigor in OAM-supported research, had been signed by four Nobel Laureates and other prominent scientists. (This was supported by the National Institutes of Health (NIH).)
In March 2009 a "Washington Post" staff writer reported that the impending national discussion about broadening access to health care, improving medical practice and saving money was giving a group of scientists an opening to propose shutting down the National Center for Complementary and Alternative Medicine, quoting one of them, Steven Salzberg, a genome researcher and computational biologist at the University of Maryland, saying "One of our concerns is that NIH is funding pseudoscience." They argued that the vast majority of studies were based on fundamental misunderstandings of physiology and disease, and have shown little or no effect.
Stephen Barrett, founder and operator of Quackwatch, has argued that practices labeled "alternative" should be reclassified as either genuine, experimental, or questionable. Here he defines genuine as being methods that have sound evidence for safety and effectiveness, experimental as being unproven but with a plausible rationale for effectiveness, and questionable as groundless without a scientifically plausible rationale.
Sampson has also pointed out that CAM tolerated contradiction without thorough reason and experiment. Barrett has pointed out that there is a policy at the NIH of never saying something doesn't work only that a different version or dose might give different results. Barrett also expressed concern that, just because some "alternatives" have merit, there is the impression that the rest deserve equal consideration and respect even though most are worthless, since they are all classified under the one heading of alternative medicine.
Writers such as Carl Sagan (1934-1996), a noted astrophysicist, advocate of skeptical thinking (Scientific skepticism) and the author of "The Demon–Haunted World: Science as a Candle in the Dark" (1996), have described the lack of empirical evidence to support the existence of the putative energy fields on which these therapies are predicated.
According to two writers, Wallace Sampson and K. Butler, marketing is part of the medical training required in chiropractic education, and propaganda methods in alternative medicine have been traced back to those used by Hitler and Goebels in their promotion of pseudoscience in medicine.
The NCCIH budget has been criticized because, despite the duration and intensity of studies to measure the efficacy of alternative medicine, there had been no effective CAM treatments supported by scientific evidence as of 2002, according to the QuackWatch website; the NCCIH budget has been on a sharp and sustained rise. Critics of the Center argue that the plausibility of interventions such as botanical remedies, diet, relaxation therapies and yoga should not be used to support research on implausible interventions based on superstition and belief in the supernatural, and that the plausible methods can be studied just as well in other parts of NIH, where they should be made to compete on an equal footing with other research projects.
In the UK.
Richard Dawkins, an English evolutionary biologist and author, in an essay in his book "A Devil's Chaplain" (2003) (chapter 4.4), has defined alternative medicine as a "set of practices that cannot be tested, refuse to be tested, or consistently fail tests." Another essay in the same book (chapter 1.4) quoted from an article by John Diamond in "The Independent": "There is really no such thing as alternative medicine, just medicine that works and medicine that doesn't." 
Dawkins has argued that, if a technique is demonstrated effective in properly performed trials, it ceases to be alternative and simply becomes medicine.
As it relates to ethics, in November 2011 Edzard Ernst stated that the "level of misinformation about alternative medicine has now reached the point where it has become dangerous and unethical. So far, alternative medicine has remained an ethics-free zone. It is time to change this." Ernst requested that Prince Charles recall two guides to alternative medicine published by the Foundation for Integrated Health, on the grounds that "[t]hey both contain numerous misleading and inaccurate claims concerning the supposed benefits of alternative medicine" and that "[t]he nation cannot be served by promoting ineffective and sometimes dangerous alternative treatments." In general, he believes that CAM can and should be subjected to scientific testing.
Placebo effect.
A research methods expert and author of "Snake Oil Science", R. Barker Bausell, has stated that "it's become politically correct to investigate nonsense." There are concerns that just having NIH support is being used to give unfounded "legitimacy to treatments that are not legitimate."
Use of placebos in order to achieve a placebo effect in integrative medicine has been criticized as "diverting research time, money, and other resources from more fruitful lines of investigation in order to pursue a theory that has no basis in biology".
Another critic has argued that academic proponents of integrative medicine sometimes recommend misleading patients by using known placebo treatments in order to achieve a placebo effect. However, a 2010 survey of family physicians found that 56% of respondents said they had used a placebo in clinical practice as well. Eighty-five percent of respondents believed placebos can have both psychological and physical benefits.
Integrative medicine has been criticized in that its practitioners, trained in science based medicine, deliberately mislead patients by pretending placebos are not. "quackademic medicine" is a pejorative term used for "integrative medicine", which is considered to be an infiltration of quackery into academic science-based medicine.
An analysis of trends in the criticism of complementary and alternative medicine (CAM) in five prestigious American medical journals during the period of reorganization within medicine (1965–1999) was reported as showing that the medical profession had responded to the growth of CAM in three phases, and that in each phase there had been changes in the medical marketplace which influenced the type of response in the journals. Changes included relaxed medical licensing, the development of managed care, rising consumerism, and the establishment of the USA Office of Alternative Medicine (now National Center for Complementary and Alternative Medicine). In the "condemnation" phase, from the late 1960s to the early 1970s, authors had ridiculed, exaggerated the risks, and petitioned the state to contain CAM; in the "reassessment" phase (mid-1970s through early 1990s), when increased consumer utilization of CAM was prompting concern, authors had pondered whether patient dissatisfaction and shortcomings in conventional care contributed to the trend; in the "integration" phase of the 1990s physicians began learning to work around or administer CAM, and the subjugation of CAM to scientific scrutiny had become the primary means of control.
Use and regulation.
Prevalence of use.
Complementary and alternative medicine (CAM) has been described as a broad domain of healing resources that encompasses all health systems, modalities, and practices and their accompanying theories and beliefs, other than those intrinsic to the politically dominant health system of a particular society or culture in a given historical period. CAM includes all such practices and ideas self-defined by their users as preventing or treating illness or promoting health and well-being. Boundaries within CAM and between the CAM domain and that of the dominant system are not always sharp or fixed.
About 50% of people in developed countries use some kind of complementary and alternative medicine other than prayer for health. A British telephone survey by the BBC of 1209 adults in 1998 shows that around 20% of adults in Britain had used alternative medicine in the past 12 months. About 40% of cancer patients use some form of CAM.
In developing nations, access to essential medicines is severely restricted by lack of resources and poverty. Traditional remedies, often closely resembling or forming the basis for alternative remedies, may comprise primary healthcare or be integrated into the healthcare system. In Africa, traditional medicine is used for 80% of primary healthcare, and in developing nations as a whole over one-third of the population lack access to essential medicines.
In the USA.
In the United States, the 1974 Child Abuse Prevention and Treatment Act (CAPTA) required states to grant religious exemptions to child neglect and abuse laws, regarding religion-based healing practices, in order to receive federal money. Thirty-one states have child-abuse religious exemptions.
In respect of taxation in the USA, the Internal Revenue Service has discriminated in favour of medical expenses for acupuncture and chiropractic (and others including Christian Science practitioners) but against homeopathy and the use of non-prescription required medicine.
The use of alternative medicine in the US has increased, with a 50 percent increase in expenditures and a 25 percent increase in the use of alternative therapies between 1990 and 1997 in America. Americans spend many billions on the therapies annually. Most Americans used CAM to treat and/or prevent musculoskeletal conditions or other conditions associated with chronic or recurring pain. In America, women were more likely than men to use CAM, with the biggest difference in use of mind-body therapies including prayer specifically for health reasons". In 2008, more than 37% of American hospitals offered alternative therapies, up from 26.5 percent in 2005, and 25% in 2004. More than 70% of the hospitals offering CAM were in urban areas.
A survey of Americans found that 88 percent agreed that "there are some good ways of treating sickness that medical science does not recognize". Use of magnets was the most common tool in energy medicine in America, and among users of it, 58 percent described it as at least "sort of scientific", when it is not at all scientific. In 2002, at least 60 percent of US medical schools have at least some class time spent teaching alternative therapies. "Therapeutic touch", was taught at more than 100 colleges and universities in 75 countries before the practice was debunked by a nine-year-old child for a school science project.
A 1997 survey found that 13.7% of respondents in the US had sought the services of both a medical doctor and an alternative medicine practitioner. The same survey found that 96% of respondents who sought the services of an alternative medicine practitioner also sought the services of a medical doctor in the past 12 months. Medical doctors are often unaware of their patient's use of alternative medical treatments as only 38.5% of the patients alternative therapies were discussed with their medical doctor.
According to Michael H. Cohen, US regulation of alternative includes state licensure of healthcare providers and scope of practice limits on practice by non-MD healthcare professionals; state-law malpractice rules (standard of care limits on professional negligence); discipline of practitioners by state regulatory boards; and federal regulation such as food and drug law. He argues that US regulation of alternative medicine "seeks to integrate biomedical, holistic, and social models of health care in ways that maximize patients’ well-being [w]hile still protecting patients from fraud."
Prevalence of use of specific therapies.
The most common CAM therapies used in the US in 2002 were prayer (45.2%), herbalism (18.9%), breathing meditation (11.6%), meditation (7.6%), chiropractic medicine (7.5%), yoga (5.1%-6.1%), body work (5.0%), diet-based therapy (3.5%), progressive relaxation (3.0%), mega-vitamin therapy (2.8%) and Visualization (2.1%)
In Britain, the most often used alternative therapies were Alexander technique, Aromatherapy, Bach and other flower remedies, Body work therapies including massage, Counseling stress therapies, hypnotherapy, Meditation, Reflexology, Shiatsu, Ayurvedic medicine, Nutritional medicine, and Yoga. Ayurvedic medicine remedies are mainly plant based with some use of animal materials. Safety concerns include the use of herbs containing toxic compounds and the lack of quality control in Ayurvedic facilities.
According to the National Health Service (England), the most commonly used complementary and alternative medicines (CAM) supported by the NHS in the UK are: acupuncture, aromatherapy, chiropractic, homeopathy, massage, osteopathy and clinical hypnotherapy.
"Complementary medicine treatments used for pain include: acupuncture, low-level laser therapy, meditation, aroma therapy, Chinese medicine, dance therapy, music therapy, massage, herbalism, therapeutic touch, yoga, osteopathy, chiropractic, naturopathy, and homeopathy."
In palliative care.
Complementary therapies are often used in palliative care or by practitioners attempting to manage chronic pain in patients. Integrative medicine is considered more acceptable in the interdisciplinary approach used in palliative care than in other areas of medicine. "From its early experiences of care for the dying, palliative care took for granted the necessity of placing patient values and lifestyle habits at the core of any design and delivery of quality care at the end of life. If the patient desired complementary therapies, and as long as such treatments provided additional support and did not endanger the patient, they were considered acceptable." The non-pharmacologic interventions of complementary medicine can employ mind-body interventions designed to "reduce pain and concomitant mood disturbance and increase quality of life."
Regulation.
In Austria and Germany complementary and alternative medicine is mainly in the hands of doctors with MDs, and half or more of the American alternative practitioners are licensed MDs. In Germany herbs are tightly regulated: half are prescribed by doctors and covered by health insurance.
Some professions of complementary/traditional/alternative medicine, such as chiropractic, have achieved full regulation in North America and other parts of the world and are regulated in a manner similar to that governing science-based medicine. In contrast, other approaches may be partially recognized and others have no regulation at all. Regulation and licensing of alternative medicine ranges widely from country to country, and state to state.
Government bodies in the USA and elsewhere have published information or guidance about alternative medicine. One of those is the U.S. Food and Drug Administration (FDA), which mentions specifically homeopathic products, traditional Chinese medicine and Ayurvedic products. A document which the FDA has issued for comment is headed "Guidance for Industry: Complementary and Alternative Medicine Products and Their Regulation by the Food and Drug Administration", last updated on March 2, 2007. The document opens with three preliminary paragraphs which explain that "in the document":
The FDA has also issued online warnings for consumers about medication health fraud. This includes a section on Alternative Medicine Fraud, such as a warning that Ayurvedic products generally have not been approved by the FDA before marketing.
Efficacy.
Alternative therapies lack the requisite scientific validation, and their effectiveness is either unproved or disproved. Many of the claims regarding the efficacy of alternative medicines are controversial, since research on them is frequently of low quality and methodologically flawed. Selective publication of results (misleading results from only publishing positive results, and not all results), marked differences in product quality and standardisation, and some companies making unsubstantiated claims, call into question the claims of efficacy of isolated examples where herbs may have some evidence of containing chemicals that may affect health. "The Scientific Review of Alternative Medicine" points to confusions in the general population - a person may attribute symptomatic relief to an otherwise-ineffective therapy just because they are taking something (the placebo effect); the natural recovery from or the cyclical nature of an illness (the regression fallacy) gets misattributed to an alternative medicine being taken; a person not diagnosed with science based medicine may never originally have had a true illness diagnosed as an alternative disease category.
Edzard Ernst characterized the evidence for many alternative techniques as weak, nonexistent, or negative and in 2011 published his estimate that about 7.4% were based on "sound evidence", although he believes that may be an overestimate due to various reasons. Ernst has concluded that 95% of the alternative treatments he and his team studied, including acupuncture, herbal medicine, homeopathy, and reflexology, are "statistically indistinguishable from placebo treatments", but he also believes there is something that conventional doctors can usefully learn from the chiropractors and homeopath: this is the therapeutic value of the placebo effect, one of the strangest phenomena in medicine.
In 2003, a project funded by the CDC identified 208 condition-treatment pairs, of which 58% had been studied by at least one randomized controlled trial (RCT), and 23% had been assessed with a meta-analysis. According to a 2005 book by a US Institute of Medicine panel, the number of RCTs focused on CAM has risen dramatically. The book cites Vickers (1998), who found that many of the CAM-related RCTs are in the Cochrane register, but 19% of these trials were not in MEDLINE, and 84% were in conventional medical journals.
As of 2005, the Cochrane Library had 145 CAM-related Cochrane systematic reviews and 340 non-Cochrane systematic reviews. An analysis of the conclusions of only the 145 Cochrane reviews was done by two readers. In 83% of the cases, the readers agreed. In the 17% in which they disagreed, a third reader agreed with one of the initial readers to set a rating. These studies found that, for CAM, 38.4% concluded positive effect or possibly positive (12.4%), 4.8% concluded no effect, 0.69% concluded harmful effect, and 56.6% concluded insufficient evidence. An assessment of conventional treatments found that 41.3% concluded positive or possibly positive effect, 20% concluded no effect, 8.1% concluded net harmful effects, and 21.3% concluded insufficient evidence. However, the CAM review used the more developed 2004 Cochrane database, while the conventional review used the initial 1998 Cochrane database.
Most alternative medical treatments are not patentable, which may lead to less research funding from the private sector. In addition, in most countries, alternative treatments (in contrast to pharmaceuticals) can be marketed without any proof of efficacy—also a disincentive for manufacturers to fund scientific research. Some have proposed adopting a prize system to reward medical research. However, public funding for research exists. Increasing the funding for research on alternative medicine techniques is the purpose of the US National Center for Complementary and Alternative Medicine. NCCIH and its predecessor, the Office of Alternative Medicine, have spent more than $2.5 billion on such research since 1992; this research has largely not demonstrated the efficacy of alternative treatments.
In the same way as for conventional therapies, drugs, and interventions, it can be difficult to test the efficacy of alternative medicine in clinical trials. In instances where an established, effective, treatment for a condition is already available, the Helsinki Declaration states that withholding such treatment is unethical in most circumstances. Use of standard-of-care treatment in addition to an alternative technique being tested may produce confounded or difficult-to-interpret results.
Cancer researcher Andrew J. Vickers has stated:
Homeopathy is based on the belief that a disease can be cured by a very low dose of substance that creates similar symptoms in a healthy person. This conflicts with fundamental concepts of physics and chemistry and there is no good evidence from reviews of research to support its use.
Conflicts of interest.
Some commentators have said that special consideration must be given to the issue of conflicts of interest in alternative medicine. Edzard Ernst has said that most researchers into alternative medicine are at risk of "unidirectional bias" because of a generally uncritical belief in their chosen subject. Ernst cites as evidence the phenomenon whereby 100% of a sample of acupuncture trials originating in China had positive conclusions. David Gorski contrasts evidence-based medicine, in which researchers try to disprove hyphotheses, with what he says is the frequent practice in pseudoscience-based research, of striving to confirm pre-existing notions. Harriet A. Hall writes that there is a contrast between the circumstances of alternative medicine practitioners and disinterested scientists: in the case of acupuncture, for example, an acupuncturist would have "a great deal to lose" if acupuncture were rejected by research; but the disinterested skeptic would not lose anything if its effects were confirmed; rather their change of mind would enhance their skeptical credentials.
Safety.
Adequacy of regulation and CAM safety.
Many of the claims regarding the safety and efficacy of alternative medicine are controversial. Some alternative treatments have been associated with unexpected side effects, which can be fatal.
One of the commonly voiced concerns about complementary alternative medicine (CAM) is the manner in which is regulated. There have been significant developments in how CAMs should be assessed prior to re-sale in the United Kingdom and the European Union (EU) in the last 2 years. Despite this, it has been suggested that current regulatory bodies have been ineffective in preventing deception of patients as many companies have re-labelled their drugs to avoid the new laws. There is no general consensus about how to balance consumer protection (from false claims, toxicity, and advertising) with freedom to choose remedies.
Advocates of CAM suggest that regulation of the industry will adversely affect patients looking for alternative ways to manage their symptoms, even if many of the benefits may represent the placebo affect. Some contend that alternative medicines should not require any more regulation than over-the-counter medicines that can also be toxic in overdose (such as paracetamol).
Interactions with conventional pharmaceuticals.
Forms of alternative medicine that are biologically active can be dangerous even when used in conjunction with conventional medicine. Examples include immuno-augmentation therapy, shark cartilage, bioresonance therapy, oxygen and ozone therapies, insulin potentiation therapy. Some herbal remedies can cause dangerous interactions with chemotherapy drugs, radiation therapy, or anesthetics during surgery, among other problems. An anecdotal example of these dangers was reported by Associate Professor Alastair MacLennan of Adelaide University, Australia regarding a patient who almost bled to death on the operating table after neglecting to mention that she had been taking "natural" potions to "build up her strength" before the operation, including a powerful anticoagulant that nearly caused her death.
To "ABC Online", MacLennan also gives another possible mechanism:
Potential side-effects.
Conventional treatments are subjected to testing for undesired side-effects, whereas alternative treatments, in general, are not subjected to such testing at all. Any treatment – whether conventional or alternative – that has a biological or psychological effect on a patient may also have potential to possess dangerous biological or psychological side-effects. Attempts to refute this fact with regard to alternative treatments sometimes use the "appeal to nature" fallacy, i.e., "that which is natural cannot be harmful".
An exception to the normal thinking regarding side-effects is Homeopathy. Since 1938, the U.S. Food and Drug Administration (FDA) has regulated homeopathic products in "several significantly different ways from other drugs." Homeopathic preparations, termed "remedies", are extremely dilute, often far beyond the point where a single molecule of the original active (and possibly toxic) ingredient is likely to remain. They are, thus, considered safe on that count, but "their products are exempt from good manufacturing practice requirements related to expiration dating and from finished product testing for identity and strength", and their alcohol concentration may be much higher than allowed in conventional drugs.
Treatment delay.
Those having experienced or perceived success with one alternative therapy for a minor ailment may be convinced of its efficacy and persuaded to extrapolate that success to some other alternative therapy for a more serious, possibly life-threatening illness. For this reason, critics argue that therapies that rely on the placebo effect to define success are very dangerous. According to mental health journalist Scott Lilienfeld in 2002, "unvalidated or scientifically unsupported mental health practices can lead individuals to forgo effective treatments" and refers to this as "opportunity cost". Individuals who spend large amounts of time and money on ineffective treatments may be left with precious little of either, and may forfeit the opportunity to obtain treatments that could be more helpful. In short, even innocuous treatments can indirectly produce negative outcomes.
Between 2001 and 2003, four children died in Australia because their parents chose ineffective naturopathic, homeopathic, or other alternative medicines and diets rather than conventional therapies.
Unconventional cancer "cures".
There have always been "many therapies offered outside of conventional cancer treatment centers and based on theories not found in biomedicine. These alternative cancer cures have often been described as 'unproven,' suggesting that appropriate clinical trials have not been conducted and that the therapeutic value of the treatment is unknown." However, "many alternative cancer treatments have been investigated in good-quality clinical trials, and they have been shown to be ineffective...The label 'unproven' is inappropriate for such therapies; it is time to assert that many alternative cancer therapies have been 'disproven'."
Edzard Ernst has stated:
Research funding.
Funding for research into effectiveness of alternative treatments comes from a variety of public and private sources. In the USA, one conduit for funding and information is the National Center for Complementary and Integrative Medicine (NCCIH). Other governments have various levels of funding; the Dutch government funded CAM research between 1986 and 2003, but formally ended it in 2006.
Appeal.
Physicians who practice complementary medicine usually discuss and advise patients as to available complementary therapies. Patients often express interest in mind-body complementary therapies because they offer a non-drug approach to treating some health conditions. Some mind-body techniques, such as cognitive-behavioral therapy, were once considered complementary medicine, but are now a part of conventional medicine in the United States.
Against alternative medicine it has been argued that in addition to the social-cultural underpinnings of the popularity of alternative medicine, there are several psychological issues that are critical to its growth. One of the most critical is the placebo effect, which is a well-established observation in medicine. Related to it are similar psychological effects such as the will to believe, cognitive biases that help maintain self-esteem and promote harmonious social functioning, and the "post hoc, ergo propter hoc" fallacy.
In the UK.
CAM's popularity may be related to other factors which Edzard Ernst mentioned in an interview in "The Independent":
Why is it so popular, then? Ernst blames the providers, customers and the doctors whose neglect, he says, has created the opening into which alternative therapists have stepped. "People are told lies. There are 40 million websites and 39.9 million tell lies, sometimes outrageous lies. They mislead cancer patients, who are encouraged not only to pay their last penny but to be treated with something that shortens their lives. "At the same time, people are gullible. It needs gullibility for the industry to succeed. It doesn't make me popular with the public, but it's the truth.
In a paper published in October 2010 entitled "The public's enthusiasm for complementary and alternative medicine amounts to a critique of mainstream medicine", Ernst described these views in greater detail and concluded:
[CAM] is popular. An analysis of the reasons why this is so points towards the therapeutic relationship as a key factor. Providers of CAM tend to build better therapeutic relationships than mainstream healthcare professionals. In turn, this implies that much of the popularity of CAM is a poignant criticism of the failure of mainstream healthcare. We should consider it seriously with a view of improving our service to patients.
In the USA and Canada.
A study published in 1998 indicates that a majority of alternative medicine use was in conjunction with standard medical treatments. Approximately 4.4 percent of those studied used alternative medicine as a replacement for conventional medicine. The research found that those having used alternative medicine tended to have higher education or report poorer health status. Dissatisfaction with conventional medicine was not a meaningful factor in the choice, but rather the majority of alternative medicine users appear to be doing so largely because "they find these healthcare alternatives to be more congruent with their own values, beliefs, and philosophical orientations toward health and life." In particular, subjects reported a holistic orientation to health, a transformational experience that changed their worldview, identification with a number of groups committed to environmentalism, feminism, psychology, and/or spirituality and personal growth, or that they were suffering from a variety of common and minor ailments – notable ones being anxiety, back problems, and chronic pain.
Authors have speculated on the socio-cultural and psychological reasons for the appeal of alternative medicines among that minority using them "in lieu" of conventional medicine. There are several socio-cultural reasons for the interest in these treatments centered on the low level of scientific literacy among the public at large and a concomitant increase in antiscientific attitudes and new age mysticism. Related to this are vigorous marketing of extravagant claims by the alternative medical community combined with inadequate media scrutiny and attacks on critics.
There is also an increase in conspiracy theories toward conventional medicine and pharmaceutical companies, mistrust of traditional authority figures, such as the physician, and a dislike of the current delivery methods of scientific biomedicine, all of which have led patients to seek out alternative medicine to treat a variety of ailments. Many patients lack access to contemporary medicine, due to a lack of private or public health insurance, which leads them to seek out lower-cost alternative medicine. Medical doctors are also aggressively marketing alternative medicine to profit from this market.
Patients can also be averse to the painful, unpleasant, and sometimes-dangerous side effects of biomedical treatments. Treatments for severe diseases such as cancer and HIV infection have well-known, significant side-effects. Even low-risk medications such as antibiotics can have potential to cause life-threatening anaphylactic reactions in a very few individuals. Also, many medications may cause minor but bothersome symptoms such as cough or upset stomach. In all of these cases, patients may be seeking out alternative treatments to avoid the adverse effects of conventional treatments.
Schofield and others, in a systematic review published in 2011, make ten recommendations which they think may increase the effectiveness of consultations in a conventional (here: oncology) setting, such as "Ask questions about CAM use at critical points in the illness trajectory"; "Respond to the person's emotional state"; and "Provide balanced, evidence-based advice". They suggest that this approach may address "... concerns surrounding CAM use [and] encourage informed decision-making about CAM and ultimately, improve outcomes for patients".

</doc>
<doc id="1847" url="http://en.wikipedia.org/wiki?curid=1847" title="Archimedean solid">
Archimedean solid

In geometry, an Archimedean solid is a highly symmetric, semi-regular convex polyhedron composed of two or more types of regular polygons meeting in identical vertices. They are distinct from the Platonic solids, which are composed of only one type of polygon meeting in identical vertices, and from the Johnson solids, whose regular polygonal faces do not meet in identical vertices.
"Identical vertices" are usually taken to mean that for any two vertices, there must be an isometry of the entire solid that takes one vertex to the other. Sometimes it is instead only required that the faces that meet at one vertex are related isometrically to the faces that meet at the other. This difference in definitions controls whether the elongated square gyrobicupola (pseudo-rhombicuboctahedron) is considered an Archimedean solid or a Johnson solid: it is the unique convex polyhedron that has regular polygons meeting in the same way at each vertex, but that does not have a global symmetry taking every vertex to every other vertex. Based on its existence, Branko Grünbaum (2009) has suggested a terminological distinction in which an Archimedean solid is defined as having the same vertex figure at each vertex (including the elongated square gyrobicupola) while a uniform polyhedron is defined as having each vertex symmetric to each other vertex (excluding the gyrobicupola).
Prisms and antiprisms, whose symmetry groups are the dihedral groups, are generally not considered to be Archimedean solids, despite meeting the above definition. With this restriction, there are only finitely many Archimedean solids. All but the elongated square gyrobicupola can be made via Wythoff constructions from the Platonic solids with tetrahedral, octahedral and icosahedral symmetry.
Origin of name.
The Archimedean solids take their name from Archimedes, who discussed them in a now-lost work. Pappus refers to it, stating that Archimedes listed 13 polyhedra. During the Renaissance, artists and mathematicians valued "pure forms" and rediscovered all of these forms. This search was almost entirely completed around 1620 by Johannes Kepler, who defined prisms, antiprisms, and the non-convex solids known as the Kepler-Poinsot polyhedra.
Kepler may have also found the elongated square gyrobicupola (pseudorhombicuboctahedron): at least, he once stated that there were 14 Archimedean solids. However, his published enumeration only includes the 13 uniform polyhedra, and the first clear statement of the pseudorhombicuboctahedron's existence was made in 1905, by Duncan Sommerville.
Classification.
There are 13 Archimedean solids (not counting the elongated square gyrobicupola; 15 if the mirror images of two enantiomorphs, see below, are counted separately).
Here the "vertex configuration" refers to the type of regular polygons that meet at any given vertex. For example, a vertex configuration of (4,6,8) means that a square, hexagon, and octagon meet at a vertex (with the order taken to be clockwise around the vertex).
Some definitions of semiregular polyhedron include one more figure, the elongated square gyrobicupola or "pseudo-rhombicuboctahedron".
Properties.
The number of vertices is 720° divided by the vertex angle defect.
The cuboctahedron and icosidodecahedron are edge-uniform and are called quasi-regular.
The duals of the Archimedean solids are called the Catalan solids. Together with the bipyramids and trapezohedra, these are the face-uniform solids with regular vertices.
Chirality.
The snub cube and snub dodecahedron are known as "chiral", as they come in a left-handed (Latin: levomorph or laevomorph) form and right-handed (Latin: dextromorph) form. When something comes in multiple forms which are each other's three-dimensional mirror image, these forms may be called enantiomorphs. (This nomenclature is also used for the forms of certain chemical compounds).
Construction of Archimedean solids.
The different Archimedean and Platonic solids can be related to each other using a handful of general constructions. Starting with a Platonic solid, truncation involves cutting away of corners. To preserve symmetry, the cut is in a plane perpendicular to the line joining a corner to the center of the polyhedron and is the same for all corners. Depending on how much is truncated (see table below), different Platonic and Archimedean (and other) solids can be created. Expansion or cantellation involves moving each face away from the center (by the same distance so as to preserve the symmetry of the Platonic solid) and taking the convex hull. Expansion with twisting also involves rotating the faces, thus breaking the rectangles corresponding to edges into triangles. The last construction we use here is truncation of both corners and edges. Ignoring scaling, expansion can also be viewed as truncation of corners and edges but with a particular ratio between corner and edge truncation.
Note the duality between the cube and the octahedron, and between the dodecahedron and the icosahedron. Also, in part due to self-duality of the tetrahedron, only one Archimedean solid has only tetrahedral symmetry.

</doc>
<doc id="1851" url="http://en.wikipedia.org/wiki?curid=1851" title="Antiprism">
Antiprism

In geometry, an "n"-sided antiprism is a polyhedron composed of two parallel copies of some particular "n"-sided polygon, connected by an alternating band of triangles. Antiprisms are a subclass of the prismatoids.
Antiprisms are similar to prisms except the bases are twisted relative to each other, and that the side faces are triangles, rather than quadrilaterals.
In the case of a regular "n"-sided base, one usually considers the case where its copy is twisted by an angle 180°/"n". Extra regularity is obtained by the line connecting the base centers being perpendicular to the base planes, making it a right antiprism. As faces, it has the two "n"-gonal bases and, connecting those bases, 2"n" isosceles triangles.
Uniform antiprism.
A uniform antiprism has, apart from the base faces, 2"n" equilateral triangles as faces. As a class, the uniform antiprisms form an infinite series of vertex-uniform polyhedra, as do the uniform prisms. For we have as degenerate case the regular tetrahedron as a "digonal antiprism", and for the non-degenerate regular octahedron as a "triangular antiprism".
The dual polyhedra of the antiprisms are the trapezohedra. Their existence was first discussed and their name was coined by Johannes Kepler.
Cartesian coordinates.
Cartesian coordinates for the vertices of a right antiprism with "n"-gonal bases and isosceles triangles are
with "k" ranging from 0 to 2"n"−1; if the triangles are equilateral,
Volume and surface area.
Let "a" be the edge-length of a uniform antiprism. Then the volume is
and the surface area is
Related polyhedra.
There are an infinite set of truncated antiprisms, including a lower-symmetry form of the truncated octahedron (truncated triangular antiprism). These can be alternated to create snub antiprisms, two of which are Johnson solids, and the "snub triangular antiprism" is a lower symmetry form of the icosahedron.
Symmetry.
The symmetry group of a right "n"-sided antiprism with regular base and isosceles side faces is D"n"d of order 4"n", except in the case of a tetrahedron, which has the larger symmetry group Td of order 24, which has three versions of D2d as subgroups, and the octahedron, which has the larger symmetry group Oh of order 48, which has four versions of D3d as subgroups.
The symmetry group contains inversion if and only if "n" is odd.
The rotation group is D"n" of order 2"n", except in the case of a tetrahedron, which has the larger rotation group T of order 12, which has three versions of D2 as subgroups, and the octahedron, which has the larger rotation group O of order 24, which has four versions of D3 as subgroups.
Star antiprism.
Uniform star antiprisms are named by their star polygon bases, {p/q}, and exist in prograde and retrograde (crossed) solutions. Crossed forms have intersecting vertex figures, and are denoted by inverted fractions, p/(p-q) instead of p/q, like 5/3 versus 5/2.
In the retrograde forms but not in the prograde forms, the triangles joining the star bases intersect the axis of rotational symmetry.
Some retrograde star antiprisms with regular star polygon bases cannot be constructed with equal edge lengths, so are not uniform polyhedra. Star antiprism compounds also can be constructed where p and q have common factors; thus a 10/4 antiprism is the compound of two 5/2 star antiprisms. 

</doc>
<doc id="1853" url="http://en.wikipedia.org/wiki?curid=1853" title="Natural history of Africa">
Natural history of Africa

The natural history of Africa encompasses some of the well known megafauna of that continent.
Natural history is the study and description of organisms and natural objects, especially their origins, evolution, and interrelationships.
Flora.
The vegetation of Africa follows very closely the distribution of heat and moisture. The northern and southern temperate zones have a flora distinct from that of the continent generally, which is tropical. In the countries bordering the Mediterranean, there are groves of orange and olive trees, evergreen oaks, cork trees and pines, intermixed with cypresses, myrtles, arbutus and fragrant tree-heaths.
South of the Atlas Range the conditions alter. The zones of minimum rainfall have a very scanty flora, consisting of plants adapted to resist the great dryness. Characteristic of the Sahara is the date palm, which flourishes where other vegetation can scarcely maintain existence, while in the semidesert regions the acacia, from which gum arabic is obtained, is abundant.
The more humid regions have a richer vegetation; dense forest where the rainfall is greatest and variations of temperature least, conditions found chiefly on the tropical coasts, and in the west African equatorial basin with its extension towards the upper Nile; and savanna interspersed with trees on the greater part of the plateaus, passing as the desert regions are approached into a scrub vegetation consisting of thorny acacias, etc. Forests also occur on the humid slopes of mountain ranges up to a certain elevation. In the coast regions the typical tree is the mangrove, which flourishes wherever the soil is of a swamp character.
The dense forests of West Africa contain, in addition to a great variety of hardwoods, two palms, "Elaeis guineensis" (oil palm) and "Raphia vinifera" (bamboo palm), not found, generally speaking, in the savanna regions. "Bombax" or silk-cotton trees attain gigantic proportions in the forests, which are the home of the India rubber-producing plants and of many valuable kinds of timber trees, such as odum ("Chlorophora excelsa"), ebony, mahogany ("Khaya senegalensis"), Oldfieldia ("Oldfieldia africana") and camwood ("Baphia nitida"). The climbing plants in the tropical forests are exceedingly luxuriant and the undergrowth or "bush" is extremely dense.
In the savannas the most characteristic trees are the monkey bread tree or baobab ("Adanisonia digitata"), doum palm ("Hyphaene") and euphorbias. The coffee plant grows wild in such widely separated places as Liberia and southern Ethiopia. The higher mountains have a special flora showing close agreement over wide intervals of space, as well as affinities with the mountain flora of the eastern Mediterranean, the Himalaya and Indo-China.
In the swamp regions of north-east Africa papyrus and associated plants, including the soft-wooded ambach, flourish in immense quantities, and little else is found in the way of vegetation. South Africa is largely destitute of forest save in the lower valleys and coast regions. Tropical flora disappears, and in the semi-desert plains the fleshy, leafless, contorted species of kapsias, mesembryanthemums, aloes and other succulent plants make their appearance. There are, too, valuable timber trees, such as the Yellow-wood ("Podocarpus elongatus"), stinkwood ("Ocotea"), sneezewood or Cape ebony ("Pteroxylon utile") and ironwood. Extensive miniature woods of heaths are found in almost endless variety and covered throughout the greater part of the year with innumerable blossoms in which red is very prevalent. Of the grasses of Africa alfa is very abundant in the plateaus of the Atlas range.
Fauna.
The fauna again shows the effect of the characteristics of the vegetation. The open savannas are the home of large ungulates, especially antelopes, the giraffe (peculiar to Africa), zebra, buffalo, wild donkey and four species of rhinoceros; and of carnivores, such as the lion, leopard, hyena, etc. The okapi (a genus restricted to Africa) is found only in the dense forests of the Congo basin. Bears are confined to the Atlas region, wolves and foxes to North Africa. The elephant (though its range has become restricted through the attacks of hunters) is found both in the savannas and forest regions, the latter being otherwise poor in large game, though the special habitat of the chimpanzee and gorilla. Baboons and mandrills, with few exceptions, are peculiar to Africa. The single-humped camel, as a domestic animal, is
especially characteristic of the northern deserts and steppes.
The rivers in the tropical zone abound with hippopotami and crocodiles, the former entirely confined to Africa. The vast herds of game, formerly so characteristic of many parts of Africa, have much diminished with the increase of intercourse with the interior. Game reserves have, however, been established in South Africa, British Central Africa, British East Africa, Somaliland, etc., while measures for the protection of wild animals were laid down in an international convention signed in May 1900.
The ornithology of northern Africa presents a close resemblance to that of southern Europe, scarcely a species being found which does not also occur in the other countries bordering the Mediterranean. Among the birds most characteristic of Africa are the ostrich and the secretary-bird. The ostrich is widely dispersed, but is found chiefly in the desert and steppe regions. The secretary-bird is common in the south. The weaver birds and their allies, including the long-tailed whydahs, are abundant, as are, among game-birds, the francolin and guineafowl. Many of the smaller birds, such as the sunbirds, bee-eaters, the parrots and kingfishers, as well as the larger plantain-eaters, are noted for the brilliance of their plumage.
Of reptiles the lizard and chameleon are common, and there are a number of venomous snakes, though these are not so numerous as in other tropical countries.
The scorpion is abundant. Of insects Africa has many thousand different kinds; of these the locust is the proverbial scourge of the continent, and the ravages of the termites are almost incredible. The spread of malaria by means of mosquitoes is common. The tsetse fly, whose bite is fatal to all domestic animals, is common in many districts of South and East Africa. It is found nowhere outside Africa.

</doc>
<doc id="1854" url="http://en.wikipedia.org/wiki?curid=1854" title="Geography of Africa">
Geography of Africa

Africa is a continent comprising 62 political territories, representing the largest of the great southward projections from the main mass of Earth's surface. Within its regular outline, it comprises an area of 30368609 km2, including adjacent islands. Its highest mountain is Mount Kilamanjaro, its largest lake is Lake Victoria and its longest river is the Nile.
Separated from Europe by the Mediterranean Sea and from much of Asia by the Red Sea, Africa is joined to Asia at its northeast extremity by the Isthmus of Suez (which is transected by the Suez Canal), 130 km wide. For geopolitical purposes, the Sinai Peninsula of Egypt – east of the Suez Canal – is often considered part of Africa. From the most northerly point, Ras ben Sakka in Tunisia, in 37°21′ N, to the most southerly point, Cape Agulhas in South Africa, 34°51′15″ S, is a distance approximately of 8000 km; from Cap-Vert, 17°31′13″W, the westernmost point, to Ras Hafun in Somalia, 51°27′52″ E, the most easterly projection, is a distance (also approximately) of 7400 km. The length of coastline is 26000 km and the absence of deep indentations of the shore is shown by the fact that Europe, which covers only 10400000 km2, has a coastline of 32000 km.
The main structural lines of the continent show both the east-to-west direction characteristic, at least in the eastern hemisphere, of the more northern parts of the world, and the north-to-south direction seen in the southern peninsulas. Africa is thus mainly composed of two segments at right angles, the northern running from east to west, and the southern from north to south.
Main features.
The average elevation of the continent approximates closely to 600 m above sea level, roughly near to the mean elevation of both North and South America, but considerably less than that of Asia, 950 m. In contrast with other continents, it is marked by the comparatively small area of either very high or very low ground, lands under 180 m occupying an unusually small part of the surface; while not only are the highest elevations inferior to those of Asia or South America, but the area of land over 3000 m is also quite insignificant, being represented almost entirely by individual peaks and mountain ranges. Moderately elevated tablelands are thus the characteristic feature of the continent, though the surface of these is broken by higher peaks and ridges. (So prevalent are these isolated peaks and ridges that a specialised term ["Inselberg-landschaft"] has been adopted in Germany to describe this kind of country, thought to be in great part the result of wind action.)
As a general rule, the higher tablelands lie to the east and south, while a progressive diminution in altitude towards the west and north is observable. Apart from the lowlands and the Atlas mountain range, the continent may be divided into two regions of higher and lower plateaus, the dividing line (somewhat concave to the north-west) running from the middle of the Red Sea to about 6 deg. S. on the west coast...
Africa can be divided into a number of geographic zones:
Plateau region.
The high southern and eastern plateaus, rarely falling below 600 m, have a mean elevation of about 1000 m. The South African Plateau, as far as about 12° S, is bounded east, west and south by bands of high ground which fall steeply to the coasts. On this account South Africa has a general resemblance to an inverted saucer. Due south the plateau rim is formed by three parallel steps with level ground between them. The largest of these level areas, the Great Karoo, is a dry, barren region, and a large tract of the plateau proper is of a still more arid character and is known as the Kalahari Desert.
The South African Plateau is connected towards East African plateau, with probably a slightly greater average elevation, and marked by some distinct features. It is formed by a widening out of the eastern axis of high ground, which becomes subdivided into a number of zones running north and south and consisting in turn of ranges, tablelands and depressions. The most striking feature is the existence of two great lines of depression, due largely to the subsidence of whole segments of the Earth's crust, the lowest parts of which are occupied by vast lakes. Towards the south the two lines converge and give place to one great valley (occupied by Lake Nyasa), the southern part of which is less distinctly due to rifting and subsidence than the rest of the system.
Farther north the western depression, known as the Albertine Rift is occupied for more than half its length by water, forming the Great Lakes of Tanganyika, Kivu, Lake Edward and Lake Albert, the first-named over 400 miles (600 km) long and the longest freshwater lake in the world. Associated with these great valleys are a number of volcanic peaks, the greatest of which occur on a meridional line east of the eastern trough. The eastern branch of the East African Rift, contains much smaller lakes, many of them brackish and without outlet, the only one comparable to those of the western trough being Lake Turkana or Basso Norok.
At no great distance east of this rift-valley is Mount Kilimanjaro - with its two peaks Kibo and Mawenzi, the latter being 5889 m, and the culminating point of the whole continent — and Mount Kenya, which is 5184 m. Hardly less important is the Ruwenzori Range, over 5060 m, which lies east of the western trough. Other volcanic peaks rise from the floor of the valleys, some of the Kirunga (Mfumbiro) group, north of Lake Kivu, being still partially active. This could cause most of the citys and states to be flooded with lava and ash.
The third division of the higher region of Africa is formed by the Ethiopian Highlands, a rugged mass of mountains forming the largest continuous area of its altitude in the whole continent, little of its surface falling below 1500 m, while the summits reach heights of 4600 m to 4900 m (15,000 to 16,000 ft). This block of country lies just west of the line of the great East African Trough, the northern continuation of which passes along its eastern escarpment as it runs up to join the Red Sea. There is, however, in the centre a circular basin occupied by Lake Tsana.
Both in the east and west of the continent the bordering highlands are continued as strips of plateau parallel to the coast, the Ethiopian mountains being continued northwards along the Red Sea coast by a series of ridges reaching in places a height of 2000 m. In the west the zone of high land is broader but somewhat lower. The most mountainous districts lie inland from the head of the Gulf of Guinea (Adamawa, etc.), where heights of 1800 m to 2400 m (6000 to 8000 ft) are reached. Exactly at the head of the gulf the great peak of the Cameroon, on a line of volcanic action continued by the islands to the south-west, has a height of 4075 m, while Clarence Peak, in Fernando Po, the first of the line of islands, rises to over 2700 m. Towards the extreme west the Futa Jallon highlands form an important diverging point of rivers, but beyond this, as far as the Atlas chain, the elevated rim of the continent is almost wanting.
Plains.
The area between the east and west coast highlands, which north of 17° N is mainly desert, is divided into separate basins by other bands of high ground, one of which runs nearly centrally through North Africa in a line corresponding roughly with the curved axis of the continent as a whole. The best marked of the basins so formed (the Congo basin) occupies a circular area bisected by the equator, once probably the site of an inland sea.
Running along the south of desert is the plains region known as the Sahel.
The arid region, the Sahara — the largest desert in the world, covering 9000000 km2 — extends from the Atlantic to the Red Sea. Though generally of slight elevation, it contains mountain ranges with peaks rising to 2400 m Bordered N.W. by the Atlas range, to the northeast a rocky plateau separates it from the Mediterranean; this plateau gives place at the extreme east to the delta of the Nile. That river (see below) pierces the desert without modifying its character. The Atlas range, the north-westerly part of the continent, between its seaward and landward heights encloses elevated steppes in places 160 km broad. From the inner slopes of the plateau numerous wadis take a direction towards the Sahara. The greater part of that now desert region is, indeed, furrowed by old water-channels.
Mountains.
The following table gives the approximate altitudes of the chief mountains of the continent:
Rivers.
From the outer margin of the African plateaus, a large number of streams run to the sea with comparatively short courses, while the larger rivers flow for long distances on the interior highlands, before breaking through the outer ranges. The main drainage of the continent is to the north and west, or towards the basin of the Atlantic Ocean.
To the main African rivers belong: Nile (the longest river of Africa), Congo (river with the highest water discharge on the continent) and the Niger, which flows half of its length through the arid areas. The largest lakes are the following: Lake Victoria (Lake Ukerewe), Lake Chad, in the centre of the continent, Lake Tanganika, lying between the Democratic Republic of Congo, Burundi, Tanzania and Zambia. There is also the considerably large Lake Malawi stretching along the eastern border of one of the poorest countries in the world -Malawi. There are also numerous water dams throughout the continent: Kariba on the river of Zambezi, Asuan in Egypt on the river of Nile and the biggest dam of the continent lying completely in The republic of Ghana is called Akosombo on the Volta river (Fobil 2003).
The high lake plateau of the African Great Lakes region contains the headwaters of both the Nile and the Congo.
The upper Nile receives its chief supplies from the mountainous region adjoining the Central African trough in the neighbourhood of the equator. From there, streams pour eastward into Lake Victoria, the largest lake in Africa (covering over 26,000 square m.), and to the west and north into Lake Edward and Lake Albert. To the latter of these, the effluents of the other two lakes add their waters. Issuing from there, the Nile flows northward, and between the latitudes of 7 and 10 degrees north it traverses a vast marshy level, where its course is liable to being blocked by floating vegetation. After receiving the Bahr-el-Ghazal from the west and the Sobat, Blue Nile and Atbara from the Ethiopian Highlands (the chief gathering ground of the flood-water), it separates the great desert with its fertile watershed, and enters the Mediterranean at a vast delta.
The most remote head-stream of the Congo is the Chambezi, which flows southwest into the marshy Lake Bangweulu. From this lake issues the Congo, known in its upper course by various names. Flowing first south, it afterwards turns north through Lake Mweru and descends to the forest-clad basin of west equatorial Africa. Traversing this in a majestic northward curve, and receiving vast supplies of water from many great tributaries, it finally turns southwest and cuts a way to the Atlantic Ocean through the western highlands.
North of the Congo basin, and separated from it by a broad undulation of the surface, is the basin of Lake Chad – a flat-shored, shallow lake filled principally by the Chari coming from the southeast.
West of this is the basin of the Niger, the third major river of Africa. With its principal source in the far west, it reverses the direction of flow exhibited by the Nile and Congo, and ultimately flows into the Atlantic — a fact that eluded European geographers for many centuries. An important branch, however — the Benue—flows from the southeast.
These four river basins occupy the greater part of the lower plateaus of North and West Africa — the remainder consists of arid regions watered only by intermittent streams that do not reach the sea.
Of the remaining rivers of the Atlantic basin, the Orange, in the extreme south, brings the drainage from the Drakensberg on the opposite side of the continent, while the Kunene, Kwanza, Ogowe and Sanaga drain the west coastal highlands of the southern limb; the Volta, Komoe, Bandama, Gambia and Senegal the highlands of the western limb. North of the Senegal, for over 1000 miles (1600 km) of coast, the arid region reaches to the Atlantic. Farther north are the streams, with comparatively short courses, reaching the Atlantic and Mediterranean from the Atlas mountains.
Of the rivers flowing to the Indian Ocean, the only one draining any large part of the interior plateaus is the Zambezi, whose western branches rise in the western coastal highlands. The main stream has its rise in 11°21′3″ S 24°22′ E, at an elevation of 5000 ft. It flows to the west and south for a considerable distance before turning eastward. All the largest tributaries, including the Shire, the outflow of Lake Nyasa, flow down the southern slopes of the band of high ground stretching across the continent from 10° to 12° S. In the southwest, the Zambezi system interlaces with that of the Taukhe (or Tioghe), from which it at times receives surplus water. The rest of the water of the Taukhe, known in its middle course as the Okavango, is lost in a system of swamps and saltpans that was formerly centred in Lake Ngami, now dried up.
Farther south, the Limpopo drains a portion of the interior plateau, but breaks through the bounding highlands on the side of the continent nearest its source. The Rovuma, Rufiji and Tana principally drain the outer slopes of the African Great Lakes highlands.
In the Horn region to the north, the Jubba and the Shebelle rivers begin in the Ethiopian Highlands. These rivers mainly flow southwards, with the Jubba emptying in the Indian Ocean. The Shebelle River reaches a point to the southwest. After that, it consists of swamps and dry reaches before finally disappearing in the desert terrain near the Jubba River. Another large stream, the Hawash, rising in the Ethiopian mountains, is lost in a saline depression near the Gulf of Aden.
Between the basins of the Atlantic and Indian Oceans, there is an area of inland drainage along the centre of the Ethiopian plateau, directed chiefly into the lakes in the Great Rift Valley. The largest river is the Omo, which, fed by the rains of the Ethiopian highlands, carries down a large body of water into Lake Rudolf. The rivers of Africa are generally obstructed either by bars at their mouths, or by cataracts at no great distance upstream. But when these obstacles have been overcome, the rivers and lakes afford a vast network of navigable waters.
The area of the Congo basin is greater than that of any other river except the Amazon, while the African inland drainage area is greater than that of any continent but Asia, where the corresponding area is 4,000,000 square miles (10 Mm²).
Lakes.
The principal lakes of Africa are situated in the African Great Lakes plateau. As a rule, the lakes found within the Great Rift Valley have steep sides and are very deep. This is the case with the two largest of the type, Tanganyika and Nyasa, the latter with depths of 430 fathoms (790 m).
Others, however, are shallow, and hardly reach the steep sides of the valleys in the dry season. Such are Lake Rukwa, in a subsidiary depression north of Nyasa, and Eiassi and Manyara in the system of the Great Rift Valley. Lakes of the broad type are of moderate depth, the deepest sounding in Lake Victoria being under 50 fathoms (90 m).
Besides the African Great Lakes, the principal lakes on the continent are: Lake Chad, in the northern inland watershed; Bangweulu and Mweru, traversed by the head-stream of the Congo; and Lake Mai-Ndombe and Ntomba (Mantumba), within the great bend of that river. All, except possibly Mweru, are more or less shallow, and Lake Chad appears to be drying up.
Divergent opinions have been held as to the mode of origin of the African Great Lakes, especially Tanganyika, which some geologists have considered to represent an old arm of the sea, dating from a time when the whole central Congo basin was under water; others holding that the lake water has accumulated in a depression caused by subsidence. The former view is based on the existence in the lake of organisms of a decidedly marine type. They include jellyfish, molluscs, prawns, crabs, etc.
Islands.
With the exception of Madagascar the African islands are small. Madagascar, with an area of 229820 sqmi, is, after Greenland, New Guinea and Borneo, the fourth largest island on the Earth. It lies in the Indian Ocean, off the S.E. coast of the continent, from which it is separated by the deep Mozambique channel, 250 mi wide at its narrowest point. Madagascar in its general structure, as in flora and fauna, forms a connecting link between Africa and southern Asia. East of Madagascar are the small islands of Mauritius and Réunion. There are also islands in the Gulf of Guinea on which lies the Republic of Sao Tomé and Príncipe (islands of São Tomé and Príncipe). Part of the Republic of Equatorial Guinea is lying on the island of Bioko (with the capital Malabo and the town of Lubu) and the island of Annobón. Socotra lies E.N.E. of Cape Guardafui. Off the north-west coast are the Canary and Cape Verde archipelagoes. which, like some small islands in the Gulf of Guinea, are of volcanic origin. The South Atlantic Islands of Saint Helena and Ascension are classed as Africa but are situated on the Mid-Atlantic Ridge half way to South America.
Climatic Conditions.
Lying almost entirely within the tropics, and equally to north and south of the equator, Africa does not show excessive variations of temperature.
Great heat is experienced in the lower plains and desert regions of North Africa, removed by the great width of the continent from the influence of the ocean, and here, too, the contrast between day and night, and between summer and winter, is greatest. (The rarity of the air and the great radiation during the night cause the temperature in the Sahara to fall occasionally to freezing point.)
Farther south, the heat is to some extent modified by the moisture brought from the ocean, and by the greater elevation of a large part of the surface, especially in East Africa, where the range of temperature is wider than in the Congo basin or on the Guinea coast.
In the extreme north and south the climate is a warm temperate one, the northern countries being on the whole hotter and drier than those in the southern zone; the south of the continent being narrower than the north, the influence of the surrounding ocean is more felt.
The most important climatic differences are due to variations in the amount of rainfall. The wide heated plains of the Sahara, and in a lesser degree the corresponding zone of the Kalahari in the south, have an exceedingly scanty rainfall, the winds which blow over them from the ocean losing part of their moisture as they pass over the outer highlands, and becoming constantly drier owing to the heating effects of the burning soil of the interior; while the scarcity of mountain ranges in the more central parts likewise tends to prevent condensation. In the inter-tropical zone of summer precipitation, the rainfall is greatest when the sun is vertical or soon after. It is therefore greatest of all near the equator, where the sun is twice vertical, and less in the direction of both tropics.
The rainfall zones are, however, somewhat deflected from a due west-to-east direction, the drier northern conditions extending southwards along the east coast, and those of the south northwards along the west. Within the equatorial zone certain areas, especially on the shores of the Gulf of Guinea and in the upper Nile basin, have an intensified rainfall, but this rarely approaches that of the rainiest regions of the world. The rainiest district in all Africa is a strip of coastland west of Mount Cameroon, where there is a mean annual rainfall of about 390 in as compared with a mean of 458 in at Cherrapunji, in Meghalaya, India.
The two distinct rainy seasons of the equatorial zone, where the sun is vertical at half-yearly intervals, become gradually merged into one in the direction of the tropics, where the sun is overhead but once. Snow falls on all the higher mountain ranges, and on the highest the climate is thoroughly Alpine.
The countries bordering the Sahara are much exposed to a very dry wind, full of fine particles of sand, blowing from the desert towards the sea. Known in Egypt as the khamsin, on the Mediterranean as the sirocco, it is called on the Guinea coast the harmattan. This wind is not invariably hot; its great dryness causes so much evaporation that cold is not infrequently the result. Similar dry winds blow from the Kalahari Desert in the south. On the eastern coast the monsoons of the Indian Ocean are regularly felt, and on the southeast hurricanes are occasionally experienced.
Health.
The climate of Africa lends itself to certain environmental diseases, the most serious of which are: malaria, sleeping sickness and yellow fever. Malaria is the most deadly environmental disease in Africa. It is transmitted by a genus of mosquito (anopheles mosquito) native to Africa, and can be contracted over and over again. There is not yet a vaccine for malaria, which makes it difficult to prevent the disease from spreading in Africa. Recently, the dissemination of mosquito netting has helped lower the rate of malaria.
Yellow fever is a disease also transmitted by mosquitoes native to Africa. Unlike malaria, it cannot be contracted more than once. Like chicken pox, it is a disease that tends to be severe the later in life a person contracts the disease.
Sleeping sickness, or African trypanosomiasis, is a disease that usually affects animals, but has been known to be fatal to some humans as well. It is transmitted by the tse tse fly, and is found almost exclusively in Sub-Saharan Africa. This disease has had a significant impact on African development not because of its deadly nature, like Malaria, but because it has prevented Africans from pursuing agriculture (as the sleeping sickness would kill their livestock).
Extreme points.
These are the points that are farther north, south, east or west than any other location on the continent.
The highest point in Africa is Mount Kilimanjaro, 5891.8 m in Tanzania. The lowest point is Lake Asal, 153 m below sea level, in Djibouti.
See also.
Richard Grant 2014. Africa. Geographies of Change. New York: Oxford University Press.
External links.
 

</doc>
<doc id="1857" url="http://en.wikipedia.org/wiki?curid=1857" title="Approval voting">
Approval voting

Approval voting is a single-winner voting method used for elections. Each voter may 'approve' of (i.e., select) any number of candidates. The winner is the most-approved candidate.
Guy Ottewell first described the system in 1977. and also by Robert J. Weber, who coined the term "Approval Voting." It was more fully published in 1978 by political scientist Steven Brams and mathematician Peter Fishburn.
Description.
Approval voting can be considered a form of score voting, with the range restricted to two values, 0 and 1—or a form of majority judgment, with grades restricted to "good" and "poor". Approval Voting can also be compared to plurality voting, without the rule that discards ballots that vote for more than one candidate.
By treating each candidate as a separate question, "Do you approve of this person for the job?" approval voting lets each voter indicate support for one, some, or all candidates. All votes count equally, and everyone gets the same number of votes: one vote per candidate, either for or against. Final tallies show how many voters support each candidate, and the winner is the candidate whom the most voters support.
Approval voting ballots show, for each office being contested, a list of the candidates running for that seat. Next to each name is a checkbox, or another similar way to mark 'Yes' or 'No' for that candidate. This "check yes or no" approach means approval voting provides one of the simplest ballots for a voter to understand.
Ballots on which the voter marked every candidate the same (whether yes or no) have no effect on the outcome of the election. Each ballot can, therefore, be viewed as a small "delta" that separates two groups of candidates, those supported and those that are not. Each candidate approved is considered preferred to any candidate not approved, while the voter's preferences among approved candidates is unspecified, and likewise the voter's preferences among unapproved candidates is also unspecified.
Uses.
Approval voting has been adopted by the Mathematical Association of America (1986), the Institute of Management Sciences (1987) (now the Institute for Operations Research and the Management Sciences), the American Statistical Association (1987), and the Institute of Electrical and Electronics Engineers (1987). According to Steven J. Brams and Peter C. Fishburn, the IEEE board in 2002 rescinded its decision to use approval voting. IEEE Executive Director Daniel J. Senese stated that approval voting was abandoned because "few of our members were using it and it was felt that it was no longer needed."
Approval voting was used for Dartmouth Alumni Association elections for seats on the College Board of Trustees, but after some controversy it was replaced with traditional runoff elections by an alumni vote of 82% to 18% in 2009. Dartmouth students started to use approval voting to elect their student body president in 2011. In the first election, the winner secured the support of 41% of voters against several write-in candidates. In 2012, Suril Kantaria won with the support of 32% of the voters. In 2013, the winner earned the support of just under 40% of the voters.
Historically, several voting methods that incorporate aspects of approval voting have been used:
Effect on elections.
Approval voting advocates Steven Brams and Dudley R. Herschbach predict that approval voting should increase voter participation, prevent minor-party candidates from being spoilers, and reduce negative campaigning. The effect of this system as an electoral reform measure is not without critics, however. FairVote has a position paper arguing that approval voting has three flaws that undercut it as a method of voting and political vehicle. They argue that it can result in the defeat of a candidate who would win an absolute majority in a plurality system, can allow a candidate to win who might not win any support in a plurality elections, and has incentives for tactical voting.
One study showed that approval voting would not have chosen the same two winners as plurality voting (Chirac and Le Pen) in France's presidential election of 2002 (first round) – it instead would have chosen Chirac and Jospin as the top two to proceed to a runoff. Le Pen lost by a very high margin in the runoff, 82.2% to 17.8%, a sign that the true top two had not been found. Straight approval voting without a runoff, from the study, still would have selected Chirac, but with an approval percentage of only 36.7%, compared to Jospin at 32.9%. Le Pen, in that study, would have received 25.1%. In the real primary election, the top three were Chirac, 19.9%, Le Pen, 16.9%, and Jospin, 16.2%.
A generalized version of the Burr dilemma applies to approval voting when two candidates are appealing to the same subset of voters. Although approval voting differs from the voting system used in the Burr dilemma, approval voting can still leave candidates and voters with the generalized dilemma of whether to compete or cooperate.
While in the modern era there have been relatively few competitive approval voting elections where tactical voting is more likely, Brams argues that approval voting usually elects Condorcet winners in practice. Critics of the use of approval voting in the alumni elections for the Dartmouth Board of Trustees in 2009 placed its ultimately successful repeal before alumni voters, arguing that the system has not been electing the most centrist candidates. "The Dartmouth" editorialized that "When the alumni electorate fails to take advantage of the approval voting process, the three required Alumni Council candidates tend to split the majority vote, giving petition candidates an advantage. By reducing the number of Alumni Council candidates, and instituting a more traditional one-person, one-vote system, trustee elections will become more democratic and will more accurately reflect the desires of our alumni base."
Strategic voting.
Overview.
Approval voting is vulnerable to Bullet Voting and Compromising, while it is immune to Push-Over and Burying.
Bullet Voting occurs when a voter approves "only" candidate 'a' instead of "both" 'a' and 'b' for the reason that voting for 'b' can cause 'a' to lose.
Compromising occurs when a voter approves an "additional" candidate who is otherwise considered unacceptable to the voter to prevent an even worse alternative from winning.
Strategic Approval voting differs from ranked choice voting methods where voters might "reverse" the preference order of two options. Strategic Approval voting, with more than two options, involves the voter changing their approval threshold. The voter decides which options to give the "same" rating, despite having a strict preference order between them.
Sincere voting.
Approval voting experts describe sincere votes as those "... that directly reflect the true preferences of a voter, i.e., that do not report preferences 'falsely.'" They also give a specific definition of a sincere approval vote in terms of the voter's ordinal preferences as being any vote that, if it votes for one candidate, it also votes for any more preferred candidate. This definition allows a sincere vote to treat strictly preferred candidates the same, ensuring that every voter has at least one sincere vote. The definition also allows a sincere vote to treat equally preferred candidates differently. When there are two or more candidates, every voter has at least three sincere approval votes to choose from. Two of those sincere approval votes do not distinguish between any of the candidates: vote for none of the candidates and vote for all of the candidates. When there are three or more candidates, every voter has more than one sincere approval vote that distinguishes between the candidates.
Examples.
Based on the definition above, if there are four candidates, A, B, C, and D, and a voter has a strict preference order, preferring A to B to C to D, then the following are the voter's possible sincere approval votes:
If the voter instead equally prefers B and C, while A is still the most preferred candidate and D is the least preferred candidate, then all of the above votes are sincere and the following combination is also a sincere vote:
The decision between the above ballots is equivalent to deciding an arbitrary "approval cutoff." All candidates preferred to the cutoff are approved, all candidates less preferred are not approved, and any candidates equal to the cutoff may be approved or not arbitrarily.
Sincere strategy with ordinal preferences.
A sincere voter with multiple options for voting sincerely still has to choose which sincere vote to use. Voting strategy is a way to make that choice, in which case strategic approval voting includes sincere voting, rather than being an alternative to it. This differs from other voting systems that typically have a unique sincere vote for a voter.
When there are three or more candidates, the winner of an approval voting election can change, depending on which sincere votes are used. In some cases, approval voting can sincerely elect any one of the candidates, including a Condorcet winner and a Condorcet loser, without the voter preferences changing. To the extent that electing a Condorcet winner and not electing a Condorcet loser is considered desirable outcomes for a voting system, approval voting can be considered vulnerable to sincere, strategic voting. In one sense, conditions where this can happen are robust and are not isolated cases. On the other hand, the variety of possible outcomes has also been portrayed as a virtue of approval voting, representing the flexibility and responsiveness of approval voting, not just to voter ordinal preferences, but cardinal utilities as well.
Dichotomous preferences.
Approval voting avoids the issue of multiple sincere votes in special cases when voters have dichotomous preferences. For a voter with dichotomous preferences, approval voting is strategy-proof (also known as strategy-free). When all voters have dichotomous preferences and vote the sincere, strategy-proof vote, approval voting is guaranteed to elect the Condorcet winner, if one exists. However, having dichotomous preferences when there are three or more candidates is not typical. It is an unlikely situation for all voters to have dichotomous preferences when there are more than a few voters.
Having dichotomous preferences means that a voter has bi-level preferences for the candidates. All of the candidates are divided into two groups such that the voter is indifferent between any two candidates in the same group and any candidate in the top-level group is preferred to any candidate in the bottom-level group. A voter that has strict preferences between three candidates—prefers A to B and B to C—does not have dichotomous preferences.
Being strategy-proof for a voter means that there is a unique way for the voter to vote that is a strategically best way to vote, regardless of how others vote. In approval voting, the strategy-proof vote, if it exists, is a sincere vote.
Approval threshold.
Another way to deal with multiple sincere votes is to augment the ordinal preference model with an approval or acceptance threshold. An approval threshold divides all of the candidates into two sets, those the voter approves of and those the voter does not approve of. A voter can approve of more than one candidate and still prefer one approved candidate to another approved candidate. Acceptance thresholds are similar. With such a threshold, a voter simply votes for every candidate that meets or exceeds the threshold.
With threshold voting, it is still possible to not elect the Condorcet winner and instead elect the Condorcet loser when they both exist. However, according to Steven Brams, this represents a strength rather than a weakness of approval voting. Without providing specifics, he argues that the pragmatic judgements of voters about which candidates are acceptable should take precedence over the Condorcet criterion and other social choice criteria.
Strategy with cardinal utilities.
Voting strategy under approval is guided by two competing features of approval voting. On the one hand, approval voting fails the later-no-harm criterion, so voting for a candidate can cause that candidate to win instead of a more preferred candidate. On the other hand, approval voting satisfies the monotonicity criterion, so not voting for a candidate can never help that candidate win, but can cause that candidate to lose to a less preferred candidate. Either way, the voter can risk getting a less preferred election winner. A voter can balance the risk-benefit trade-offs by considering the voter's cardinal utilities, particularly via the von Neumann–Morgenstern utility theorem, and the probabilities of how others vote.
A rational voter model described by Myerson and Weber specifies an approval voting strategy that votes for those candidates that have a positive prospective rating. This strategy is optimal in the sense that it maximizes the voter's expected utility, subject to the constraints of the model and provided the number of other voters is sufficiently large.
An optimal approval vote always votes for the most preferred candidate and not for the least preferred candidate. However, an optimal vote can require voting for a candidate and not voting for a more preferred candidate if there 4 candidates or more.
Other strategies are also available and coincide with the optimal strategy in special situations. For example:
Another strategy is to vote for the top half of the candidates, the candidates that have an above-median utility. When the voter thinks that others are balancing their votes randomly and evenly, the strategy maximizes the voter's power or efficacy, meaning that it maximizes the probability that the voter will make a difference in deciding which candidate wins.
Optimal strategic approval voting fails to satisfy the Condorcet criterion and can elect a Condorcet loser. Strategic approval voting can guarantee electing the Condorcet winner in some special circumstances. For example, if all voters are rational and cast a strategically optimal vote based on a common knowledge of how all the other voters vote except for small-probability, statistically independent errors in recording the votes, then the winner will be the Condorcet winner, if one exists.
Strategy examples.
In the example election described here, assume that the voters in each faction share the following von Neumann-Morgenstern utilities, fitted to the interval between 0 and 100. The utilities are consistent with the rankings given earlier and reflect a strong preference each faction has for choosing its city, compared to weaker preferences for other factors such as the distance to the other cities.
Using these utilities, voters choose their optimal strategic votes based on what they think the various pivot probabilities are for pairwise ties. In each of the scenarios summarized below, all voters share a common set of pivot probabilities.
In the first scenario, voters all choose their votes based on the assumption that all pairwise ties are equally likely. As a result, they vote for any candidate with an above-average utility. Most voters vote for only their first choice. Only the Knoxville faction also votes for its second choice, Chattanooga. As a result, the winner is Memphis, the Condorcet loser, with Chattanooga coming in second place.
In the second scenario, all of the voters expect that Memphis is the likely winner, that Chattanooga is the likely runner-up, and that the pivot probability for a Memphis-Chattanooga tie is much larger than the pivot probabilities of any other pair-wise ties. As a result, each voter votes for any candidate they prefer more than the leading candidate, and also vote for the leading candidate if they prefer that candidate more than the expected runner-up. Each remaining scenario follows a similar pattern of expectations and voting strategies.
In the second scenario, there is a three-way tie for first place. This happens because the expected winner, Memphis, was the Condorcet loser and was also ranked last by any voter that did not rank it first.
Only in the last scenario does the actual winner and runner-up match the expected winner and runner-up. As a result, this can be considered a stable strategic voting scenario. In the language of game theory, this is an "equilibrium." In this scenario, the winner is also the Condorcet winner.
Compliance with voting system criteria.
Most of the mathematical criteria by which voting systems are compared were formulated for voters with ordinal preferences. In this case, approval voting requires voters to make an additional decision of where to put their approval cutoff (see examples above). Depending on how this decision is made, approval voting satisfies different sets of criteria.
There is no ultimate authority on which criteria should be considered, but the following are criteria that many voting theorists accept and considered desirable:
Multiple winners.
Approval voting can be extended to multiple winner elections. The naive way to do so is as "block approval voting", a simple variant on block voting where each voter can select an unlimited number of candidates and the candidates with the most approval votes win. This does not provide proportional representation and is subject to the Burr dilemma, among other problems.
Other ways of extending Approval voting to multiple winner elections have been devised. Among these are proportional approval voting for determining a proportional assembly, and Minimax Approval for determining a consensus assembly where the least satisfied voter is satisfied the most.
Ballot types.
Approval ballots can be of at least four semi-distinct forms. The simplest form is a blank ballot on which voters hand-write the names of the candidates they support. A more structured ballot lists all candidates, and voters mark each candidate they support. A more explicit structured ballot can list the candidates and provide two choices by each. (Candidate list ballots can include spaces for write-in candidates as well.)
All four ballots are theoretically equivalent. The more structured ballots may aid voters in offering clear votes so they explicitly know all their choices. The Yes/No format can help to detect an "undervote" when a candidate is left unmarked and allow the voter a second chance to confirm the ballot markings are correct. The "single bubble" format is incapable of producing invalid ballots (which might otherwise be rejected in counting).
Unless the second or fourth format is used, fraudulently adding votes to an approval voting ballot does not invalidate the ballot (that is, it does not make it appear inconsistent). Thus, approval voting raises the importance of ensuring that the "chain of custody" of ballots is secure.

</doc>
<doc id="1859" url="http://en.wikipedia.org/wiki?curid=1859" title="Arizona State University">
Arizona State University

Arizona State University (commonly referred to as ASU or Arizona State) is a public flagship metropolitan research university located on five campuses across the Phoenix, Arizona, Metropolitan Area. A sixth campus located in northwestern Arizona is known as the ASU Colleges at Lake Havasu City. ASU is the largest public university by enrollment in the United States.
ASU's charter, approved by the board of regents in 2014, is based on the "New American University" model created by current ASU President Michael Crow. It defines ASU as “a comprehensive public research university, measured not by whom we exclude, but rather by whom we include and how they succeed; advancing research and discovery of public value; and assuming fundamental responsibility for the economic, social, cultural and overall health of the communities it serves.”
ASU is classified as a research university with very high research activity (RU/VH) by the Carnegie Classification of Institutions of Higher Education. Since 2005 ASU has been ranked among the top research universities, public and private, in the U.S. based on research output, innovation, development, research expenditures, number of awarded patents and awarded research grant proposals. The Center for Measuring University Performance currently ranks ASU 31st among top U.S. public research universities.ASU was classified as a Research I institute in 1994; thus, making it one of the newest major research universities (public or private) in the nation.
Students will compete in 24 varsity sports beginning in 2016. In conjunction with the transition of the men's ACHA club hockey team to Division I of the NCAA, the 24th varsity sport will be an NCAA women’s team: Rowing is among the favored possibilities. The Arizona State Sun Devils are members of the Pacific-12 Conference and have won 23 NCAA championships. Along with multiple athletic clubs and recreational facilities, ASU is home to more than 1,100 registered student organizations, reflecting the diversity of the student body. To keep pace with the growth of the student population, the university is continuously renovating and expanding infrastructure. The demand for new academic halls, athletic facilities, student recreation centers, and residential halls is being addressed with donor contributions and public-private investments. ASU's residential halls accommodate one of the largest residential populations in the nation.
History.
1885–1929.
Arizona State University was established as the Territorial Normal School at Tempe on March 12, 1885, when the 13th Arizona Territorial Legislature passed an act to create a normal school to train teachers for the Arizona Territory. The campus consisted of a single, four-room schoolhouse on a 20-acre plot largely donated by Tempe residents George and Martha Wilson. Classes began with 33 students on February 8, 1886. The curriculum evolved over the years and the name was changed several times; the institution was also known as Arizona Territorial Normal School (1889–1896), Arizona Normal School (1896–1899), Normal School of Arizona (1899–1901), and Tempe Normal School (1901–1925). The school accepted both high school students and graduates, and awarded high school diplomas and teaching certificates to those who completed the requirements.
In 1923 the school stopped offering high school courses and added a high school diploma to the admissions requirements. In 1925 the school became the Tempe State Teachers College and offered four-year Bachelor of Education degrees as well as two-year teaching certificates. In 1929, the legislature authorized Bachelor of Arts in Education degrees as well, and the school was renamed the Arizona State Teachers College. Under the 30-year tenure of president Arthur John Matthews the school was given all-college student status. The first dormitories built in the state were constructed under his supervision. Of the 18 buildings constructed while Matthews was president, six are still currently in use. Matthews envisioned an "evergreen campus," with many shrubs brought to the campus, and implemented the planting of Palm Walk, now a landmark of the Tempe campus. His legacy is being continued to this day with the main campus having been declared a nationally recognized arboretum.
During the Great Depression, Ralph W. Swetman was hired as president for a three-year term. Although enrollment increased by almost 100 percent during his tenure due to the depression, many faculty were terminated and faculty salaries were cut.
1930–1989.
In 1933, Grady Gammage, then president of Arizona State Teachers College at Flagstaff, became president of ASU, a tenure that would last for nearly 28 years. Like his predecessor, Gammage oversaw construction of a number of buildings on the Tempe campus. He also oversaw the development of the university, graduate programs. The school's name was changed to Arizona State College in 1945, and finally to Arizona State University in 1958.
By the 1960s, with the presidency of G. Homer Durham, the University began to expand its academic curriculum by establishing several new colleges and beginning to award Doctor of Philosophy and other doctoral degrees.
The next three presidents—Harry K. Newburn, 1969–71, John W. Schwada, 1971–81, and J. Russell Nelson, 1981–89—and Interim President Richard Peck, 1989, led the university to increased academic stature, creation of the West campus, and rising enrollment.
1990–present.
Under the leadership of Lattie F. Coor, from 1990 to 2002, ASU grew through the creation of the Polytechnic campus and extended education sites. Increased commitment to diversity, quality in undergraduate education, research, and economic development occurred over his 12-year tenure. Part of his legacy to the university was a successful fund-raising campaign: through private donations, more than $500 million was invested in areas that would significantly impact the future of ASU. Among the campaign's achievements were the naming and endowing of Barrett, The Honors College, and the Katherine K. Herberger College of Fine Arts; the creation of many new endowed faculty positions; and hundreds of new scholarships and fellowships.
On July 1, 2002, Michael M. Crow became the university's 16th president. At his inauguration, he outlined his vision for transforming ASU into a "New American University"—one that would be open and inclusive, with a goal set for the university to meet Association of American Universities (AAU) criteria and to become a member. Furthermore, he initiated the idea of transforming ASU into "One University in Many Places" by merging ASU's several campuses into a single institution, sharing students, faculty, staff and accreditation. Aided by hundreds of millions of dollars in donations, ASU embarked on a years-long research facility capital building effort, resulting in the establishment of the Biodesign Institute and several large interdisciplinary research buildings. Along with the research facilities, the university faculty was expanded, including the addition of three Nobel Laureates. In addition, ASU's Downtown Phoenix campus was vastly expanded with several of the University's colleges and schools relocated to the downtown campus. Since fiscal year 2002 the university's research expenditures have tripled and more than 1.5 million sq. ft. of new research space has been added to the university's research facilities.
The economic downturn that began in 2008 took a particularly hard toll on Arizona, resulting in large cuts to ASU's budget. In response to these cuts, ASU underwent several rounds of reorganizations, combining academic departments, consolidating colleges and schools, and reducing university staff and administrators; however, with an economic recovery underway in 2011, the university continued its campaign to expand the West and Polytechnic Campuses, and establishing a set of low-cost, teaching-focused extension campuses in Lake Havasu City and Payson, Arizona. The university has announced that a new building for the Sandra Day O’Connor College of Law will be built on the Downtown Phoenix Campus, relocating faculty and students from the Tempe Campus. The university plans to establish the Arizona Center for Law and Society in 2016.
Organization and Administration.
The Arizona Board of Regents governs Arizona State University as well as the other state's public universities; University of Arizona and Northern Arizona University. The Board of Regents is composed of twelve members including eleven voting, and one non-voting member. Members of the board include the Governor and the Superintendent of Public Instruction acting as ex-officio members, eight volunteer Regent members with eight years term that are appointed by the Governor, and two Student Regents with two years term, serving a one-year term as non-voting apprentices. ABOR provides policy guidance to the state universities of Arizona. ASU has multiple campus locations, covering the Phoenix metropolitan areas including the Main Tempe campus in Tempe, the West campus and Downtown Phoenix campus both in Phoenix, and the Polytechnic campus in Mesa. It also offers courses and degrees through ASU online to advance the mission of the university.
The Arizona Board of Regents appoints and elects the president of the university, who is considered the chief executive officer and the chief budget officer of the institution. The president is responsible for the execution of measures enacted by the Board of Regents, controls the property of the university, and acts as the official representative of the university to the Board of Regents. The chief executive officer is assisted through the administration of the institution by the provost, vice presidents, deans, faculty, directors, department chairs, and other officers. The president also selects and appoints administrative officers and general counsels. The 16th and current ASU president is Michael M. Crow, and has served since July 1, 2002.
Campuses and locations.
ASU's academic programs are spread across campuses in the Phoenix Metropolitan Area; however, unlike most multi-campus institutions, ASU describes itself as "one university in many places," inferring that there is "not a system with separate campuses, and not one main campus with branch campuses." The university considers each campus "distinctive" and academically focused on certain aspects of the overall university mission. The Tempe Campus is the university's research and graduate school center. Undergraduate studies on the Tempe campus are research-based programs designed to prepare students for graduate school, professional school, or employment. The Polytechnic campus is designed with an emphasis on professional and technological programs for direct workforce preparation. The Polytechnic campus is the location of many of the university's simulators and laboratories dedicated for project-based learning. The West campus is focused on interdisciplinary degrees and the liberal arts, while maintaining professional programs with a direct impact on the community and society. The Downtown Campus focuses on direct urban and public programs such as nursing, public policy, criminal justice, mass communication, and journalism. ASU recently relocated some nursing and health related programs to its new ASU-Mayo Medical School Campus. Inter-campus shuttles and light rail allow students and faculty to easily travel between the campuses. In addition to the physical campuses, ASU's "virtual campus", housed at the university's SkySong Innovation Center, provides online and extended education.
Tempe campus.
ASU's Tempe campus is located in downtown Tempe, Arizona, about eight miles (13 km) east of downtown Phoenix. The campus is considered urban, and is approximately 642 acre in size. The campus is arranged around broad pedestrian malls and is completely encompassed by an arboretum. The Tempe campus is also the largest of ASU's campuses, with 59,794a[›] students enrolled in at least one class on campus.
The Tempe campus is ASU's original campus, and Old Main, the first building constructed, still stands today. There are many notable landmarks on campus, including Grady Gammage Memorial Auditorium, designed by Frank Lloyd Wright, Palm Walk, which is lined by 111 palm trees, Charles Trumbull Hayden Library, the University Club Building, Margaret Gisolo Dance Theatre, and University Bridge. In addition, the campus has an extensive public art collection, considered one of the ten best among university public art collections in America according to "Public Art Review". Against the northwest edge of campus is the Mill Avenue district (part of downtown Tempe) which has a college atmosphere that attracts many students to its restaurants and bars. The Tempe campus is also home to all of the university's athletic facilities.
West campus.
The West campus was established in 1984 by the Arizona Legislature and sits on 250 acre in a suburban area of northwest Phoenix. The West campus lies about 12 mi northwest of downtown Phoenix, and about 18 mi northwest of the Tempe campus. The West campus is designated as a Phoenix Point of Pride, and is nearly completely powered by a 4.6MW solar array. This campus is home to the New College of Interdisciplinary Arts and Sciences, the Mary Lou Fulton Teachers College, and selected programs of the W.P. Carey School of Business. The campus, patterned after the University of Oxford’s architecture, has recently opened a new residence hall, dining facility and recreation center.
Polytechnic campus.
Founded in 1996 as "ASU East," the ASU Polytechnic campus serves 10,521 students and is home to more than 40 bachelor’s, master’s and doctoral degrees in professional and technical programs through the College of Technology and Innovation, and selected programs of the W.P. Carey School of Business/Morrison School of Management and Agribusiness, Mary Lou Fulton Teachers College, the School of Letters and Sciences, and focuses on professional and technological programs including simulators and lab space in various fields of study. The 600 acre campus is located in southeast Mesa, Arizona, approximately 25 mi southeast of the Tempe campus, and 33 mi southeast of downtown Phoenix. The Polytechnic campus sits on the former Williams Air Force Base.
Downtown Phoenix campus.
The newest of ASU's four campuses, the Downtown Phoenix campus was established in 2006 on the north side of Downtown Phoenix. The campus has an urban design, with several large modern academic buildings intermingled with commercial and retail office buildings. In addition to the new buildings, the campus included the adaptive reuse of several existing structures, including a 1930s era Post Office that is on the National Register of Historic Places. Serving 17,151 students, the campus houses the College of Health Solutions, College of Nursing and Health Innovation, College of Public Service and Community Solutions and Walter Cronkite School of Journalism and Mass Communication. In the summer of 2013, the campus added the Sun Devil Fitness Center in conjunction with the original YMCA building.
ASU Colleges at Lake Havasu City.
In response to demands for lower-cost public higher education in Arizona, ASU developed the small, undergraduate-only college in Lake Havasu City. ASU Colleges will be teaching-focused and will provide a selection of popular undergraduate majors. The Lake Havasu City campus offers high-demand undergraduate degrees with lower tuition rates than other Arizona research universities.
Online degree programs.
ASU offers more than 70 undergraduate and graduate degree programs through an entirely online platform, known as ASU Online. The degree programs delivered online hold the same accreditation as the university's traditional face-to-face programs, and students earn many of the same degrees as those who attend courses in person. Online students are taught by the same faculty and receive the same diploma as on-campus students. ASU is a top-rated provider of online degrees and certificates. As of fall 2014 7,437 students were enrolled at ASU Online. ASU Online is headquartered at ASU's SkySong campus in Scottsdale, Arizona. ASU Online was ranked in the Top 10 for Best Online Bachelor's Programs by U.S. News & World Report.
ASU-Mayo Medical School Campus.
In 2011 ASU launched a collaboration with the Mayo Clinic to establish a medical school in Arizona. As part of the collaboration with Mayo Medical, ASU moved some academic departments onto the Mayo Clinic campus in Scottsdale. Mayo Medical and ASU have created an undergraduate "Barrett-Mayo Pre-medical Scholars Program" offered through ASU's Barrett, The Honors College. Partnerships with organizations and hospitals throughout the region has been created as a vehicle to establish a network for knowledge sharing and peer testing of the innovations that arise as a result of the partnerships. Real-world training for students researching medical issues affecting the community will be a priority of the school which ranks in the top 25 for best medical schools in the research category. ASU-Mayo Medical School began enrolling its first students in 2014. As a part of the preparation for the medical school opening, ASU began offering health and nursing degree programs on the Mayo Clinic Campus. The program at the ASU-Mayo Clinic Campus began in the Fall of 2012 and provides hands-on education in world-class medical facilities to its students. Unique MD degrees, believed to be the first in the nation, will be granted under the governance and oversight of Mayo Medical School and Arizona State University with a specialized master’s degree in the Science of Health Care Management.
Academics.
Admissions.
Fall Freshman Statistics
Admission to any of the public universities in Arizona is ensured to residents in the top 25% of their high-school class with a GPA of 3.0 in core competencies. For fall 2013, ASU admitted 80.2% of all applicants and is considered a "more selective" university by U.S. News & World Report. Average GPA of high school graduates enrolling full-time is 3.46. All freshman are required to live on campus.
Barrett, The Honors College is ranked 1st in the nation among peer institutions (1300–1400 minimum SAT), 4th in Honors Factors, and 5th in Overall Excellence among all universities. Like most of ASU's colleges and schools (e.g. Walter Cronkite School of Journalism and Mass Communication, W.P. Carey School of Business, Sandra Day O'Connor College of Law, Mary Lou Fulton Teachers College, College of Nursing and Health Innovation, etc.), Barrett College maintains much more strict admissions standards. Furthermore, Barrett College provides a more rigorous curriculum with smaller classes and increased faculty interaction. Although there are no set minimum admissions criteria for Barrett College, the average GPA of incoming freshmen was 3.84, with average SAT scores of 1314/1600 and ACT scores of 29. The Honors college enrolls 5,416 undergraduate students, with more than 700 people who are National Merit Scholars.
ASU currently enrolls 8,787 international students, 10.7% of the total student population. The international student body represents more than 130 nations and more than 60 student clubs and organizations exist at ASU to serve the growing number of students from abroad.<ref name="http://about.asu.edu/facts.html">http://about.asu.edu/facts.html</ref> The growth in the number of international students in 2014 at ASU is a 33.6% increase over the 2012 figure.
Academic programs.
ASU offers over 250 majors to undergraduate students, and more than 100 graduate programs leading to numerous masters and doctoral degrees in the liberal arts and sciences, design and arts, engineering, journalism, education, business, law, nursing, public policy, technology, and sustainability. These programs are divided into 16 colleges and schools which are spread across ASU's six campuses. ASU uses a plus-minus grading system with highest cumulative GPA awarded of 4.0 (at time of graduation). Arizona State University is accredited by the North Central Association of Colleges and Schools.
Rankings.
The 2014 "U.S. News & World Report" ranking of US colleges and universities ranked ASU's undergraduate program 63rd among public universities and 129th of 280 "national universities." ASU also ranked 2nd in the "Up and Coming" category of universities for making the most promising and innovative changes in the areas of academics, faculty and student life. In addition, ASU is ranked 106th in the world and 57th in the U.S. by the "Center for World University Rankings". "Forbes" magazine named ASU one of "America's Best College Buys". Money Magazine ranked Arizona State 214th in the country out of the nearly 1500 schools it evaluated for its 2014 Best Colleges ranking. The Daily Beast ranked Arizona State 172nd in the country out of the nearly 2000 schools it evaluated for its 2014 Best Colleges ranking.
In 2012, "ASU students ranked fifth among all public universities in National Science Foundation grants for graduate study and 11th among all universities, including the schools of the Ivy League. Among other things, the high achievement in this area of excellence points to consistently strong advising and support, a logical outcome of Barrett (Arizona State University's honor college) investing more in honors staff than any other honors program that" Public University Honors reviewed.
Arizona State is ranked 5th in the nation by The Wall Street Journal for best qualified graduates. For its efforts to be a national leader in campus sustainability, ASU was named one of the top 20 "cool schools" by the Sierra Club in 2009, was named to the "Green Honor Roll" by the Princeton Review, and earned an "A-" grade on the 2010 College Sustainability Report Card.
Several of ASU's colleges and schools also appear among the top of the U.S. News & World Report rankings, including the 27th-ranked W. P. Carey School of Business (along with its 3rd-ranked program in Supply Chain Management and the 15th-ranked program in Information Systems), the 22nd-ranked Herberger Institute for Design and the Arts (along with its 7th-ranked program in Ceramics, 11th-ranked program in Photography and 5th-ranked program in Printmaking), the 12th-ranked School of Criminology and Criminal Justice, the 31st-ranked Sandra Day O'Connor College of Law (along with its 8th-ranked program in Legal Writing and 10th-ranked program in Dispute Resolution), the 43rd-ranked Ira A. Fulton School of Engineering (including five individual programs ranked in the top 30), the 16th-ranked School of Public Affairs (along with its 2nd-ranked program in City Management and Urban Policy, 10th-ranked program in Environmental Policy and Management, 16th-ranked program in Public Finance and Budgeting and 19th-ranked program in Public Management and Administration), the 18th-ranked Mary Lou Fulton Teachers College, the 21st-ranked , and 25th-ranked Healthcare Management. In addition, the individual Ph.D. programs in Materials Science and Engineering (20th), Physics (46th), Psychology (38th), Earth Science (17th), and Economics (36th) earned high rankings.
ASU's Walter Cronkite School of Journalism and Mass Communication has been ranked in the top 10 for journalism Schools by various publications and organizations over the last decade. The most recent rankings (2012) include: NewsPro (6th), Quality Education and Jobs (6th), and International Student (1st). In 2011, ASU was included in the Quacquarelli Symonds (QS) list as the 21st best school in the world for biological sciences.
Research and Institutes.
ASU consistently ranks among the top 20 universities—without a traditional medical school—for research expenditures. It shares this designation with schools such as: Caltech, Georgia Tech, MIT, Purdue, Rockefeller, UC-Berkeley, and University of Texas at Austin. ASU is classified as a “RU/VH: Research University (very high research activity)” by the Carnegie Classification of Institutions of Higher Education. The university has tripled research expenditures since 2002 and now receives more than $385 million annually. Like its research budget, the university's endowment continues to grow and now exceeds $500 million (2013). ASU is a NASA designated national space-grant institute and a member of the Universities Research Association.
ASU is one of the nation's most successful universities in terms of creating start-up companies through research. The university attracted over $200 million in financing during 2012, aiding in the creation of more than 55 companies. ASU ranks #2 in the nation for proprietary start-ups “created for every $10 million in research expenditures.” In 2013, ASU researchers were issued 47 patents, a significant increase over 2012 when 26 patents were granted. ASU ranks 1st for Arizona Technology Transfers/Start-ups (AzTE) in fiscal year 2013: 14 AzTE Start-ups were created by all three state universities (which include Northern Arizona University and University of Arizona) and ASU accounted for 11 of those technology firms. According to the Switzerland-based University Business Incubator (UBI) Index for 2013, ASU is one of the top universities in the world for business incubation, ranking 17th out of the top 25. ASU is one of only 14 universities and institutes to make the list from the United States and the only university representing Arizona. UBI reviewed 550 universities and associated business incubators from around the world using an assessment framework that takes more than 50 performance indicators into consideration. As an example, one of ASU's spin-offs (Heliae Development, LLC) raised more than $28 million in venture capital in 2013 alone.
The university's push to create various institutes has led to greater funding and an increase in the number of researchers in multiple fields. Among the most notable and famed institutes at ASU are: The Biodesign Institute, Institute of Human Origins, L. William Seidman Research Institute (W.P. Carey School of Business), the Julie Ann Wrigley Global Institute of Sustainability, Learning Sciences Institute, Herberger Research Institute, Hispanic Research Center, and the International Institute for Species Exploration. Much of the research conducted at ASU is considered cutting edge with its focus on interdisciplinarity. The Biodesign Institute for instance, conducts research on issues such as biomedical and healthcare outcomes as part of a collaboration with the Mayo Clinic to diagnose and treat rare diseases, including cancer. Biodesign Institute researchers have also developed various techniques for reading and detecting biosignatures which expanded in 2006 with an $18 million grant from the National Human Genome Research Institute of the National Institutes of Health. The institute also is heavily involved in sustainability research, primarily through reuse of CO2 via biological feedback and various biomasses (e.g. algae) to synthesize clean biofuels. Heliae is a Biodesign Institute spin-off and much of its business centers on Algal-derived, high value products. Furthermore, the institute is heavily involved in security research including technology that can detect biological and chemical changes in the air and water. The university has received more than $30.7 million in funding from the Department of Defense for adapting this technology for use in detecting the presence of biological and chemical weapons.
World renowned scholars have been integral to the successes of the various institutes associated with the university. ASU students and researchers have been selected as Marshall, Truman, Rhodes, and Fulbright Scholars with the university ranking 4th for total recipients of the prestigious Fulbright Scholarship in the 2012–2013 academic year. ASU faculty includes Nobel Laureates, Royal Society members, National Academy members, and members of the National Institutes of Health, to name a few. ASU Professor Donald Johanson, who discovered the 3.18 million year old fossil hominid Lucy (Australopithecus) in Ethiopia, established the Institute of Human Origins (IHO) in 1981. The institute was first established in Berkeley, California and later moved to ASU in 1997. As one of the leading research organization in the United States devoted to the science of human origins, IHO pursues a transdisciplinary strategy for field and analytical paleoanthropological research. The Herberger Institute Research Center supports the scholarly inquiry, applied research and creative activity of more than 400 faculty and 4,373 students. The renowned ASU Art Museum, Herberger Institute Community Programs, urban design, and other outreach and initiatives in the arts community round out the research and creative activities of the Herberger Institute. Among well known professors within the Herberger Institute is Johnny Saldaña of the School of Theatre and Film. Saldaña received the 1996 Distinguished Book Award and the prestigious Judith Kase Cooper Honorary Research Award, both from the American Alliance for Theatre Education (AATE). The Julie Ann Wrigley Global Institute of Sustainability is the center of ASU's initiatives focusing on practical solutions to environmental, economic, and social challenges. The institute has partnered with various cities, universities, and organizations from around the world to address issues affecting the global community.
ASU is also involved with NASA in the field of space exploration. In order to meet the needs of NASA programs, ASU built the LEED Gold Certified, 298,000-square-foot Interdisciplinary Science and Technology Building IV (ISTB 4) at a cost of $110 million in 2012. The building includes space for the School of Earth and Space Exploration (SESE) and includes labs and other facilities for the Ira A. Fulton Schools of Engineering. One of the main projects at ISTB 4 includes the OSIRIS-REx Thermal Emission Spectrometer (OTES). Although ASU built the spectrometers aboard the Martian rovers Spirit and Opportunity, OTES will be the first major scientific instrument completely designed and built at ASU for a NASA space mission. Phil Christensen, the principal investigator for the Mars Global Surveyor Thermal Emission Spectrometer (TES), is a Regents' Professor at ASU. He also serves as the principal investigator for the Mars Odyssey THEMIS instruments, as well as co-investigator for the Mars Exploration Rovers. ASU scientists are responsible for the Mini-TES instruments aboard the Mars Exploration Rovers. The Center for Meteorite Studies, which is home to rare Martian meteorites and exotic fragments from space, and the Mars Space Flight Facility are both located on ASU's Tempe campus.
The Army Research Laboratory extended funding for the Arizona State University Flexible Display Center (FDC) in 2009 with a $50 million grant. The university has partnered with the Pentagon on such endeavors since 2004 with an initial $43.7 million grant. The university’s FDC built the world’s largest flexible screen color organic light emitting display (OLED) prototype using advanced mixed oxide thin-film transistors (TFTs). The technology delivers high-performance while remaining cost-effective during the manufacturing process. Vibrant colors, high switching speeds for video and reduced power consumption are some of the features the center has been able to successfully integrate into the technology. In 2012, ASU successfully eliminated the need for specialized equipment and processing, thereby reducing costs compared to competitive approaches.
Libraries.
ASU's faculty and students are served by two dedicated general-topic libraries: Hayden Library, which is the largest of the ASU libraries and is located on the Tempe campus, and Fletcher Library, located on the West campus. In addition, the Ross-Blakley Law Library and the Noble Science Library are housed in dedicated facilities on the Tempe campus. Music and Architecture collections are housed in facilities within the schools of Music and Architecture, respectively. Smaller library facilities are also located on the Polytechnic and Downtown campuses.
As of 2013, ASU's libraries held 4.5 million volumes. In total, there are 7 libraries that service the university community. The Arizona State University library system is ranked the 34th largest research library in the United States and Canada, according to criteria established by the Association of Research Libraries that measures various aspects of quality and size of the collection. The University continues to grow its rare special collections, such as the recent addition of a privately held collection of manuscripts by poet Rubén Darío.
Hayden Library is located on Cady Mall in the center of the Tempe campus. It opened in 1966 and serves as the library system's reference, periodical, and administrative center and houses the most extensive special collections in ASU’s library system. An expansion in 1989 created the subterranean entrance underneath Hayden Lawn and is attached to the above ground portion of the original library. There are two floors underneath Hayden Lawn with a landmark known as the "Beacon of Knowledge" rising form the center. The beacon is lit at night by the underground library’s lights.
The 2013 Capital Improvement Plan, approved by the Arizona Board of Regents, incorporates a $35 million repurposing and renovation project for Hayden Library. The moat area that is currently open air and serves as an outdoor study space will be enclosed in order to increase indoor space for the library. Along with increasing space and renovating the facility, the front entrance of Hayden Library will be rebuilt.
Sustainability.
As of April 2013, ASU is the only institution of higher education in the United States to generate over 24 megawatts (MW) of electricity from solar arrays containing 81,424 solar panels. This is an increase over the June 2012 total of 15.3 MW. ASU has 72 solar photovoltaic (PV) installations across all four campuses. The largest concentration of solar PV installations are on the Tempe campus, producing over 12.8 MW.
Additionally, there are six wind turbines installed on the roof of the Julie Ann Wrigley Global Institute of Sustainability building on the Tempe campus that have been in operation since October 2008. Under normal conditions, the six turbines produce enough electricity to power approximately 36 computers.
ASU's School of Sustainability is the first school in the United States dedicated to exploring the principles of sustainability. ASU's School of Sustainability is part of the Wrigley Global Institute of Sustainability. The School was established in spring 2007 and began enrolling undergraduates in fall 2008. ASU is also home to the Sustainability Consortium which was founded by Jay Golden in 2009.
The School of Sustainability has been essential in establishing the university as "a leader in the academics of sustainable business." The university is widely considered to be one of the most ambitious and principled organizations when it comes to embedding sustainable practices into its operating model. The university has embraced several challenging sustainability benchmarks. Among the numerous benchmarks outlined in the university's prospectus, is the creation of a large recycling and composting operation that by 2015, will eliminate 90% of the solid waste generated by all on-campus activities. This endeavor will be aided by educating students about the benefits of avoiding overconsumption that contributes to excessive waste. Sustainability courses have been expanded to attain this goal and many of the university's individual colleges and schools have integrated such material into their lectures and courses. Second, ASU is on track to reduce its rate of water consumption by 50%. The university's most aggressive benchmark is to be the first, large research university to achieve carbon neutrality as it pertains to its Scope 1, 2 and non-transportation Scope 3 greenhouse gas (GHG) emissions.
Traditions.
Maroon and Gold.
Gold is the oldest color associated with Arizona State University and dates back to 1896 when the school was named the Tempe Normal School. Maroon and white were later added to the color scheme in 1898. Gold signifies the “golden promise” of ASU. The promise includes every student receiving a valuable educational experience. Gold also signifies the sunshine Arizona is famous for; including the power of the sun and its influence on the climate and the economy. The first uniforms worn by athletes associated with the university were black and white when the “Normals” were the name of the athletic teams. The student section, known as The Inferno, wears gold on game days.
Mascot and Spirit Squad.
Sparky the Sun Devil is the mascot of Arizona State University and was named by vote of the student body on November 8, 1946. Sparky often travels with the team across the country and has been at every football bowl game in which the university has participated in. The university's mascot is not to be confused with the university’s new emblem and logo, The Trident, colloquially referred to as the "fork" or the "pitchfork" which is a hand gesture used by those associated with the university. The new logo and emblem are used on various university property, sport facilities, uniforms and documents. Arizona State Teacher’s College had a different mascot and the sports teams were known as the Owls and later, the Bulldogs. When the school was first established, the Tempe Normal School’s teams were simply known as the Normals. Sparky is visible on the sidelines of every home game played in Sun Devil Stadium or other ASU athletic facilities. His routine at football games includes pushups after every touchdown scored by the Sun Devils. He is aided by Sparky's Crew, male yell leaders that must meet physical requirements in order to participate as members. The female members are known as the Spirit Squad and are categorized into a dance line and spirit line. They are the official squad that represents ASU. The spirit squad competes every year at the ESPN Universal Dance Association (UDA) College Nationals in the Jazz and Hip-Hop categories. They were chosen by the UDA to represent the USA at the World Dance Championship 2013 in the Jazz category. Currently, ASU's varsity intercollegiate cheerleading team is not allowed to participate at athletic events (e.g. football and basketball games) due to dismissal regarding prior misconduct. ASU Cheerleading has since become a club sport, through the Student Recreation Center, competing locally and nationally as a Collegiate Co-Ed Division IA-Level VI team. They have reestablished their commitment to excellence, winning various championships. The team has a strict code of conduct and is seeking reinstatement from the university to participate at athletic events.
“A” Mountain.
A letter has existed on the slope of the mountain since 1918. A "T" followed by an "N" were the first letters to grace the landmark. Tempe Butte, home to "A" Mountain, has had the "A" installed on the slope of its south face since 1938 and is visible from campus just to the south. The original "A" was destroyed by vandals in 1952 with pipe bombs and a new "A", constructed of reinforced concrete, was built in 1955. The vandals were never identified but many speculate that the conspirators were students from the rival in-state university (University of Arizona). Many ancient Hohokam petroglyphs were destroyed by the bomb; nevertheless, many of these archeological sites around the mountain remain. There are many traditions surrounding "A" Mountain, including a revived "guarding of the 'A'" in which students camp on the mountainside before games with rival schools. "Whitewashing" of the "A" is a tradition in which incoming freshmen paint the letter white during orientation week. After the painting of the "A", new students learn the history of ASU and its other traditions.
Lantern Walk and Homecoming.
The Lantern Walk is one of the oldest traditions at ASU and dates back to 1917. It is considered one of ASU’s “most cherished” traditions and is an occasion used to mark the work of those associated with ASU throughout history. Anyone associated with ASU is free to participate in the event, including students, alumni, faculty, employees, and friends. This differs slightly from the original tradition in which the seniors would carry lanterns up "A" Mountain followed by the freshman. The senior class president would describe ASU's traditions and the freshman would repeat an oath of allegiance to the university. It was described as a tradition of "good will between the classes" and a way of ensuring new students would continue the university's traditions with honor. In modern times, the participants walk through campus and follow a path up to “A” Mountain in order to “light up” Tempe. Keynote speakers, performances, and other events are used to mark the occasion. The night is culminated with a fireworks display. The Lantern Walk was held after the Spring Semester (June) but is now held the week before Homecoming, a tradition that dates back to 1924 at ASU. It is held in the fall and in conjunction with a football game.
Victory Bell.
Arizona State University reintroduced the tradition of ringing a bell after each win for the football team in 2012. The ROTC cadets associated with the university are responsible for the transportation of the bell to various events and for ringing the bell after games are won by the Sun Devils. The first Victory Bell, in various forms, was used in the 1930s but the tradition faded in the 1970s when the bell in use was removed from Memorial Union for renovations. The bell cracked and was no longer capable of ringing. That bell is located on the southeast corner of Sun Devil Stadium near the entrance to the student section. That bell, given to the university in the late 1960s, is painted gold and is a campus landmark today.
Sun Devil Marching Band, Devil Walk and Songs of the University.
The Arizona State University Sun Devil Marching Band, created in 1915 and known as the "Pride of the Southwest", was the first of only two marching bands in the Pac-12 to be awarded the prestigious Sudler Trophy. The John Philip Sousa Foundation awarded the band the trophy in 1991. The Sun Devil Marching Band remains one of only 28 bands in the nation to have earned the designation. The band performs at every football game played in Sun Devil Stadium. Smaller ensembles of band members perform at other sport venues including basketball games at Wells Fargo Arena and baseball games at Packard Stadium. The Devil Walk is held in Wells Fargo Arena by the football team and involves a more formal introduction of the players to the community; a new approach to the tradition added in 2012 with the arrival of head coach Todd Graham. It begins 2 hours and 15 minutes prior to the game and allows the players to establish rapport with the fans. The walk ends as the team passes the band and fans lined along the path to Sun Devil Stadium. The most recognizable songs played by the band are "Alma Mater" and ASU’s fight songs titled "Maroon and Gold" and the "Al Davis Fight Song". "Alma Mater" was composed by former Music Professor and Director of Sun Devil Marching Band (then known as Bulldog Marching Band), Miles A. Dresskell, in 1937. "Maroon and Gold" was authored by former Director of Sun Devil Marching Band, Felix E. McKernan, in 1948. The "Al Davis Fight Song" (also known as "Go, Go Sun Devils" and "Arizona State University Fight Song") was composed by ASU alumnus Albert Oliver Davis in the 1940s without any lyrics. Recently lyrics were added to the song.
Student life.
Extracurricular programs.
Arizona State University has an active extracurricular involvement program (Sun Devil Involvement Center). Located on the 3rd floor of the Memorial Union, the Sun Devil Involvement Center (SDIC) provides opportunities for student involvement through clubs, sororities, fraternities, community service, leadership, student government, and co-curricular programming.
Changemaker Central is student-run centralized resource hub for student involvement in social entrepreneurship, civic engagement, service learning and community service that catalyzes student-driven social change. Changemaker Central locations have opened on all campuses in Fall 2011, providing flexible, creative workspaces for everyone in the ASU community. The project is entirely student run and advances ASU’s institutional commitments to social embeddedness and entrepreneurship. The space allows students to meet, work and join new networks and collaborative enterprises while taking advantage of ASU’s many resources and opportunities for engagement. Changemaker Central has signature programs, including Innovation Challenge and 10,000 Solutions, that support students in their journey to become changemakers by creating communities of support around new solutions/ideas and increasing access to early stage seed funding. The Innovation Challenge seeks undergraduate and graduate students from across the university who are dedicated to making a difference in our local and global communities through innovation. Students can win up to $10,000 to make their innovative project, prototype, venture or community partnership ideas happen. The 10,000 Solutions Project leverages the power of collaborative imagination and innovation to create a solutions bank. As an experimental problem solving platform, the project showcases and collects ideas at scale with local and global impact. The 10,000 Solutions Project aims to see what can be accomplished when passionate people join a collaborative community that builds upon each other’s innovative ideas.
In addition to Changemaker Central, the Freshman Year Residential Experience (FYRE) and the Greek community (Greek Life) at Arizona State University have been important in binding students to the university, and providing social outlets. The Freshman Year Residential Experience at Arizona State University was developed to improve the freshman experience at Arizona State University and increase student retention figures. FYRE provides advising, computer labs, free walk-in tutoring, workshops, and classes for students. In 2003, "U.S. News & World Report" ranked FYRE as the 23rd best first year program in the nation. ASU is also home to one of the nation's first and fastest growing gay fraternities, Sigma Phi Beta, founded in 2003; considered a sign of the growing university's commitment to supporting diversity and inclusion.
The second Eta chapter of Phrateres, a non-exclusive, non-profit social-service club, was installed here in 1958. Between 1924 and 1967, 23 chapters of Phrateres were installed in universities across North America.
Student media.
"The State Press" is the university's independent, student-operated news publication. The State Press covers news and events on all four ASU campuses. Student editors and managers are solely responsible for the content of the State Press website. These publications are overseen by an independent board and guided by a professional adviser employed by the University.
"The Downtown Devil" is a student-run news publication website for the Downtown Phoenix Campus, produced by students at the Walter Cronkite School of Journalism and Mass Communication.
ASU has two radio stations. KASC The Blaze 1330 AM, is a broadcast station that is owned and funded by the Cronkite School of Journalism, and is completely student-run save for a faculty and professional adviser. The Blaze broadcasts local, alternative and independent music 24 hours a day, and also features news and sports updates at the top and bottom of every hour. W7ASU is an amateur radio station that was first organized in 1935. W7ASU has about 30 members that enjoy amateur radio, and is primarily a contesting club.
Student government.
Associated Students of Arizona State University (ASASU) is the student government at Arizona State University. It is composed of the Undergraduate Student Government and the Graduate & Professional Student Association (GPSA). Members and officers of ASASU are elected annually by the student body.
The Residence Hall Association (RHA) of Arizona State University is the student government for every ASU student living on-campus. Each ASU campus has an RHA that operates independently of each other. The purpose of RHA is to enhance the quality of residence hall life and provide a cohesive voice for the residents by addressing the concerns of the on-campus populations to university administrators and other campus organizations; providing cultural, diversity, educational, and social programming; establishing and working with individual community councils.
Athletics.
Arizona State University's Division I athletic teams are called the Sun Devils, which is also the nickname used to refer to students and alumni of the university. They compete in the Pac-12 Conference in 20 varsity sports. Historically, the university has highly performed in men's, women's, and mixed archery; men's, women's, and mixed badminton; women's golf; women's swimming and diving; baseball; and football. Arizona State University's NCAA Division I-A program competes in 9 varsity sports for men and 11 for women. ASU's current athletic director is Steve Patterson, who was appointed to the position in 2012 after Lisa Love, the former Senior Associate Athletic Director at the University of Southern California, was relieved of her duties. Love was responsible for the hiring of coaches Herb Sendek, the men's basketball coach, and Dennis Erickson, the men's football coach. Erickson was fired in 2011 and replaced by Todd Graham.
ASU has won 23 national collegiate team championships in the following sports: baseball (5), men's golf (2), women's golf (7), men's gymnastics (1), softball (2), men's indoor track (1), women's indoor track (2), men's outdoor track (1), women's outdoor track (1), and wrestling (1).
In September 2009 criticism over the seven-figure salaries earned by various coaches at Arizona's public universities (including ASU) prompted the Arizona Board of Regents to re-evaluate the salary and benefit policy for athletic staff. With the 2011 expansion of the Pacific-12 Conference, a new $3 billion contract for revenue sharing among all the schools in the conference was established. With the infusion of funds, the salary issue and various athletic department budgeting issues at ASU were addressed. The Pac-12's new media contract with ESPN allowed ASU to hire a new coach in 2012. A new salary and bonus package (maximum bonus of $2.05 million) was instituted and is one of the most lucrative in the conference. ASU also plans to expand its athletic facilities with a public-private investment strategy to create an amateur sports district that can accommodate the Pan American Games and operate as an Olympic Training Center. The athletic district will include a $300 million renovation of Sun Devil Stadium that will include new football facilities. The press box and football offices in Sun Devil Stadium were remodeled in 2012.
Arizona State Sun Devils football was founded in 1897 under coach Fred Irish. Currently, the team has played in the 2012 Fight Hunger Bowl, the 2011 Las Vegas bowl, and the 2007 Holiday Bowl. The Sun Devils played in the 1997 Rose Bowl and won the Rose Bowl in 1987. The team has appeared in the Fiesta Bowl in 1983, 1977, 1975, 1973, 1972, and 1971 winning 5 of 6. In 1970 and 1975 they were champions of the NCAA Division I FBS National Football Championship. The Sun Devils were Pac-12 Champions in 1986, 1996, and 2007. Altogether, the football team has 17 Conference Championships and has participated in a total of 26 bowl games as of 2012.
The university also participates in the American Collegiate Hockey Association (ACHA) and is billed as the top program within that league. Beginning in 2013, ASU will be a founding member of the new Western Collegiate Hockey League (WCHL). ASU Sun Devils Hockey will compete with NCAA Division 1 schools for the first time in 2012, largely due to the success of the program.
People.
Alumni.
Arizona State University has produced over 300,000 alumni worldwide. The university has produced many notable figures over its 125-year history, including: U.S. Senator Carl Hayden (who was instrumental in the growth of Central Arizona), former Congressman Barry Goldwater, Jr., and Silver Star recipient Pat Tillman who left his professional football career to enlist in the United States Army in the aftermath of the September 11, 2001 terrorist attacks. Other notable alumni include current U.S. Congressional Representatives Ed Pastor, Harry Mitchell, Kyrsten Sinema (the first openly bisexual person elected to Congress), among others; Eric Crown, CEO and co-founder of Insight Enterprises, Inc.; Ira A. Fulton, philanthropist and founder of Fulton Homes; Craig Weatherup, former Chairman of PepsiCo; Kate Spade, namesake and co-Founder of Kate Spade New York; Larry Carter, CFO of Cisco Systems; Doug Ducey, former partner and CEO of Coldstone Creamery and the 32nd Treasurer of Arizona; Mari J. Matsuda, American lawyer, activist, and law professor; and Scott Smith, the current mayor of the City of Mesa.
In addition to Pat Tillman (football), ASU has had many famous athletes attend the school. Those athletes include: Phil Mickelson (golf), Reggie Jackson (baseball), Barry Bonds (baseball), James Harden (basketball), and Terrell Suggs (football). ASU alumni that are enshrined in the Pro Football Hall of Fame include: Curley Culp, Mike Haynes, John Henry Johnson, Randall McDaniel, and Charley Taylor. Other notable athletes that attended ASU are Dustin Pedroia (baseball), Jake Plummer (football), Danny White (football), Lionel Hollins (basketball), Fat Lever (basketball), and Byron Scott (basketball).
Famous celebrities include: television host and comedian Jimmy Kimmel; comedian and the first host of The Tonight Show, Steve Allen; actor David Spade; actress and singer Lynda Carter; and actor Tyler Hoechlin from MTV's Teen Wolf. Influential writers and novelists include: Allison Dubois, whose novels and work with various law enforcement agencies inspired the TV miniseries "Medium"; novelist Amanda Brown; author, speaker and spiritual teacher Howard Falco; and best-selling author and Doctor of Animal Science Temple Grandin, whose work inspired the film, "Temple Grandin" starring Claire Danes. Journalists and commentators include: Al Michaels, NBC Sports' play-by-play commentator including the National Football League's broadcast of "Monday Night Football" and Jerry Dumas, writer, essayist, cartoonist, and a columnist for the "Greenwich Time" and best known for his "Sam and Silo" comic strip. Radio host and author Michael Reagan, the son of President Ronald Reagan and actress Jane Wyman, also attended ASU.
Among American research universities, Arizona State is ranked 4th for total recipients of the prestigious Fulbright Scholarship in the 2012–2013 academic year. ASU has made this list for more than 9 consecutive years. ASU alumni and students are also noted for their service to the community and have officially been recognized as a top university for contributing to the public good. The Arizona State University Alumni Association is located on the Tempe campus in Old Main. The Alumni Association is responsible for continuing many of the traditions of the university.
Faculty.
ASU faculty have included former CNN host Aaron Brown, meta-analysis developer Gene V. Glass, feminist and author Gloria Feldt, physicist Paul Davies, and Pulitzer Prize winner and "The Ants" coauthor Bert Hölldobler. Donald Johanson, who discovered the 3.18 million year old fossil hominid Lucy (Australopithecus) in Ethiopia, is also a professor at ASU, as well as George Poste, Chief Scientist for the Complex Adaptive Systems Initiative. Current Nobel laureate faculty include Leland Hartwell, and Edward C. Prescott. On June 12, 2012 Elinor Ostrom, ASU's third Nobel laureate, died at the age of 78.
ASU faculty's achievements as of 2012 include:
Sexual assault investigation.
On May 1, 2014, ASU was listed as one of fifty five higher education institutions under investigation by the Office of Civil Rights "for possible violations of federal law over the handling of sexual violence and harassment complaints" by Barack Obama's White House Task Force To Protect Students from Sexual Assault. The publicly announced investigation followed two Title IX suits. In July 2014, a group of at least nine current and former students who alleged that they were harassed or assaulted asked that the federal investigation be expanded.
In August 2014 ASU President Michael Crow appointed a task force comprising faculty and staff, students, and members of the university police force to review the university’s efforts to address sexual violence. Crow accepted the recommendations of the task force in November 2014.
Notes.
^ a: Campus enrollment figures at ASU are defined by the number of students taking at least one course offered by a department housed on a particular campus. Students who are enrolled in classes on more than one campus (estimated to be 27,484) are counted within each campus's total.
^ b: ASU is the largest research university in the US under a single administration (one President, Provost, VPs, etc.). In addition ASU's Tempe campus is one of the largest single university campuses in the US.

</doc>
<doc id="1862" url="http://en.wikipedia.org/wiki?curid=1862" title="April 14">
April 14

April 14 is the day of the year in the Gregorian calendar.

</doc>
<doc id="1864" url="http://en.wikipedia.org/wiki?curid=1864" title="Astoria, Oregon">
Astoria, Oregon

Astoria is the seat of Clatsop County, Oregon, United States. Situated near the mouth of the Columbia River, the city was named after the American investor John Jacob Astor. His American Fur Company founded Fort Astoria at the site in 1811. Astoria was incorporated by the Oregon Legislative Assembly on October 20, 1876.
Located on the south shore of the Columbia river, the city is served by the deepwater Port of Astoria. Transportation includes the Astoria Regional Airport with U.S. Route 30 and U.S. Route 101 as the main highways, and the 4.2 mi Astoria–Megler Bridge connecting to neighboring Washington across the river. The population was 9,477 at the 2010 census.
History.
The Lewis and Clark Expedition spent the winter of 1805–1806 at Fort Clatsop, a small log structure south and west of modern-day Astoria. The expedition had hoped a ship would come by to take them back east, but instead endured a torturous winter of rain and cold, then returned east the way they came. Today the fort has been recreated and is now a historical park.
The Pacific Fur Company, a subsidiary of John Jacob Astor's American Fur Company, was created to begin fur trading in the Oregon Country. Its primary fur-trading post Fort Astoria was built in 1811, holding the distinction of being the first permanent U.S. settlement on the Pacific coast. It was an extremely important post for American exploration of the continent and was later used as an American claims in the Oregon boundary dispute with other European nations. British explorer David Thompson was the first European to navigate the entire length of the Columbia River in 1811. Thompson reached the partially constructed Fort Astoria at the mouth of the Columbia, arriving two months after the Pacific Fur Company's ship, the "Tonquin".
With the ongoing War of 1812, the Pacific Fur Company officers sold the company assets to their Canadian rivals, the North West Company in 1813. The house was restored to the U.S. in 1818, though the fur trade would remain under British control until American pioneers following the Oregon Trail began filtering into the port town in the mid-1840s. The Treaty of 1818 established joint U.S. – British occupancy of the Oregon Country. In 1846 the Oregon Treaty divided the mainland at 49th parallel north, the southern portion of Vancouver Island south of this line being awarded to the British.
Washington Irving, a prominent American writer with a European reputation, was approached by John Jacob Astor to mythologize the three-year reign of his Pacific Fur Company. "Astoria" (1835), written while Irving was Astor's guest, cemented the importance of the region in the American psyche. In Irving's words, the fur traders were "Sinbads of the wilderness", and their venture was a staging point for the spread of American economic power into both the continental interior and into the Pacific.
As the Oregon Territory grew and became increasingly more colonised by Americans, Astoria likewise grew as a port city at the mouth of the great river that provided the easiest access to the interior. The first U.S. Post Office west of the Rocky Mountains was established in Astoria in 1847. In 1876, the community was incorporated by the state.
Astoria attracted a host of immigrants beginning in the late 19th century: Nordic settlers, primarily Finns, and Chinese soon became significant parts of the population. The Finns mostly lived in Uniontown, near the present-day end of the Astoria–Megler Bridge, and took fishing jobs; the Chinese tended to do cannery work, and usually lived either downtown or in bunkhouses near the canneries. In 1883, and again in 1922, downtown Astoria was devastated by fire, partly because it was mostly wood and entirely raised off the marshy ground on pilings. Even after the first fire, the same format was used, and the second time around the flames spread quickly again, as collapsing streets took out the water system. Frantic citizens resorted to dynamite, blowing up entire buildings to stop the fire from going further.
Astoria has served as a port of entry for over a century and remains the trading center for the lower Columbia basin, although it has long since been eclipsed by Portland, Oregon, and Seattle, Washington, as an economic hub on the coast of the Pacific Northwest. Astoria's economy centered on fishing, fish processing, and lumber. In 1945, about 30 canneries could be found along the Columbia; however, in 1974 Bumblebee Seafood moved its headquarters out of Astoria, and gradually reduced its presence until 1980 when the company closed its last Astoria cannery. The timber industry likewise declined; Astoria Plywood Mill, the city's largest employer, closed in 1989, and the Burlington Northern and Santa Fe Railway discontinued service in 1996.
From 1921 to 1966, a ferry route across the Columbia River connected Astoria with Pacific County, Washington. In 1966 the Astoria–Megler Bridge was opened; it completed U.S. Route 101 and linked Astoria with Washington on the opposite shore of the Columbia, and replaced the ferries.
Today, tourism, Astoria's growing art scene, and light manufacturing are the main economic activities of the city. It is a port of call for cruise ships since 1982, after $10 million in pier improvements to accommodate cruise ships. To avoid Mexican ports of call during the Swine Flu outbreak of 2009, many cruises were re-routed to include Astoria. The residential community "The World" visited Astoria in June 2009.
In addition to the replicated Fort Clatsop, a popular point of interest is the Astoria Column, a tower 125 ft high, built atop Coxcomb Hill above the town, with an inner circular staircase allowing visitors to climb to see a panoramic view of the town, the surrounding lands, and the Columbia flowing into the Pacific. The column was built by the Astor family in 1926 to commemorate the region's early history.
Since 1998, artistically-inclined fishermen and women from Alaska and the Pacific Northwest have traveled to Astoria for the Fisher Poets Gathering, where poets and singers tell their tales to honor the fishing industry and lifestyle.
Astoria is also the western terminus of the TransAmerica Trail, a bicycle touring route created by the American Cycling Association.
Astoria is home to three United States Coast Guard ships: the "Steadfast", "Alert", and "Fir".
Geography.
According to the United States Census Bureau, the city has a total area of 10.11 sqmi, of which, 6.16 sqmi is land and 3.95 sqmi is water.
Climate.
Astoria lies within the Mediterranean climate zone (Köppen "Csb"), with very mild temperatures year-round, some of the most consistent in the contiguous United States; winters are mild for this latitude (it usually remains above freezing at night) and wet. Summers are cool, although short heat waves can occur. Rainfall is most abundant in late fall and winter and is lightest in July and August. Snowfall is relatively rare, occurring in only three-fifths of years. Nevertheless, when conditions are ripe, significant snowfalls can occur.
Astoria is tied with Lake Charles, Louisiana, and Port Arthur, Texas, as the most humid city in the contiguous United States. The average relative humidity in Astoria is 89% in the morning and 73% in the afternoon.
Annually, there is an average of only 4.2 days with temperatures reaching 80 °F or higher, and 90 °F readings are rare. Normally there are only one or two nights per year when the temperature remains at or above 60 °F. There are an average of 31 days with minimum temperatures at or below the freezing mark. The record high temperature was 101 °F on July 1, 1942. The record low temperature was 6 °F on December 8, 1972, and on December 21, 1990.
There are an average of 191 days with measurable precipitation. The wettest year was 1950 with 113.34 in and the driest year was 1985 with 41.58 in. The most rainfall in one month was 36.07 in in December 1933, and the most in 24 hours was 5.56 in on November 25, 1998. The most snowfall in one month was 26.9 in in January 1950, and the most snow in 24 hours was 12.5 in on December 11, 1922.
Demographics.
2010 census.
As of the census of 2010, there were 9,477 people, 4,288 households, and 2,274 families residing in the city. The population density was 1538.5 PD/sqmi. There were 4,980 housing units at an average density of 808.4 /sqmi. The racial makeup of the city was 89.2% White, 0.6% African American, 1.1% Native American, 1.8% Asian, 0.1% Pacific Islander, 3.9% from other races, and 3.3% from two or more races. Hispanic or Latino of any race were 9.8% of the population.
There were 4,288 households of which 24.6% had children under the age of 18 living with them, 37.9% were married couples living together, 10.8% had a female householder with no husband present, 4.3% had a male householder with no wife present, and 47.0% were non-families. 38.8% of all households were made up of individuals and 15.1% had someone living alone who was 65 years of age or older. The average household size was 2.15 and the average family size was 2.86.
The median age in the city was 41.9 years. 20.3% of residents were under the age of 18; 8.6% were between the ages of 18 and 24; 24.3% were from 25 to 44; 29.9% were from 45 to 64; and 17.1% were 65 years of age or older. The gender makeup of the city was 48.4% male and 51.6% female.
2000 census.
As of the census of 2000, there were 9,813 people, 4,235 households, and 2,469 families residing in the city. The population density was 1,597.6 people per square mile (617.1 per km²). There were 4,858 housing units at an average density of 790.9 per square mile (305.5 per km²). The racial makeup of the city was:
5.98% of the population were Hispanic or Latino of any race.
14.2% were of German, 11.4% Irish, 10.2% English, 8.3% United States or American, 6.1% Finnish, 5.6% Norwegian, and 5.4% Scottish ancestry according to Census 2000.
There were 4,235 households out of which 28.8% had children under the age of 18 living with them, 43.5% were married couples living together, 11.2% had a female householder with no husband present, and 41.7% were non-families. 35.4% of all households were made up of individuals and 13.6% had someone living alone who was 65 years of age or older. The average household size was 2.26 and the average family size was 2.93.
In the city the population was spread out with:
The median age was 38 years. For every 100 females there were 92.3 males. For every 100 females age 18 and over, there were 89.9 males.
The median income for a household in the city was $33,011, and the median income for a family was $41,446. Males had a median income of $29,813 versus $22,121 for females. The per capita income for the city was $18,759. About 11.6% of families and 15.9% of the population were below the poverty line, including 22.0% of those under age 18 and 9.6% of those age 65 or over.
Education.
The Astoria School District has four primary and secondary schools, including Astoria High School. Clatsop Community College is the city's two-year college. It also has a library and many parks with historical significance. As well as the second oldest Job Corps facility, Tongue Point Job Corps.
Media.
"The Daily Astorian" is a newspaper serving Astoria, the local NPR station is KMUN 91.9, and KAST 1370 is a local news-talk radio station. The "Coast River Business Journal" is a monthly business magazine covering Astoria, Clatsop County, and the Northwest Oregon coast.
In popular culture.
"Shanghaied in Astoria" is a musical about Astoria's history that has been performed in Astoria every year since 1984.
Astoria was the setting of the 1985 movie "The Goonies", which was filmed on location. Other movies filmed in Astoria include "Short Circuit", "The Black Stallion", "Kindergarten Cop", "Free Willy", "", "Teenage Mutant Ninja Turtles III", "Benji the Hunted", "The Ring Two", "Into the Wild", "The Guardian" and "Cthulhu".
The early 1960s television series "Route 66" filmed the episode entitled "One Tiger to a Hill" in Astoria; it was broadcast on September 21, 1962.
The fourth full-length album by the pop punk band The Ataris was named "So Long, Astoria" as an allusion to "The Goonies". "So Long, Astoria" is also the first track on the album. The rear album art also features news clippings such as a picture of the port's water tower from an article from 2002 of the water tower being pulled down.
Sister cities.
Astoria has one sister city, as designated by Sister Cities International:
Warships named "Astoria".
Two US Navy Cruisers were named USS "Astoria": a heavy cruiser (CA-34) and a light cruiser (CL-90). The former was lost in combat in August 1942 at the World War II Pacific ocean Battle of Savo Island, and the latter was scrapped in 1971 after being removed from active duty in 1949.

</doc>
<doc id="1866" url="http://en.wikipedia.org/wiki?curid=1866" title="Alarums and Excursions">
Alarums and Excursions

Alarums and Excursions (A&E) is an amateur press association started in June 1975 by Lee Gold (at the request of Bruce Pelz, who felt that discussion of "Dungeons & Dragons" was taking up too much space in Apa-L, the APA of the Los Angeles Science Fantasy Society). It was the first publication to focus solely on role-playing games. 
Each issue is a collection of contributions from different authors, often featuring game design discussions, rules variants, write-ups of game sessions, reviews, and comments on others contributions. It was a four-time winner of the Charles Roberts/Origins Award, winning "Best Amateur Adventure Gaming Magazine" in 1984, "Best Amateur Game Magazine" in 1999, and "Best Amateur Game Periodical" in 2000 and 2001.
Although game reports and social reactions are common parts of many "A&E" contributions, it has also, over the years, become a testing ground for new ideas on the development of the RPG as a genre and an art form. The idea that role-playing games "are" an art form took strong root in this zine, and left a lasting impression on many of the RPG professionals who contributed.
The July 2013 collation of "Alarums and Excursions" was #466.
Over the years, contributors have included: 
The role-playing game "Over the Edge" was inspired by discussions in "A&E".
"Alarums and excursions" is a stage direction for the moving of soldiers across a stage, used in Elizabethan drama.

</doc>
<doc id="1869" url="http://en.wikipedia.org/wiki?curid=1869" title="Alfred Jarry">
Alfred Jarry

Alfred Jarry (]; 8 September 1873 – 1 November 1907) was a French symbolist writer who is best known for his play "Ubu Roi" (1896). He also coined the term and philosophical concept of 'pataphysics.
Jarry was born in Laval, Mayenne, France, and his mother was from Brittany. He was associated with the Symbolist movement. His play "Ubu Roi" (1896) is often cited as a forerunner to the Surrealist and Futurist movements of the 1920s and 1930s. Jarry wrote in a variety of hybrid genres and styles, prefiguring the Postmodern. He wrote plays, novels, poetry, essays and speculative journalism. His texts present us with pioneering work in the fields of absurdist literature and postmodern philosophy.
Biography and works.
At the lycée in Rennes when he was 15, he led a group of boys who enjoyed poking fun at their well-meaning, but obese and incompetent physics teacher, a man named Hébert. Jarry and his classmate, Henri Morin, wrote a play they called "Les Polonais" and performed it with marionettes in the home of one of their friends. The main character, "Père Heb", was a blunderer with a huge belly; three teeth (one of stone, one of iron, and one of wood); a single, retractable ear; and a misshapen body. In Jarry's later work "Ubu Roi", Père Heb would develop into Ubu, one of the most monstrous and astonishing characters in French literature.
At 17 Jarry passed his baccalauréat and moved to Paris to prepare for admission to the École Normale Supérieure. Though he was not admitted, he soon gained attention for his original poems and prose-poems. A collection of his work, "Les minutes de sable mémorial", was published in 1893.
That same year, both his parents died, leaving him a small inheritance which he quickly spent.
Jarry had meantime discovered the pleasures of alcohol, which he called "my sacred herb" or, when referring to absinthe, the "green goddess". A story is told that he once painted his face green and rode through town on his bicycle in its honour (and possibly under its influence).
When he was drafted into the army in 1894, his gift for turning notions upside down defeated attempts to instill military discipline. The sight of the small man in a uniform much too large for his less than 5-foot frame—the army did not issue uniforms small enough—was so disruptively funny that he was excused from parades and marching drills. Eventually the army discharged him for medical reasons. His military experience eventually inspired his novel "Days and Nights".
Jarry returned to Paris and applied himself to writing, drinking, and the company of friends who appreciated his witty, sweet-tempered, and unpredictable conversation. This period is marked by his intense involvement with Remy de Gourmont in the publication of "L'Ymagier", a luxuriously produced "art" magazine devoted to the symbolic analysis of medieval and popular prints. Symbolism as an art movement was in full swing at this time, and "L'Ymagier" provided a nexus for many of its key contributors. Jarry's play "Caesar Antichrist" (1895) drew on this movement for material. This is a work that bridges the gap between serious symbolic meaning and the type of critical absurdity with which Jarry would soon become associated. Using the biblical Book of Revelation as a point of departure, "Caesar Antichrist" presents a parallel world of extreme formal symbolism in which Christ is resurrected not as an agent of spirituality but as an agent of the Roman Empire that seeks to dominate spirituality. It is a unique narrative that effectively links the domination of the soul to contemporaneous advances in the field of Egyptology such as the 1894 excavation of the Narmer Palette, an ancient artifact used for situating the rebus within hermeneutics. The character Ubu Roi first appears in this play.
The spring of 1896 saw the publication, in Paul Fort's review "Le Livre d'art", of Jarry's 5-act play "Ubu Roi"—the rewritten and expanded "Les Polonais" of his school days. "Ubu Roi"'s savage humor and monstrous absurdity, unlike anything thus far performed in French theater, seemed unlikely to ever actually be performed on stage. However, impetuous theater director Aurélien-Marie Lugné-Poe took the risk, producing the play at his Théâtre de l'Oeuvre.
On opening night (10 December 1896), with traditionalists and the avant-garde in the audience, King Ubu (played by Firmin Gémier) stepped forward and intoned the opening word, "Merdre!" (often translated as "Pshit" or "Shittr!" in English). A quarter of an hour of pandemonium ensued: outraged cries, booing, and whistling by the offended parties, countered by cheers and applause by the more degenerate contingent. Such interruptions continued through the evening. At the time, only the dress rehearsal and opening night performance were held, and the play was not revived until after Jarry's death.
The play brought fame to the 23-year-old Jarry, and he immersed himself in the fiction he had created. Gémier had modeled his portrayal of Ubu on Jarry's own staccato, nasal vocal delivery, which emphasized each syllable (even the silent ones). From then on, Jarry would always speak in this style. He adopted Ubu's ridiculous and pedantic figures of speech; for example, he referred to himself using the royal "we", and called the wind "that which blows" and the bicycle he rode everywhere "that which rolls".
Jarry moved into a flat which the landlord had created through the unusual expedient of subdividing a larger flat by means of a horizontal rather than a vertical partition. The diminutive Jarry could just manage to stand up in the place, but guests had to bend or crouch. Jarry also took to carrying a loaded revolver. In response to a neighbor's complaint that his target shooting endangered her children, he replied, "If that should ever happen, ma-da-me, we should ourselves be happy to get new ones with you".
With Franc-Nohain and Claude Terrasse, he co-founded the Théatre des Pantins, which in 1898 was the site of marionette performances of "Ubu Roi".
Living in worsening poverty, neglecting his health, and drinking excessively, Jarry went on to write the novel, "Le Surmâle" ("The Supermale"), which is partly a satire on the Symbolist ideal of self-transcendence.
Unpublished until after his death, his fiction "Exploits and Opinions of Dr. Faustroll, Pataphysician" ("Gestes et opinions du docteur Faustroll, pataphysicien") describes the exploits and teachings of a sort of antiphilosopher who, born at age 63, travels through a hallucinatory Paris in a sieve and subscribes to the tenets of "'pataphysics". 'Pataphysics deals with "the laws which govern exceptions and will explain the universe supplementary to this one". In 'pataphysics, every event in the universe is accepted as an extraordinary event.
Jarry once wrote, expressing some of the bizarre logic of 'pataphysics, "If you let a coin fall and it falls, the next time it is just by an infinite coincidence that it will fall again the same way; hundreds of other coins on other hands will follow this pattern in an infinitely unimaginable fashion".
In his final years, he was a legendary and heroic figure to some of the young writers and artists in Paris. Guillaume Apollinaire, André Salmon, and Max Jacob sought him out in his truncated apartment. Pablo Picasso was fascinated with Jarry. After Jarry's death Picasso acquired his revolver and wore it on his nocturnal expeditions in Paris. He later bought many of his manuscripts as well as executing a fine drawing of him.
Jarry died in Paris on 1 November 1907 of tuberculosis, aggravated by drug and alcohol use. It is recorded that his last request was for a toothpick. He was interred in the Cimetière de Bagneux, near Paris.
The complete works of Alfred Jarry are published in three volumes by Gallimard in the collection "Bibliothèque de la Pléiade".

</doc>
<doc id="1870" url="http://en.wikipedia.org/wiki?curid=1870" title="Amalric">
Amalric

Amalric or Amalaric (also Americ, Almerich, Emeric, Emerick and other variations) is a personal name derived from the tribal name "Amal" (referring to the Gothic Amali) and "ric" (Gothic "reiks") meaning "ruler, prince". 
Equivalents in different languages include:

</doc>
<doc id="1871" url="http://en.wikipedia.org/wiki?curid=1871" title="Amalric of Jerusalem">
Amalric of Jerusalem

Amalric (Latin "Amalricus", French "Amaury"; 1136 – 11 July 1174) was King of Jerusalem from 1163, and Count of Jaffa and Ascalon before his accession. He was the second son of Melisende of Jerusalem and Fulk of Jerusalem, and succeeded his older brother Baldwin III. During his reign, Jerusalem became more closely allied with the Byzantine Empire, and the two states launched an unsuccessful invasion of Egypt. Meanwhile, the Muslim territories surrounding Jerusalem began to be united under Nur ad-Din and later Saladin. He was the father of three future rulers of Jerusalem, Sibylla, Baldwin IV, and Isabella I.
Older scholarship mistook the two names Amalric and Aimery as variant spellings of the same name, so these historians erroneously added numbers, making Amalric to be Amalric I (1163–74) and King Aimery (1197–1205) to be "Amalric II". Now scholars recognize that the two names were not the same and no longer add the number for either king. Confusion between the two names was common even among contemporaries.
Youth.
Amalric was born in 1136 to King Fulk, the former count of Anjou who had married the heiress of the kingdom, Melisende, daughter of King Baldwin II. After the death of Fulk in a hunting accident in 1143, the throne passed jointly to Melisende and Amalric's older brother Baldwin III, who was still only 13 years old. Melisende did not step down when Baldwin came of age two years later, and by 1150 the two were becoming increasingly hostile towards each other. In 1152 Baldwin had himself crowned sole king, and civil war broke out, with Melisende retaining Jerusalem while Baldwin held territory further north. Amalric, who had been given the County of Jaffa as an apanage when he reached the age of majority in 1151, remained loyal to Melisende in Jerusalem, and when Baldwin invaded the south, Amalric was besieged in the Tower of David with his mother. Melisende was defeated in this struggle and Baldwin ruled alone thereafter. In 1153 Baldwin captured the Egyptian fortress of Ascalon, which was then added to Amalric's fief of Jaffa (see Battle of Ascalon).
Amalric married Agnes of Courtenay in 1157. Agnes, daughter of Joscelin II of Edessa, had lived in Jerusalem since the western regions of the former crusader County of Edessa were lost in 1150. Patriarch Fulcher objected to the marriage on grounds of consanguinity, as the two shared a great-great-grandfather, Guy I of Montlhéry, and it seems that they waited until Fulcher's death to marry. Agnes bore Amalric three children: Sibylla, the future Baldwin IV (both of whom would come to rule the kingdom in their own right), and Alix, who died in childhood.
Succession.
Baldwin III died on 10 February 1163 and the kingdom passed to Amalric, although there was some opposition among the nobility to Agnes; they were willing to accept the marriage in 1157 when Baldwin III was still capable of siring an heir, but now the "Haute Cour" refused to endorse Amalric as king unless his marriage to Agnes was annulled. The hostility to Agnes, it must be admitted, may be exaggerated by the chronicler William of Tyre, whom she prevented from becoming Latin Patriarch of Jerusalem decades later, as well as from William's continuators like Ernoul, who hints at a slight on her moral character: "car telle n'est que roine doie iestre di si haute cite comme de Jherusalem" ("there should not be such a queen for so holy a city as Jerusalem"). Nevertheless, consanguinity was enough for the opposition. Amalric agreed and ascended the throne without a wife, although Agnes continued to hold the title Countess of Jaffa and Ascalon and received a pension from that fief's income. Agnes soon thereafter married Hugh of Ibelin, to whom she had been engaged before her marriage with Amalric. The church ruled that Amalric and Agnes' children were legitimate and preserved their place in the order of succession. Through her children Agnes would exert much influence in Jerusalem for almost 20 years.
Conflicts with the Muslim states.
During Baldwin III's reign, the County of Edessa, the first crusader state established during the First Crusade, was conquered by Zengi, the Turkic emir of Aleppo. Zengi united Aleppo, Mosul, and other cities of northern Syria, and intended to impose his control on Damascus in the south. The Second Crusade in 1148 had failed to conquer Damascus, which soon fell to Zengi's son Nur ad-Din. Jerusalem also lost influence to Byzantium in northern Syria when the Empire imposed its suzerainty over the Principality of Antioch. Jerusalem thus turned its attention to Egypt, where the Fatimid dynasty was suffering from a series of young caliphs and civil wars. The crusaders had wanted to conquer Egypt since the days of Baldwin I, who died during an expedition there. The capture of Ascalon by Baldwin III made the conquest of Egypt more feasible.
Invasions of Egypt.
Amalric led his first expedition into Egypt in 1163, claiming that the Fatimids had not paid the yearly tribute that had begun during the reign of Baldwin III. The vizier, Dirgham, had recently overthrown the vizier Shawar, and marched out to meet Amalric at Pelusium, but was defeated and forced to retreat to Bilbeis. The Egyptians then opened up the Nile dams and let the river flood, hoping to prevent Amalric from invading any further. Amalric returned home but Shawar fled to the court of Nur ad-Din, who sent his general Shirkuh to settle the dispute in 1164. In response Dirgham sought help from Amalric, but Shirkuh and Shawar arrived before Amalric could intervene and Dirgham was killed. Shawar, however, feared that Shirkuh would seize power for himself, and he too looked to Amalric for assistance. Amalric returned to Egypt in 1164 and besieged Shirkuh in Bilbeis until Shirkuh retreated to Damascus.
Amalric could not follow up on his success in Egypt because Nur ad-Din was active in Syria, having taken Bohemund III of Antioch and Raymond III of Tripoli prisoner at the Battle of Harim during Amalric's absence. Amalric rushed to take up the regency of Antioch and Tripoli and secured Bohemund's ransom in 1165 (Raymond remained in captivity until 1173). The year 1166 was relatively quiet, but Amalric sent envoys to the Byzantine Empire seeking an alliance and a Byzantine wife, and throughout the year had to deal with raids by Nur ad-Din, who captured Banias.
In 1167, Nur ad-Din sent Shirkuh back to Egypt and Amalric once again followed him, establishing a camp near Cairo; Shawar again allied with Amalric and a treaty was signed with the caliph al-Adid himself. Shirkuh encamped on the opposite side of the Nile. After an indecisive battle, Amalric retreated to Cairo and Shirkuh marched north to capture Alexandria; Amalric followed and besieged Shirkuh there, aided by a Pisan fleet from Jerusalem. Shirkuh negotiated for peace and Alexandria was handed over to Amalric. However, Amalric could not remain there indefinitely, and returned to Jerusalem after exacting an enormous tribute.
Byzantine alliance.
After his return to Jerusalem in 1167, Amalric married Maria Comnena, a great-grandniece of Byzantine emperor Manuel I Comnenus. The negotiations had taken two years, mostly because Amalric insisted that Manuel return Antioch to Jerusalem. Once Amalric gave up on this point he was able to marry Maria in Tyre on August 29, 1167. During this time the queen dowager, Baldwin III's widow Theodora, eloped with her cousin Andronicus to Damascus, and Acre, which had been in her possession, reverted into the royal domain of Jerusalem. It was also around this time that William of Tyre was promoted to archdeacon of Tyre, and was recruited by Amalric to write a history of the kingdom.
In 1168 Amalric and Manuel negotiated an alliance against Egypt, and William of Tyre was among the ambassadors sent to Constantinople to finalize the treaty. Although Amalric still had a peace treaty with Shawar, Shawar was accused of attempting to ally with Nur ad-Din, and Amalric invaded. The Knights Hospitaller eagerly supported this invasion, while the Knights Templar refused to have any part in it. In October, without waiting for any Byzantine assistance (and in fact without even waiting for the ambassadors to return), Amalric invaded and seized Bilbeis. The inhabitants were either massacred or enslaved. Amalric then marched to Cairo, where Shawar offered Amalric two million pieces of gold. Meanwhile Nur ad-Din sent Shirkuh back to Egypt as well, and upon his arrival Amalric retreated.
Rise of Saladin.
In January 1169 Shirkuh had Shawar assassinated. Shirkuh became vizier, although he himself died in March, and was succeeded by his nephew Saladin. Amalric became alarmed and sent Frederick de la Roche, Archbishop of Tyre, to seek help from the kings and nobles of Europe, but no assistance was forthcoming. Later that year however a Byzantine fleet arrived, and in October Amalric launched yet another invasion and besieged Damietta by sea and by land. The siege was long and famine broke out in the Christian camp; the Byzantines and crusaders blamed each other for the failure, and a truce was signed with Saladin. Amalric returned home.
Now Jerusalem was surrounded by hostile enemies. In 1170 Saladin invaded Jerusalem and took the city of Eilat, severing Jerusalem's connection with the Red Sea. Saladin, who was set up as Vizier of Egypt, was declared Sultan in 1171 upon the death of the last Fatimid caliph. Saladin's rise to Sultan was an unexpected reprieve for Jerusalem, as Nur ad-Din was now preoccupied with reining in his powerful vassal. Nevertheless, in 1171 Amalric visited Constantinople himself and envoys were sent to the kings of Europe for a second time, but again no help was received. Over the next few years the kingdom was threatened not only by Saladin and Nur ad-Din, but also by the Hashshashin; in one episode, the Knights Templar murdered some Hashshashin envoys, leading to further disputes between Amalric and the Templars.
Death.
Nur ad-Din died in 1174, upon which Amalric immediately besieged Banias. On the way back after giving up the siege he fell ill from dysentery, which was ameliorated by doctors but turned into a fever in Jerusalem. William of Tyre explains that "after suffering intolerably from the fever for several days, he ordered physicians of the Greek, Syrian, and other nations noted for skill in diseases to be called and insisted that they give him some purgative remedy." Neither they nor Latin doctors could help, and he died on July 11, 1174.
Maria Comnena had borne Amalric two daughters: Isabella, who would eventually marry four husbands in turn and succeed as queen, was born in 1172; and a stillborn child some time later. On his deathbed Amalric bequeathed Nablus to Maria and Isabella, both of whom would retire there. The leprous child Baldwin IV succeeded his father and brought his mother Agnes of Courtenay (now married to her fourth husband) back to court.
Physical characteristics.
William was a good friend of Amalric and described him in great detail. "He had a slight impediment in his speech, not serious enough to be considered as a defect but sufficient to render him incapable of ready eloquence. He was far better in counsel than in fluent or ornate speech." Like his brother Baldwin III, he was more of an academic than a warrior, who studied law and languages in his leisure time: "He was well skilled in the customary law by which the kingdom was governed – in fact, he was second to no one in this respect." He was probably responsible for an assize making all rear-vassals directly subject to the king and eligible to appear at the Haute Cour. Amalric had an enormous curiosity, and William was reportedly astonished to find Amalric questioning, during an illness, the resurrection of the body. He especially enjoyed reading and being read to, spending long hours listening to William read early drafts of his history. He did not enjoy games or spectacles, although he liked to hunt. He was trusting of his officials, perhaps too trusting, and it seems that there were many among the population who despised him, although he refused to take any action against those who insulted him publicly.
He was tall and fairly handsome; "he had sparkling eyes of medium size; his nose, like that of his brother, was becomingly aquiline; his hair was blond and grew back somewhat from his forehead. A comely and very full beard covered his cheeks and chin. He had a way of laughing immoderately so that his entire body shook." He did not overeat or drink to excess, but his corpulence grew in his later years, decreasing his interest in military operations; according to William, he "was excessively fat, with breasts like those of a woman hanging down to his waist."
Amalric was pious and attended mass every day, although he also "is said to have absconded himself without restraint to the sins of the flesh and to have seduced married women…" Despite his piety he taxed the clergy, which they naturally opposed.
As William says, "he was a man of wisdom and discretion, fully competent to hold the reins of government in the kingdom." He is considered the last of the "early" kings of Jerusalem, after whom there was no king able to save Jerusalem from its eventual collapse. Within a few years, Emperor Manuel died as well, and Saladin remained the only strong leader in the east.

</doc>
<doc id="1872" url="http://en.wikipedia.org/wiki?curid=1872" title="Aimery of Jerusalem">
Aimery of Jerusalem

Aimery (Latin "Aimericus"; 1145 – 1 April 1205), born Aimery of Lusignan, was the first King of Cyprus (1194–1205) and tenth King of Jerusalem (1197–1205). He was an older brother of Guy of Lusignan. Older scholarship mistook the names Aimery and Amalric (Amaury) as variant spellings of the same name, so these historians erroneously added numbers for kings Amalric I (1163–74) and Amalric II (actually Aimery). Now scholars recognize that the two names were not the same and no longer add the number for either king. Confusion between the two names was common even among contemporaries.
The Lusignan family was noted for its many Crusaders. Aimery and Guy were sons of Hugh VIII of Lusignan, who had himself campaigned in the Holy Land in the 1160s. After being expelled from Poitou by their overlord, Richard the Lion-hearted, for the murder of Patrick of Salisbury, 1st Earl of Salisbury, Aimery arrived in Palestine c. 1174, Guy possibly later. Aimery married Eschiva, daughter of Baldwin of Ibelin. He then took service with Agnes of Courtenay, wife of Reginald of Sidon and mother of Baldwin IV of Jerusalem. The pro-Ibelin "Chronicle of Ernoul" later claimed that he was her lover, but it is likely that she and Baldwin IV were attempting to separate him from the political influence of his wife's family. He was appointed Constable of Jerusalem soon after 22 April 1179. Guy married the king's widowed older sister, Sibylla of Jerusalem in 1180, and so gained a claim to the kingdom of Jerusalem.
Aimery was among those captured with his brother after the disastrous Battle of Hattin in 1187. In 1194, on the death of Guy, he became King of Cyprus. By his first wife, Eschiva of Ibelin, he was the father of Hugh I of Cyprus and was crowned in Nicosia on 22 September 1197. After Eschiva's death in October 1197 he married Isabella, the daughter of Amalric of Jerusalem by his second marriage, and became King of Jerusalem in right of his wife and was crowned at Acre in January 1198. This was only possible, because the candidacy for the crown of Aimery, who was a vassal of Roman-German Emperor Henry VI., was supported by the German crusaders.
In 1198, at the end of the Crusade of 1197, he was able to procure a five years' truce with the Muslims, owing to the struggle between Saladin's brothers and his sons for the inheritance of his territories. The truce was disturbed by raids on both sides, but in 1204 it was renewed for six years.
Many members of the royal family died in rapid succession in early 1205, including Aimery himself. Aimery's two older sons, Guy and John, boys of about eight years of age, died early in 1205. Aimery died of dysentery (allegedly brought on by "a surfeit of white mullet") or even poisoned at Saint Jean d'Acre on 1 April 1205, just after his son Aimery and four days before his wife, and was buried at Saint Sophia, Nicosia. The kingdom of Cyprus passed to Hugh, his only surviving son, while the Kingdom of Jerusalem passed to Maria, the daughter of Isabella by her previous marriage with Conrad of Montferrat.
Wives and children.
His first wife, married before 29 October 1174, was Eschiva of Ibelin (c. 1160 – Cyprus in Winter 1196–1197), daughter of Baldwin of Ibelin and first wife Richilde de Bethsan or Bessan. They had six children:
His second wife was Queen Isabella of Jerusalem, married January 1198 in Acre. They had three children:

</doc>
<doc id="1873" url="http://en.wikipedia.org/wiki?curid=1873" title="Anthemius of Tralles">
Anthemius of Tralles

Anthemius of Tralles (c. 474 – before 558; Ancient Greek: Ἀνθέμιος ὁ Τραλλιανός) was a Greek professor of Geometry in Constantinople and architect, who collaborated with Isidore of Miletus to build the church of Hagia Sophia by the order of Justinian I. Anthemius came from an educated family, one of five sons of Stephanus of Tralles, a physician. Of his brothers, Dioscorus followed his father's profession in Tralles; Alexander became at Rome one of the most celebrated medical men of his time; Olympius was deeply versed in Roman jurisprudence; and Metrodorus was a distinguished grammarian in Constantinople.
As an architect he is best known for replacing the old church of Hagia Sophia at Constantinople in 532; his daring plans for the church strikingly displayed his knowledge. His skills seem also to have extended to engineering for he repaired the flood defences at Daras.
Anthemius was also a capable mathematician. He described the string construction of the ellipse and he wrote a book on conic sections, which was excellent preparation for designing the elaborate vaulting of Hagia Sophia. He compiled a survey of mirror configurations in his work on remarkable mechanical devices which was known to Arab mathematicians such as Ibn al-Haytham.
A fragment of his treatise "On burning-glasses" was published as "Περί παραδόξων μηχανημάτων" ("Concerning wondrous machines") by L. Dupuy in 1777, and also appeared in 1786 in the forty-second volume of the "Histoire de l'Academie des Instrumentistes". A. Westermann gave a revised edition of it in his "Παραδοξογράφοι" ("Scriptores rerum mirabilium Graeci", "Greek marvel-writers") in 1839. In the course of the constructions for surfaces to reflect to one and the same point
Anthemius assumes a property of an ellipse not found in Apollonius's work, that the equality of the angles subtended at a focus by two tangents drawn from a point, and having given the focus and a double ordinate he goes on to use the focus and directrix to obtain any number of points on a parabola—the first instance on record of the practical use of the directrix.

</doc>
<doc id="1874" url="http://en.wikipedia.org/wiki?curid=1874" title="Absalon">
Absalon

Absalon or Axel ( 1128 – 21 March 1201) was a Danish archbishop and statesman, who was the Bishop of Roskilde from 1158 to 1192 and Archbishop of Lund from 1178 until his death. He was the foremost politician and churchfather of Denmark in the second half of the 12th century, and was the closest advisor of King Valdemar I of Denmark. He was a key figure in the Danish policies of territorial expansion in the Baltic Sea, Europeanization in close relationship with the Holy See, and reform in the relation between the Church and the public. He combined the ideals of Gregorian Reform ideals with loyal support of a strong monarchical power.
Absalon was born into the powerful "Hvide" clan, and owned great land possessions. He endowed several church institutions, most prominently his family's Sorø Abbey. He was granted lands by the crown, and built the first fortification of the city that evolved into modern-day Copenhagen. His titles were passed on to his nephews Anders Sunesen and Peder Sunesen. He died in 1201, and was interred at Sorø Abbey.
Early life.
Absalon was born around 1128 near Sorø, Zealand. Due to a name which is unusual in Denmark, it is speculated that he was christened on the Danish "Absalon" name day, October 30. He was the son of Asser Rig, a magnate of the "Hvide" clan from Fjenneslev on Zealand. He was also a kinsman of Archbishop Eskil of Lund. He grew up at the castle of his father, and was brought up alongside his older brother Esbern Snare and the young prince Valdemar, who later became King Valdemar I of Denmark. During the civil war following the death of Eric III of Denmark in 1146, Absalon travelled abroad to study theology in Paris, while Esbern fought for Valdemar's ascension to the throne. At Paris, he was influenced by the Gregorian Reform ideals of churchly independence from Monarchical rule. He also befriended the canon William of Æbelholt at the Abbey of St Genevieve, whom he later made abbott of Eskilsø Abbey.
Absalon first appears in Saxo Grammaticus's contemporary chronicle "Gesta Danorum" at the end of the civil war, at the brokering of the peace agreement between Sweyn III and Valdemar at St. Alban's Priory, Odense. He was a guest at following Roskilde banquet given in 1157 by Sweyn to his rivals Canute V and Valdemar. Both Absalon and Valdemar narrowly escaped assassination at the hands of Sweyn on this occasion, and escaped to Jutland, whither Sweyn followed them. Absalon probably did not take part in the following battle of Grathe Heath in 1157, in which Sweyn was defeated and slain and led to Valdemar ascending the Danish throne. On Good Friday 1158, bishop Asser of Roskilde died, and Absalon was eventually elected bishop of Roskilde on Zealand with the help of Valdemar, as the king's reward for the "Hvide" family support.
Bishop and advisor.
Absalon was a close counsellor of Valdemar, and chief promoter of the Danish crusades against the Wends. During the Danish civil war, Denmark had been open to coastal raids by the Wends. It was Absalon's intention to clear the Baltic Sea of the Wendish pirates who inhabited its southern littoral zone which was later called Pomerania. The pirates had raided the Danish coasts during the civil war of Sweyn III, Canute V, and Valdemar, to the point where at the accession of Valdemar one-third of Denmark lay wasted and depopulated. Absalon formed a guardian fleet, built coastal defenses, and led several campaigns against the Wends. He even advocated forgiving the earlier enemies of Valdemar, which helped stabilize Denmark internally.
Wendish campaigns.
The first expedition against the Wends that was conducted by Absalon in person, set out in 1160. These expeditions were successful, but brought no lasting victories. What started out as mere retribution, eventually evolved into full-fledged campaigns of expansion with religious crusader motives. In 1164 began twenty years of crusades against the Wends, sometimes with the help of German duke Henry the Lion, sometimes in opposition to him.
In 1168 the chief Wendish fortress at Arkona in Rügen, containing the sanctuary of their god Svantevit, was conquered. The Wends agreed to accept Danish suzerainty and the Christian religion at the same time. From Arkona, Absalon proceeded by sea to Charenza, in the midst of Rügen, the political capital of the Wends and an all but impregnable stronghold. But the unexpected fall of Arkona had terrified the garrison, which surrendered unconditionally at the first appearance of the Danish ships. Absalon, with only Bishop Sweyn of Aarhus, and twelve "housecarls" thereupon disembarked, passed between a double row of Wendish warriors, 6000 strong, along the narrow path winding among the morasses, to the gates of the fortress, and, proceeding to the temple of the seven-headed god Rugievit, caused the idol to be hewn down, dragged forth and burnt. The whole population of Garz was then baptized, and Absalon laid the foundations of twelve churches in the isle of Rügen. Rügen was then subjected to Absalon's Bishopric of Roskilde.
The destruction of this chief sally-port of the Wendish pirates enabled Absalon considerably to reduce the Danish fleet. But he continued to keep a watchful eye over the Baltic, and in 1170 destroyed another pirate stronghold, farther eastward, at Dziwnów on the isle of Wolin. Absalon's last military exploit came in 1184, off Stralsund at Whitsun, when he soundly defeated a Pomeranian fleet that had attacked Denmark's vassal, Jaromar of Rügen.
Policies.
Absalon's main political goal was to free Denmark from entanglements with the Holy Roman Empire. Absalon reformed the Danish church organisation to closer match Holy See praxis, and worked to keep Denmark a close ally of the Holy See. However, during the schism between Pope Alexander III and Antipope Victor IV, Absalon stayed loyal to Valdemar even as he joined the Holy Roman Emperor Frederick Barberossa in supporting Victor IV. This caused a split within the Danish church, as it possibly forced Eskil into exile around 1161, despite Abaslon's attempts to keep the Danish church united. It was contrary to Absalon's advice and warnings that Valdemar I rendered fealty to the emperor Frederick Barbarossa at Dole in 1162. When Valdemar returned to Denmark, he was convinced into strengthening the Danevirke fortifications at the German border, with the support of Absalon.
Absalon built churches and monasteries, supporting international religious orders like the Cistercians and Augustinians, founding schools and doing his utmost to promote civilization and enlightenment. In 1162, Absalon transformed the Sorø Abbey of his family from Benedictine to Cistercian, granting it lands from his personal holdings. In 1167, Absalon was granted the land around the city of "Havn" (English: Harbour), and built there a castle in the coastal defense against the Wends. Havn quickly expanded as one of Scandinavia's most important centers of trade, and eventually evolved into modern-day Copenhagen. It was also Absalon who held the first Danish Synod at Lund in 1167. He was also interested in history and culture, and commissioned Saxo Grammaticus to write "Gesta Danorum", a comprehensive chronicle of the history of the Danes. In 1171, Absalon issued the "Zealand church law" (Danish: "Sjællandske Kirkelov"), which reduced the number of Canonical Law offenses for which the church could fine the public, while instituting the tithe payment system. Eventual violation of the law was specified as subject to a secular legal process.
Archbishop of Lund.
Archbishop Eskil returned from exile in 1167. Eskil agreed on canonizing Valdemar's father Knud Lavard in 1170, with Absalon assisting him at the feast. When Eskil stepped down as Archbishop of Lund in 1177, he chose Absalon as his successor. Absalon initially resisted the new position, as he did not want to lose his power position on Zealand, but complied with Papal orders to do so in 1178. By a unique Papal dispensation, Absalon was allowed to simultaneously maintain his post as Bishop of Roskilde. As the Archbishop of Lund, Absalon utilized ombudsmen from Zealand, demanded unfree labour from the peasantry, and instituted tithes. He was a harsh and effective ruler, who cleared all Orthodox Christian liturgic remnants in favour of Papal standards. A rebellion in the Scanian peasantry forced him to flee to Zealand in 1180, from where he returned and subdued the Scanians with the help of Valdemar.
When Valdemar died in 1182, his son succeeded him as Canute VI, and Absalon served as Canute VI's counsellor. Under Canute VI, Absalon was the chief policymaker in Danish politics. Absalon kept his hostile attitude to the Holy Roman Empire. On the accession of Canute VI in 1182, an imperial ambassador arrived at Roskilde to get the new king to swear fealty to Frederick Barbarossa, but Absalon resolutely withstood him. This represented the final Danish rejection of German supremacy.
Death.
When Absalon retired from military service in 1184 at the age of fifty-seven, he resigned the command of fleets and armies to younger men, like Duke Valdemar, the later king Valdemar II. He instead confined himself to the administration of the Danish empire. In 1192, Absalon made his nephew Peder Sunesen his successor as Bishop of Roskilde, while his other nephew Anders Sunesen was named the chancellor of Canute VI. Absalon died at Sorø Abbey on March 21, 1201, 73 years old, with his last will granting his personal holdings to the Abbey, apart from Fjenneslev which went to Esbern Snarre. He had already given Copenhagen to the Bishopric of Roskilde. Absalon was interred at Sorø Abbey, and was succeeded as Archbishop of Lund by Anders Sunesen.
Legacy.
Saxo Grammaticus' "Gesta Danorum" was not finished until after the death of Absalon, but Absalon was one of the chief heroic figures of the chronicle, which was to be the main source of knowledge about early Danish history. Absalon left a legacy as the foremost politician and churchfather of Denmark in the 12th century. Absalon was equally great as churchman, statesman and warrior. His policy of expansion was to give Denmark the dominion of the Baltic for three generations. That he enjoyed warfare there can be no doubt; yet he was not like the ordinary fighting bishops of the Middle Ages, whose sole indication of their religious role was to avoid the "shedding of blood" by using a mace in battle instead of a sword. Absalon never neglected his ecclesiastical duties, and even his wars were of the nature of crusades.
In the 2000s, "Absalon" was adopted as the name for a class of Royal Danish Navy vessels, and the lead vessel of the class. HDMS Absalon (L16) and "Esbern Snare" (L17) were launched and commissioned by Denmark in 2004 and 2005. In December 2008, "HDMS Absalon" was involved in the rescue of putative Somali pirates 90 miles off Yemen in the Gulf of Aden. The craft from Somalia was reported to hold rocket-propelled grenades and AK-47 assault rifles, and to have been adrift for several days. Also per the report, the "Absalon" took the sailors and weapons aboard, sunk the craft, and turned the sailors over to the Yemen coast guard. The "Absalon", according to "The New York Times" report, "was deployed in the Gulf of Aden [in] September ['08] as part of an international effort to curb piracy," part of Combined Task Force 150.

</doc>
<doc id="1875" url="http://en.wikipedia.org/wiki?curid=1875" title="Adhemar of Le Puy">
Adhemar of Le Puy

 
Adhemar (also known as Adémar, Aimar, or Aelarz) de Monteil (died 1 August 1098), one of the principal figures of the First Crusade, was bishop of Puy-en-Velay from before 1087.
Life.
At the Council of Clermont in 1095, Adhemar showed great zeal for the crusade (there is evidence Urban II had conferred with Adhemar before the council). Adhemar was named apostolic legate and appointed to lead the crusade by Pope Urban II on 27 November 1095. In part, Adhemar was selected to lead because he had already undertaken a pilgrimage to Jerusalem in 1086 and 1087. Departing 15 August 1096, he accompanied Raymond IV, Count of Toulouse, to the east. Whilst Raymond and the other leaders often quarrelled with each other over the leadership of the crusade, Adhemar was always recognized as the spiritual leader of the crusade.
Adhemar negotiated with Alexius I Comnenus at Constantinople, reestablished at Nicaea some discipline among the crusaders, fought a crucial role at the Battle of Dorylaeum and was largely responsible for sustaining morale during the siege of Antioch through various religious rites including fasting and special observances of holy days. After the capture of the city in June 1098, and the subsequent siege led by Kerbogha, Adhemar organized a procession through the streets, and had the gates locked so that the Crusaders, many of whom had begun to panic, would be unable to desert the city. He was extremely skeptical of Peter Bartholomew's discovery in Antioch of the Holy Lance, especially because he knew such a relic already existed in Constantinople; however, he was willing to let the Crusader army believe it was real if it raised their morale.
When Kerbogha was defeated, Adhemar organized a council in an attempt to settle the leadership disputes, but he died on 1 August 1098, probably of typhus. The disputes among the higher nobles went unsolved, and the march to Jerusalem was delayed for months. However, the lower-class foot soldiers continued to think of Adhemar as a leader; some of them claimed to have been visited by his ghost during the siege of Jerusalem, and reported that Adhemar instructed them to hold another procession around the walls. This was done, and Jerusalem was taken by the Crusaders in 1099.

</doc>
<doc id="1878" url="http://en.wikipedia.org/wiki?curid=1878" title="Alphonse, Count of Poitiers">
Alphonse, Count of Poitiers

Alphonse or Alfonso (11 November 1220 – 21 August 1271) was the Count of Poitou from 1225 and Count of Toulouse (as Alphonse II) from 1249.
Life.
Birth and early life.
Born at Poissy, Alphonse was a son of Louis VIII, King of France and Blanche of Castile. He was a younger brother of Louis IX of France and an older brother of Charles I of Sicily. In 1229, his mother, who was regent of France, forced the Treaty of Paris on Raymond VII of Toulouse after his rebellion. It stipulated that a brother of King Louis was to marry Joan of Toulouse, daughter of Raymond VII of Toulouse, and so in 1237 Alphonse married her. Since she was Raymond's only child, they became rulers of Toulouse at Raymond's death in 1249.
By the terms of his father's will he received an "appanage" of Poitou and Auvergne. To enforce this Louis IX won the battle of Taillebourg in the Saintonge War together with Alphonse against a revolt allied with king Henry III of England, who also participated in the battle.
Crusades.
Alphonse took part in two crusades with his brother, St Louis, in 1248 (the Seventh Crusade) and in 1270 (the Eighth Crusade). For the first of these, he raised a large sum and a substantial force, arriving in Damietta on 24 October 1249, after the town had already been captured. He sailed for home on 10 August 1250. His father-in-law had died while he was away, and he went directly to Toulouse to take possession. There was some resistance to his accession as count, which was suppressed with the help of his mother Blanche of Castile who was acting as regent in the absence of Louis IX. The county of Toulouse, since then, was joined to the Alphonse's "appanage".
Later life.
In 1252, on the death of his mother, Blanche of Castile, Alphonse was joint regent with Charles of Anjou until the return of Louis IX. During that time he took a great part in the campaigns and negotiations which led to the Treaty of Paris in 1259, under which King Henry III of England recognized his loss of continental territory to France (including Normandy, Maine, Anjou, and Poitou) in exchange for France withdrawing support from English rebels.
Aside from the crusades, Alphonse stayed primarily in Paris, governing his estates by officials, inspectors who reviewed the officials work, and a constant stream of messages. His main work was on his own estates. There he repaired the evils of the Albigensian war and made a first attempt at administrative centralization, thus preparing the way for union with the crown. The charter known as "Alphonsine," granted to the town of Riom, became the code of public law for Auvergne. Honest and moderate, protecting the middle classes against exactions of the nobles, he exercised a happy influence upon the south, in spite of his naturally despotic character and his continual and pressing need of money. He is noted for ordering the first recorded local expulsion of Jews, when he did so in Poitou in 1249.
When Louis IX again engaged in a crusade (the Eighth Crusade), Alphonse again raised a large sum of money and accompanied his brother. This time, however, he did not return to France, dying while on his way back, probably at Savona in Italy, on 21 August 1271. He had been appointed a Knight of the Order of the Ship by his brother.
Death and Legacy.
Alphonse's death without heirs raised some questions as to the succession to his lands. One possibility was that they should revert to the crown, another that they should be redistributed to his family. The latter was claimed by Charles of Anjou, but in 1283 Parlement decided that the County of Toulouse should revert to the crown, if there were no male heirs. Alphonse's wife Joan (who died four days after Alphonse) had attempted to dispose of some of her inherited lands in her will. Joan was the only surviving child and heiress of Raymond VII, Count of Toulouse, Duke of Narbonne, and Marquis of Provence, so under Provençal and French law, the lands should have gone to her nearest male relative. But, her will was invalidated by Parlement in 1274. One specific bequest in Alphonse's will, giving his wife's lands in the Comtat Venaissin to the Holy See, was allowed, and it became a Papal territory, a status that it retained until 1791.

</doc>
<doc id="1879" url="http://en.wikipedia.org/wiki?curid=1879" title="Alfonso Jordan">
Alfonso Jordan

Alfonso Jordan (French: "Alphonse Jourdain"; Latin: "Ildefonsus") (1103–1148) was the Count of Tripoli (1105–09), Count of Rouergue (1109–48) and Count of Toulouse, Margrave of Provence and Duke of Narbonne (1112–48, as Alfonso I). 
Life.
He was the son of Raymond IV of Toulouse by his third wife, Elvira of Castile. He was born in the castle of Mont Pèlerin in Tripoli while his father was on the First Crusade. He was given the name "Jordan" after being baptised in the Jordan River.
Alfonso's father died when he was two years old and he remained under the guardianship of his cousin, William Jordan, Count of Cerdagne, until he was five. He was then taken to Europe, where his half-brother Bertrand had given him the county of Rouergue. Upon Bertrand's death in 1112, Alfonso succeeded to the county of Toulouse and marquisate of Provence. In 1114, Duke William IX of Aquitaine, who claimed Toulouse by right of his wife Philippa, daughter of Count William IV, invaded the county and conquered it. Alfonso recovered a part in 1119, but he was not in full control until 1123. When at last successful, he was excommunicated by Pope Callixtus II for having expelled the monks of Saint-Gilles, who had aided his enemies.
Alfonso next had to fight for his rights in Provence against Count Raymond Berengar III of Barcelona. Not until September 1125 did their war end in "peace and concord" ("pax et concordia"). At this stage, Alfonso was master of the regions lying between the Pyrenees and the Alps, the Auvergne and the sea. His ascendancy was, according to one commentator, an unmixed good to the country, for during a period of fourteen years art and industry flourished. 
In March 1126, Alfonso was at the court of Alfonso VII of León when he acceded to the throne. 
According to the "Chronica Adefonsi imperatoris", Alfonso and Suero Vermúdez took the city of León from opposition magnates and handed it over to Alfonso VII. Among those who may have accompanied Alfonso on one of his many extended stays in Spain was the troubadour Marcabru.
About 1134 Alfonso seized the viscounty of Narbonne and ruled it during the minority of the Viscountess Ermengarde, only restoring it to her in 1143. 
In 1141 King Louis VII pressed the claim of Philippa on behalf of his wife, Eleanor of Aquitaine, even besieging Toulouse, but without result. 
That same year Alfonso Jordan was again in Spain, making a pilgrimage to Saint James of Compostela, when he proposed a peace between the king of León and García VI of Navarre, which became the basis for subsequent negotiations.
In 1144, Alfonso again incurred the displeasure of the church by siding with the citizens of Montpellier against their lord. 
In 1145, Bernard of Clairvaux addressed a letter to him full of concern about a heretic named Henry in the diocese of Toulouse. 
Bernard even went there to preach against the heresy, an early expression of Catharism. 
A second time he was excommunicated; but in 1146 he took the cross (i.e., vowed to go on crusade) at a meeting in Vézelay called by Louis VII. 
In August 1147, he embarked for the near east on the Second Crusade.
He lingered on the way in Italy and probably in Constantinople, where he may have met the Emperor Manuel I.
Alfonso finally arrived at Acre in 1148. 
Among his companions he had made enemies and he was destined to take no share in the crusade he had joined. 
He died at Caesarea, and there were accusations of poisoning, usually levelled against either by Eleanor of Aquitaine, the wife of Louis, or Melisende, the mother of King Baldwin III of Jerusalem. 
By his wife since 1125, Faydiva d'Uzès, he left two legitimate sons: Raymond, who succeeded him, and Alfonso. His daughter Faydiva (died 1154) married Count Humbert III of Savoy. He left two other daughters: the legitimate Agnes (died 1187) and the illegitimate Laurentia, who married Count Bernard III of Comminges.

</doc>
<doc id="1880" url="http://en.wikipedia.org/wiki?curid=1880" title="Ambroise">
Ambroise

Ambroise, sometimes Ambroise of Normandy, (flourished c. 1190) was a Norman poet and chronicler of the Third Crusade, author of a work called "L'Estoire de la guerre sainte", which describes in rhyming Old French verse the adventures of Richard Coeur de Lion as a crusader. The poem is known to us only through one Vatican manuscript, and long escaped the notice of historians.
The credit for detecting its value belongs to Gaston Paris, although his edition (1897) was partially anticipated by the editors of the "Monumenta Germaniae Historica", who published some selections in the twenty-seventh volume of their Scriptores (1885). Ambroise followed Richard I as a noncombatant, and not improbably as a court-minstrel. He speaks as an eye-witness of the king's doings at Messina, in Cyprus, at the siege of Acre, and in the abortive campaign which followed the capture of that city.
Ambroise is surprisingly accurate in his chronology; though he did not complete his work before 1195, it is evidently founded upon notes which he had taken in the course of his pilgrimage. He shows no greater political insight than we should expect from his position; but relates what he had seen and heard with a naïve vivacity which compels attention. He is by no means an impartial source: he is prejudiced against the Saracens, against the French, and against all the rivals or enemies of his master, including the "Polein" party which supported Conrad of Montferrat against Guy of Lusignan. He is rather to be treated as a biographer than as a historian of the Crusade in its broader aspects. Nonetheless he is an interesting primary source for the events of the years 1190–1192 in the Kingdom of Jerusalem.
Books 2–6 of the "Itinerarium Regis Ricardi", a Latin prose narrative of the same events apparently compiled by Richard, a canon of Holy Trinity, London, are closely related to Ambroise's poem. They were formerly sometimes regarded as the first-hand narrative on which Ambroise based his work, but that can no longer be maintained.

</doc>
<doc id="1881" url="http://en.wikipedia.org/wiki?curid=1881" title="Art Deco">
Art Deco

Art Deco (), or Deco, is an influential visual arts design style that first appeared in France after World War I and began flourishing internationally in the 1920s, 1930s and 1940s before its popularity waned after World War II. It is an eclectic style that combines traditional craft motifs with Machine Age imagery and materials. The style is often characterized by rich colours, bold geometric shapes and lavish ornamentation.
Deco emerged from the interwar period when rapid industrialisation was transforming culture. One of its major attributes is an embrace of technology. This distinguishes Deco from the organic motifs favoured by its predecessor Art Nouveau.
Historian Bevis Hillier defined Art Deco as "an assertively modern style [that] ran to symmetry rather than asymmetry, and to the rectilinear rather than the curvilinear; it responded to the demands of the machine and of new material [and] the requirements of mass production".
During its heyday, Art Deco represented luxury, glamour, exuberance and faith in social and technological progress.
Etymology.
The first use of the term "Art Deco" has been attributed to architect Le Corbusier, who penned a series of articles in his journal "L'Esprit nouveau" under the headline "1925 Expo: Arts Déco". He was referring to the 1925 Exposition Internationale des Arts Décoratifs et Industriels Modernes (International Exposition of Modern Decorative and Industrial Arts).
The term came into more general use in 1966, when a French exhibition celebrating the 1925 event was held under the title "Les Années 25: Art Déco/Bauhaus/Stijl/Esprit Nouveau". Here the term was used to distinguish the new styles of French decorative crafts that had emerged since the Belle Epoque. The term Art Deco has since been applied to a wide variety of works produced during the Interwar period ("L'Entre Deux Guerres"), and even to those of the Bauhaus in Germany. However, Art Deco originated in France. It has been argued that the term should be applied to French works and those produced in countries directly influenced by France.
Art Deco gained currency as a broadly applied stylistic label in 1968 when historian Bevis Hillier published the first book on the subject: "Art Deco of the '20s and '30s". Hillier noted that the term was already being used by art dealers and cites "The Times" (2 November 1966) and an essay named "Les Arts Déco" in "Elle" magazine (November 1967) as examples of prior usage. In 1971, Hillier organised an exhibition at the Minneapolis Institute of Arts, which he details in his book about it, "The World of Art Deco".
Origins.
Some historians trace Deco's roots to the Universal Exposition of 1900. After this show a group of artists established an informal collective known as "La Société des artistes décorateurs" (Society of Decorator Artists) to promote French crafts. Among them were Hector Guimard, Eugène Grasset, Raoul Lachenal, Paul Bellot, Maurice Dufrêne and Emile Decoeur. These artists are said to have influenced the principles of Art Deco.
The Art Deco era is often anecdotally dated from 1925 when the Exposition Internationale des Arts Décoratifs et Industriels Modernes was organized to showcase new ideas in applied arts, although the style had been in full force in France for several years before that date. Deco was heavily influenced by pre-modern art from around the world and observable at the Musée du Louvre, Musée de l'Homme and the Musée national des Arts d'Afrique et d'Océanie. During the 1920s, affordable travel permitted "in situ" exposure to other cultures. There was also popular interest in archeology due to excavations at Pompeii, Troy, the tomb of Tutankhamun, etc. Artists and designers integrated motifs from ancient Egypt, Mesopotamia, Greece, Rome, Asia, Mesoamerica and Oceania with Machine Age elements.
Deco was also influenced by Cubism, Constructivism, Functionalism, Modernism, and Futurism.
In 1905, before the onset of Cubism, Eugène Grasset wrote and published "Méthode de Composition Ornementale, Éléments Rectilignes," within which he systematically explored the decorative (ornamental) aspects of geometric elements, forms, motifs and their variations, in contrast with (and as a departure from) the undulating Art Nouveau style of Hector Guimard, so popular in Paris a few years earlier. Grasset stresses the principle that various simple geometric shapes like triangles and squares are the basis of all compositional arrangements.
At the 1907 Salon d'Automne in Paris, Georges Braque exhibited "Viaduc à l'Estaque" (a proto-Cubist work), now at the Minneapolis Institute of Arts. Simultaneously, there was a retrospective exhibition of 56 works by Paul Cézanne, as a tribute to the artist who died in 1906. Cézanne was interested in the simplification of forms to their geometric essentials: the cylinder, the sphere, the cone.
Paul Iribe created for the couturier Paul Poiret esthetic designs that shocked the Parisian milieu with its novelty. These illustrations were compiled into an album, "Les Robes de Paul Poiret racontée par Paul Iribe", published in 1908.
At the 1910 Salon des Indépendants, Jean Metzinger, Henri Le Fauconnier and Robert Delaunay, shown together in Room 18, elaborated upon Cézannian syntax, revealing to the general public for the first time a "mobile perspective" in their art, soon to become known as Cubism. Several months later, the Salon d'Automne saw the invitation of Munich artists who for several years had been working with simple geometric shapes. Leading up to 1910 and culminating in 1912, the French designers André Mare and Louis Sue turned towards the quasi-mystical Golden ratio, in accord with Pythagorean and Platonic traditions, giving their works a Cubist sensibility.
Between 1910 and 1913, Paris saw the construction of the Théâtre des Champs-Élysées, 15 avenue Montaigne, another sign of the radical aesthetic change experienced by the Parisian milieu of the time. The rigorous composition of its facade, designed by Auguste Perret, is a major example of early Art Deco. The building includes exterior bas reliefs by Antoine Bourdelle, a dome by Maurice Denis, paintings by Édouard Vuillard and Jacqueline Marval, and a stage curtain design by Ker-Xavier Roussel.
The artists of the Section d'Or exhibited (in 1912) works considerably more accessible to the general public than the analytical Cubism of Picasso and Braque. The Cubist vocabulary was poised to attract fashion, furniture and interior designers.
These revolutionary changes occurring at the outset of the 20th century are summarized in the 1912 writings of André Vera. "Le Nouveau style", published in the journal "L'Art décoratif", expressed the rejection of Art Nouveau forms (asymmetric, polychrome and picturesque) and called for "simplicité volontaire, symétrie manifeste, l'ordre et l'harmonie", themes that would eventually become ubiquitous within the context of Art Deco.
Order, color and geometry: the essence of Art Deco vocabulary was made manifest before 1914.
Several years after World War I, in 1927, Cubists Joseph Csaky, Jacques Lipchitz, Louis Marcoussis, Henri Laurens, the sculptor Gustave Miklos, and others collaborated in the decoration of a Studio House, rue Saint-James, Neuilly-sur-Seine, designed by the architect Paul Ruaud and owned by the French fashion designer Jacques Doucet, also a collector of Post-Impressionist and Cubist paintings (including Les Demoiselles d'Avignon, which he bought directly from Picasso's studio). Laurens designed the fountain, Csaky designed Doucet's staircase, Lipchitz made the fireplace mantel, and Marcoussis made a Cubist rug.
"La Maison Cubiste" (The Cubist House).
In the "Art Décoratif" section of the 1912 Salon d'Automne, an architectural installation was exhibited that quickly became known as "La Maison Cubiste" ("The Cubist House"). The facade was designed by Raymond Duchamp-Villon and the interior by André Mare along with a group of collaborators. "Mare's ensembles were accepted as frames for Cubist works because they allowed paintings and sculptures their independence", writes Christopher Green, "creating a play of contrasts, hence the involvement not only of Gleizes and Metzinger themselves, but of Marie Laurencin, the Duchamp brothers (Raymond Duchamp-Villon designed the facade) and Mare's old friends Léger and Roger de La Fresnaye".
"La Maison Cubiste" was a fully furnished house, with a staircase, wrought iron banisters, a living room—the "Salon Bourgeois", where paintings by Marcel Duchamp, Jean Metzinger, Albert Gleizes, Marie Laurencin and Fernand Léger were hung—and a bedroom. It was an early example of "L'art décoratif", a home within which Cubist art could be displayed in the comfort and style of modern, bourgeois life. Spectators at the Salon d'Automne passed through the full-scale 10-by-3-meter plaster model of the ground floor of the facade. This architectural installation was subsequently exhibited at the 1913 Armory Show, New York, Chicago and Boston, listed in the catalogue of the New York exhibit as Raymond Duchamp-Villon, number 609, and entitled "Facade architectural, plaster" ("Façade architecturale").
Attributes.
Deco emphasizes geometric forms: spheres, polygons, rectangles, trapezoids, zigzags, chevrons, and sunburst motifs. Elements are often arranged in symmetrical patterns. Modern materials such as aluminum, stainless steel, Bakelite, chrome, and plastics are frequently used. Stained glass, inlays, and lacquer are also common. Colors tend to be vivid and high contrast.
Influence.
Art Deco was a globally popular style and affected many areas of design. It was used widely in consumer products such as automobiles, furniture, cookware, china, textiles, jewelry, clocks, and electronic items such as radios, telephones, and jukeboxes. It also influenced architecture, interior design, industrial design, fashion, graphic arts, and cinema.
During the 1930s, Art Deco was used extensively for public works projects, railway stations, ocean liners (including the "Île de France", "Queen Mary", and "Normandie"), movie palaces, and amusement parks.
The austerities imposed by World War II caused Art Deco to decline in popularity: it was perceived by some as gaudy and inappropriately luxurious. A resurgence of interest began during the 1960s. Deco continues to inspire designers and is often used in contemporary fashion, jewelry, and toiletries.
Streamline Moderne.
A style related to Art Deco is Streamline Moderne (or Streamline) which emerged during the mid-1930s. Streamline was influenced by modern aerodynamic principles developed for aviation and ballistics to reduce air friction at high velocities. Designers applied these principles to cars, trains, ships, and even objects not intended to move, such as refrigerators, gas pumps, and buildings.
One of the first production vehicles in this style was the Chrysler Airflow of 1933. It was unsuccessful commercially, but the beauty and functionality of its design set a precedent.
Streamlining quickly influenced automotive design and evolved the rectangular "horseless carriage" into sleek vehicles with aerodynamic lines, symmetry, and V-shapes. These designs continued to be popular after World War II.
Surviving examples.
Europe.
Belgium.
One of the largest Art Deco buildings in Western Europe is the Basilica of the Sacred Heart in Koekelberg, Brussels. In 1925, architect Albert van Huffel won the Grand Prize for Architecture with his scale model of the basilica at the "Exposition Internationale des Arts Décoratifs et Industriels Modernes" in Paris.
Germany.
In Germany two variations of Art Deco flourished in the 1920s and 30s: The Neue Sachlichkeit (New Objectivity) employed the same curving horizontal lines and nautical motifs that are known as Streamline Moderne in the Anglophone world. While Neue Sachlichkeit was rather austere and reduced (eventually merging with the Bauhaus style), Expressionist architecture came up with a more emotional use of shapes, colors and textures, partly reinterpreting shapes from the German and Baltic Brick Gothic style. Notable examples are Erich Mendelsohn's "Mossehaus" and "Schaubühne" theater in Berlin, Fritz Höger's "Chilehaus" in Hamburg and his "Kirche am Hohenzollernplatz" in Berlin, the "Anzeiger Tower" in Hannover and the "Borsig Tower" in Berlin. Art deco architecture was revived in the late-20th century by architects like Hans Kollhoff (see his tower on Potsdamer Platz), Jan Kleihues and Tobias Nöfer.
The 1921 Mossehaus in Berlin by Erich Mendelsohn was a pioneering design in Art Deco and Streamline Moderne, that displays how the Deco style spread and evolved in Europe.
Greece.
Art Deco in Athens incorporated insolently many of the structural and formal characteristics of the Classical idiom, at times transforming them to mere decorative elements, or oppositely, imprinting to them a functionality. Thematically it moved beyond the Classical period and looked for its models in the Mycenaean, Archaic, Hellenistic and Byzantine arts. The classicizing trends however, as one would expect in the city of Parthenon, held strongly, and despite what it has been sometimes suggested, Art Deco was never really independent in Athens. Rather, it accommodated itself in the midst of a strong and ideologically charged classicizing tradition and produced some of the most original and less expected works of the Greek architectural heritage.
Lithuania.
Like Romania, Lithuania too experienced booming industrial growth during the Interwar period. This resulted in the rapid modernization of the city of Kaunas in particular. At this time it became the temporary capital of Lithuania. Vytautas the Great War Museum, built in 1936 and located in downtown Kaunas, along with the Central Post Building and the Pienocentras HQ Building (1934) are the three most prominent Art Deco structures in the city. Today many of these buildings still stand, and apartment complexes and large government buildings alike survive from this time, even through the Nazi and Soviet occupations of Kaunas. Many other buildings around the city were built in the Bauhaus style.
Norway.
An example of Art Deco in Norway is found in the Student Society in Trondheim (built 1927–29). Its interior is based on an abandoned circus, so that the exterior exhibits a characteristic round shape.
Romania.
As a result of the inter-war period of rapid development, cities in Romania have numerous Art Deco buildings, including government buildings, hotels, and private houses. The best representative in this regard is the capital, Bucharest, which, despite the widespread destruction of its architecture during Communist times, still has many Art Deco examples, both on its main boulevards and in the lesser known parts of the city. 
Constanta has the second number of Art Deco buildings after Bucharest.
Ploieşti also has many Art Deco houses.
Spain.
Valencia was built profusely in Art Deco style during the period of economic bounty between wars in which Spain remained neutral. Particularly remarkable are the famous bath house Las Arenas, the building hosting the rectorship of the University of Valencia and the cinemas Rialto (currently the Filmoteca de la Generalitat Valenciana), Capitol (reconverted into an office building) and Naruto.
United Kingdom.
During the 1930s, Art Deco had a noticeable effect on house design in the United Kingdom, as well as the design of various public buildings. Straight, white-rendered house frontages rising to flat roofs, sharply geometric door surrounds and tall windows, as well as convex-curved metal corner windows, were all characteristic of that period.
North America.
Canada.
In Canada Art Deco structures that survive are mainly in urban centres like Montreal, Toronto, Hamilton, Ontario, and Vancouver. They range from public buildings like Vancouver City Hall to commercial buildings (College Park) to public works (R. C. Harris Water Treatment Plant). 
United States.
The U.S. has many examples of Art Deco architecture. Chicago, Los Angeles, and New York have many Art Deco buildings: the famous skyscrapers are the best-known, but notable Art Deco buildings can be found in various neighborhoods. Art deco was popular during the later years of the movie palace era of theatre construction. Excellent examples of Art Deco theatres, such as the Fargo Theatre in Fargo, North Dakota, and The Campus Theatre in Lewisburg, Pennsylvania, still exist throughout the United States.
Oceania.
Australia.
Australia also has many surviving examples of Art Deco architecture. Among the most notable are:
Gallery.
Antoine Bourdelle, 1910–12, "Apollon et sa méditation entourée des 9 muses (The Meditation of Apollo and the Muses)", bas-relief, Théâtre des Champs Elysées, Paris. This work represents one of the earliest examples of what would become known as Art Deco sculpture
See also.
See also: the categories , , , , , , , and .
Bibliography.
</dl>

</doc>
<doc id="1884" url="http://en.wikipedia.org/wiki?curid=1884" title="ASCII art">
ASCII art

ASCII art is a graphic design technique that uses computers for presentation and consists of pictures pieced together from the 95 printable (from a total of 128) characters defined by the ASCII Standard from 1963 and ASCII compliant character sets with proprietary extended characters (beyond the 128 characters of standard 7-bit ASCII). The term is also loosely used to refer to text based visual art in general. ASCII art can be created with any text editor, and is often used with free-form languages. Most examples of ASCII art require a fixed-width font (non-proportional fonts, as on a traditional typewriter) such as Courier for presentation.
Among the oldest known examples of ASCII art are the
creations by computer-art pioneer Kenneth Knowlton from around 1966, who was working for Bell Labs at the time. "Studies in Perception I" by Ken Knowlton and Leon Harmon from 1966 shows some examples of their early ASCII art.
One of the main reasons ASCII art was born was because early printers often lacked graphics ability and thus characters were used in place of graphic marks. Also, to mark divisions between different print jobs from different users, bulk printers often used ASCII art to print large banners, making the division easier to spot so that the results could be more easily separated by a computer operator or clerk. ASCII art was also used in early e-mail when images could not be embedded. ASCII art can also be used for typesetting initials.
History.
Typewriter art.
Since 1867 typewriters have been used for creating visual art. The oldest known preserved example of typewriter art is a picture of a butterfly made in 1898 by Flora Stacey. 
Typewriter portraits by Hobart Reese gained attention in 1922.
Typewriter art was also called keyboard art. 
In the 1954 short film "Stamp Day for Superman", typewriter art was a feature of the plot.
TTY and RTTY.
TTY stands for "TeleTYpe" or "TeleTYpewriter" and is also known as Teleprinter or Teletype.
RTTY stands for Radioteletype; character sets such as Baudot code, which predated ASCII, were used. According to a chapter in the "RTTY Handbook", text images have been sent via teletypewriter as early as 1923. However, none of the "old" RTTY art has been discovered yet. What is known is that text images appeared frequently on radioteletype in the 1960s and the 1970s.
Line-printer art.
In the 1960s, Andries van Dam published a representation of an electronic circuit produced on an IBM 1403 line printer. At the same time, Kenneth Knowlton was producing realistic images, also on line printers, by overprinting several characters on top of one another.
ASCII art.
The widespread usage of ASCII art can be traced to the computer bulletin board systems of the late 1970s and early 1980s. The limitations of computers of that time period necessitated the use of text characters to represent images. Along with ASCII's use in communication, however, it also began to appear in the underground online art groups of the period. An ASCII comic is a form of webcomic which uses ASCII text to create images. In place of images in a regular comic, ASCII art is used, with the text or dialog usually placed underneath.
During the 1990s, graphical browsing and variable-width fonts became increasingly popular, leading to a decline in ASCII art. Despite this, ASCII art continued to survive through online MUDs, an acronym for "Multi-User Dungeon", (which are textual multiplayer role-playing video games), Internet Relay Chat, E-mail, message boards and other forms of online communication which commonly employ the needed fixed-width.
ANSI.
ASCII and more importantly, ANSI were staples of the early technological era; terminal systems relied on coherent presentation using color and control signals standard in the terminal protocols.
Over the years, warez groups began to enter the ASCII art scene. Warez groups usually release .nfo files with their software, cracks or other general software reverse-engineering releases. The ASCII art will usually include the warez group's name and maybe some ASCII borders on the outsides of the release notes, etc.
BBS systems were based on ASCII and ANSI art, as were most DOS and similar console applications, and the precursor to AOL.
Uses.
ASCII art is used wherever text can be more readily printed or transmitted than graphics, or in some cases, where the transmission of pictures is not possible. This includes typewriters, teleprinters, non-graphic computer terminals, printer separators, in early computer networking (e.g., BBSes), e-mail, and Usenet news messages. ASCII art is also used within the source code of computer programs for representation of company or product logos, and flow control or other diagrams. In some cases, the entire source code of a program is a piece of ASCII art – for instance, an entry to one of the earlier International Obfuscated C Code Contest is a program that adds numbers, but visually looks like a binary adder drawn in logic ports.
Examples of ASCII-style art predating the modern computer era can be found in the June 1939, July 1948 and October 1948 editions of Popular Mechanics.
"0verkill" is a 2D platform multiplayer shooter game designed entirely in colour ASCII art. MPlayer and VLC media player can display videos as ASCII art. ASCII art is used in the making of DOS-based ZZT games.
Many game walkthrough guides come as part of a basic .txt file; this file often contains the name of the game in ASCII art. Such as below, word art is created using backslashes and other ASCII values in order to create the illusion of 3D. 
Types and styles.
Different techniques could be used in ASCII art to obtain different artistic effects. Electronic circuits and diagrams were implemented by typewriter or teletype and provided the pretense for ASCII.
Line art, for creating shapes: 
 .--. /\ ____
 '--' /__\ (^._.^)~ <(o.o )>
Solid art, for creating filled objects:
 .g@8g. db
 'Y8@P' d88b
Shading, using symbols with various intensities for creating gradients or contrasts:
 :$#$: "4b. ':.
 :$#$: "4b. ':.
Combinations of the above, often used as signatures, for example, at the end of an email:
 |\_/| **************************** (\_/)
 / @ @ \ * "Purrrfectly pleasant" * (='.'=)
 ( > º < ) * Poppy Prinz * (")_(")
 `Â»xÂ«´ * (pprinz@example.com) *
 / O \ ****************************
As-Pixel Characters, use combinations of ░ , █ , ▄ and ▀ to make pictures:<br>
 ▄▄▄▄▄▄▄░▄▄▄▄▄▄▄░▄▄▄▄▄▄░▄▄▄▄▄<br>
 ░░▀███░░░░▀██░░░░██▀░░░░██░░<br>
 ░░░▀██░░░░░▀██░░▄█░░░░░▄█░░░<br>
 ░░░░███░░░░░▀██▄█░░░░░░█░░░░<br>
 ░░░░░███░░░░░▀██░░░░░░█▀░░░░<br>
 ░░░░░░███░░░░▄███░░░░█▀░░░░░<br>
 ░░░░░░░██▄░░▄▀░███░░█▀░░░░░░<br>
 ░░░░░░░▀██▄█▀░░░███▄▀░░░░░░░<br>
 ░░░░░░░░▀██▀░░░░░███░░░░░░░░<br>
Emoticons and verticons.
The simplest forms of ASCII art are combinations of two or three characters for expressing emotion in text. They are commonly referred to as 'emoticon', 'smilie', or 'smiley'.
There is another type of one-line ASCII art that does not require the mental rotation of pictures, which is widely known in Japan as kaomoji (literally "face characters".) Traditionally, they are referred to as "ASCII face".
More complex examples use several lines of text to draw large symbols or more complex figures.
Popular smileys.
Hundreds of different text smileys were developed over time, but only a few were generally accepted, used and understood.
ASCII comic.
An ASCII comic is a form of webcomic.
The Adventures of Nerd Boy.
The Adventures of Nerd Boy, or just Nerd Boy is an ASCII comic by Joaquim Gândara between 6 August 2001 and 17 July 2007, consisting of 600 strips. They were posted to ASCII art newsgroup alt.ascii-art and on the website. Some strips have been translated to Polish and French.
Styles of the computer underground text art scene.
Atari 400/800 ATASCII.
The Atari 400/800 which were released in 1979 did not follow the ASCII standard and had its own character set, called ATASCII. The emergence of ATASCII art coincided with the growing popularity of BBS Systems caused by availability of the acoustic couplers that were compatible with the 8-bit home computers. ATASCII text animations are also referred to as "break animations" by the Atari sceners.
C-64 PETSCII.
The Commodore 64, which was released in 1982, also did not follow the ASCII standard. The C-64 character set is called PETSCII, an extended form of ASCII-1963. As with the Atari's ATASCII art, C-64 fans developed a similar scene that used PETSCII for their creations.
"Block ASCII" / "High ASCII" style ASCII art on the IBM PC.
So-called "block ASCII" or "high ASCII" uses the extended characters of the 8-bit code page 437, which is a proprietary standard introduced by IBM in 1979 (ANSI Standard x3.16) for the IBM PC DOS and MS-DOS operating systems. "Block ASCIIs" were widely used on the PC during the 1990s until the Internet replaced BBSes as the main communication platform. Until then, "block ASCIIs" dominated the PC Text Art Scene.
The first art scene group that focused on the extended character set of the PC in their art work was called "Aces of ANSI Art," or "AAA." Some members left in 1990, and formed a group called ACiD, "ANSI Creators in Demand." In that same year the second major underground art scene group was founded, ICE, "Insane Creators Enterprise".
There is some debate between ASCII and block ASCII artist, with "Hardcore" ASCII artists maintaining that block ASCII art is in fact not ANSI art, because it does not use the 128 characters of the original ASCII standard. On the other hand, block ASCII artists argue that if their art uses only characters of the computers character set, then it is to be called ASCII, regardless if the character set is proprietary or not.
Microsoft Windows does not support the ANSI Standard x3.16. One can view block ASCIIs with a text editor using the font "Terminal", but it will not look exactly as it was intended by the artist. With a special ASCII/ANSI viewer, such as ACiDView for Windows (see ASCII and ANSI art viewers), one can see block ASCII and ANSI files properly. An example that illustrates the difference in appearance is part of this article. Alternatively, one could look at the file using the Type command in the command prompt.
"Amiga"/"Oldskool" style ASCII art.
In the art scene one popular ASCII style that used the 7-bit standard ASCII character set was the so-called "Oldskool" Style. It is also called "Amiga style", due to its origin and widespread use on the Commodore Amiga Computers. The style uses primarily the characters: _/\-+=.()<>:. The "oldskool" art looks more like the outlined drawings of shapes than real pictures.
This is an example of "Amiga style" (also referred to as "old school" or "oldskool" style) scene ASCII art.
The Amiga ASCII Scene surfaced in 1992, seven years after the introduction of the Commodore Amiga 1000. The Commodore 64 PETSCII scene did not make the transition to the Commodore Amiga as the C64 demo and warez scenes did. Among the first Amiga ASCII art groups were ART, Epsilon Design, Upper Class, Unreal (later known as "DeZign"). This means that the text art scene on the Amiga was actually younger than the text art scene on the PC. The Amiga artists also did not call their ASCII art style "Oldskool". That term was introduced on the PC. When and by whom is unknown and lost in history.
The Amiga style ASCII artwork was most often released in the form of a single text file, which included all the artwork (usually requested), with some design parts in between, as opposed to the PC art scene where the art work was released as a ZIP archive with separate text files for each piece. Furthermore, the releases were usually called "ASCII collections" and not "art packs" like on the IBM PC.
In text editors.
This kind of ASCII art is handmade in a text editor. Popular editors used to make this kind of ASCII art include CygnusEditor a.k.a. CED (Amiga) and EditPlus2 (PC).
Oldskool font example from the PC, which was taken from the ASCII Editor FIGlet.
Newskool style ASCII art.
"Newskool" is a popular form of ASCII art which capitalizes on character strings like "$#Xxo". In spite of its name, the style is not "new"; on the contrary, it was very old but fell out of favor and was replaced by "Oldskool" and "Block" style ASCII art. It was dubbed "Newskool" upon its comeback and renewed popularity at the end of the 1990s.
Newskool changed significantly as the result of the introduction of extended proprietary characters. The classic 7-bit standard ASCII characters remain predominant, but the extended characters are often used for "fine tuning" and "tweaking". The style developed further after the introduction and adaptation of Unicode.
Methods for generating ASCII art.
While some prefer to use a simple text editor to produce ASCII art, specialized programs, such as JavE have been developed that often simulate the features and tools in bitmap image editors. For Block ASCII art and ANSI art the artist almost always uses a special text editor, because the required characters are not available on a standard keyboard.
The special text editors have sets of special characters assigned to existing keys on the keyboard. Popular MS DOS based editors, such as TheDraw and ACiDDraw had multiple sets of different special characters mapped to the F-Keys to make the use of those characters easier for the artist who can switch between individual sets of characters via basic keyboard shortcuts. PabloDraw is one of the very few special ASCII/ANSI art editors that were developed for MS Windows XP.
Image to text conversion.
Other programs allow one to automatically convert an image to text characters, which is a special case of vector quantization. A method is to sample the image down to grayscale with less than 8-bit precision, and then assign a character for each value. Such ASCII art generators often allow users to choose the intensity and contrast of the generated image.
3 factors limitate the fidelity of the conversion, especially of photographs:
Examples of converted images are given below.
This is one of the earliest forms of ASCII art, dating back to the early days of the 1960s minicomputers and teletypes. During the 1970s it was popular in malls to get a t-shirt with a photograph printed in ASCII art on it from an automated kiosk manned by a computer. With the advent of the web and HTML and CSS, many ASCII conversion programs will now quantize to a full RGB colorspace, enabling colorized ASCII images.
Still images or movies can also be converted to ASCII on various UNIX and UNIX-Like systems using the or graphics device driver, or the VLC media player under Windows, Linux or OS X; all of which render the screen using ASCII symbols instead of pixels. See also .
There are also a number of smartphone applications, such as ASCII cam for Android, that generate ASCII art in real-time using input from the phone's camera. These applications typically allow the ASCII art to be saved as either a text file or as an image made up of ASCII text.
Non fixed-width ASCII.
Most ASCII art is created using a monospaced font, where all characters are identical in width (Courier is a popular monospaced font). Early computers in use when ASCII art came into vogue had monospaced fonts for screen and printer displays. Today most of the more commonly used fonts in word processors, web browsers and other programs are proportional fonts, such as Helvetica or Times Roman, where different widths are used for different characters. ASCII art drawn for a fixed width font will usually appear distorted, or even unrecognizable when displayed in a proportional font.
Some ASCII artists have produced art for display in proportional fonts. These ASCIIs, rather than using a purely shade-based correspondence, use characters for slopes and borders and use block shading. These ASCIIs generally offer greater precision and attention to detail than fixed-width ASCIIs for a lower character count, although they are not as universally accessible since they are usually relatively font-specific.
Animated ASCII art.
Animated ASCII art started in 1970 from so-called VT100 animations produced on vt100 terminals. These animations were simply text with cursor movement instructions, deleting and erasing the characters necessary to appear animated. Usually, they represented a long hand-crafted process undertaken by a single person to tell a story.
Contemporary web browser revitalized animated ASCII art again. It became possible to display animated ASCII art via JavaScript or Java applets. Static ASCII art pictures are loaded and displayed one after another, creating the animation, very similar to how movie projectors unreel film reel and project the individual pictures on the big screen at movie theaters. A new term was born: "ASCIImation" – another name of Animated ASCII Art. A seminal work in this arena is the Star Wars ASCIImation. More complicated routines in JavaScript generate more elaborate ASCIImations showing effects like Morphing effects, star field emulations, fading effects and calculated images, such as mandelbrot fractal animations.
There are now many tools and programs that can transform raster images into text symbols; some of these tools can operate on streaming video. For example, the music video for pop singer Beck Hansen's song "Black Tambourine" is made up entirely of ASCII characters that approximate the original footage. Vlc, a media player software, can render any video in colored ASCII through the libcaca module.
Other text-based visual art.
There are a variety of other types of art using text symbols from character sets other than ASCII and/or some form of color coding. Despite not being pure ASCII, these are still often referred to as "ASCII art". The character set portion designed specifically for drawing is known as the line drawing characters or pseudo-graphics.
ANSI art.
The IBM PC graphics hardware in text mode uses 16 bits per character. It supports a variety of configurations, but in its default mode under DOS they are used to give 256 glyphs from one of the IBM PC code pages (Code page 437 by default), 16 foreground colors, eight background colors, and a flash option. Such art can be loaded into screen memory directly. ANSI.SYS, if loaded, also allows such art to be placed on screen by outputting escape sequences that indicate movements of the screen cursor and color/flash changes. If this method is used then the art becomes known as ANSI art. The IBM PC code pages also include characters intended for simple drawing which often made this art appear much cleaner than that made with more traditional character sets. Plain text files are also seen with these characters, though they have become far less common since Windows GUI text editors (using the Windows ANSI code page) have largely replaced DOS based ones.
Shift_JIS.
A large character selection and the availability of fixed-width characters allow Japanese users to use Shift JIS as a text-based art on Japanese websites.
Special circumstances of Japan.
Japanese mainly refer to ASCII-art (AA) as Shift-JIS Art in Japan.
In this background,Independently generation author of the Japanese original 
in Japan, the ASCII-NET(People with disabilities of related Bulletin board system.) SIG Operator (シグオペ, "The Forum's Leaders") Mr.Yasushi Wakabayashi (若林泰志, わかばやしやすし)," the author eastern emoticons. The PC communications in June 20, 1986 00:28:26(JST) From the "face mark" was published 
Art by the derived has an eastern emoticon, has been recognized as a Character of actors, that a reason.
In other words, the "ASCII", does not refer to the American Standard Code, refers to ASCII-NET's art as ASCII Corporation.
"See List of common emoticons#Eastern".
Unicode.
Unicode would seem to offer the ultimate flexibility in producing text based art with its huge variety of characters. However, finding a suitable fixed-width font is likely to be difficult if a significant subset of Unicode is desired. (Modern UNIX-style operating systems do provide complete fixed-width Unicode fonts, e.g. for xterm. Windows has the Courier New font which includes characters like ┌╥─╨┐♥☺Ƹ̵̡Ӝ̵̨̄Ʒ) Also, the common practice of rendering Unicode with a mixture of variable width fonts is likely to make predictable display hard if more than a tiny subset of Unicode is used. ≽ʌⱷ҅ᴥⱷʌ≼ is an adequate representation of a cat's face in a font with varying character widths.
Control and combining characters.
The combining characters mechanism of Unicode provides considerable ways of customizing the style, even obfuscating the text (e.g. via an online generator like , which focuses on the ). Glitcher is one example of Unicode art, initiated in 2012: « "These symbols, intruding up and down, are made by combining lots of diacritical marks. It’s a kind of art. There’s quite a lot of artists who use the Internet or specific social networks as their canvas." » The corresponding creations are favored in web browsers (thanks to their always better ), as geekily stylized usernames for social networks. With a fair compatibility, and among different online tools, showcases various types of Unicode art, mainly for aesthetic purpose (Ɯıḳĭƥḙȡḯả Wîkipêȡıẚ Ẉǐḳîṗȅḍȉā Ẃįḵįṗẻḑìẵ Ẉĭḵɪṕḗdïą Ẇïƙỉpểɗĭà Ẅȉḱïṕȩđĩẵ etc.). Besides, the creations can be hand-crafted (by programming), or pasted from mobile applications (e.g. the category of tools on Android). The underlaying technique dates back to the old systems that incorporated control characters, though. E.g. the German composite codice_1 would be imitated on ZX Spectrum by codice_2 after backspace and codice_3. (Cf. below.)
Overprinting (surprint).
In the 1970s and early 1980s it was popular to produce a kind of text art that relied on overprinting. This could be produced either on a screen or on a printer by typing a character, backing up, and then typing another character, just as on a typewriter. This developed into sophisticated graphics in some cases, such as the PLATO system (c. 1973), where superscript and subscript allowed a wide variety of graphic effects. A common use were for emoticons, with WOBTAX and VICTORY both producing convincing smiley faces. Overprinting had previously been used on typewriters, but the low-resolution pixelation of characters on video terminals meant that overprinting here produced seamless pixel graphics, rather than visibly overstruck combinations of letters on paper.
Beyond pixel graphics, this was also used for printing photographs, as the overall darkness of a particular character space dependent on how many characters, as well as the choice of character, were printed in a particular place. Thanks to the increased granularity of tone, photographs were often converted to this type of printout. Even manual typewriters or daisy wheel printers could be used. The technique has fallen from popularity since all cheap printers can easily print photographs, and a normal text file (or an e-mail message or Usenet posting) cannot represent overprinted text. However, something similar has emerged to replace it: shaded or colored ASCII art, using ANSI video terminal markup or color codes (such as those found in HTML, IRC, and many internet message boards) to add a bit more tone variation. In this way, it is possible to create ASCII art where the characters only differ in color.
Creation.
ASCII art text editors are used to create ASCII art from scratch, or to edit existing ASCII art files.
ASCII art may be created from an existing digital image using an ASCII art converter, an online tool or a software application that automatically converts an image into ASCII art, using vector quantization. Typically, this is done by sampling the image down to grayscale with less than 8-bit precision, so that each value corresponds to different ASCII character.
Further reading.
</dl>

</doc>
<doc id="1887" url="http://en.wikipedia.org/wiki?curid=1887" title="Alexius">
Alexius

Alexius is the Latinized form of the given name Alexios (Greek: Αλέξιος, polytonic Ἀλέξιος, "defender", cf. Alexander), especially common in the later Byzantine Empire. Variants include Alexis with the Russian Aleksey and its Ukrainian counterpart Oleksa/Oleksiy deriving from this form. The female form is Alexia (Greek: Αλεξία) and its variants such as Alessia. 
It may refer to:

</doc>
<doc id="1890" url="http://en.wikipedia.org/wiki?curid=1890" title="American English">
American English

American English, or U.S. English, is a set of dialects of the English language used mostly in the United States. Approximately two-thirds of the world's native speakers of English live in the U.S. The variety of American English that is considered by many speakers to be the most free from regional, ethnic, or cultural distinctions is the dialect known as General American.
English is the most widely spoken language in the United States. English is the common language used by the federal government and is considered the "de facto" language of the country because of its widespread use. English has been given official status by 30 of the 50 state governments. As an example, while both Spanish and English have equivalent status in the local courts of the Commonwealth of Puerto Rico, under federal law, English is the official language for any matters being referred to the United States District Court for the territory.
The use of English in the United States is a result of British colonization. The first wave of English-speaking settlers arrived in North America during the 17th century, followed by further migrations in the 18th and 19th centuries. Since then, American English has been influenced by the languages of West Africa, the Native American population, German, Irish, Spanish, and other languages of successive waves of immigrants to the U.S.
Phonology.
Compared with English as spoken in England, North American English is more homogeneous. Some distinctive accents can be found on the East Coast (for example, in eastern New England, New York City, Philadelphia, and Baltimore) partly because these areas were in close contact with England and imitated prestigious varieties of English at a time when these were undergoing changes. In addition, many speech communities on the East Coast have existed in their present locations for centuries, while the interior of the country was settled by people from all regions of the existing United States and developed a far more general linguistic pattern.
Studies on historical usage of English in the United States and the United Kingdom suggest that spoken American English did not simply evolve from period British English, but rather retained many archaic features contemporary British English has since lost. Most North American speech is rhotic, because in the 17th century, when English was brought to the Americas, most English in England was rhotic. Rhoticity has been further supported by the influences of Hiberno-English, West Country English and Scottish English. In most varieties of North American English, the sound corresponding to the letter "r" is a postalveolar approximant [ɹ̠] or retroflex approximant [ɻ] rather than a trill or a tap (as often heard, for example, in Scotland or India). A unique "bunched tongue" variant of the approximant "r" sound is also associated with the United States, and seems particularly noticeable in the Midwest and South. The loss of syllable-final "r" in North America, known as non-rhoticity, is confined mostly to the accents of eastern New England, New York City and surrounding areas and the coastal portions of the South, and African American Vernacular English.
In rural tidewater Virginia and eastern New England, 'r' is non-rhotic in accented (such as "bird", "work", "first", "birthday") as well as unaccented syllables, although this is declining among the younger generation of speakers. Dropping of syllable-final "r" sometimes happens in natively rhotic dialects if "r" is located in unaccented syllables or words and the next syllable or word begins in a consonant (for example, many North Americans drop the first 'r' in "particular"). In England, the lost "r" was often changed into [ə] (schwa), giving rise to a new class of falling diphthongs. Furthermore, the "er" sound of "fur or "butter, is realized in AmE as a monophthongal r-colored vowel (stressed [ɝ] or unstressed [ɚ] as represented in the IPA). This does not happen in the non-rhotic varieties of North American speech.
Some other English changes in which most North American dialects do not participate:
On the other hand, North American English has undergone some sound changes not found in other varieties of English speech:
Some mergers found in most varieties of both American and British English include:
Vocabulary.
North America has given the English lexicon many thousands of words, meanings, and phrases. Several thousand are now used in English as spoken internationally.
Creation of an American lexicon.
The process of coining new lexical items started as soon as the colonists began borrowing names for unfamiliar flora, fauna, and topography from the Native American languages. Examples of such names are "opossum, raccoon, squash" and "moose" (from Algonquian). Other Native American loanwords, such as "wigwam" or "moccasin", describe articles in common use among Native Americans. The languages of the other colonizing nations also added to the American vocabulary; for instance, "cookie", "cruller", "stoop", and "pit" (of a fruit) from Dutch; "angst, kindergarten, sauerkraut" from German, "levee, portage" ("carrying of boats or goods") and (probably) "gopher" from French; "barbecue, stevedore, and rodeo" from Spanish.
Among the earliest and most notable regular "English" additions to the American vocabulary, dating from the early days of colonization through the early 19th century, are terms describing the features of the North American landscape; for instance, "run, branch, fork, snag, bluff, gulch, neck" (of the woods), "barrens, bottomland, notch, knob, riffle, rapids, watergap, cutoff, trail, timberline" and "divide". Already existing words such as "creek, slough, sleet" and (in later use) "watershed" received new meanings that were unknown in England.
Other noteworthy American toponyms are found among loanwords; for example, "prairie, butte" (French); "bayou" (Choctaw via Louisiana French); "coulee" (Canadian French, but used also in Louisiana with a different meaning); "canyon, mesa, arroyo" (Spanish); "vlei, skate, kill" (Dutch, Hudson Valley).
The word "corn", used in England to refer to wheat (or any cereal), came to denote the plant "Zea mays", the most important crop in the U.S., originally named "Indian corn" by the earliest settlers; wheat, rye, barley, oats, etc. came to be collectively referred to as "grain". Other notable farm related vocabulary additions were the new meanings assumed by "barn" (not only a building for hay and grain storage, but also for housing livestock) and "team" (not just the horses, but also the vehicle along with them), as well as, in various periods, the terms "range, (corn) crib, truck, elevator, sharecropping" and "feedlot."
"Ranch," later applied to a house style, derives from Mexican Spanish; most Spanish contributions came after the War of 1812, with the opening of the West. Among these are, other than toponyms, "chaps" (from "chaparreras), plaza, lasso, bronco, , rodeo;" examples of "English" additions from the cowboy era are "bad man, maverick, chuck" ("food") and "Boot Hill;" from the California Gold Rush came such idioms as "hit pay dirt" or "strike it rich." The word "blizzard" probably originated in the West. A couple of notable late 18th century additions are the verb "belittle" and the noun "bid," both first used in writing by Thomas Jefferson.
With the new continent developed new forms of dwelling, and hence a large inventory of words designating real estate concepts "(land office, lot, outlands, waterfront," the verbs "locate" and "relocate, betterment, addition, subdivision)," types of property "(log cabin, adobe" in the 18th century; "frame house, apartment, tenement house, shack, " in the 19th century; "project, condominium, townhouse, split-level, mobile home, multi-family" in the 20th century), and parts thereof "(driveway, breezeway, backyard, dooryard; clapboard, siding, trim, baseboard; stoop" (from Dutch), "family room, den;" and, in recent years, "HVAC, central air, walkout basement)."
Ever since the American Revolution, a great number of terms connected with the U.S. political institutions have entered the language; examples are "run (i.e, for office), gubernatorial, primary election, carpetbagger" (after the Civil War), "repeater", "lame duck" (a British term used originally in Banking) and "pork barrel." Some of these are internationally used (for example, "caucus, gerrymander, filibuster, exit poll)."
19th century onwards.
The development of industry and material innovations throughout the 19th and 20th centuries were the source of a massive stock of distinctive new words, phrases and idioms. Typical examples are the vocabulary of "railroading" (see further at rail terminology) and "transportation" terminology, ranging from names of roads (from "dirt roads" and "back roads" to "freeways" and "parkways)" to road infrastructure "(parking lot, overpass, rest area)," and from automotive terminology to "public transit" (for example, in the sentence ""riding" the "subway downtown""); such American introductions as "commuter" (from "commutation ticket), concourse, to board" (a vehicle), "to park, double-park" and "parallel park" (a car), "" or the noun "terminal" have long been used in all dialects of English.
Trades of various kinds have endowed (American) English with household words describing jobs and occupations "(bartender, longshoreman, patrolman, hobo, bouncer, bellhop, roustabout, white collar, blue collar, employee, boss" [from Dutch], "intern, busboy, mortician, senior citizen)," businesses and workplaces "(department store, supermarket, thrift store, gift shop, drugstore, motel, main street, gas station, hardware store, savings and loan, hock" [also from Dutch]), as well as general concepts and innovations "(automated teller machine, smart card, cash register, dishwasher, reservation" [as at hotels], "pay envelope, movie, mileage, shortage, outage, blood bank)."
Already existing English words—such as "store, shop, dry goods, haberdashery, lumber"—underwent shifts in meaning; some—such as "mason, student, clerk", the verbs "can" (as in "canned goods"), "ship, fix, carry, enroll" (as in school), "run" (as in "run a business"), "release" and "haul"—were given new significations, while others (such as "tradesman)" have retained meanings that disappeared in England. From the world of business and finance came "breakeven, merger, , downsize, disintermediation, bottom line;" from sports terminology came, jargon aside, "Monday-morning quarterback, cheap shot, game plan" (football); "in the ballpark, out of left field, off base, hit and run," and many other idioms from baseball; gamblers coined "bluff, , ante, bottom dollar, raw deal, pass the buck, ace in the hole, freeze-out, showdown;" miners coined "bedrock, bonanza, peter out, pan out" and the verb "prospect" from the noun; and railroadmen are to be credited with "make the grade, sidetrack, head-on," and the verb "railroad." A number of Americanisms describing material innovations remained largely confined to North America: "elevator, ground, gasoline;" many automotive terms fall in this category, although many do not "(hatchback, sport utility vehicle, station wagon, tailgate, motorhome, truck, pickup truck, to exhaust)."
In addition to the above-mentioned loans from French, Spanish, Mexican Spanish, Dutch, and Native American languages, other accretions from foreign languages came with 19th and early 20th century immigration; notably, from Yiddish "(chutzpah, schmooze, tush") and German—"hamburger" and culinary terms like "frankfurter/franks, liverwurst, sauerkraut, wiener, deli(catessen); scram, kindergarten, gesundheit;" musical terminology "(whole note, half note," etc.); and apparently "cookbook, fresh" ("impudent") and "what gives?" Such constructions as "Are you coming with?" and "I like to dance" (for "I like dancing") may also be the result of German or Yiddish influence.
Finally, a large number of English colloquialisms from various periods are American in origin; some have lost their American flavor (from "OK" and "cool" to "nerd" and "24/7)," while others have not "(have a nice day, for sure);" many are now distinctly old-fashioned "(swell, groovy)." Some English words now in general use, such as "hijacking, disc jockey, boost, bulldoze" and "jazz," originated as American slang. Among the many English idioms of U.S. origin are "get the hang of, bark up the wrong tree, keep tabs, run scared, take a backseat, have an edge over, stake a claim, take a shine to, in on the ground floor, bite off more than one can chew, off/on the wagon, stay put, inside track, stiff upper lip, bad hair day, throw a monkey wrench, under the weather, jump bail, come clean, come again?, it ain't over till it's over, what goes around comes around," and "will the real x please stand up?"
Morphology.
American English has always shown a marked tendency to use nouns as verbs. Examples of verbed nouns are "interview, advocate, vacuum, lobby, pressure, rear-end, transition, feature, profile, spearhead, skyrocket, showcase, service" (as a car), "corner, torch, exit" (as in "exit the lobby"), "factor" (in mathematics), "gun" ("shoot"), "author" (which disappeared in English around 1630 and was revived in the U.S. three centuries later) and, out of American material, "proposition, graft" (bribery), "bad-mouth, vacation, major, backpack, backtrack, intern, ticket" (traffic violations), "hassle, blacktop, peer-review, dope" and "OD", and, of course "verbed" as used at the start of this sentence.
Compounds coined in the U.S. are for instance "foothill, flatlands, badlands, landslide" (in all senses), "overview" (the noun), ", teenager, brainstorm, , hitchhike, smalltime, , frontman, lowbrow" and "highbrow, hell-bent, foolproof, nitpick, about-face" (later verbed), "upfront" (in all senses), "fixer-upper, no-show;" many of these are phrases used as adverbs or (often) hyphenated attributive adjectives: "non-profit, for-profit, free-for-all, ready-to-wear, catchall, low-down, down-and-out, down and dirty, in-your-face, nip and tuck;" many compound nouns and adjectives are open: "happy hour, fall guy, capital gain, road trip, wheat pit, head start, plea bargain;" some of these are colorful "(empty nester, loan shark, , buzz saw, ghetto blaster, dust bunny)," others are euphemistic "(differently abled (physically challenged), human resources, affirmative action, correctional facility)."
Many compound nouns have the form verb plus preposition: ", stopover, lineup, , tryout, spin-off, rundown" ("summary"), "shootout, holdup, hideout, comeback, cookout, kickback, makeover, takeover, rollback" ("decrease"), "rip-off, come-on, shoo-in, fix-up, tie-in, tie-up" ("stoppage"), "stand-in." These essentially are nouned phrasal verbs; some prepositional and phrasal verbs are in fact of American origin "(spell out, figure out, hold up, brace up, size up, rope in, back up/off/down/out, step down, miss out, kick around, cash in, rain out, check in" and "check out" (in all senses), "fill in" ("inform"), "kick in" or "throw in" ("contribute"), "square off, sock in, sock away, factor in/out, come down with, give up on, lay off" (from employment), "run into" and "across" ("meet"), "stop by, pass up, put up" (money), "set up" ("frame"), "trade in, pick up on, pick up after, lose out)."
Noun endings such as "-ee (retiree), -ery (bakery), -ster (gangster)" and "-cian (beautician)" are also particularly productive. Some verbs ending in "-ize" are of U.S. origin; for example, "fetishize, prioritize, burglarize, accessorize, itemize, editorialize, customize, notarize, weatherize, winterize, Mirandize;" and so are some back-formations "(locate, fine-tune, evolute, curate, donate, emote, upholster, peeve" and "enthuse)." Among syntactical constructions that arose in the U.S. are "as of" (with dates and times), "outside of, headed for, meet up with, back of, convince someone to, not about to" and "lack for."
Americanisms formed by alteration of some existing words include notably "pesky, phony, rambunctious, pry" (as in "pry open", from "prize), putter" (verb), "buddy, sundae, skeeter, sashay" and "kitty-corner." Adjectives that arose in the U.S. are for example, "lengthy, bossy, cute" and "cutesy, grounded" (of a child), "punk" (in all senses), "sticky" (of the weather), "through" (as in "through train", or meaning "finished"), and many colloquial forms such as "peppy" or "wacky". American blends include "motel, guesstimate, infomercial" and "televangelist."
English words that survived in the United States and not in the United Kingdom.
A number of words and meanings that originated in Middle English or Early Modern English and that have been in everyday use in the United States dropped out in most varieties of British English; some of these have cognates in Lowland Scots. Terms such as "fall" ("autumn"), "faucet" ("tap"), "diaper" ("nappy"), "candy" ("sweets"), "skillet", "eyeglasses" and "obligate" are often regarded as Americanisms. "Fall" for example came to denote the season in 16th century England, a contraction of Middle English expressions like "fall of the leaf" and "fall of the year".
During the 17th century, English immigration to the British colonies in North America was at its peak and the new settlers took the English language with them. While the term "fall" gradually became obsolete in Britain, it became the more common term in North America. "Gotten" (past participle of "get") is often considered to be an Americanism, although there are some areas of Britain, such as Lancashire and North East England, that still continue to use it and sometimes also use "putten" as the past participle for "put" (which is not done by most speakers of American English).
Other words and meanings, to various extents, were brought back to Britain, especially in the second half of the 20th century; these include "hire" ("to employ"), "quit" ("to stop", which spawned "quitter" in the U.S.), "I guess" (famously criticized by H. W. Fowler), "baggage", "hit" (a place), and the adverbs "overly" and "presently" ("currently"). Some of these, for example "monkey wrench" and "wastebasket", originated in 19th century Britain.
The mandative subjunctive (as in "the City Attorney suggested that the case "not be closed"") is livelier in American English than it is in British English. It appears in some areas as a spoken usage and is considered obligatory in contexts that are more formal. The adjectives "mad" meaning "angry", "smart" meaning "intelligent", and "sick" meaning "ill" are also more frequent in American (these meanings are also frequent in Hiberno-English) than British English.
Regionally distinct English words within the United States.
Linguist Bert Vaux created a survey polling English speakers across the United States about the specific words they use for a variety of given definitions. This 2003 study concludes that:
Regional differences.
While written American English is (in general) standardized across the country, there are several recognizable variations in the spoken language, both in pronunciation and in vernacular vocabulary. "General American" is the name given to any American accent that is often considered relatively free of noticeable regional influences.
Eastern seaboard.
After the Civil War, the settlement of the western territories by migrants from the Eastern U.S. led to dialect mixing and leveling, (koineization) so that regional dialects are most strongly differentiated along the Eastern seaboard. The Connecticut River and Long Island Sound is usually regarded as the southern/western extent of New England speech, which has its roots in the speech of the Puritans from East Anglia who settled in the Massachusetts Bay Colony.
The Potomac River generally divides a group of Northern coastal dialects from the beginning of the Coastal Southern dialect area; in between these two rivers several local variations exist, chief among them the one that prevails in and around New York City and northern New Jersey, which developed on a Dutch substratum after the English conquered New Amsterdam. The main features of Coastal Southern speech can be traced to the speech of the English from the West Country who settled in Virginia after leaving England at the time of the English Civil War.
Midwest.
A distinctive speech pattern also appears near the border between Canada and the United States, centered on the Great Lakes region (but only on the American side). This is the Inland North Dialect—the "standard Midwestern" speech that was the basis for General American in the mid-20th century (although it has been recently modified by the northern cities vowel shift). Those not from this area frequently confuse it with the North Midland dialect treated below, referring to both collectively as "Midwestern" in the Mid-Atlantic region or "Northern" in the Southern US. The so-called '"Minnesotan" dialect is also prevalent in the cultural Upper Midwest, and is characterized by influences from the German and Scandinavian settlers of the region (like "yah" for yes, pronounced similarly to "ja" in German, Norwegian and Swedish). In parts of Pennsylvania and Ohio, another dialect known as Pennsylvania Dutch English is also spoken.
Interior.
In the interior, the situation is very different. West of the Appalachian Mountains begins the broad zone of what is generally called "Midland" speech. This is divided into two discrete subdivisions, the North Midland that begins north of the Ohio River valley area, and the South Midland speech; sometimes the former is designated simply "Midland" and the latter is reckoned as "Highland Southern". The North Midland speech continues to expand westward until it becomes the closely related Western dialect which contains Pacific Northwest English as well as California English, although in the immediate San Francisco area some older speakers do not possess the cot–caught merger and thus retain the distinction between words such as cot and caught which reflects a historical Mid-Atlantic heritage.
The South Midland or Highland Southern dialect follows the Ohio River in a generally southwesterly direction, moves across Arkansas and Oklahoma west of the Mississippi, and peters out in West Texas. It is a version of the Midland speech that has assimilated some coastal Southern forms (outsiders often mistakenly believe South Midland speech and coastal South speech to be the same).
Although no longer region-specific, African American Vernacular English, which remains prevalent among African Americans, has a close relationship to Southern varieties of American English and has greatly influenced everyday speech of many Americans.
The island state of Hawaii has a distinctive Hawaiian Pidgin.
Finally, dialect development in the United States has been notably influenced by the distinctive speech of such important cultural centers as Baltimore, Boston, Buffalo, Charleston, Cleveland, Chicago, Detroit, Miami, New Orleans, New York City, Philadelphia and Pittsburgh, which imposed their marks on the surrounding areas.
Differences between British and American English.
American English and British English (BrE) often differ at the levels of phonology, phonetics, vocabulary, and, to a much lesser extent, grammar and orthography.
The first large American dictionary, "An American Dictionary of the English Language", was written by Noah Webster in 1828, codifying several of these spellings.
Differences in grammar are relatively minor, and normally do not affect mutual intelligibility; these include: different use of some verbal auxiliaries; formal (rather than notional) agreement with collective nouns; different preferences for the past forms of a few verbs (for example, AmE/BrE: "learned"/"learnt", "burned"/"burnt", "snuck/sneaked", "dove/dived") although the purportedly "British" forms can occasionally be seen in American English writing as well; different prepositions and adverbs in certain contexts (for example, AmE "in school," BrE "at school"); and whether or not a definite article is used, in very few cases (AmE "to the hospital", BrE "to hospital"; contrast, however, AmE "actress Elizabeth Taylor", BrE "the actress Elizabeth Taylor"). Often, these differences are a matter of relative preferences rather than absolute rules; and most are not stable, since the two varieties are constantly influencing each other, and American English is not a standardized set of dialects.
Differences in orthography are also minor. The main differences are that American English usually uses spellings such as "flavor" for British "flavour", "fiber" for "fibre", "defense" for "defence", "analyze" for "analyse", "catalog" for "catalogue" and "traveling" for "travelling". Noah Webster popularized such spellings in America, but he did not invent most of them. Rather, "he chose already existing options [...] on such grounds as simplicity, analogy or etymology". Other differences are due to the francophile tastes of 19th century Victorian England (for example they preferred "programme" for "program", "manoeuvre" for "maneuver", "cheque" for "check", etc.). AmE almost always uses "-ize" in words like "realize". BrE prefers "-ise", but also uses "-ize" (see Oxford spelling).
There are a few differences in punctuation rules. British English is more tolerant of run-on sentences, called "comma splices" in American English, and American English requires that periods and commas be placed inside closing quotation marks even in cases in which British rules would place them outside. American English also favors the double quotation mark over single.
AmE sometimes favors words that are morphologically more complex, whereas BrE uses clipped forms, such as AmE "transportation" and BrE "transport" or where the British form is a back-formation, such as AmE "burglarize" and BrE "burgle" (from "burglar"). However, while individuals usually use one or the other, both forms will be widely understood and mostly used alongside each other within the two systems.

</doc>
<doc id="1893" url="http://en.wikipedia.org/wiki?curid=1893" title="Albert Spalding">
Albert Spalding

Albert Goodwill Spalding (September 2, 1849 – September 9, 1915) was an American pitcher, manager and executive in the early years of professional baseball, and the co-founder of A.G. Spalding sporting goods company. He played major league baseball between 1871 and 1878. In 1877, he became the first well-known player to use a fielding glove; such gloves were among the items sold at his sporting goods store.
After his retirement as a player, Spalding remained active with the Chicago White Stockings as president and part-owner. In the 1880s, he took players on the first world tour of baseball. With William Hulbert, Spalding organized the National League. He later called for the commission that investigated the origins of baseball and credited Abner Doubleday with creating the game. He also wrote the first set of official baseball rules.
Baseball career.
Player.
Having played baseball throughout his youth, Spalding first played competitively with the Rockford Pioneers, a youth team, which he joined in 1865. After pitching his team to a 26–2 victory over a local men's amateur team (the Mercantiles), he was approached at the age of 15 by another, the Forest Citys, for whom he played for two years. In the autumn of 1867 he accepted a $40 per week contract, nominally as a clerk, but really to play professionally for the Chicago Excelsiors, not an uncommon arrangement used to circumvent the rules of the time, which forbade the hiring of professional players. Following the formation of baseball's first professional organization, the National Association of Professional Base Ball Players (which became known as the National Association, the Association, or NA) in 1871, Spalding joined the Boston Red Stockings (precursor club to the modern Atlanta Braves) and was highly successful; winning 206 games (and losing only 53) as a pitcher and batting .323 as a hitter.
William Hulbert, principal owner of the Chicago White Stockings, did not like the loose organization of the National Association and the gambling element that influenced it, so he decided to create a new organization, which he dubbed the National League of Baseball Clubs. To aid him in this venture, Hulbert enlisted the help of Spalding. Playing to the pitcher's desire to return to his Midwestern roots and challenging Spalding's integrity, Hulbert convinced Spalding to sign a contract to play for the White Stockings (now known as the Chicago Cubs) in 1876. Spalding then coaxed teammates Deacon White, Ross Barnes and Cal McVey, as well as Philadelphia Athletics players Cap Anson and Bob Addy, to sign with Chicago. This was all done under complete secrecy during the playing season because players were all free agents in those days and they did not want their current club and especially the fans to know they were leaving to play elsewhere the next year. News of the signings by the Boston and Philadelphia players leaked to the press before the season ended and all of them faced verbal abuse and physical threats from the fans of those cities.
He was "the premier pitcher of the 1870s", leading the league in victories for each of his six full seasons as a professional. During each of those years he was his team's only pitcher. In 1876, Spalding won 47 games as the prime pitcher for the White Stockings and led them to win the first-ever National League pennant by a wide margin.
In 1877, Spalding began to use a glove to protect his catching hand. People had used gloves previously, but never had a star like Spalding used one. Spalding had an ulterior motive for doing so: he now owned a sporting goods store which sold baseball gloves and wearing one himself was good advertising for his business.
Spalding retired from playing baseball in 1878 at the age of 27, although he continued as president and part owner of the White Stockings and a major influence on the National League. Spalding's .796 career winning percentage (from an era when teams played about once or twice a week) is the highest ever achieved by a baseball pitcher.
Organizer and executive.
In the months after signing for Chicago, Hulbert and Spalding organized the National League by enlisting the four major teams in the East and the three other top teams in what was then considered to be the West. Joining Chicago initially were the leading teams from Cincinnati, Louisville, and St. Louis. The owners of these western clubs accompanied Hulbert and Spalding to New York where they secretly met with owners from New York, Philadelphia, Hartford, and Boston. Each signed the league's constitution, and the National League was officially born. "Spalding was thus involved in the transformation of baseball from a game of gentlemen athletes into a business and a professional sport." Although the National Association held on for a few more seasons, it was no longer recognized as the premier organization for professional baseball. Gradually, it faded out of existence and was replaced by myriad minor leagues and associations around the country.
In 1905, after Henry Chadwick wrote an article saying that baseball grew from the British sports of cricket and rounders, Spalding called for a commission to find out the real source of baseball. The commission called for citizens who knew anything about the founding of baseball to send in letters. After three years of searching, on December 30, 1907, Spalding received a letter that (erroneously) declared baseball to be the invention of Abner Doubleday. The commission, though, was biased, as Spalding would not appoint anyone to the commission if they believed the sport was somewhat related to the English sport of rounders. Just before the commission, in a letter to sportswriter Tim Murnane, Spalding noted, "Our good old American game of baseball must have an American Dad." The project, later called the Mills Commission, concluded that "Base Ball had its origins in the United States" and "the first scheme for playing baseball, according to the best evidence available to date, was devised by Abner Doubleday at Cooperstown, N.Y., in 1839."
Receiving the archives of Henry Chadwick in 1908, Spalding combined these records with his own memories (and biases) to write "America's National Game" (published 1911) which, despite its flaws, was probably the first scholarly account of the history of baseball.
Businessman.
In 1874 while Spalding was playing and organizing the league, Spalding and his brother Walter began a sporting goods store in Chicago, which grew rapidly (14 stores by 1901) and expanded into a manufacturer and distributor of all kinds of sporting equipment. The company became "synonymous with sporting goods" and is still a going concern.
Spalding published the first official rules guide for baseball. In it he stated that only Spalding balls could be used (previously, the quality of the balls used had been subpar). Spalding also founded the "Baseball Guide," which at the time was the most widely read baseball publication.
In 1888–1889, Spalding took a group of major league players around the world to promote baseball and Spalding sporting goods. This was the first-ever world baseball tour. Playing across the western U.S., the tour made stops in Hawaii (although no game was played), New Zealand, Australia, Ceylon, Egypt, Italy, France, and England. The tour returned to grand receptions in New York, Philadelphia, and Chicago. The tour included future Hall of Famers Cap Anson and John Montgomery Ward. While the players were on the tour, the National League instituted new rules regarding player pay that led to a revolt of players, led by Ward, who started the Players League the following season (1890). The league lasted one year, partially due to the anti-competitive tactics of Spalding to limit its success. The tour and formation of the Player's League is depicted in the 2015 movie "Deadball."
In 1900 Spalding was appointed by President McKinley as the USA's Commissioner at that year's Summer Olympic Games.
Other activities.
Spalding had been a prominent member of the Theosophical Society under William Quan Judge. In 1900, Spalding moved to San Diego with his newly acquired second wife, Elizabeth and became a prominent member and supporter of the Theosophical community Lomaland, which was being developed on Point Loma by Katherine Tingley. He built an estate in the Sunset Cliffs area of Point Loma where he lived with Elizabeth for the rest of his life. The Spaldings raised race horses and collected Chinese fine furniture and art.
The Spaldings had an extensive library which included many volumes on Theosophy, art, and literature. In 1907-1909 he was the driving force behind the development of a paved road, known as the "Point Loma boulevard", from downtown San Diego to Point Loma and Ocean Beach; the road also provided good access to Lomaland. It later provided the basis for California State Route 209. He proposed the project, supervised it on behalf of the city, and paid a portion of the cost out of his own pocket. He joined with George Marston and other civic-minded businessmen to purchase the site of the original Presidio of San Diego, which they developed as a historic park and eventually donated to the city of San Diego. He ran unsuccessfully for the United States Senate in 1910. He helped to organize the 1915 Panama-California Exposition, serving as second vice-president.
Death.
He died on September 9, 1915 in San Diego, and his ashes were scattered at his request.
Legacy.
He was elected to the Baseball Hall of Fame by the Veterans Committee in 1939, as one of the first inductees from the 19th century at that summer's opening ceremonies. His plaque in the Hall of Fame reads "Albert Goodwill Spalding. Organizational genius of baseball's pioneer days. Star pitcher of Forest City Club in late 1860s, 4-year champion Bostons 1871-1875 and manager-pitcher of champion Chicagos in National League's first year. Chicago president for 10 years. Organizer of baseball's first round-the-world tour in 1888."
His nephew, also named Albert Spalding, was a renowned violinist.

</doc>
<doc id="1894" url="http://en.wikipedia.org/wiki?curid=1894" title="Africa Alphabet">
Africa Alphabet

The Africa Alphabet (also International African Alphabet or IAI alphabet) was developed in 1928 under the lead of Diedrich Westermann. He developed it with a group of Africanists at the International Institute of African Languages and Cultures (later the IAI) in London. Its aim was to enable people to write all the African languages for practical and scientific purposes without diacritics. It is based on the International Phonetic Alphabet with a few differences, such as "j" and "y", which instead have the same (consonant) sound values as in English.
This alphabet has influenced development of orthographies of many African languages (serving "as the basis for the transcription" of about 60, by one count), but not all, and discussions of harmonization of systems of transcription that led to, among other things, adoption of the African reference alphabet.
The African Alphabet was used, with the International Phonetic Alphabet, as a basis for the World Orthography.

</doc>
<doc id="1896" url="http://en.wikipedia.org/wiki?curid=1896" title="Acquire">
Acquire

Acquire is a board game designed by Sid Sackson. The game was originally published in 1962 by 3M as a part of their bookshelf games series. In most versions, the theme of the game is investing in hotel chains. In the 1990s Hasbro edition, the hotel chains were replaced by generic corporations, though the actual gameplay was unchanged. The game is currently published by Hasbro under the Avalon Hill brand, and the companies are once again hotel chains.
The object of the game is to earn the most money by developing and merging hotel chains. When a chain in which a player owns stock is acquired by a larger chain, players earn money based on the size of the acquired chain. At the end of the game, all players liquidate their stock in order to determine which player has the most money.
Components.
The components of the game have varied over the years. In particular, the tiles have been made from wood, plastic, and cardboard in various editions of the game. In the current 2008 version, the tiles are cardboard. The following components are included in all versions:
The array on the game board is arranged with lettered rows (A through I) and numbered columns (1 through 12). The 108 tiles correspond to each of the squares: 5E, 10B, and so forth.
Rules.
"Acquire" is a game for three to six players, though earlier editions included special rules for two players. Standard tournament games are played with four players.
Setup.
At the beginning of the game, each player receives $6000 in cash. Each player draws a tile and places it on the board. The player whose tile is in the topmost row (closest to row A) goes first. If more than one player selects a tile in that row, then the player whose tile is in the leftmost column (closest to 1) goes first. All players place these tiles on the board. Then, starting with the first player, each player draws six tiles.
Play of the game.
A turn consists of three steps:
Tile placement falls in one of four categories. The tile placed could be an orphan, adjacent to no other tile on the board. The tile could create a new chain of tiles, and the player who placed it on the board would have the opportunity to found a new chain. The tile could increase the length of an existing chain already on the board. Or the tile could link two chains, causing a merger of two or more chains. Since there are only seven hotel chains in the game, placing a tile that would create an eighth chain is not permitted.
When a player founds a chain, he receives one free share of stock in that chain. If, however, there are no shares left when the chain is founded, then the founding player does not receive the free share.
Chains are deemed "safe" if they have 11 or more links; placing a tile that would cause such a chain to be acquired by a larger chain is also not permitted.
After a player places a tile, and the results of that placement have been handled, he may purchase up to three shares of stock. A player may only purchase shares of stock in chains that have already been founded. The price of a share depends on the size of the chain, according to a chart that lists prices according to size. A player may purchase shares in one, two, or three existing chains (assuming at least three chains are currently in play), in any combination up to a total of three shares.
Finally, the player replaces the tile he played, ensuring that he has six tiles at the end of his turn.
Growing and merging chains.
A chain is a conglomeration of tiles that are linked to each other either horizontally or vertically but not diagonally. For example, adjacent to square 5F are squares 4F, 6F, 5E, and 5G, but not 6E or 4G. If there is a tile in 5F, then placing either tile 4F or 5G would result in founding a new hotel chain. A chain grows when a player increases the length of a chain. Suppose a chain consists of squares 8D, 8E, and 8F. Playing tile 9F would add to the length of the chain. Playing tile 9C would not.
Chains merge when a player places the tile that eliminates the empty space between them. Suppose there is a chain at 1A, 2A, 3A, and 4A, along with another chain at 6A and 7A. Placing tile 5A would cause these two chains to merge. When a merger occurs, the larger hotel chain always acquires the smaller hotel chain. That is, the hotel chain with more tiles will continue to exist and now grows to include the smaller hotel chain (after bonuses have been calculated according to the steps outlined below). If a tile is placed between two hotel chains of the same size, the individual player who places the tile decides which hotel chain remains on the board and which is acquired. In this situation, there are a number of strategic reasons why an individual player might select one hotel chain over another to be the one that remains on the board. However, often it is most advantageous for the player selecting to choose to let the more expensive chains remain on the board (and trade in their stock of the less expensive chain at the 2-to-1 ratio described below).
Mergers.
The merger is the mechanism by which the players compete. Mergers yield bonuses for the two shareholders who hold, respectively, the largest and second-largest interests in a chain. Mergers also give each player who holds any interest at all in a chain a chance to sell his stock or to trade it in for shares of the acquiring chain. A merger takes place in three steps:
If placing a tile causes three or four chains to merge, then the merger steps are handled between the largest and second-largest chain, then with the third-largest chain, and finally with the smallest chain.
Rules issues.
The rules allow any player to count the number of shares available in the bank. However, the rules do not specify whether a player should hold his shares of stock face up or face down. That is, the rules do not say whether one player may ask another how many shares of stock he or she owns in a particular chain. Whether this is public or private information should be agreed upon between players before the game begins.
The current rules do not provide for a two-player game. However, the stock market was used as a "third shareholder" in previous versions of the game. By this rule, a tile is drawn whenever a merger is declared. The number on the tile indicates how many shares the stock market owns in the company that is being acquired. The players must compete with the market as well as with each other in order to receive bonuses.
Ending the game.
Any player may declare the game over at any time during his turn if either of two conditions is true: one chain has 41 or more tiles, or there is at least one chain on the board and every chain on the board has 11 or more tiles. Upon declaring the game over, the player is allowed to complete his turn (including buying stock). Ending the game is optional - if he believes it is to his advantage not to end the game, he may refrain from doing so. Once the game ends, the minority and majority bonuses are paid to the minority and majority holders in each of the remaining chains; each player sells his or her shares of stock in each of the remaining chains; and the player with the most money wins. Because ending the game is optional, and a player may not realize that he can end it, it is unethical to say "good game", or in any other way indicate that the game could be ended, until after a player actually has ended the game.
Corner Cases.
These conditions can be used to handle unusual situations.
Awards.
The game was short-listed for the first Spiel des Jahres board game awards in 1979.
"GAMES" magazine has inducted "Acquire" into their buyers' guide Hall of Fame. The magazine's stated criteria for the Hall of Fame encompasses "games that have met or exceeded the highest standards of quality and play value and have been continuously in production for at least 10 years; i.e., classics."
It was inducted into the Academy of Adventure Gaming Arts & Design's Hall of Fame, along with Sackson, in 2011.
Online play.
"Acquire" is playable online through the GameTable Online site.
Acquire is also playable for free as NetAcquire. Various NetAcquire clients are available including Visual Basic (for Windows) by Kensit, JAVA (for all) by Tim Styer, and a Mac version by Nolan Waite. Details can be found at http://www.netacquire.ca.

</doc>
<doc id="1897" url="http://en.wikipedia.org/wiki?curid=1897" title="Australian English">
Australian English

Australian English (AusE, AuE, AusEng, en-AU) is a major variety of the English language and is used throughout Australia. Although English has no official status in the Constitution, Australian English is Australia's "de facto" official language and is the first language of the majority of the population.
Australian English began to diverge from British English after the founding of the colony of New South Wales in 1788 and was recognised as being different from British English by 1820. It arose from the intermingling of early settlers from a great variety of mutually intelligible dialectal regions of the British Isles and quickly developed into a distinct variety of English.
Australian English differs from other varieties of English in vocabulary, accent, pronunciation, register, grammar and spelling.
History.
The earliest form of Australian English was first spoken by the children of the colonists born into the colony of New South Wales. This first generation of children created a new dialect that was to become the language of the nation. The Australian-born children in the new colony were exposed to a wide range of dialects from all over the British Isles, in particular from Ireland and South East England.
The native-born children of the colony created the new dialect from the speech they heard around them, and with it expressed peer solidarity. Even when new settlers arrived, this new dialect was strong enough to blunt other patterns of speech.
A quarter of the convicts were Irish. Many had been arrested in Ireland, and some in Great Britain. Many, if not most, of the Irish convicts spoke either no English at all, or spoke it poorly and rarely. There were other significant populations of convicts from non-English speaking part of Britain, such as the Scottish Highlands and Wales.
Records from the early 19th century show the distinct dialect that had surfaced in the colonies since first settlement in 1788, with Peter Miller Cunningham's 1827 book "Two Years in New South Wales", describing the distinctive accent and vocabulary of the native-born colonists, different from that of their parents and with a strong London influence. Anthony Burgess writes that "Australian English may be thought of as a kind of fossilised Cockney of the Dickensian era." 
The first of the Australian gold rushes, in the 1850s, began a large wave of immigration, during which about two per cent of the population of the United Kingdom emigrated to the colonies of New South Wales and Victoria. According to linguist Bruce Moore, "the major input of the various sounds that went into constructing the Australian accent was from south-east England".
Some elements of Aboriginal languages have been adopted by Australian English—mainly as names for places, flora and fauna (for example dingo) and local culture. Many such are localised, and do not form part of general Australian use, while others, such as "kangaroo", "boomerang", "budgerigar", "wallaby" and so on have become international. Other examples are "cooee" and "hard yakka". The former is used as a high-pitched call, for attracting attention, (pronounced /kʉː.iː/) which travels long distances. "Cooee" is also a notional distance: "if he's within cooee, we'll spot him". "Hard yakka" means "hard work" and is derived from "yakka", from the Jagera/Yagara language once spoken in the Brisbane region.
Also of Aboriginal origin is the word "bung", from the Sydney pidgin English (and ultimately from the Sydney Aboriginal language), meaning "dead", with some extension to "broken" or "useless". Many towns or suburbs of Australia have also been influenced or named after Aboriginal words. The best-known example is the capital, Canberra, named after a local language word meaning "meeting place".
Among the changes starting in the 19th century was the introduction of words, spellings, terms and usages from North American English. The words imported included some later considered to be typically Australian, such as "bushwhacker" and "squatter".
This American influence continued with the popularity of American films and the influx of American military personnel in World War II; seen in the enduring persistence of such terms as "okay", "you guys" and "gee".
Phonology and pronunciation.
The primary way in which Australian English is distinctive from other varieties of English is through its unique pronunciation. It shares most similarity with other Southern Hemisphere accents, in particular New Zealand English. Like most dialects of English it is distinguished primarily by its vowel phonology.
Vowels.
The vowels of Australian English can be divided according to length. The long vowels, which include monophthongs and diphthongs, mostly correspond to the tense vowels used in analyses of Received Pronunciation (RP) as well as its centring diphthongs. The short vowels, consisting only of monophthongs, correspond to the RP lax vowels. There exist pairs of long and short vowels with overlapping vowel quality giving Australian English phonemic length distinction, which is unusual amongst the various dialects of English, though not unknown elsewhere, such as in regional south-eastern dialects of the UK and eastern seaboard dialects in the US. As with General American and New Zealand English, the weak-vowel merger is complete in Australian English: unstressed /ɪ/ (sometimes written as /ɨ/ or /ᵻ/) is merged into /ə/ (schwa), unless it is followed by a velar consonant.
Consonants.
There is little variation with respect to the sets of consonants used in various English dialects. There are, however, variations in how these consonants are used. Australian English is no exception.
Australian English is non-rhotic; in other words, the /r/ sound does not appear at the end of a syllable or immediately before a consonant. However, a linking /r/ can occur when a word that has a final <r> in the spelling comes before another word that starts with a vowel. An intrusive /r/ may similarly be inserted before a vowel in words that do not have <r> in the spelling in certain environments, namely after the long vowel /oː/ and after word final /ə/.
There is some degree of allophonic variation in the alveolar stops. As with North American English, Intervocalic alveolar flapping is a feature of Australian English: prevocalic /t/ and /d/ surface as the alveolar tap [ɾ] after sonorants other than /ŋ/, /m/as well as at the end of a word or morpheme before any vowel in the same breath group. For many speakers, /t/ and /d/ in the combinations /tr/-/tw/ and /dr/-/dw/ are also palatalised, thus /tʃr/-/tʃw/ and /dʒr/-/dʒw/, as Australian /r/ is only very slightly retroflex, the tip remaining below the level of the bottom teeth in the same position as for /w/; it is also somewhat rounded ("to say 'r' the way Australians do you need to say 'w' at the same time"), where older English /wr/ and /r/ have fallen together as /ʷr/. The wine–whine merger is complete in Australian English.
"Yod"-dropping occurs after /r/, /l/, /s/, /z/, /θ/, /tʃ/, /dʒ/, /j/ and /ɹ/, . Other cases of /sj/ and /zj/, along with /tj/ and /dj/, have coalesced to /ʃ/, /ʒ/, /tʃ/ and /dʒ/ respectively for many speakers. /j/ is generally retained in other consonant clusters.
In common with American English and Scottish English, the phoneme /l/ is pronounced as a "dark" (velarised) L in all positions, unlike other dialects such as English English and Hiberno (Irish) English, where a light (palatised) L is used in many positions.
Pronunciation.
Differences in stress, weak forms and standard pronunciation of isolated words occur between Australian English and other forms of English, which while noticeable do not impair intelligibility.
The affixes "-ary", "-ery", "-ory", "-bury", "-berry" and "-mony" (seen in words such as "necessary, mulberry" and "matrimony") can be pronounced either with a full vowel or a schwa. Although some words like "necessary" are almost universally pronounced with the full vowel, older generations of Australians are relatively likely to pronounce these affixes with a schwa while younger generations are relatively likely to use a full vowel.
Words ending in unstressed "-ile" derived from Latin adjectives ending in "-ilis" are pronounced with a full vowel (/ɑel/), so that "fertile" rhymes with "fur tile" rather than "turtle".
In addition, miscellaneous pronunciation differences exist when compared with other varieties of English in relation to seemingly random words. For example, as with American English, the vowel in "yoghurt" is pronounced as /əʉ/ ("long 'O'") rather than /ɔ/ ("short o"), and "vitamin" is pronounced with /ɑe/ ("long 'I'") in the first syllable, rather than /ɪ/ ("short 'I'"). As with British English, "advertisement" is pronounced with /ɪ/, and "buoy" is pronounced as /bɔɪ/ rather than /buːi/. Two examples of miscellaneous pronunciations which contrast with both standard American and British usages are "data", which is pronounced with /ɑː/ ("dah") as opposed to /eɪ/ ("day"); and "maroon", pronounced with /oʊ/ ("own") as opposed to /uː/ ("oon").
Variation.
Academic research has shown that the most notable variation within Australian English is largely sociocultural. This is mostly evident in phonology, which is divided into three sociocultural varieties: "broad", "general" and "cultivated".
A limited range of word choices is strongly regional in nature. Consequently, the geographical background of individuals can be inferred, if they use words that are peculiar to particular Australian states or territories and, in some cases, even smaller regions.
In addition, some Australians speak creole languages derived from Australian English, such as Australian Kriol, Torres Strait Creole and Norfuk.
Sociocultural.
The "broad", "general" and "cultivated" accents form a continuum that reflects minute variations in the Australian accent. They can reflect the social class, education and urban or rural background of speakers, though such indicators are not always reliable. According to linguists, the general Australian variant emerged some time before 1900. Recent generations have seen a comparatively smaller proportion of the population speaking with the broad variant, along with the near extinction of the cultivated Australian accent. The growth and dominance of general Australian accents perhaps reflects its prominence on radio and television during the late 20th century.
Australian Aboriginal English is made up of a range of forms which developed differently in different parts of Australia, and are said to vary along a continuum, from forms close to Standard Australian English to more non-standard forms. There are distinctive features of accent, grammar, words and meanings, as well as language use.
The ethnocultural dialects are diverse accents in Australian English that are spoken by the minority groups, which are of non-English speaking background. A massive immigration from Asia has made a large increase in diversity and the will for people to show their cultural identity within the Australian context. These ethnocultural varieties contain features of General Australian English as adopted by the children of immigrants blended with some non-English language features, such as the Afro-Asiatic and Asian languages.
Regional variation.
Although Australian English is relatively homogeneous, some regional variations are notable. The dialects of English spoken in South Australia, Western Australia, New South Wales, Victoria, Tasmania, Queensland and the Torres Strait Islands differ slightly from each other. Differences exist both in terms of vocabulary and phonology.
Most regional differences come down to word usage. For example, swimming clothes are known as "cossies" or "swimmers" in New South Wales, "togs" in Queensland, and "bathers" in Victoria, Western Australia and South Australia; what is referred to as a "stroller" in most of Australia is called a "pusher" in Victoria and usually a "pram" in Western Australia. Preference for synonymous words also differs between states. For example, "garbage" (i.e. garbage bin, garbage truck) dominates over "rubbish" in New South Wales and Queensland, while "rubbish" is more popular in Victoria, Western Australia and South Australia. The word "footy" generally refers to the most popular football code in the particular state or territory; that is, rugby league in New South Wales and Queensland, and Australian rules football elsewhere. Beer glasses are also named differently in different states. Distinctive grammatical patterns exist such as the use of the interrogative "eh" (also spelled "ay" or "aye"), which is particularly associated with Queensland.
There are some notable regional variations in the pronunciations of certain words. The extent to which the trap‑bath split has taken hold is one example. This phonological development is more advanced in South Australia, which had a different settlement chronology and type than other parts of the country, which resulted in a prolonged British English influence that outlasted that of the other colonies. Words such as "dance", "advance", "plant", "graph", "example" and "answer" are pronounced far more frequently with the older /æ/ (as in "mad") outside of South Australia, but with /aː/ (as in "father") within South Australia. "L"-vocalisation is also more common in South Australia than other states. In Western Australian and Queensland English, the vowels in "near" and "square" are typically realised as centring diphthongs ("nee-ya"), whereas in the other states they may also be realised as monophthongs. A feature common in Victorian English is salary–celery merger, whereby a Victorian pronunciation of "Ellen" may sound like "Alan" to speakers from other states. There is also regional variation in /uː/ before /l/ (as in "school" and "pool"), typically pronounced as /iːu/ in Queensland and New South Wales but /uː/ in South Australia and Western Australia.
Vocabulary.
Australian English has many words and idioms which are unique to the dialect and have been written on extensively, with the "Macquarie Dictionary", widely regarded as the national standard, incorporating numerous Australian terms.
Internationally well-known examples of Australian terminology include "outback", meaning a remote, sparsely populated area, "the bush", meaning either a native forest or a country area in general, and "g'day", a greeting. "Dinkum", or "fair dinkum" means "true" or "is that true?", among other things, depending on context and inflection. The derivative "dinky-di" means "true" or devoted: a "dinky-di Aussie" is a "true Australian".
Australian poetry, such as "The Man from Snowy River", as well as folk songs such as "Waltzing Matilda", contain many historical Australian words and phrases that are understood by Australians even though some are not in common usage today.
Australian English, in common with several British English dialects (for example, Cockney, Scouse, Glaswegian and Geordie), uses the word "mate". Many words used by Australians were at one time used in England but have since fallen out of usage or changed in meaning there.
For example, "creek" in Australia, as in North America, means a stream or small river, whereas in the UK it means a small watercourse flowing into the sea; "paddock" in Australia means field, whereas in the UK it means a small enclosure for livestock; "bush" or "scrub" in Australia, as in North America, means a wooded area, whereas in England they are commonly used only in proper names (such as Shepherd's Bush and Wormwood Scrubs).
Litotes, such as "not bad", "not much" and "you're not wrong", are also used, as are diminutives, which are commonly used and are often used to indicate familiarity. Some common examples are "arvo" (afternoon), "barbie" (barbecue), "smoko" (cigarette break), "Aussie" (Australian) and "pressie" (present/gift). This may also be done with people's names to create nicknames (other English speaking countries create similar diminutives). For example, "Gazza" from Gary, or "Smitty" from John Smith. The use of the suffix "-o" originates in Irish Gaelic (Irish "ó"), which is both a postclitic and a suffix with much the same meaning as in Australian English.
In informal speech, incomplete comparisons are sometimes used, such as "sweet as" (as in "That car is sweet as."). "Full", "fully" or "heaps" may precede a word to act as an intensifier (as in "The waves at the beach were heaps good."). This was more common in regional Australia and South Australia but has been in common usage in urban Australia for decades. The suffix "-ly" is sometimes omitted in broader Australian English. For instance, "really good" can become "real good".
Australia's switch to the metric system in the 1970s changed the country's vocabulary of measurement from Imperial towards metric measures.
Comparison with other varieties.
Where British and American vocabulary differs, Australians sometimes favour a usage different from both varieties, as with footpath (for US sidewalk, UK pavement) or capsicum (for US bell pepper, UK green/red pepper). In other instances, it either shares a term with American English, as with truck (UK: lorry) or eggplant (UK: aubergine), or with British English, as with mobile phone (US: cell phone) or bonnet (US: hood).
A non-exhaustive selection of common British English terms not commonly used in Australian English include (Australian usage in brackets): artic/articulated lorry (semi-trailer); aubergine (eggplant); bank holiday (public holiday); bedsit (one-bedroom apartment); bespoke (custom); bin lorry (garbage truck); black pudding (blood sausage); cagoule (raincoat); candy floss (fairy floss); cash machine (automatic teller machine/ATM); child-minder (babysitter); clingfilm (glad wrap/cling-wrap); courgette (zucchini); crisps (chips/potato chips); doddle (bludge); dungarees (overalls); dustbin (garbage/rubbish bin); dustcart (garbage/rubbish truck); duvet (doona); elastoplast/plaster (band-aid); estate car (station wagon); fairy cake (cupcake/patty cake); flannel ((face) washer/wash cloth); free phone (toll-free); football (soccer); high street (main street); hoover (vacuum cleaner); ice lolly (ice block/icy pole); kitchen roll (paper towel); lavatory (toilet); lorry (truck); off-licence (bottle shop); pavement (footpath); red/green pepper (capsicum); pillar box (mail box); plimsoll (sandshoe); pushchair (pram/stroller); saloon (sedan); sweets (lollies); utility room (laundry); Wellington boots (gumboots).
A non-exhaustive list of American English terms not commonly found in Australian English include: acclimate (acclimatise); aluminum (aluminium); bangs (fringe); bell pepper (capsicum); bellhop (hotel porter); broil (grill); burglarize (burgle); busboy (included under the broader term of waiter); candy (lollies); cell phone (mobile phone); cilantro (coriander); cookie (biscuit); counter-clockwise (anticlockwise); diaper (nappy); drywall (plasterboard); emergency brake (handbrake); faucet (tap); flashlight (torch); gasoline (petrol); hood (bonnet); jell-o (jelly); jelly (jam); math (maths); pacifier (dummy); parking lot (car park); popsicle (ice block/icy pole); railway ties (sleepers); rear view mirror (rear vision mirror); row house (terrace house); scallion (spring onion); stickshift (manual transmission); streetcar (tram); takeout (takeaway); trash can (garbage/rubbish bin); trunk (boot); turn signal (indicator/blinker); vacation (holiday); windshield (windscreen).
Terms shared by British and American English but not so commonly found in Australian English include: abroad (overseas); cooler/ice box (esky); pickup truck (ute); wildfire (bushfire).
Australian English is particularly divergent from other varieties with respect to geographical terminology, due to the country's unique geography. This is particularly true when comparing with British English, due to that country's dramatically different geography. British geographical terms not in common use in Australia include: coppice (cleared bushland); fen (swamp); heath (shrubland); meadow (grassy plain); moor (swampland); spinney (shrubland); stream (creek); woods (bush) and village (even the smallest settlements in Australia are called "towns" or "stations").
In addition, a number of words in Australian English have different meanings to those ascribed in other varieties of English. Clothing-related examples are notable. "Pants" in Australian English refer to British English "trousers" but in British English refer to Australian English "underpants"; "vest" in Australian English refers to British English "waistcoat" but in British English refers to Australian English "singlet"; "thong" in both American and British English refers to underwear otherwise known as a "G-string", while in Australian English it refers to British and American English "flip-flop" (footwear).
Grammar.
As with American English, but unlike British English, collective nouns are almost always singular in construction, e.g. "the government was unable to decide" as opposed to "the government were unable to decide".
In common with British English, the past tense and past participles of the verbs "learn", "spell" and "smell" are often irregular ("learnt", "spelt", "smelt").
"Shan't" and the use of "should" as in "I should be happy if...", common in upper-register British English, are almost never encountered in Australian (or North American) English.
While prepositions before days may be omitted in American English, i.e. "She resigned Thursday", they must be retained in Australian English, as in British English: "She resigned on Thursday". Ranges of dates use "to", i.e. "Monday to Friday", as with British English, rather than "Monday through Friday" in American English.
"River" generally follows the name of the river in question as in North America, i.e. "Darling River", rather than the British convention of coming before the name, e.g. "River Thames". Note in South Australia however, the British convention applies—for example, the "River Murray" or the "River Torrens".
When saying or writing out numbers, "and" is inserted before the tens and units, i.e. "one hundred and sixty-two", as with British practice. However Australians, like Americans, are more likely to pronounce numbers such as 1200 as "twelve hundred", rather than "one thousand two hundred".
As with American English, "on the weekend" and "studied medicine" are used rather than the British "at the weekend" and "read medicine".
Spelling and style.
As in most English-speaking countries, there is no official governmental regulator or overseer of correct spelling and grammar. The "Macquarie Dictionary" is used by some universities and some other organisations as a standard for Australian English spelling. The "Style Manual: For Authors, Editors and Printers", the "Cambridge Guide to Australian English Usage" and the "Australian Guide to Legal Citation" are prominent style guides.
Australian spelling is closer to British than American spelling. As with British spelling, the "u" is retained in words such as "colour", "honour", "labour" and "favour". While the Macquarie Dictionary lists the "-our" ending and follows it with the "-or" ending as an acceptable variant, the latter is rarely found in actual use today. Australian print media, including digital media, today strongly favour "-our" endings. A notable exception to this rule is the Australian Labor Party, which adopted the American spelling in 1912 as a result of "-or" spellings' comparative popularity at that time. Consistent with British spellings, "-re", rather than "-er", is the only listed variant in Australian dictionaries in words such as "theatre", "centre" and "manoeuvre". Unlike British English, which is split between "-ise" and "-ize" in words such as "organise" and "realise", with "-ize" favoured by the Oxford English Dictionary and "-ise" listed as a variant, "-ize" is rare in Australian English and designated as a variant by the Macquarie Dictionary. "Ae" and "oe" are often maintained in words such as "manoeuvre", "paedophilia" and "foetus" (excepting those listed below), however the Macquarie dictionary lists forms with "e" (e.g. pedophilia, fetus) as acceptable variants and notes a tendency within Australian English towards using only "e". Individual words spelt differently from British spelling, according to the "Macquarie Dictionary", include "program" (in all contexts) as opposed to "programme", "inquire" (for all meanings) and derivatives "inquired", "inquiring", "inquiry", "inquirer", etc. as opposed to "enquire" and derivatives, "analog" as opposed to "analogue" (as with American English, "analog" is used in the context of information transmission and "analogue" in the sense of "something analogous to"), "livable" as opposed to "liveable", "guerilla" as opposed to "guerrilla", "yoghurt" as opposed to "yogurt", "verandah" as opposed to "veranda", "sulfur" and derivatives "sulfide", "sulfidic" and "sulfuric" as opposed to "sulphur" and derivatives, "burqa" as opposed to "burka", "pastie" (food) as opposed to "pasty", "onto" "or" "on to" as opposed to "on to", "anytime" as opposed to "any time", "alright" "or" "all right" as opposed to "all right", and "anymore" as opposed to "any more". Both "acknowledgement" and "acknowledgment", as well as "abridgement" and "abridgment" are used, with the shorter forms being endorsed by Australian governments. "Okay", rather than "OK", is listed as the preferred variant.
Different spellings have existed throughout Australia's history. A pamphlet entitled "The So-Called "American Spelling"", published in Sydney some time in the 19th century, argued that "there is no valid etymological reason for the preservation of the "u" in such words as "honor", "labor", etc." The pamphlet also claimed that "the tendency of people in Australasia is to excise the u, and one of the Sydney morning papers habitually does this, while the other generally follows the older form." What are today regarded as American spellings were popular in Australia throughout the late 19th and early 20th centuries, with the Victorian Department of Education endorsing them into the 1970s and "The Age" newspaper until the 1990s. This influence can be seen in the spelling of the Australian Labor Party and also in some place names such as Victor Harbor. The "Concise Oxford English Dictionary" has been attributed with re-establishing the dominance of the British spellings in the 1920s and 1930s. For a short time during the late 20th century, Harry Lindgren's 1969 spelling reform proposal ("Spelling Reform 1" or "SR1") gained some support in Australia: in 1975, the Australian Teachers' Federation adopted SR1 as a policy. SR1 calls for the short /e/ sound (as in "bet") to be spelt with E (for example "friend→frend, head→hed"). 
Both single and double quotation marks are in use (with double quotation marks being far more common in print media), with logical (as opposed to typesetter's) punctuation. Spaced and unspaced em-dashes remain in mainstream use, as with American and Canadian English. The DD/MM/YYYY date format is followed, however the 12-hour clock is generally used in everyday life (as opposed to service, police, and airline applications).
Computer keyboards.
There are two major English language keyboard layouts, the United States layout and the United Kingdom layout. Australia universally uses the United States keyboard layout, which lacks Pound Sterling, Euro currency and negation symbols. Punctuation symbols are also placed differently from British keyboards.
References.
</dl>

</doc>
<doc id="1902" url="http://en.wikipedia.org/wiki?curid=1902" title="American Airlines Flight 77">
American Airlines Flight 77

American Airlines Flight 77 was a scheduled domestic passenger flight that was hijacked by five men affiliated with al-Qaeda on September 11, 2001, as part of the September 11 attacks. They deliberately crashed it into the Pentagon in Arlington County, Virginia near Washington, D.C., killing all 64 people on board including the five hijackers and six crew as well as 125 people in the building. The Boeing 757-223 aircraft was flying American Airlines' daily scheduled morning transcontinental service from Washington Dulles International Airport, in Dulles, Virginia to Los Angeles International Airport in Los Angeles, California.
Less than 35 minutes into the flight, the hijackers stormed the cockpit. They forced the passengers, crew, and pilots to the rear of the aircraft. Hani Hanjour, one of the hijackers who was trained as a pilot, assumed control of the flight. Unknown to the hijackers, passengers aboard made telephone calls to loved ones and relayed information on the hijacking.
The hijackers crashed the aircraft into the western side of the Pentagon at 9:37 a.m. (EDT). Dozens of people witnessed the crash, and news sources began reporting on the incident within minutes. The impact severely damaged an area of the Pentagon and caused a large fire. At 10:10 a.m., a portion of the Pentagon collapsed; firefighters spent days trying to fully extinguish the blaze. The damaged sections of the Pentagon were rebuilt in 2002, with occupants moving back into the completed areas on August 15, 2002.
The 184 victims of the attack are memorialized in the Pentagon Memorial adjacent to the Pentagon. The 1.93 acre park contains a bench for each of the victims, arranged according to their year of birth, ranging from 1930 (age 71) to 1998 (age 3).
Hijackers.
The hijackers on American Airlines Flight 77 were led by Hani Hanjour, who piloted the aircraft into the Pentagon. Hanjour first came to the United States in 1990.
Hanjour trained at the CRM Airline Training Center in Scottsdale, Arizona, earning his FAA commercial pilot's certificate in April 1999. He had wanted to be a commercial pilot for the Saudi national airline but was rejected when he applied to the civil aviation school in Jeddah in 1999. Hanjour's brother later explained that, frustrated at not finding a job, Hanjour "increasingly turned his attention toward religious texts and cassette tapes of militant Islamic preachers". Hanjour returned to Saudi Arabia after being certified as a pilot, but left again in late 1999, telling his family that he was going to the United Arab Emirates to work for an airline. Hanjour likely went to Afghanistan, where Al Qaeda recruits were screened for special skills they may have. Already having selected the Hamburg Cell members, Al Qaeda leaders selected Hanjour to lead the fourth team of hijackers.
Alec Station, the CIA's unit dedicated to tracking Osama bin Laden, had discovered that two of the other hijackers, al-Hazmi and al-Mihdhar, had multiple entry visas to the United States well before 9/11. Two FBI agents inside the unit tried to alert FBI headquarters, but CIA officers rebuffed them.
In December 2000, Hanjour arrived in San Diego, joining "muscle" hijackers Nawaf al-Hazmi and Khalid al-Mihdhar, who had been there since January 2000. Soon after arriving, Hanjour and Hazmi left for Mesa, Arizona, where Hanjour began refresher training at Arizona Aviation.
In April 2001, they relocated to Falls Church, Virginia, where they awaited the arrival of the remaining "muscle" hijackers. One of these men, Majed Moqed, arrived on May 2, 2001, with Flight 175 hijacker Ahmed al-Ghamdi from Dubai at Dulles International Airport. They moved into an apartment with Hazmi and Hanjour. While living in Falls Church, Hazmi attended the mosque in the community.
On May 21, 2001, Hanjour rented a room in Paterson, New Jersey, where he stayed with other hijackers through the end of August. The last Flight 77 "muscle" hijacker, Salem al-Hazmi, arrived on June 29, 2001, with Abdulaziz al-Omari (a hijacker of Flight 11) at John F. Kennedy International Airport from the United Arab Emirates. They stayed with Hanjour.
Hanjour received ground instruction and did practice flights at Air Fleet Training Systems in Teterboro, New Jersey, and at Caldwell Flight Academy in Fairfield, New Jersey. Hanjour moved out of the room in Paterson and arrived at the Valencia Motel in Laurel, Maryland, on September 2, 2001. While in Maryland, Hanjour and fellow hijackers trained at Gold's Gym in Greenbelt. On September 10, he completed a certification flight, using a terrain recognition system for navigation, at Congressional Air Charters in Gaithersburg, Maryland.
On September 10, Nawaf al-Hazmi, accompanied by other hijackers, checked into the Marriott in Herndon, Virginia, near Dulles Airport.
Suspected accomplices.
According to a U.S. State Department cable leaked in the Wikileaks dump in February 2010, the FBI has investigated another suspect, Mohammed al-Mansoori. He had associated with three Qatari citizens who flew from Los Angeles to London (via Washington) and Qatar on the eve of the attacks, after allegedly surveying the World Trade Center and the White House. U.S. law enforcement officials said that the data about the four men was "just one of many leads that were thoroughly investigated at the time and never led to terrorism charges". An official added that the three Qatari citizens have never been questioned by the FBI. Eleanor Hill, the former staff director for the congressional joint inquiry on the September 11 attacks, said the cable reinforces questions about the thoroughness of the FBI's investigation. She also said that the inquiry concluded that the hijackers had a support network that helped them in different ways.
The three Qatari men were booked to fly from Los Angeles to Washington on September 10, 2001, on the same plane that was hijacked and piloted into the Pentagon on the following day. Instead, they flew from Los Angeles to Qatar, via Washington and London. While the cable said that Mansoori was currently under investigation, U.S. law enforcement officials said that there was no active investigation of him or of the Qatari citizens mentioned in the cable.
Flight.
The American Airlines Flight 77 aircraft was a Boeing 757-223 (registration N644AA). The flight crew included pilot Charles Burlingame (a Naval Academy graduate and former fighter pilot), First Officer David Charlebois, and flight attendants Michele Heidenberger, Jennifer Lewis, Kenneth Lewis, and Renee May. The capacity of the aircraft was 188 passengers, but with 58 passengers on September 11, the load factor was 33 percent. American Airlines said that Tuesdays were the least-traveled day of the week, with the same load factor seen on Tuesdays in the previous three months for Flight 77.
Boarding and departure.
On the morning of September 11, 2001, the five hijackers arrived at Washington Dulles International Airport. At 07:15, Khalid al-Mihdhar and Majed Moqed checked in at the American Airlines ticket counter for Flight 77, arriving at the passenger security checkpoint a few minutes later at 07:18. Both men set off the metal detector and were put through secondary screening. Moqed continued to set off the alarm, so he was searched with a hand wand. The Hazmi brothers checked in together at the ticket counter at 07:29. Hani Hanjour checked in separately and arrived at the passenger security checkpoint at 07:35. Hanjour was followed minutes later at the checkpoint by Salem and Nawaf al-Hazmi, who also set off the metal detector's alarm. The screener at the checkpoint never resolved what set off the alarm. As seen in security footage later released, Nawaf Hazmi appeared to have an unidentified item in his back pocket. Utility knives up to four inches were permitted at the time by the Federal Aviation Administration (FAA) as carry-on items. The passenger security checkpoint at Dulles International Airport was operated by Argenbright Security, under contract with United Airlines.
The hijackers were all selected for extra screening of their checked bags. Hanjour, al-Mihdhar, and Moqed were chosen by the Computer Assisted Passenger Prescreening System criteria, while the brothers Nawaf and Salem al-Hazmi were selected because they did not provide adequate identification and were deemed suspicious by the airline check-in agent. Hanjour, Mihdhar, and Nawaf al-Hazmi did not check any bags for the flight. Checked bags belonging to Moqed and Salem al-Hazmi were held until they boarded the aircraft. By 07:50, the five hijackers, carrying knives and box cutters, had made it through the airport security checkpoint.
Flight 77 was scheduled to depart for Los Angeles at 08:10; 58 passengers boarded through Gate D26, including the five hijackers. Excluding the hijackers, of the 59 other passengers and crew on board, there were 26 men, 22 women, and five children ranging in age from three to eleven. On the flight, Hani Hanjour was seated up front in 1B, while Salem and Nawaf al-Hazmi were seated in first class in seats 5E and 5F. Majed Moqed and Khalid al-Mihdhar were seated further back in 12A and 12B, in economy class. Flight 77 left the gate on time and took off from Runway 30 at Dulles at 8:20 am.
Hijacking.
The 9/11 Commission estimated that the flight was hijacked between 08:51 and 08:54, shortly after American Airlines Flight 11 struck the World Trade Center and not too long after United Airlines Flight 175 had been hijacked. The last normal radio communications from the aircraft to air traffic control occurred at 08:50:51. Unlike the other three flights, there were no reports of anyone being stabbed or a bomb threat and the pilots were not immediately killed but shoved to the back of the plane with the rest of the passengers. At 08:54, the plane began to deviate from its normal, assigned flight path and turned south. The hijackers set the flight's autopilot heading for Washington, D.C. By 08:56, the flight was turned around, and the transponder had been disabled.
The FAA was aware at this point that there was an emergency on board the airplane. By this time, American Airlines Flight 11 had already crashed into the North Tower of the World Trade Center, and United Airlines Flight 175 was known to have been hijacked and was within minutes of striking the South Tower. After learning of this second hijacking involving an American Airlines aircraft and the hijacking involving United Airlines, American Airlines' Executive Vice President Gerard Arpey ordered a nationwide ground stop for the airline. The Indianapolis Air Traffic Control Center, as well as American Airlines dispatchers, made several failed attempts to contact the aircraft. At the time the airplane was hijacked, it was flying over an area of limited radar coverage. With air controllers unable to contact the flight by radio, an Indianapolis official declared that the Boeing 757 had possibly crashed at 09:09.
Two people on the aircraft made phone calls to contacts on the ground. At 09:12, flight attendant Renee May called her mother, Nancy May, in Las Vegas. During the call, which lasted nearly two minutes, May said her flight was being hijacked by six persons, and staff and passengers had been moved to the rear of the airplane. May asked her mother to contact American Airlines, which she and her husband promptly did; American Airlines was already aware of the hijacking. Between 09:16 and 09:26, passenger Barbara Olson called her husband, United States Solicitor General Theodore Olson, and reported that the airplane had been hijacked and that the assailants had box cutters and knives. She reported that the passengers, including the pilots, had been moved to the back of the cabin and that the hijackers were unaware of her call. A minute into the conversation, the call was cut off. Theodore Olson contacted the command center at the Department of Justice, and tried unsuccessfully to contact Attorney General John Ashcroft. About five minutes later, Barbara Olson called again, told her husband that the "pilot" (possibly Hanjour on the cabin intercom) had announced the flight was hijacked, and asked "what do I tell the pilot to do?" Ted Olson asked her location and she reported the plane was flying low over a residential area. He told her of the attacks on the World Trade Center. Soon afterward, the call cut off again.
An airplane was detected again by Dulles controllers on radar screens as it approached Washington, turning and descending rapidly. Controllers initially thought this was a military fighter, due to its high speed and maneuvering. Reagan Airport controllers asked a passing Air National Guard Lockheed C-130 Hercules to identify and follow the aircraft. The pilot, Lt. Col. Steven O'Brien, told them it was a Boeing 757 or 767, and its silver fuselage meant it was probably an American Airlines jet. He had difficulty picking out the airplane in the "East Coast haze", but then saw a "huge" fireball, and initially assumed it had hit the ground. Approaching the Pentagon, he saw the impact site on the building's west side and reported to Reagan control, "Looks like that aircraft crashed into the Pentagon, sir."
Crash.
According to the 9/11 Commission Report, as Flight 77 was 5 mi west-southwest of the Pentagon, it made a 330-degree turn. At the end of the turn, it was descending through 2200 ft, pointed toward the Pentagon and downtown Washington. Hani Hanjour advanced the throttles to maximum power and dived toward the Pentagon. While level above the ground and seconds from the crash, the wings knocked over five street lampposts and the right wing struck a portable generator, creating a smoke trail seconds before smashing into the Pentagon. Flight 77, flying at 530 mph (853 km/h, 237 m/s, or 460 knots) over the Navy Annex Building adjacent to Arlington National Cemetery, crashed into the western side of the Pentagon in Arlington County, Virginia, just south of Washington, D.C., at 09:37:46, killing all 64 people on board: 59 victims (including six crewmembers) and the five perpetrators. The plane hit the Pentagon at the first-floor level, and at the moment of impact, the airplane was rolled slightly to the left, with the right wing elevated. The front part of the fuselage disintegrated on impact, while the mid and tail sections moved for another fraction of a second, with tail section debris penetrating furthest into the building. In all, the airplane took eight-tenths of a second to fully penetrate 310 ft into the three outermost of the building's five rings and unleashed a fireball that rose 200 ft above the building.
At the time of the attacks, approximately 18,000 people worked in the Pentagon, which was 4,000 fewer than before renovations began in 1998. The section of the Pentagon, which had recently been renovated at a cost of $250 million, housed the Naval Command Center and other Pentagon offices, as well as some unoccupied offices. The crash and subsequent fire penetrated three outer ring sections of the western side. The outermost ring section was largely destroyed, and a large section collapsed.
In all, there were 189 deaths at the Pentagon site, including the 125 in the Pentagon building in addition to the 64 on board the aircraft. Passenger Barbara Olson was en route to a taping of "Politically Incorrect". A group of children, their chaperones, and National Geographic Society staff members were also on board, embarking on an educational trip west to the Channel Islands National Marine Sanctuary near Santa Barbara, California. The fatalities at the Pentagon included 55 military personnel and 70 civilians. Of those 125 killed, 92 were on the first floor, 31 were on the second floor, and two were on the third. Seven Defense Intelligence Agency employees were killed while the Office of the Secretary of Defense lost one contractor. The Army suffered 75 fatalities — 53 civilians (47 employees and six contractors) and 22 soldiers — while the Navy suffered 42 fatalities — nine civilians (six employees and three contractors) and 33 sailors. Lieutenant General Timothy Maude, an Army Deputy Chief of Staff, was the highest-ranking military officer killed at the Pentagon; also killed was retired Rear Admiral Wilson Flagg, a passenger on the plane. LT Mari-Rae Sopper, JAGC, USNR, was also on board the flight, and was the first Navy Judge Advocate ever to be killed. Another 106 were injured on the ground and were treated at area hospitals. 
On the side where the plane hit, the Pentagon is bordered by Interstate 395 and Washington Boulevard. Motorist Mary Lyman, who was on I-395, saw the airplane pass over at a "steep angle toward the ground and going fast" and then saw the cloud of smoke from the Pentagon. Omar Campo, another witness, was cutting the grass on the other side of the road when the airplane flew over his head.
"I was cutting the grass and it came in screaming over my head. I felt the impact. The whole ground shook and the whole area was full of fire. I could never imagine I would see anything like that here".Afework Hagos, a computer programmer, was on his way to work and stuck in a traffic jam near the Pentagon when the airplane flew over. "There was a huge screaming noise and I got out of the car as the plane came over. Everybody was running away in different directions. It was tilting its wings up and down like it was trying to balance. It hit some lampposts on the way in." Daryl Donley witnessed the crash and took some of the first photographs of the site.
"USA Today" reporter Mike Walter was driving on Washington Boulevard when he witnessed the crash, which he recounted,
"I looked out my window and I saw this plane, this jet, an American Airlines jet, coming. And I thought, 'This doesn't add up, it's really low.' And I saw it. I mean it was like a cruise missile with wings. It went right there and slammed right into the Pentagon".Terrance Kean, who lived in a nearby apartment building, heard the noise of loud jet engines, glanced out his window, and saw a "very, very large passenger jet". He watched "it just plow right into the side of the Pentagon. The nose penetrated into the portico. And then it sort of disappeared, and there was fire and smoke everywhere." Tim Timmerman, who is a pilot himself, noticed American Airlines markings on the aircraft as he saw it hit the Pentagon. Other drivers on Washington Boulevard, Interstate 395, and Columbia Pike witnessed the crash, as did people in Pentagon City, Crystal City, and other nearby locations.
Former Georgetown University basketball coach John Thompson had originally booked a ticket on Flight 77. As he would tell the story many times in the following years, including a September 12, 2011 interview on Jim Rome's radio show, he had been scheduled to appear on that show on September 12, 2001. Thompson was planning to be in Las Vegas for a friend's birthday on September 13, and initially insisted on traveling to Rome's Los Angeles studio on the 11th. However, this did not work for the show, which wanted him to travel on the day of the show. After a Rome staffer personally assured Thompson that he would be able to travel from Los Angeles to Las Vegas immediately after the show, Thompson changed his travel plans. He felt the impact from the crash at his home near the Pentagon.
Rescue and recovery.
Rescue efforts began immediately after the crash. Almost all the successful rescues of survivors occurred within half an hour of the impact. Initially, rescue efforts were led by the military and civilian employees within the building. Within minutes, the first fire companies arrived and found these volunteers searching near the impact site. The firemen ordered them to leave as they were not properly equipped or trained to deal with the hazards. The Arlington County Fire Department (ACFD) assumed command of the immediate rescue operation within 10 minutes of the crash. ACFD Assistant Chief James Schwartz implemented an incident command system (ICS) to coordinate response efforts among multiple agencies. It took about an hour for the ICS structure to become fully operational. Firefighters from Fort Myer and Reagan National Airport arrived within minutes. Rescue and firefighting efforts were impeded by rumors of additional incoming planes. Chief Schwartz ordered two evacuations during the day in response to these rumors.
As firefighters attempted to extinguish the fires, they watched the building in fear of a structural collapse. One firefighter remarked that they "pretty much knew the building was going to collapse because it started making weird sounds and creaking". Officials saw a cornice of the building move and ordered an evacuation. Minutes later, at 10:10, the upper floors of the damaged area of the Pentagon collapsed. The collapsed area was about 95 ft at its widest point and 50 ft at its deepest. The amount of time between impact and collapse allowed everyone on the fourth and fifth levels to evacuate safely before the structure collapsed. After the collapse, the interior fires intensified, spreading through all five floors. After 11:00, firefighters mounted a two-pronged attack against the fires. Officials estimated temperatures of up to 2000 °F. While progress was made against the interior fires by late afternoon, firefighters realized a flammable layer of wood under the Pentagon's slate roof had caught fire and begun to spread. Typical firefighting tactics were rendered useless by the reinforced structure as firefighters were unable to reach the fire to extinguish it. Firefighters instead made firebreaks in the roof on September 12 to prevent further spreading. At 18:00 on the 12th, Arlington County issued a press release stating the fire was "controlled" but not fully "extinguished". Firefighters continued to put out smaller fires that ignited in the succeeding days.
Various pieces of aircraft debris were found within the wreckage at the Pentagon. While on fire and escaping from the Navy Command Center, Lt. Kevin Shaeffer observed a chunk of the aircraft's nose cone and the nose landing gear in the service road between rings B and C. Early in the morning on Friday, September 14, Fairfax County Urban Search and Rescue Team members Carlton Burkhammer and Brian Moravitz came across an "intact seat from the plane's cockpit", while paramedics and firefighters located the two black boxes near the punch out hole in the A-E drive, nearly 300 ft into the building. The cockpit voice recorder was to retrieve any information, though the flight data recorder yielded useful information. Investigators also found a part of Nawaf al-Hazmi's driver's license in the North Parking Lot rubble pile. Personal effects belonging to victims were found and taken to Fort Myer.
Remains.
Army engineers determined by 5:30 p.m. on the first day that no one remained alive in the damaged section of the building. In the days after the crash, news reports emerged that up to 800 people had died. Army troops from Fort Belvoir were the first teams to survey the interior of the crash site and noted the presence of human remains. Federal Emergency Management Agency (FEMA) Urban Search and Rescue teams, including Fairfax County Urban Search and Rescue assisted the search for remains, working through the National Interagency Incident Management System (NIIMS). Kevin Rimrodt, a Navy photographer surveying the Navy Command Center after the attacks, remarked that "there were so many bodies, I'd almost step on them. So I'd have to really take care to look backwards as I'm backing up in the dark, looking with a flashlight, making sure I'm not stepping on somebody". Debris from the Pentagon was taken to the Pentagon's north parking lot for more detailed search for remains and evidence.
Remains that were recovered from the Pentagon were photographed, and turned over to the Armed Forces Medical Examiner office, located at Dover Air Force Base in Delaware. The medical examiner's office was able to identify remains belonging to 179 of the victims. Investigators eventually identified 184 of the 189 people who died in the attack. The remains of the five hijackers were identified through a process of elimination, and were turned over as evidence to the Federal Bureau of Investigation (FBI). On September 21, the ACFD relinquished control of the crime scene to the FBI. The Washington Field Office, National Capital Response Squad (NCRS), and the Joint Terrorism Task Force (JTTF) led the crime scene investigation at the Pentagon.
By October 2, 2001, the search for evidence and remains was complete and the site was turned over to Pentagon officials. In 2002, the remains of 25 victims were buried collectively at Arlington National Cemetery, with a five-sided granite marker inscribed with the names of all the victims in the Pentagon. The ceremony also honored the five victims whose remains were never found.
Flight Data Recorder and Cockpit Voice Recorder.
At around 3:40 a.m on September 14, a paramedic and a firefighter who were searching through the debris of the impact site found two dark boxes, about 1.5 ft by 2 ft long. They called for an FBI agent, who in turn called for someone from the National Transportation Safety Board (NTSB). The NTSB employee confirmed that these were the flight recorders ("black boxes") from American Airlines Flight 77. Dick Bridges, deputy manager for Arlington County, Virginia, said the cockpit voice recorder that used magnetic tape was damaged on the outside and the flight data recorder that used a solid-state drive was charred. Bridges said the recorders were found "right where the plane came into the building."
The cockpit voice recorder that used magnetic tape was transported to the NTSB lab in Washington, D.C., to see what data was salvageable. In its report on the cockpit voice recorder, the NTSB identified the unit as an L-3 Communications, Fairchild Aviation Recorders model A-100A cockpit voice recorder; a device which records on magnetic tape. The NTSB reported that "The majority of the recording tape was fused into a solid block of charred plastic." No usable segments of tape were found inside the recorder. All the data from the flight data recorder that used a solid-state drive was recovered.
Continuity of operations.
At the moment of impact, Secretary of Defense Donald Rumsfeld was in his office on the other side of the Pentagon, away from the crash site. He ran to the site and assisted the injured. Rumsfeld returned to his office, and went to a conference room in the Executive Support Center where he joined a secure videoteleconference with Vice President Dick Cheney and other officials. On the day of the attacks, DoD officials considered moving their command operations to Site R, a backup facility in Pennsylvania. Secretary of Defense Rumsfeld insisted he remain at the Pentagon, and sent Deputy Secretary Paul Wolfowitz to Site R. The National Military Command Center (NMCC) continued to operate at the Pentagon, even as smoke entered the facility. Engineers and building managers manipulated the ventilation and other building systems that still functioned to draw smoke out of the NMCC and bring in fresh air.
During a press conference held inside the Pentagon at 18:42, Rumsfeld announced, "The Pentagon's functioning. It will be in business tomorrow." Pentagon employees returned the next day to offices in mostly unaffected areas of the building. By the end of September, more workers returned to the lightly damaged areas of the Pentagon.
Aftermath.
Early estimates on rebuilding the damaged section of the Pentagon were that it would take three years to complete. However, the project moved forward at an accelerated pace and was completed by the one-year anniversary of the attack. The rebuilt section of the Pentagon includes a small indoor memorial and chapel at the point of impact. An outdoor memorial, commissioned by the Pentagon and designed by Julie Beckman and Keith Kaseman, was completed on schedule for its dedication on September 11, 2008.
Security camera video.
On May 16, 2006, the Department of Defense released filmed footage that was recorded by a security camera of American Airlines Flight 77 crashing into the Pentagon, with a plane visible in one frame, as a "thin white blur" and an explosion following. The images were made public in response to a December 2004 Freedom of Information Act request by Judicial Watch. Some still images from the video had previously been released and publicly circulated, but this was the first official release of the edited video of the crash.
A nearby Citgo service station also had security cameras, but a video released on September 15, 2006 did not show the crash because the camera was pointed away from the crash site.
The Doubletree Hotel, located nearby in Crystal City, Virginia, also had a security camera video. On December 4, 2006, the FBI released the video in response to a FOIA lawsuit filed by Scott Bingham. The footage is "grainy and the focus is soft, but a rapidly growing tower of smoke is visible in the distance on the upper edge of the frame as the plane crashes into the building".
Memorials.
On September 12, 2002, Defense Secretary Donald Rumsfeld and General Richard Myers, Chairman of the Joint Chiefs of Staff, dedicated the Victims of Terrorist Attack on the Pentagon Memorial at Arlington National Cemetery. The memorial specifically honors the five individuals for whom no identifiable remains were found. This included Dana Falkenberg, age three, who was aboard American Airlines Flight 77 with her parents and older sister. A portion of the remains of 25 other victims are also buried at the site. The memorial is a pentagonal granite marker 4.5 ft high. On five sides of the memorial along the top are inscribed the words "Victims of Terrorist Attack on the Pentagon September 11, 2001". Aluminum plaques, painted black, are inscribed with the names of the 184 victims of the terrorist attack. The site is located in Section 64, on a slight rise, which gives it a view of the Pentagon.
At the National September 11 Memorial, the names of the Pentagon victims are inscribed on the South Pool, on Panels S-1 and S-72 – S-76.

</doc>
<doc id="1905" url="http://en.wikipedia.org/wiki?curid=1905" title="Ambush">
Ambush

An ambush is a long-established military tactic, in which combatants take advantage of concealment and the element of surprise to attack unsuspecting enemy combatants from concealed positions, such as among dense underbrush or behind hilltops. Ambushes have been used consistently throughout history, from ancient to modern warfare.
History.
The use by early humans of the ambush may date as far back as two million years when anthropologists have recently suggested that ambush techniques were used to hunt large game. 
More recently, an ambush often might involve thousands of soldiers on a large scale, such as over a mountain pass. Ambushes appear many times in military history. One outstanding example from ancient times is the Battle of the Trebia river. Hannibal encamped within striking distance of the Romans with the Trebia River between them, and placed a strong force of cavalry and infantry in concealment, near the battle zone. He had noticed, says Polybius, a "“place between the two camps, flat indeed and treeless, but well adapted for an ambuscade, as it was traversed by a water-course with steep banks, densely overgrown with brambles and other thorny plants, and here he proposed to lay a stratagem to surprise the enemy”". 
When the Roman infantry became entangled in combat with his army, the hidden ambush force attacked the legionnaires in the rear. The result was slaughter and defeat for the Romans. Nevertheless the battle also displays the effects of good tactical discipline on the part of the ambushed force. Although most of the legions were lost, about 10,000 Romans cut their way through to safety, maintaining unit cohesion. This ability to maintain discipline and break out or maneuver away from a killing zone is a hallmark of good troops and training in any ambush situation. See Ranger reference below.
Another famous ambush was that sprung by Germanic warchief Arminius against the Romans at Battle of the Teutoburg Forest. This particular ambush was to have an impact on the course of Western history. The Germanic forces demonstrated several principles needed for a successful ambush. They took cover in difficult forested terrain, allowing the warriors time and space to mass without detection. They had the element of surprise, and this was also aided by the defection of Arminius from Roman ranks prior to the battle. They sprung the attack when the Romans were most vulnerable- when they had left their fortified camp, and were on the march in a pounding rainstorm. 
They did not dawdle at the hour of decision but attacked quickly, using a massive series of short, rapid, vicious charges against the length of the whole Roman line, with charging units sometimes withdrawing to the forest to regroup while others took their place. The Germans also made use of blocking obstacles, erecting a trench and earthen wall to hinder Roman movement along the route of the killing zone. The result was mass slaughter of the Romans, and the destruction of 3 legions. The Germanic victory caused a limit on Roman expansion in the West. Ultimately, it established the Rhine as the boundary of the Roman Empire for the next four hundred years, until the decline of the Roman influence in the West. The Roman Empire made no further concerted attempts to conquer Germania beyond the Rhine.
Arabia during Muhammad's era.
The Islamic Prophet Muhammad made use of ambush tactics in his military campaigns. His first such use was during the Caravan raids, in the Kharrar caravan raid Sa`d ibn Abi Waqqas was ordered to lead a raid against the Quraysh. His group consisted of about twenty Muhajirs. This raid was done about a month after the previous. Sa'd, with his soldiers, set up an ambush in the valley of Kharrar on the road to Mecca and waited to raid a returning Meccan caravan from Syria. But the caravan had already passed and the Muslims returned to Medina without a fight.
Other Arab tribes during Muhammad's era also made use of ambush tactics. One such example was during the First Raid on Banu Thalabah. The Banu Thalabah tribe were already aware of the impending attack; so they lay in wait for the Muslims, and when Muhammad ibn Maslama arrived at the site.The Banu Thalabah, with 100 men ambushed them, while the Muslims were making preparation to sleep; and after a brief resistance killed all of Muhammad ibn Maslama’s men. Muhammad ibn Maslama pretended to be dead. A Muslim who happened to pass that way found him and assisted him to return to Medina. The raid was unsuccessful.
Procedure.
In modern warfare, an ambush is most often employed by ground troops up to platoon size against enemy targets, which may be other ground troops, or possibly vehicles. However, in some situations, especially when deep behind enemy lines, the actual attack will be carried out by a platoon, a company-sized unit will be deployed to support the attack group, setting up and maintaining a forward patrol harbour from which the attacking force will deploy, and to which they will retire after the attack.
Planning.
Ambushes are complex, multi-phase operations, and are, therefore, usually planned in some detail. First, a suitable killing zone is identified. This is the place where the ambush will be laid. It is generally a place where enemy units are expected to pass, and which gives reasonable cover for the deployment, execution, and extraction phases of the ambush patrol. A path along a wooded valley floor would be a typical example.
Ambush can be described geometrically as:
Viet Cong ambush techniques.
Ambush criteria: The terrain for the ambush had to meet strict criteria:
One important feature of the ambush was that the target units should 'pile up' after being attacked, thus preventing them any easy means of withdrawal from the kill zone and hindering their use of heavy weapons and supporting fires. Terrain was usually selected which would facilitate this and slow down the enemy. The terrain around the ambush site which was not favorable to the ambushing force, or which offered some protection to the target, was heavily mined and booby trapped or pre-registered for mortars.
Ambush units: The NVA/VC ambush formations consisted of:
Other elements might also be included if the situation demanded, such as a sniper screen along a nearby avenue of approach to delay enemy reinforcement.
Command posts: When deploying into an ambush site, the NVA first occupied several observation posts, placed to detect the enemy as early as possible and to report on the formation it was using, its strength and firepower, as well as to provide early warning to the unit commander. Usually one main OP and several secondary OP's were established. Runners and occasionally radios were used to communicate between the OP's and the main command post. The OP's were located so that they could observe enemy movement into the ambush and often they would remain in position throughout the ambush in order to report routes of reinforcement and withdrawal by the enemy as well as his maneuver options. Frequently the OP's were reinforced to squad size and served as flank security. The command post was situated in a central location, often on terrain which afforded it a vantage point overlooking the ambush site.
Recon methods: Recon elements observing a potential ambush target on the move generally stayed 300–500 meters away. Sometimes a "leapfrogging" recon technique was used. Surveillance units were echeloned one behind the other. As the enemy drew close to the first, it fell back behind the last recon team, leaving an advance group in its place. This one in turn fell back as the enemy again closed the gap, and the cycle rotated. This method helped keep the enemy under continuous observation from a variety of vantage points, and allowed the recon groups to cover one another.

</doc>
<doc id="1908" url="http://en.wikipedia.org/wiki?curid=1908" title="Abzyme">
Abzyme

An abzyme (from antibody and enzyme), also called "catmab" (from "catalytic monoclonal antibody"), and most often called "catalytic antibody", is a monoclonal antibody with catalytic activity. Abzymes are usually raised in lab animals immunized against synthetic haptans, but some natural abzymes can be found in normal humans (anti-vasoactive intestinal peptide autoantibodies) and in patients with autoimmune diseases such as systemic lupus erythematosus, where they can bind to and hydrolyze DNA. To date abzymes display only weak, modest catalytic activity and have not proved to be of any practical use. They are, however, subjects of considerable academic interest. Studying them has yielded important insights into reaction mechanisms, enzyme structure and function, catalysis, and the immune system itself.
Enzymes function by lowering the activation energy of the transition state of a chemical reaction, thereby enabling the formation of an otherwise less-favorable molecular intermediate between the reactant(s) and the product(s). If an antibody is developed to bind to a molecule that's structurally and electronically similar to the transition state of a given chemical reaction, the developed antibody will bind to, and stabilize, the transition state, just like a natural enzyme, lowering the activation energy of the reaction, and thus catalyzing the reaction. By raising an antibody to bind to a stable transition-state analog, a new and unique type of enzyme is produced.
So far, all catalytic antibodies produced have displayed only modest, weak catalytic activity. The reasons for low catalytic activity for these molecules have been widely discussed. Possibilities indicate that factors beyond the binding site may play an important, in particular through protein dynamics. Some abzymes have been engineered to use metal ions and other cofactors to improve their catalytic activity.
History.
The possibility of catalyzing a reaction by means of an antibody which binds the transition state was first suggested by William P. Jencks in 1969. In 1994, Peter G. Schultz and Richard A. Lerner received the prestigious Wolf Prize in Chemistry for developing catalytic antibodies for many reactions and popularizing their study into a significant sub-field of enzymology.
Potential HIV treatment.
In a June 2008 issue of the journal Autoimmunity Review, researchers S Planque, Sudhir Paul, Ph.D, and Yasuhiro Nishiyama, Ph.D of the University Of Texas Medical School at Houston announced that they have engineered an abzyme that degrades the superantigenic region of the gp120 CD4 binding site. This is the one part of the HIV virus outer coating that does not change, because it is the attachment point to T lymphocytes, the key cell in cell-mediated immunity. Once infected by HIV, patients produce antibodies to the more changeable parts of the viral coat. The antibodies are ineffective because of the virus' ability to change their coats rapidly. Because this protein gp120 is necessary for HIV to attach, it does not change across different strains and is a point of vulnerability across the entire range of the HIV variant population.
The abzyme does more than bind to the site, it catalytically destroys the site, rendering the virus inert, and then can attack other HIV viruses. A single abzyme molecule can destroy thousands of HIV viruses.

</doc>
<doc id="1909" url="http://en.wikipedia.org/wiki?curid=1909" title="Adaptive radiation">
Adaptive radiation

In evolutionary biology, adaptive radiation is a process in which organisms diversify rapidly into a multitude of new forms, particularly when a change in the environment makes new resources available, creates new challenges and opens environmental niches. Starting with a recent single ancestor, this process results in the speciation and phenotypic adaptation of an array of species exhibiting different morphological and physiological traits with which they can exploit a range of divergent environments.
Adaptive radiation, a characteristic example of cladogenesis, can be graphically illustrated as a "bush", or clade, of coexisting species (on the tree of life).
 Caribbean anoline lizards are a particularly interesting example of an adaptive radiation. The Hawaiian islands are very isolated and contribute numerous examples of adaptive radiation. An exceptional example of adaptive radiation would be the avian species of the Hawaiian honeycreepers. Via natural selection, these birds adapted rapidly and converged based on the different environments of the Hawaiian islands.
Much research has been done on adaptive radiation due to its dramatic effects on the diversity of a population. However, more research is needed, especially to fully understand the many factors affecting adaptive radiation. Both empirical and theoretical approaches are helpful, though each has its disadvantages. In order to procure the largest amount of data, empirical and theoretical approaches must be united.
Identification.
Four features can be used to identify an adaptive radiation:
Causes.
Innovation.
The evolution of a novel feature may permit a clade to diversify by making new areas of morphospace accessible. A classic example is the evolution of a fourth cusp in the mammalian tooth. This trait permits a vast increase in the range of foodstuffs which can be fed on. Evolution of this character has thus increased the number of ecological niches available to mammals. The trait arose a number of times in different groups during the Cenozoic, and in each instance was immediately followed by an adaptive radiation. Birds find other ways to provide for each other, i.e. the evolution of flight opened new avenues for evolution to explore, initiating an adaptive radiation.
Other examples include placental gestation (for eutherian mammals), or bipedal locomotion (in hominins).
Opportunity.
Adaptive radiations often occur as a result of an organism arising in an environment with unoccupied niches, such as a newly formed lake or isolated island chain. The colonizing population may diversify rapidly to take advantage of all possible niches.
In Lake Victoria, an isolated lake which formed recently in the African rift valley, over 300 species of cichlid fish adaptively radiated from one parent species in just 15,000 years.
Adaptive radiations commonly follow mass extinctions: following an extinction, many niches are left vacant. A classic example of this is the replacement of the non-avian dinosaurs with mammals at the end of the Cretaceous, and of brachiopods by bivalves at the Permo-Triassic boundary.
Examples.
Darwin's Finches.
One famous example where adaptive radiation is seen is with Darwin's finches. It has been observed by many evolutionary biologists that fragmented landscapes oftentimes are a prime location for adaptive radiation to occur. The differences in geography throughout disjointed landscapes such as islands are believed to promote such diversification. Darwin’s finches occupy the fragmented landscape of the Galapagos Islands and are diversified into many different species which differ in ecology, song, and morphology, specifically the size and shapes of their beaks. The first obvious explanation for these differences is allopatric speciation, speciation that occurs when populations of the same species become isolated geographically and evolve separately. Because the finches are divided amongst the islands, the birds have been evolving separately for several million years. However, this does not account for the fact that many of the species occur in sympatry, with seven or more species inhabiting the same island. This raises the question as to why these species split when living in the same environment with all the same resources. Petren, Grant, Grant, and Keller proposed that the speciation of the finches occurred in two parts: an initial, easily observable allopatric event followed by a less clear sympatric event. This sympatric event which occurred second was adaptive radiation. This occurred largely to promote specialization upon each island. One major morphological difference among species sharing one island is beak size and shape. Adaptive radiation led to the evolution of different beaks which could access different food and resources. Those with short beaks are better adapted to eating seeds on the ground, those with thin, sharp beaks eat insects, and those with long beaks use their beaks to probe for food inside cacti. With these specializations, seven or more species of finches are able to inhabit the same environments without competition or lack of resources killing several off. In other words, these morphological differences in beak size and shape brought about by adaptive radiation allow the island diversification to persist.
Cichlid Fish.
Another famous example is the cichlid fishes of the Great Rift Valley in East Africa. The lakes in that area are believed to support and sustain about 2,000 different species of these fish, each with different ecological and morphological characteristics such as body size. Like the Galapagos Islands, these lakes form a fragmented landscape that isolates the cichlid fish from one another, allowing them, and many of the organisms they live with, to evolve separately. The diversity of the lakes is in fact quite extraordinary because the adaptive radiation here is so young and so new. One thing that has interested scientists about the cichlid fish case is the possibility of convergent evolution. Some believe that, though these species are isolated, some may have developed analogous structures that were not present in the last common ancestor simply by living in similar environments. Studies on the convergent evolution of the fish are limited, but Losos claims that while some convergence may have occurred, the adaptive radiation in the area has successfully created many species which are ecomorphologically diverse and specialized as seen partially with their differing body sizes. Rather, the convergence is mainly observed in pictures of the similar-looking species of cichlid fish. These pictures, however, do not prove that they are similar in any way besides appearance. It would seem that the adaptive radiation thoroughly diversified the species so that none are similar anymore.
Hawaiian Honeycreepers.
Another example of an adaptive radiation would be an endemic species of the Hawaiian Islands. The Hawaiian Honeycreepers are a large, highly diverse species which have been part of a vast adaptive radiation, that began as the Hawaiian Islands started to form. The Honeycreeper species was shaped by island formation and natural selection. The mechanism by which this adaptive radiation occurred, can be described as allopatric speciation via the peripheral isolate model. Each time a new island formed, a dispersal event would occur which would result in new community structures on each island. New selection pressures forced the adaptive radiation of the Hawaiian Honeycreepers, as they needed to exploit new resources from the different environments of each island. It has been determined that many of the similar morphologies and behaviors of the Hawaiian Honeycreepers, located on distant islands, are due to convergence of analogous traits caused by similar environments.
Hawaiian Silverswords.
Though the most famously recognized cases of adaptive radiation have occurred in animals such as Darwin’s finches or the cichlid fish, adaptive radiation certainly occurs in plant species as well. The most famous example of adaptive radiation in plants is quite possibly the Hawaiian silverswords. The Hawaiian silversword alliance consists of twenty-eight species of Hawaiian plants which range from trees to shrubs to vines. This is exceptional diversification as can be seen through the significant morphological differences between each species of the Hawaiian silverswords. With some species, it’s virtually impossible to distinguish visually that they were ever part of one species to begin with. These radiations occurred millions of years ago, but through studies over the past few decades, it has been suggested that the rate of speciation and diversification was extremely high. These high rates, as well as the fragmented landscape of the Hawaiian Islands, are key characteristics which point directly to adaptive radiation.
Anolis lizards.
Anolis lizards have been radiating widely in many different environments, including Central and South America, as well as the West Indies and experience great diversity of species just as the finches, cichlid fish, and silverswords. Studies have been done to determine whether radiations occur similarly for these lizards on the mainland as they do on the Caribbean islands or if differences can be observed in how they speciated. It has been observed that in fact, the radiations are very different, and ecological and morphological characteristics that these lizards developed as part of their speciation on the islands and on the mainland are unique. They have clearly evolved differently to the environments they inhabit. The environmental pressures on the Anolis lizards are not the same on the mainland as they are on the islands. There is a significantly larger amount of predators preying on the Anolis lizards on the mainland. This is but one environmental difference. Other factors play a role in what sort of adaptive radiation will develop. Among the Caribbean islands, a larger perch diameter correlates with longer forelimbs, larger body mass, longer tails, and longer hind limbs. However, on the mainland, a larger perch diameter correlates with shorter tails. This shows that these lizards adapted differently to their environment depending on whether they were located on the mainland or the islands. These differing characteristics reconfirm that most of the adaptive radiation between the mainland and the islands occurred independently. On the islands specifically, species have adapted to certain “microhabitats” in which they require different morphological traits to survive. Irschick (1997) divides these microhabitats into six groups: “trunk–ground, trunk–crown, grass–bush, crown–giant, twig, and trunk.” Different groups of lizards would acquire traits for one of these particular areas that made them more specialized for survival in this microhabitat and not so much in others. Adaptive radiation allows species to acquire the traits they need to survive in these microhabitats and reduce competition to allow the survival of a greater number of organisms as seen in many of the examples before.

</doc>
<doc id="1910" url="http://en.wikipedia.org/wiki?curid=1910" title="Agarose gel electrophoresis">
Agarose gel electrophoresis

Agarose gel electrophoresis is a method of gel electrophoresis used in biochemistry, molecular biology, and clinical chemistry to separate a mixed population of DNA or proteins in a matrix of agarose. The proteins may be separated by charge and/or size (isoelectric focusing agarose electrophoresis is essentially size independent), and the DNA and RNA fragments by length. Biomolecules are separated by applying an electric field to move the charged molecules through an agarose matrix, and the biomolecules are separated by size in the agarose gel matrix.
Agarose gels are easy to cast and are particularly suitable for separating DNA of size range most often encountered in laboratories, which accounts for the popularity of its use. The separated DNA may be viewed with stain, most commonly under UV light, and the DNA fragments can be extracted from the gel with relative ease. Most agarose gels used are between 0.7 - 2% dissolved in a suitable electrophoresis buffer.
Properties of agarose gel.
Agarose gel is a three-dimensional matrix formed of helical agarose molecules in supercoiled bundles that are aggregated into three-dimensional structures with channels and pores through which biomolecules can pass. The 3-D structure is held together with hydrogen bonds and can therefore be disrupted by heating back to a liquid state. The melting temperature is different from the gelling temperature, depending on the sources, agarose gel has a gelling temperature of 35-42°C and a melting temperature of 85-95°C. Low-melting and low-gelling agaroses made through chemical modifications are also available.
Agarose gel has large pore size and good gel strength that made it particularly suitable as an anticonvection medium for the electrophoresis of DNA and large protein molecules. The pore size of a 1% gel has been estimated from 100 nm to 200-500 nm, and its gel strength allows gels as dilute as 0.15% to form slab for gel electrophoresis. Low-concentration gels (0.1 - 0.2%) however are fragile and therefore hard to handle. Agarose gel has lower resolving power than polyacrylamide gel for DNA but has a greater range of separation, and is therefore used for DNA fragments of usually 50-20,000 bp in size. The limit of resolution for standard agarose gel electrophoresis is around 750 kb, but resolution of over 6 Mb is possible with pulsed field gel electrophoresis (PFGE). It can also be used to separate large protein, and it is the preferred matrix for the gel electrophoresis of particles with effective radii larger than 5-10 nm. A 0.9% agarose gel has pores large enough for the entry of bacteriophage T4.
The agarose polymer contains charged groups, in particular pyruvate and sulphate. These negatively charged groups create a flow of water in the opposite direction to the movement of DNA in a process called electroendosmosis (EEO), and can therefore retard the movement of DNA and cause blurring of bands. Higher concentration gel would have higher electroosmotic flow. Low EEO agarose is therefore generally preferred for use in agarose gel electrophoresis of nucleic acids, but high EEO agarose may be used for other purposes. The lower sulphate content of low EEO agarose, particularly low-melting point (LMP) agarose, is also beneficial in cases where the DNA extracted from gel is to be used for further manipulation as the presence of contaminating sulphate may affect some subsequent procedures, such as ligation and PCR. Zero EEO agaroses however are undesirable for some applications as they may be made by adding positively charged group and such groups can affect subsequent enzyme reactions.
Migration of nucleic acids in agarose gel.
Factors affect migration of nucleic acid in gel.
A number of factors can affect the migration of nucleic acids: the dimension of the gel pores (gel concentration), size of DNA being electrophoresed, the voltage used, the ionic strength of the buffer, and the concentration of intercalating dye such as ethidium bromide if used during electrophoresis.
Smaller molecules travel faster than larger molecules in gel, and double-stranded DNA moves at a rate that is inversely proportional to the log10 of the number of base pairs. This relationship however breaks down with very large DNA fragments, and separation of very large DNA fragments requires the use of pulsed field gel electrophoresis (PFGE).
For standard agarose gel electrophoresis, larger molecules are resolved better using a low concentration gel while smaller molecules separate better at high concentration gel. High concentrations gel however requires longer run times (sometimes days).
The movement of the DNA may be affected by the conformation of the DNA molecule, for example, supercoiled DNA usually moves faster than relaxed DNA because it is tightly coiled and hence more compact. In a normal plasmid DNA preparation, multiple forms of DNA may be present. Gel electrophoresis of the plasmids would normally show the negatively supercoiled form as the main band, while nicked DNA (open circular form) and the relaxed closed circular form appears as minor bands. The rate at which the various forms move however can change using different electrophoresis conditions, and the mobility of larger circular DNA may be more strongly affected than linear DNA by the pore size of the gel.
Ethidium bromide which intercalates into circular DNA can change the charge, length, as well as the superhelicity of the DNA molecule, therefore its presence in gel during electrophoresis can affect its movement. Agarose gel electrophoresis can be used to resolve circular DNA with different supercoiling topology.
DNA damage due to increased cross-linking will also reduce electrophoretic DNA migration in a dose-dependent way.
The rate of migration of the DNA is proportional to the voltage applied, i.e. the higher the voltage, the faster the DNA moves. The resolution of large DNA fragments however is lower at high voltage. The mobility of DNA may also change in an unsteady field - in a field that is periodically reversed, the mobility of DNA of a particular size may drop significantly at a particular cycling frequency. This phenomenon can result in band inversion in field inversion gel electrophoresis (FIGE), whereby larger DNA fragments move faster than smaller ones.
Mechanism of migration and separation.
The negative charge of its phosphate backbone moves the DNA towards the positively charged anode during electrophoresis. However, the migration of DNA molecules in solution, in the absence of a gel matrix, is independent of molecular weight during electrophoresis. The gel matrix is therefore responsible for the separation of DNA by size during electrophoresis, and a number of models exist to explain the mechanism of separation of biomolecules in gel matrix. A widely accepted one is the Ogston model which treats the polymer matrix as a sieve. A globular protein or a random coil DNA moves through the interconnected pores, and the movement of larger molecules is more likely to be impeded and slowed down by collisions with the gel matrix, and the molecules of different sizes can therefore be separated in this sieving process.
The Ogston model however breaks down for large molecules whereby the pores are significantly smaller than size of the molecule. For DNA molecules of size greater than 1 kb, a reptation model (or its variants) is most commonly used. This model assumes that the DNA can crawl in a "snake-like" fashion (hence "reptation") through the pores as an elongated molecule. At higher electric field strength, this turned into a biased reptation model, whereby the leading end of the molecule become strongly biased in the forward direction and pulls the rest of the molecule along. Real-time fluorescence microscopy of stained molecules, however, showed more subtle dynamics during electrophoresis, with the DNA showing considerable elasticity as it alternately stretching in the direction of the applied field and then contracting into a ball, or becoming hooked into a U-shape when it gets caught on the polymer fibres.
General procedure.
The details of an agarose gel electrophoresis experiment may vary depending on methods, but most follow a general procedure.
Casting of gel.
The gel is prepared by dissolving the agarose powder in an appropriate buffer, such as TAE or TBE, to be used in electrophoresis. The agarose is dispersed in the buffer before heating it to near-boiling point, but avoid boiling. The melted agarose is allowed to cool sufficiently before pouring the solution into a cast as the cast may warp or crack if the agarose solution is too hot. A comb is placed in the cast to create wells for loading sample, and the gel should be completely set before use.
The concentration of gel affects the resolution of DNA separation. For a standard agarose gel electrophoresis, a 0.8% gives good separation or resolution of large 5–10kb DNA fragments, while 2% gel gives good resolution for small 0.2–1kb fragments. 1% gels are common for many applications. The concentration is measured in weight of agarose over volume of buffer used. High percentage gels are often brittle and may not set evenly, while low percentage gels (0.1-0.2%) are fragile and not easy to handle. Low-melting-point (LMP) agarose gels are also more fragile than normal agarose gel. PFGE and FIGE are often done with high percentage agarose gels.
Loading of samples.
Once the gel has set, the comb is removed, leaving wells where DNA samples can be loaded. Loading buffer is mixed with the DNA sample before the mixture is loaded into the wells. The loading buffer contains a dense compound, which may be glycerol, sucrose, or Ficoll, that raises the density of the sample so that the DNA sample may sink to the bottom of the well. If the DNA sample contains residual ethanol after its preparation, it may float out of the well. The loading buffer also include colored dyes such as xylene cyanol and bromophenol blue used to monitor the progress of the electrophoresis. The DNA samples are loaded using a pipette.
Electrophoresis.
Agarose gel electrophoresis is most commonly done horizontally in a submarine mode whereby the slab gel is completely submerged in buffer during electrophoresis. It is also possible, but less common, to perform the electrophoresis vertically, as well as horizontally with the gel raised on agarose legs using the appropriate apparatus. The buffer used in the gel is the same as the running buffer in the electrophoresis tank, which is why electrophoresis in the submarine mode is possible with agarose gel.
For optimal resolution of DNA greater than 2kb in size in standard gel electrophoresis, 5 to 8 V/cm is recommended (the distance in cm refers to the distance between electrodes, therefore this recommended voltage would be 5 to 8 multiplied by the distance between the electrodes in cm). Voltage may also be limited by the fact that it heats the gel and may cause the gel to melt if it is run at high voltage for a prolonged period, especially if the gel used is LMP agarose gel. Too high a voltage may also reduce resolution, as well as causing band streaking for large DNA molecules. Too low a voltage may lead to broadening of band for small DNA fragments due to dispersion and diffusion.
Since DNA is not visible in natural light, the progress of the electrophoresis is monitored using colored dyes. Xylene cyanol (light blue color) comigrates large DNA fragments, while Bromophenol blue (dark blue) comigrates with the smaller fragments. Less commonly used dyes include Cresol Red and Orange G which migrate ahead of bromophenol blue. A DNA marker is also run together for the estimation of the molecular weight of the DNA fragments. Note however that the size of a circular DNA like plasmids cannot be accurately gauged using standard markers unless it has been linearized by restriction digest, alternatively a supercoiled DNA marker may be used.
Staining and visualization.
DNA as well as RNA are normally visualized by staining with ethidium bromide, which intercalates into the major grooves of the DNA and fluoresces under UV light. The ethidium bromide may be added to the agarose solution before it gels, or the DNA gel may be stained later after electrophoresis. Destaining of the gel is not necessary but may produce better images. Other methods of staining are available; examples are SYBR Green, GelRed, methylene blue, brilliant cresyl blue, Nile blue sulphate, and crystal violet. SYBR Green, GelRed and other similar commercial products are sold as safer alternatives to ethidium bromide as it has been shown to be mutagenic in Ames test, although the carcinogenicity of ethidium bromide has not actually been established. SYBR Green requires the use of a blue-light transilluminator. DNA stained with crystal violet can be viewed under natural light without the use of a UV transilluminator which is an advantage, however it may not produce a strong band.
When stained with ethidium bromide, the gel is viewed with an ultraviolet (UV) transilluminator. Standard transilluminators use wavelengths of 302/312-nm (UV-B), however exposure of DNA to UV radiation for as little as 45 seconds can produce damage to DNA and affect subsequent procedures, for example reducing the efficiency of transformation, "in vitro" transcription, and PCR. Exposure of the DNA to UV radiation therefore should be limited. Using a higher wavelength of 365 nm (UV-A range) causes less damage to the DNA but also produces much weaker fluorescence with ethidium bromide. Where multiple wavelengths can be selected in the transillumintor, the shorter wavelength would be used to capture images, while the longer wavelength should be used when it is necessary to work on the gel for any extended period of time.
The transilluminator apparatus may also contain image capture devices, such as a digital or polaroid camera, that allow an image of the gel to be taken or printed.
Downstream procedures.
The separated DNA bands are often used for further procedures, and a DNA band may be cut out of the gel as a slice, dissolved and purified. The gels may also be used for blotting techniques.
Buffers.
In general, the ideal buffer should have good conductivity, produce less heat and have a long life. There are a number of buffers used for agarose electrophoresis; common ones for nucleic acids include Tris/Acetate/EDTA (TAE) and Tris/Borate/EDTA (TBE). Many other buffers have been proposed, e.g. lithium borate (LB), which is almost never used, based on Pubmed citations, iso electric histidine, pK matched goods buffers, etc.; in most cases the purported rationale is lower current (less heat) and or matched ion mobilities, which leads to longer buffer life. Tris-phosphate buffer has high buffering capacity but cannot be used if DNA extracted is to be used in phosphate sensitive reaction. Borate is problematic; Borate can polymerize, and/or interact with cis diols such as those found in RNA. TAE has the lowest buffering capacity but provides the best resolution for larger DNA. This means a lower voltage and more time, but a better product. LB is relatively new and is ineffective in resolving fragments larger than 5 kbp; However, with its low conductivity, a much higher voltage could be used (up to 35 V/cm), which means a shorter analysis time for routine electrophoresis. As low as one base pair size difference could be resolved in 3% agarose gel with an extremely low conductivity medium (1 mM lithium borate). The buffers used contain EDTA to inactivate many nucleases which require divalent cation for their function.
Applications.
Agarose gels are easily cast and handled compared to other matrices and nucleic acids are not chemically altered during electrophoresis. Samples are also easily recovered. After the experiment is finished, the resulting gel can be stored in a plastic bag in a refrigerator.
Electrophoresis is performed in buffer solutions to reduce pH changes due to the electric field, which is important because the charge of DNA and RNA depends on pH, but running for too long can exhaust the buffering capacity of the solution. Further, different preparations of genetic material may not migrate consistently with each other, for morphological or other reasons.

</doc>
<doc id="1911" url="http://en.wikipedia.org/wiki?curid=1911" title="Allele">
Allele

An allele ( or ), or allel, is one of a number of alternative forms of the same gene or same genetic locus. Sometimes, different alleles can result in different observable phenotypic traits, such as different pigmentation. However, most genetic variations result in little or no observable variation.
Most multicellular organisms have two sets of chromosomes; that is, they are diploid. These chromosomes are referred to as homologous chromosomes. Diploid organisms have one copy of each gene (and, therefore, one allele) on each chromosome. If both alleles are the same, they and the organism are homozygous with respect to that gene. If the alleles are different, they and the organism are heterozygous with respect to that gene.
The word "allele" is a short form of allelomorph ("other form"), which was used in the early days of genetics to describe variant forms of a gene detected as different phenotypes. It derives from the Greek prefix "ἀλλήλ", "allel", meaning "reciprocal" or "each other", which itself is related to the Greek adjective ἄλλος (allos; cognate with Latin "alius"), meaning "other".
Dominant and recessive alleles.
In many cases, genotypic interactions between the two alleles at a locus can be described as dominant or recessive, according to which of the two homozygous phenotypes the heterozygote most resembles. Where the heterozygote is indistinguishable from one of the homozygotes, the allele involved is said to be dominant to the other, which is said to be recessive to the former. The degree and pattern of dominance varies among loci. This type of interaction was first formally described by Gregor Mendel. However, many traits defy this simple categorization and the phenotypes are modeled by co-dominance and polygenic inheritance.
The term "wild type" allele is sometimes used to describe an allele that is thought to contribute to the typical phenotypic character as seen in "wild" populations of organisms, such as fruit flies ("Drosophila melanogaster"). Such a "wild type" allele was historically regarded as dominant, common, and normal, in contrast to "mutant" alleles regarded as recessive, rare, and frequently deleterious. It was formerly thought that most individuals were homozygous for the "wild type" allele at most gene loci, and that any alternative "mutant" allele was found in homozygous form in a small minority of "affected" individuals, often as genetic diseases, and more frequently in heterozygous form in "carriers" for the mutant allele. It is now appreciated that most or all gene loci are highly polymorphic, with multiple alleles, whose frequencies vary from population to population, and that a great deal of genetic variation is hidden in the form of alleles that do not produce obvious phenotypic differences.
Multiple alleles.
A population or species of organisms typically includes multiple alleles at each locus among various individuals. Allelic variation at a locus is measurable as the number of alleles (polymorphism) present, or the proportion of heterozygotes in the population.
For example, at the gene locus for the ABO blood type carbohydrate antigens in humans, classical genetics recognizes three alleles, IA, IB, and i, that determine compatibility of blood transfusions. Any individual has one of six possible genotypes (IAIA, IAi, IBIB, IBi, IAIB, and ii) that produce one of four possible phenotypes: "Type A" (produced by IAIA homozygous and IAi heterozygous genotypes), "Type B" (produced by IBIB homozygous and IBi heterozygous genotypes), "Type AB" produced by IAIB heterozygous genotype, and "Type O" produced by ii homozygous genotype. It is now known that each of the A, B, and O alleles is actually a class of multiple alleles with different DNA sequences that produce proteins with identical properties: more than 70 alleles are known at the ABO locus. An individual with "Type A" blood may be an AO heterozygote, an AA homozygote, or an AA heterozygote with two different "A" alleles.
Allele and genotype frequencies.
The frequency of alleles in a diploid population can be used to predict the frequencies of the corresponding genotypes (see Hardy-Weinberg principle). For a simple model, with two alleles:
where "p" is the frequency of one allele and "q" is the frequency of the alternative allele, which necessarily sum to unity. Then, "p"2 is the fraction of the population homozygous for the first allele, 2"pq" is the fraction of heterozygotes, and "q"2 is the fraction homozygous for the alternative allele. If the first allele is dominant to the second then the fraction of the population that will show the dominant phenotype is "p"2 + 2"pq", and the fraction with the recessive phenotype is "q"2.
With three alleles:
In the case of multiple alleles at a diploid locus, the number of possible genotypes (G) with a number of alleles (a) is given by the expression:
Allelic dominance in genetic disorders.
A number of genetic disorders are caused when an individual inherits two recessive alleles for a single-gene trait. Recessive genetic disorders include Albinism, Cystic Fibrosis, Galactosemia, Phenylketonuria (PKU), and Tay-Sachs Disease. Other disorders are also due to recessive alleles, but because the gene locus is located on the X chromosome, so that males have only one copy (that is, they are hemizygous), they are more frequent in males than in females. Examples include red-green color blindness and Fragile X syndrome.
Other disorders, such as Huntington disease, occur when an individual inherits only one dominant allele.

</doc>
<doc id="1912" url="http://en.wikipedia.org/wiki?curid=1912" title="Ampicillin">
Ampicillin

Ampicillin is an antibiotic useful for the treatment of a number of bacterial infections. It is a beta-lactam antibiotic that is part of the aminopenicillin family and is roughly equivalent to amoxicillin in terms of activity. It is taken either orally or intravenously. It is active against many Gram-positive and Gram-negative bacteria.
It is effective for ear infections and respiratory infections such as sinusitis caused by bacteria, acute exacerbations of COPD, and epiglottitis. It is also sometimes used for the treatment of urinary tract infections, meningitis, and salmonella infections, but resistance to ampicillin is increasingly common among the bacteria responsible for these infections. 
Common side effects include rash, diarrhea, nausea and vomiting.<Ref name =FDA></ref> It is not useful for the treatment of viral infections. 
It is on the World Health Organization's List of Essential Medicines, a list of the most important medication needed in a basic health system.
Medical uses.
Ampicillin is active against Gram-(+) bacteria including "Streptococcus pneumoniae", "Streptococcus pyogenes", some isolates of "Staphylococcus aureus" (but not penicillin-resistant or methicillin-resistant strains), and some "Enterococci". Activity against Gram-(-) bacteria includes "Neisseria meningitidis", some "Haemophilus influenzae", and some Enterobacteriaceae. Its spectrum of activity is enhanced by co-administration of sulbactam, a drug that inhibits beta lactamase, an enzyme produced by bacteria to inactivate ampicillin and related antibiotics.
It is used for the treatment of infections known to be or highly likely to be caused by these bacteria. These include common respiratory infections including sinusitis, bronchitis, and pharyngitis, as well as otitis media. In combination with vancomycin (which provides coverage of ampicillin-resistant pneumococci), it is effective for the treatment of bacterial meningitis. It is also used for gastrointestinal infections caused by consuming contaminated water or food, such as "Salmonella", "Shigella", and "Listeriosis".
Ampicillin is a first-line agent for the treatment of infections caused by "Enterococci". The bacteria are an important cause of healthcare-associated infections such as endocarditis, meningitis, and catheter-associated urinary tract infections that are typically resistant to other antibiotics.
Side effects.
Ampicillin is relatively non-toxic. Its most common side effects include rash, diarrhea, nausea and vomiting.<Ref name =FDA></ref> In very rare cases it causes severe side effects such as angioedema, anaphylaxis and "Clostridium difficile" diarrhea.
Mechanism of action.
Belonging to the penicillin group of beta-lactam antibiotics, ampicillin is able to penetrate Gram-positive and some Gram-negative bacteria. It differs from penicillin G, or benzylpenicillin, only by the presence of an amino group. That amino group helps the drug penetrate the outer membrane of Gram-negative bacteria.
Ampicillin acts as an irreversible inhibitor of the enzyme transpeptidase, which is needed by bacteria to make their cell walls. It inhibits the third and final stage of bacterial cell wall synthesis in binary fission, which ultimately leads to cell lysis; therefore ampicillin is usually bacteriocidal.
History.
Ampicillin has been used extensively to treat bacterial infections since 1961. Until the introduction of ampicillin by the British company Beecham, penicillin therapies had only been effective against Gram-positive organisms such as staphylococci and streptococci. Ampicillin (originally branded as 'Penbritin') also demonstrated activity against Gram-negative organisms such as "H. influenzae", coliforms and "Proteus" spp. 

</doc>
<doc id="1913" url="http://en.wikipedia.org/wiki?curid=1913" title="Annealing">
Annealing

Annealing may refer to:

</doc>
<doc id="1914" url="http://en.wikipedia.org/wiki?curid=1914" title="Antimicrobial resistance">
Antimicrobial resistance

Antimicrobial resistance (AMR) is when microbes are resistant to one or more antimicrobial agents, used to treat infection or as an antiseptic. Resistance is either innate or acquired – referring to whether the resistance is newly developed or has always been present in an organism. Most commonly antimicrobial resistance refers to acquired resistance – which may be a result of either novel mutation or transfer of genes causing resistance. All classes of microbes may develop resistance to medication: fungi – antifungal resistance, viruses – antiviral resistance, protozoans antiprotozoal resistance and bacteria – antibiotic resistance. Microbes which are resistant to multiple antimicrobials are termed "multidrug resistant" (MDR) (or, sometimes in the lay press, "superbugs"). Antimicrobial resistance is a growing problem in the world, and causes millions of deaths every year.
The increasing rates of antibiotic resistant infections are caused by antibiotic use both within human and veterinary medicine. Any use of antibiotics can increase selective pressure in a population of bacteria that promotes resistant bacteria to thrive and the susceptible bacteria to die off. As resistance to antibiotics becomes more common, a greater need for alternative treatments arises. Despite a call for new antibiotic therapies, there has been a continued decline in the number of newly approved drugs. Common types of drug-resistant bacteria include methicillin-resistant "Staphylococcus aureus" (MRSA), vancomycin-resistant "S. aureus" (VRSA), extended spectrum beta-lactamase (ESBL), vancomycin-resistant "Enterococcus" (VRE) and multidrug-resistant "A. baumannii" (MRAB). Viruses, fungi, and parasites can also become resistant to agents to which they were once susceptible. Resistance may take the form of a spontaneous or induced genetic mutation, or the acquisition of resistance genes from other bacterial species by horizontal gene transfer via conjugation, transduction, or transformation. Infection by resistant microbes may be community acquired or healthcare associated.
Antibiotic resistance poses a grave and growing global problem: a World Health Organization report released April 2014 stated, "this serious threat is no longer a prediction for the future, it is happening right now in every region of the world and has the potential to affect anyone, of any age, in any country. Antibiotic resistance—when bacteria change so antibiotics no longer work in people who need them to treat infections—is now a major threat to public health." Genes for resistance to antibiotics, like antibiotics themselves, are ancient.:457–461
Definition.
The WHO defines antimicrobial resistance as a microorganism's resistance to an antimicrobial drug that was once able to treat an infection by that microorganism.
A person cannot become resistant to antibiotics. Resistance is a property of the microbe, not the person or other organism infected by the microbe.
Causes.
Some bacteria with resistance to antibiotics predate the medical use of antibiotics by humans; however, widespread antibiotic use has caused more bacteria to become resistant, a process called evolutionary pressure.
Reasons for the widespread use of antibiotics are manifold and include:
Antibiotic use in livestock feed at low doses for growth promotion is an accepted practice in industrialized countries which leads to resistance. Releasing large quantities of antibiotics into the environment during pharmaceutical manufacturing by inadequate treatment of wastewater contributes to the likelihood of creating antibiotic-resistant strains. It is uncertain whether antibacterials in soaps and other products contribute to antibiotic resistance, but they are discouraged for other reasons.
Human medicine.
Certain antibiotic classes induce resistance more than others. A multiresistant bacterium carries several resistance genes. Increased rates of MRSA infections are seen when using glycopeptides, cephalosporins, and quinolones. Cephalosporins, and particularly quinolones and clindamycin, are more likely to produce colonisation with "Clostridium difficile"[] Increasing bacterial resistance correlates with the volume of antibiotic prescribed, and not lack of compliance with taking antibiotics. Inappropriate prescribing of antibiotics has been attributed to a number of causes, including people insisting on antibiotics, physicians prescribing them as they feel they do not have time to explain why they are not necessary, and physicians not knowing when to prescribe antibiotics or being overly cautious for medical and/or legal reasons. For example, a third of people believe that antibiotics are effective for the common cold, and the common cold is the most common reason antibiotics are prescribed even though antibiotics are useless against viruses. A single regimen of antibiotics even in compliant individuals leads to a greater risk of resistant organisms to that antibiotic in the person for a month to possibly a year.
Antibiotic resistance increases with duration of treatment; therefore, as long as an effective minimum is kept, shorter courses of antibiotics are likely to decrease rates of resistance, reduce cost, and have better outcomes due to fewer complications. Short course regimens exist for community-acquired pneumonia spontaneous bacterial peritonitis, suspected lung infections in ICU patients, in the so-called acute abdomen, middle ear infection, sinusitis and throat infection, and penetrating gut injury. In some situations a short course is inferior to a long course. A BMJ editorial recommended that antibiotics can often be safely stopped 72 hours after symptoms resolve. Because individuals may feel better before the infection is eradicated, doctors must provide instructions to them so they know when it is safe to stop taking a prescription. Some researchers advocate doctors' using a very short course of antibiotics, reevaluating the patient after a few days, and stopping treatment if there are no clinical signs of infection.
Factors within the intensive care unit setting such as mechanical ventilation and multiple underlying diseases also appear to contribute to bacterial resistance. Poor hand hygiene by hospital staff has been associated with the spread of resistant organisms, and an increase in hand washing compliance results in decreased rates of these organisms.
The improper use of antibiotics can often be attributed to the presence of structural violence in particular regions. Socioeconomic factors such as race and poverty affect accessibility of and adherence to drug therapy. The efficacy of treatment programs for drug-resistant strains depends on whether or not programmatic improvements take into account the effects of structural violence.
Veterinary medicine.
The use of antibiotics in animals is partly responsible for the emergence of antibiotic-resistant microorganisms in human medicine. Antibiotic use in animals can be classified into therapeutic, prophylactic, metaphylactic, and growth promotion uses of antibiotics. All four patterns select for bacterial resistance, since antibiotic resistance is a natural evolutionary process, but the non-therapeutic uses expose larger number of animals, and therefore of bacteria, for more extended periods, and at lower doses. They therefore greatly increase the cross-section for the evolution of resistance.
 Since the last third of the 20th century, antibiotics have been used extensively in animal husbandry. In 2013, 80% of antibiotics used in the US were used in animals and only 20% in humans; in 1997 half were used in humans and half in animals. Some antibiotics are not used and not considered significant for use in humans, because they either lack efficacy or purpose in humans, such as ionophores in ruminants, or because the drug has gone out of use in humans. Others are used in both animals and humans, including penicillin and some forms of tetracycline. Historically, regulation of antibiotic use in food animals has been limited to limiting drug residues in meat, egg, and milk products, rather than by direct concern over the development of antibiotic resistance. This mirrors the primary concerns in human medicine, where, in general, researchers and doctors were more concerned about effective but non-toxic doses of drugs rather than antibiotic resistance.
In 2001, the Union of Concerned Scientists estimated that greater than 70% of the antibiotics used in the U.S. are given to food animals (for example, chickens, pigs, and cattle), in the absence of disease. The amounts given are termed "sub-therapeutic", i.e., insufficient to combat disease. Despite no diagnosis of disease, the administration of these drugs (most of which are not significant to human medicine) results in decreased mortality and morbidity and increased growth in the animals so treated. It is theorized that sub-therapeutic dosages kills some, but not all, of the bacterial organisms in the animal — likely leaving those that are naturally antibiotic-resistant. Studies have shown, however, that, in essence, the overall population levels of bacteria are unchanged; only the mix of bacteria is affected. The actual mechanism by which sub-therapeutic antibiotic feed additives serve as growth promoters is thus unclear. Some people have speculated that animals and fowl may have sub-clinical infections, which would be cured by low levels of antibiotics in feed, thereby allowing the creatures to thrive. No convincing evidence has been advanced for this theory, and the bacterial load in an animal is essentially unchanged by use of antibiotic feed additives. The mechanism of growth promotion is therefore probably something other than "killing off the bad bugs."
Antibiotics are used in U.S. animal feed to promote animal productivity. In particular, poultry feed and water is a common route of administration of drugs, due to higher overall costs when drugs are administered by handling animals individually.
In research studies, occasional animal-to-human spread of drug-resistant organisms has been demonstrated. Resistant bacteria can be transmitted from animals to humans in three ways: by consuming animal products (milk, meat, eggs, etc.), from close or direct contact with animals or other humans, or through the environment. In the first pathway, food preservation methods can help eliminate, decrease, or prevent the growth of bacteria in some food classes. Evidence for the transfer of antibiotic-resistant microorganisms from animals to humans has been scant, and most evidence shows that pathogens of concern in human populations originated in humans and are maintained there, with rare cases of transference to humans.
The World Health Organization concluded that inappropriate use of antibiotics in animal husbandry is an underlying contributor to the emergence and spread of antibiotic-resistant germs, and that the use of antibiotics as growth promoters in animal feeds should be prohibited. Regarding this matter, the World Organisation for Animal Health has added to the Terrestrial Animal Health Code a series of guidelines with recommendations to its members for the creation and harmonization of national antimicrobial resistance surveillance and monitoring programs, monitoring of the quantities of antibiotics used in animal husbandry, and recommendations to ensure the proper and prudent use of antibiotic substances. Another guideline is to implement methodologies that help to establish associated risk factors and assess the risk of antibiotic resistance.
Natural occurrence.
Naturally occurring antibiotic resistance is common. The genes that confer this resistance are known as the environmental resistome. These genes may be transferred from non-disease-causing bacteria to those that do cause disease, leading to clinically significant antibiotic resistance. In 1952 it was shown that penicillin-resistant bacteria existed before penicillin treatment; and also preexistent bacterial resistance to streptomycin. In 1962, the presence of penicillinase was detected in dormant endospores of "Bacillus licheniformis", revived from dried soil on the roots of plants, preserved since 1689 in the British Museum. Six strains of "Clostridium", found in the bowels of William Braine and John Hartnell (members of the Franklin Expedition) showed resistance to cefoxitin and clindamycin. Penicillinase may have emerged as a defense mechanism for bacteria in their habitats, such as the case of penicillinase-rich "Staphylococcus aureus", living with penicillin-producing "Trichophyton", however this may be circumstantial. Search for a penicillinase ancestor has focused on the class of proteins that must be "a priori" capable of specific combination with penicillin. The resistance to cefoxitin and clindamycin in turn was attributed to Braine's and Hartnell's contact with microorganisms that naturally produce them or random mutation in the chromosomes of "Clostridium" strains. There is evidence that heavy metals and other pollutants may select for antibiotic-resistant bacteria, generating a constant source of them in small numbers.:34
Environmental impact.
Antibiotics have been polluting the environment since their introduction through human waste (medication, farming), animals, and the pharmaceutical industry. Along with antibiotic waste, resistant bacteria follow, thus introducing antibiotic-resistant bacteria into the environment. As bacteria replicate quickly, the resistant bacteria that enter the environment replicate their resistance genes as they continue to divide. In addition, bacteria carrying resistance genes have the ability to spread those genes to other species via horizontal gene transfer. Therefore, even if the specific antibiotic is no longer introduced into the environment, antibiotic-resistance genes will persist through the bacteria that have since replicated without continuous exposure.
A study the Poudre River (Colorado, United States) implicated wastewater treatment plants, as well as animal-feeding operations in the dispersal of antibiotic-resistance genes into the environment. This research was done using molecular signatures in order to determine the sources, and the location at the Poudre River was chosen due to lack of other anthropogenic influences upstream. The study indicates that monitoring of antibiotic-resistance genes may be useful in determining not only the point of origin of their release but also how these genes persist in the environment. In addition, studying physical and chemical methods of treatment may alleviate pressure of antibiotic-resistance genes in the environment, and thus their entry back into human contact.
Mechanisms.
The four main mechanisms by which microorganisms exhibit resistance to antimicrobials are:
Antibiotic resistance can be a result of horizontal gene transfer, and also of unlinked point mutations in the pathogen genome at a rate of about 1 in 108 per chromosomal replication. Mutations are rare but the fact that bacteria reproduce at such a high rate allows for the effect to be significant. A mutation may produce a change in the binding site of the antibiotic, which may allow the site to continue proper functioning in the presence of the antibiotic or prevent the binding of the antibiotic to the site altogether. Research has shown the bacterial protein LexA may play a key role in the acquisition of bacterial mutations giving resistance to quinolones and rifampicin. DNA damage induces the SOS gene repressor LexA to undergo autoproteolytic activity. This includes the transcription of genes encoding Pol II, Pol IV, and Pol V, which are three nonessential DNA polymerases that are required for mutation in response to DNA damage. The antibiotic action against the pathogen can be seen as an environmental pressure. Those bacteria with a mutation that allows them to survive live to reproduce. They then pass this trait to their offspring, which leads to the evolution of a fully resistant colony. Although these chromosomal mutations may seem to benefit the bacteria by providing antibiotic resistance, they also confer a cost of fitness. For example, a ribosomal mutation may protect a bacterial cell by changing the binding site of an antibiotic but it will also slow the process of protein synthesis. Additionally, a particular study specifically compared the overall fitness of antibiotic resistant strains of Escherichia coli and Salmonella typhimurium to their drug-sensitive revertants. They observed a reduced overall fitness in the antibiotic resistant strains, especially in growth rate.
There are three known mechanisms of fluoroquinolone resistance. Some types of efflux pumps can act to decrease intracellular quinolone concentration. In Gram-negative bacteria, plasmid-mediated resistance genes produce proteins that can bind to DNA gyrase, protecting it from the action of quinolones. Finally, mutations at key sites in DNA gyrase or topoisomerase IV can decrease their binding affinity to quinolones, decreasing the drug's effectiveness.
Antibiotic resistance can also be introduced artificially into a microorganism through laboratory protocols, sometimes used as a selectable marker to examine the mechanisms of gene transfer or to identify individuals that absorbed a piece of DNA that included the resistance gene and another gene of interest. A recent study demonstrated that the extent of horizontal gene transfer among "Staphylococcus" is much greater than previously expected—and encompasses genes with functions beyond antibiotic resistance and virulence, and beyond genes residing within the mobile genetic elements.
For a long time, it has been thought that, for a microorganism to become resistant to an antibiotic, it must be in a large population. However, recent findings show that there is no necessity of large populations of bacteria for the appearance of antibiotic resistance. We know now that small populations of E.coli in an antibiotic gradient can become resistant. Any heterogeneous environment with respect to nutrient and antibiotic gradients may facilitate the development of antibiotic resistance in small bacterial populations and this is also true for the human body. Researchers hypothesize that the mechanism of resistance development is based on four SNP mutations in the genome of E.coli produced by the gradient of antibiotic. These mutations confer the bacteria emergence of antibiotic resistance.
Organisms.
Bacteria.
"Staphylococcus aureus".
"Staphylococcus aureus" (colloquially known as "Staph aureus" or a "Staph infection") is one of the major resistant pathogens. Found on the mucous membranes and the human skin of around a third of the population, it is extremely adaptable to antibiotic pressure. It was one of the earlier bacteria in which penicillin resistance was found—in 1947, just four years after the drug started being mass-produced. Methicillin was then the antibiotic of choice, but has since been replaced by oxacillin due to significant kidney toxicity. Methicillin-resistant "Staphylococcus aureus" (MRSA) was first detected in Britain in 1961, and is now "quite common" in hospitals. MRSA was responsible for 37% of fatal cases of sepsis in the UK in 1999, up from 4% in 1991. Half of all "S. aureus" infections in the US are resistant to penicillin, methicillin, tetracycline and erythromycin.
This left vancomycin as the only effective agent available at the time. However, strains with intermediate (4-8 μg/ml) levels of resistance, termed glycopeptide-intermediate "Staphylococcus aureus" (GISA) or vancomycin-intermediate "Staphylococcus aureus" (VISA), began appearing in the late 1990s. The first identified case was in Japan in 1996, and strains have since been found in hospitals in England, France and the US. The first documented strain with complete (>16 μg/ml) resistance to vancomycin, termed vancomycin-resistant "Staphylococcus aureus" (VRSA) appeared in the United States in 2002. However, in 2011, a variant of vancomycin has been tested that binds to the lactate variation and also binds well to the original target, thus reinstating potent antimicrobial activity.
A new class of antibiotics, oxazolidinones, became available in the 1990s, and the first commercially available oxazolidinone, linezolid, is comparable to vancomycin in effectiveness against MRSA. Linezolid-resistance in "S. aureus" was reported in 2001.
Community-acquired MRSA (CA-MRSA) has now emerged as an epidemic that is responsible for rapidly progressive, fatal diseases, including necrotizing pneumonia, severe sepsis, and necrotizing fasciitis. MRSA is the most frequently identified antimicrobial drug-resistant pathogen in US hospitals. The epidemiology of infections caused by MRSA is rapidly changing. In the past 10 years, infections caused by this organism have emerged in the community. The two MRSA clones in the United States most closely associated with community outbreaks, USA400 (MW2 strain, ST1 lineage) and USA300, often contain Panton-Valentine leukocidin (PVL) genes and, more frequently, have been associated with skin and soft tissue infections. Outbreaks of CA-MRSA infections have been reported in correctional facilities, among athletic teams, among military recruits, in newborn nurseries, and among men that have sex with men. CA-MRSA infections now appear endemic in many urban regions and cause most CA-"S. aureus" infections.
"Streptococcus" and "Enterococcus".
"Streptococcus pyogenes" (Group A "Streptococcus": GAS) infections can usually be treated with many different antibiotics. Early treatment may reduce the risk of death from invasive group A streptococcal disease. However, even the best medical care does not prevent death in every case. For those with very severe illness, supportive care in an intensive-care unit may be needed. For persons with necrotizing fasciitis, surgery often is needed to remove damaged tissue. Strains of "S. pyogenes" resistant to macrolide antibiotics have emerged; however, all strains remain uniformly susceptible to penicillin.
Resistance of "Streptococcus pneumoniae" to penicillin and other beta-lactams is increasing worldwide. The major mechanism of resistance involves the introduction of mutations in genes encoding penicillin-binding proteins. Selective pressure is thought to play an important role, and use of beta-lactam antibiotics has been implicated as a risk factor for infection and colonization. "S. pneumoniae" is responsible for pneumonia, bacteremia, otitis media, meningitis, sinusitis, peritonitis and arthritis.
Multidrug-resistant "Enterococcus faecalis" and "Enterococcus faecium" are associated with nosocomial infections. Among these strains, penicillin-resistant "Enterococcus" was seen in 1983, vancomycin-resistant "Enterococcus" in 1987, and linezolid-resistant "Enterococcus" in the late 1990s.
"Pseudomonas aeruginosa".
"Pseudomonas aeruginosa" is a highly prevalent opportunistic pathogen. One of the most worrisome characteristics of "P. aeruginosa" is its low antibiotic susceptibility, which is attributable to a concerted action of multidrug efflux pumps with chromosomally encoded antibiotic resistance genes (e.g., "mexAB-oprM", "mexXY") and the low permeability of the bacterial cellular envelopes. "Pseudomonas aeruginosa" has the ability to produce 4-hydroxy-2-alkylquinolines (HAQs) and it has been found that HAQs have prooxidant effects, and overexpressing modestly increased susceptibility to antibiotics. The study experimented with the "Pseudomonas aeruginosa" biofilms and found that a disruption of relA and spoT genes produced an inactivation of the Stringent response (SR) in cells with nutrient limitation, which provides cells be more susceptible to antibiotics.
"Clostridium difficile".
"Clostridium difficile" is a nosocomial pathogen that causes diarrheal disease in hospitals world wide.
"C. difficile" colitis is most strongly associated with fluoroquinolones, cephalosporins, carbapenems, and clindamycin.
Some research suggests the overuse of antibiotics in the raising of livestock is contributing to outbreaks of bacterial infections such as C. difficile.[16]
Antibiotics, especially those with a broad activity spectrum (such as clindamycin) disrupt normal intestinal flora. This can lead to an overgrowth of C. difficile, which flourishes under these conditions. Pseudomembranous colitis can follow, creating generalized inflammation of the colon and the development of "pseudomembrane", a viscous collection of inflammatory cells, fibrin, and necrotic cells.[4] Clindamycin-resistant "C. difficile" was reported as the causative agent of large outbreaks of diarrheal disease in hospitals in New York, Arizona, Florida and Massachusetts between 1989 and 1992. Geographically dispersed outbreaks of "C. difficile" strains resistant to fluoroquinolone antibiotics, such as ciprofloxacin and levofloxacin, were also reported in North America in 2005.
"Salmonella" and "E. coli".
Infection with "Escherichia coli" and "Salmonella" can result from the consumption of contaminated food and water. Both of these bacteria are well known for causing nosocomial (hospital-linked) infections, and often, these strains found in hospitals are antibiotic resistant due to adaptations to wide spread antibiotic use. When both bacteria are spread, serious health conditions arise. Many people are hospitalized each year after becoming infected, with some dying as a result. Since 1993, some strains of "E. coli" have become resistant to multiple types of fluoroquinolone antibiotics.
Although mutation alone plays a huge role in the development of antibiotic resistance, a 2008 study found that high survival rates after exposure to antibiotics could not be accounted for by mutation alone. This study focused on the development of resistance in E. coli to three antibiotic drugs: ampicillin, tetracycline, and nalidixic acid. The researchers found that some antibiotic resistance in E. coli developed due to epigenetic inheritance rather than by direct inheritance of a mutated gene. This was further supported by data showing that reversion to antibiotic sensitivity was relatively common as well. This could only be explained by epigenetics. Epigenetics is a type of inheritance in which gene expression is altered rather than the genetic code itself. There are many modes by which this alteration of gene expression can occur, including methylation of DNA and histone modification; however, the important point is that both inheritance of random mutations and epigenetic markers can result in the expression of antibiotic resistance genes.
"Acinetobacter baumannii".
On November 5, 2004, the Centers for Disease Control and Prevention (CDC) reported an increasing number of "Acinetobacter baumannii" bloodstream infections in patients at military medical facilities in which service members injured in the Iraq/Kuwait region during Operation Iraqi Freedom and in Afghanistan during Operation Enduring Freedom were treated. Most of these showed multidrug resistance (MRAB), with a few isolates resistant to all drugs tested.
"Klebsiella pneumoniae".
Klebsiella pneumoniae carbapenemase (KPC)-producing bacteria are a group of emerging highly drug-resistant Gram-negative bacilli causing infections associated with significant morbidity and mortality whose incidence is rapidly increasing in a variety of clinical settings around the world. "Klebsiella pneumoniae" includes numerous mechanisms for antibiotic resistance, many of which are located on highly mobile genetic elements. Carbapenem antibiotics (heretofore often the treatment of last resort for resistant infections) are generally not effective against KPC-producing organisms.
"Mycobacterium tuberculosis".
Tuberculosis is increasing across the globe, especially in developing countries, over the past few years. TB resistant to antibiotics is called MDR TB (Multidrug Resistant TB). Globally, MDR TB causes 150,000 deaths annually. The rise of the HIV/AIDS epidemic has contributed to this.
TB was considered one of the most prevalent diseases, and did not have a cure until the discovery of Streptomycin by Selman Waksman in 1943. However, the bacteria soon developed resistance. Since then, drugs such as isoniazid and rifampin have been used. M. tuberculosis develops resistance to drugs by spontaneous mutations in its genomes. Resistance to one drug is common, and this is why treatment is usually done with more than one drug. Extensively Drug-Resistant TB (XDR TB) is TB that is also resistant to the second line of drugs.
Resistance of "Mycobacterium tuberculosis" to isoniazid, rifampin, and other common treatments has become an increasingly relevant clinical challenge. (For more on Drug-Resistant TB, visit the Multi-drug-resistant tuberculosis page.) Evidence is lacking for whether these bacteria have plasmids. Also "M. tuberculosis" lack the opportunity to interact with other bacteria in order to share plasmids.
"Neisseria gonorrhoeae".
Neisseria gonorrhoeae is a sexually transmitted pathogen that can cause pelvic pain, pain on urination, penile and vaginal discharge, as well as systemic symptoms. The bacteria was first identified in 1879, although some Biblical scholars believe that references to the disease can be found as early as Parshat Metzora of the Old Testament.
In the 1940s effective treatment with penicillin became available, but by the 1970s resistant strains predominated. Resistance to penicillin has developed through two mechanisms: chomasomally mediated resistance (CMRNG) and penicillinase-mediated resistance (PPNG). CMRNG involves stepwise mutation of penA, which codes for the penicilin-binding protein (PBP-2); mtr, which encodes an efflux pump to remove penicilin from the cell; and penB, which encodes the bacterial cell wall porins. PPNG involves the acquisition of a plasmid-borne beta-lactamase.
Fluoroquinolones were a useful next-line treatment until resistance was achieved through efflux pumps and mutations to the gyrA gene, which encodes DNA gyrase. Third-generation cephalosporins have been used to treat gonorrhoea since 2007, but resistant strains have emerged. Strains of Neisseria gonorrhoea have also been found to be resistant to tetracyclines and aminoglycosides. Neisseria gonorrheoea has a high affinity for horizontal gene transfer, and as a result, the existence of any strain resistant to a given drug could spread easily across strains.
As of 2010, the recommended treatment is a single 250 mg intramuscular injection of ceftriaxone, sometimes in combination with azithromycin or doxycycline.
Viruses.
Specific antiviral drugs are used to treat some viral infections. These drugs prevent viruses from reproducing by inhibiting essential stages of the virus's replication cycle in infected cells. Antivirals are used to treat HIV, hepatitis B, hepatitis C, influenza, herpes viruses including varicella zoster virus, cytomegalovirus and Epstein-Barr virus. With each virus, some strains have become resistant to the administered drugs.
Resistance to HIV antivirals is problematic, and even multi-drug resistant strains have evolved. Resistant strains of the HIV virus emerge rapidly if only one antiviral drug is used. Using three or more drugs together has helped to control this problem, but new drugs are needed because of the continuing emergence of drug-resistant HIV strains.
Fungi.
Infections by fungi are a cause of high morbidity and mortality in immunocompromised persons, such as those with HIV/AIDS, tuberculosis or receiving chemotherapy. The fungi candida, Cryptococcus neoformans and Aspergillus fumigatus cause most of these infections and antifungal resistance occurs in all of them. Multidrug resistance in fungi is increasing because of the widespread use of antifungal drugs to treat infections in immunocompromised individuals.
Parasites.
The protozoan parasites that cause the diseases malaria, trypanosomiasis, toxoplasmosis, cryptosporidiosis and leishmaniasis are important human pathogens.
Malarial parasites that are resistant to the drugs that are currently available to infections are common and this has led to increased efforts to develop new drugs. Resistance to recently developed drugs such as artemisinin has also been reported. The problem of drug resistance in malaria has driven efforts to develop vaccines.
Trypanosomes are parasitic protozoa that cause African trypanosomiasis and Chagas disease (American trypanosomiasis). There are no vaccines to prevent these infections so drugs such as pentamidine and suramin, benznidazole and nifurtimox and used to treat infections. These drugs are effective but infections caused by resistant parasites have been reported.
Leishmaniasis is caused by protozoa and is an important public health problem worldwide, especially in sub-tropical and tropical countries. Drug resistance has "become a major concern".
Prevention.
World Health Organization recommendations.
An April 30, 2014, report by the WHO addressed this issue, and a summary was described in a WHO press release as follows:
United States 2015.
For the FY 2016 budget, President Obama has suggested to nearly double the amount of federal funding to "combat and prevent" antibiotic resistance to more than $1.2 billion.
Strategies.
Excessive antibiotic use has become one of the top contributors to the development of antibiotic resistance. Since the beginning of the antibiotic era, antibiotics have been used to treat a wide range of disease and illness. Overuse of antibiotics has become the primary cause of rising levels of antibiotic resistance. The main problem is that doctors are willing to prescribe antibiotics to ill-informed individuals who believe that antibiotics can cure nearly all illnesses, including viral infections like the common cold. In fact, in a recent analysis of drug prescriptions, it was found that 35.7% of individuals with a cold or an upper respiratory infection (both viral in origin) were given prescriptions for antibiotics. These prescriptions accomplished nothing other than increasing the risk of further evolution of antibiotic resistant bacteria.
In a recent years, antimicrobial stewardship teams in hospitals have encouraged optimal use of antimicrobials. The goals of antimicrobial stewardship are to help practitioners pick the right drug at the right dose and duration of therapy while preventing misuse and minimizing the development of resistance.
Rational use.
Rational use of antimicrobials may reduce the chances of development of opportunistic infection by antibiotic-resistant bacteria due to dysbacteriosis. The immune systems will cure minor bacterial infections on its own. It is also important to note that antibiotics will not cure viral infections such as colds and the flu, and taking an antibiotic unnecessarily to treat a viral infection can lead to increased resistance.
Rapid viral testing.
It is unclear if rapid viral testing affects antibiotic use in children.
Vaccines.
Microorganisms do not develop resistance to vaccines because a vaccine enhances the body's immune system, whereas an antibiotic operates separately from the body's normal defenses. Nevertheless, new strains that escape immunity induced by vaccines may evolve; for example, an updated influenza vaccine is needed each year.
While theoretically promising, antistaphylococcal vaccines have shown limited efficacy, because of immunological variation between "Staphylococcus" species, and the limited duration of effectiveness of the antibodies produced. Development and testing of more effective vaccines is underway.
Phage therapy.
Phage therapy used as a therapeutic agent was researched especially in the Soviet Union, and represents a potential but currently underdeveloped approach to the treatment of bacterial disease. Phage therapy was widely used in the United States until the advent of antibiotics, in the early 1940s. Bacteriophages or "phages" are viruses that invade bacterial cells and, in the case of lytic phages, disrupt bacterial metabolism and cause the bacterium to lyse. Phage therapy is the therapeutic use of lytic bacteriophages to treat pathogenic bacterial infections.
Alternating therapy.
Alternating therapy is a proposed method in which two or three antibiotics are taken in a rotation versus taking just one antibiotic such that bacteria resistant to one antibiotic are killed when the next antibiotic is taken. Studies have found that this method reduces the rate at which antibiotic resistant bacteria emerge in vitro relative to a single drug for the entire duration.
Develop new drugs.
Since the discovery of antibiotics, research and development (R&D) efforts have provided new drugs in time to treat bacteria that became resistant to older antibiotics, but in the 2000s there has been concern that development has slowed enough that seriously ill people may run out of treatment options. Another concern is that doctors may become reluctant to perform routine surgeries due to the increased risk of harmful infection. Backup treatments can have serious side-effects; for example, treatment of multi-drug-resistant tuberculosis can cause deafness and insanity. The potential crisis at hand is the result of a marked decrease in industry R&D. Poor financial investment in antibiotic research has exacerbated the situation. In 2011, Pfizer, one of the last major pharmaceutical companies developing new antibiotics, shut down its primary research effort, citing poor shareholder returns relative to drugs for chronic illnesses.
In the United States, drug companies and the administration of President Barack Obama have been proposing changing the standards by which the FDA approves antibiotics targeted at resistant organisms. On 12 December 2013, the Antibiotic Development to Advance Patient Treatment (ADAPT) Act of 2013 was introduced in the U.S. Congress. The ADAPT Act aims to fast-track the drug development in order to combat the growing public health threat of 'superbugs'. Under this Act, the FDA can approve antibiotics and antifungals needed for life-threatening infections based on data from smaller clinical trials. The CDC will reinforce the monitoring of the use of antibiotics that treat serious and life-threatening infections and the emerging resistance, and make the data publicly available. The FDA antibiotics labeling process, 'Susceptibility Test Interpretive Criteria for Microbial Organisms' or 'breakpoints' is also streamlined to allow the most up-to-date and cutting-edge data available to healthcare professionals under the new Act.
On 18 September 2014 Obama signed an executive order to implement the recommendations proposed in a report by the President's Council of Advisors on Science and Technology (PCAST) which outlines strategies to stream-line clinical trials and speed up the R&D of new antibiotics. Among the proposals:
The executive order also included a $20 million prize to encourage the development of diagnostic tests to identify highly resistant bacterial infections.
The U.S. National Institutes of Health plans to fund a new research network on the issue up to $62 million from 2013 to 2019. Using authority created by the Pandemic and All Hazards Preparedness Act of 2006, the Biomedical Advanced Research and Development Authority in the U.S. Department of Health and Human Services announced that it will spend between $40 million and $200 million in funding for R&D on new antibiotic drugs under development by GlaxoSmithKline.
One major cause of antibiotic resistance is the increased pumping activity of microbial ABC transporters, which diminishes the effective drug concentration inside the microbial cell. ABC transporter inhibitors that can be used in combination with current antimicrobials are being tested in clinical trials and are available for therapeutic regimens.
Reducing antibiotic use in animals.
Europe.
In 1997, European Union health ministers voted to ban avoparcin and four additional antibiotics used to promote animal growth in 1999. In 2006 a ban on the use of antibiotics in European feed, with the exception of two antibiotics in poultry feeds, became effective. In Scandinavia, there is evidence that the ban has led to a lower prevalence of antibiotic resistance in (nonhazardous) animal bacterial populations. A corresponding change in antibiotic-resistance cases among humans has not yet been demonstrated.
United States.
The United States Department of Agriculture (USDA) and the Food and Drug Administration (FDA) collect data on antibiotic use in humans and in a more limited fashion in animals.
The FDA first determined in 1977 that there is evidence of emergence of antibiotic-resistant bacterial strains in livestock. The long-established practice of permitting OTC sales of antibiotics (including penicillin and other drugs) to lay animal owners for administration to their own animals nonetheless continued in all states.
In 2000, the FDA announced their intention to revoke approval of fluoroquinolone use in poultry production because of substantial evidence linking it to the emergence of fluoroquinolone-resistant "Campylobacter" infections in humans. Legal challenges from the food animal and pharmaceutical industries delayed the final decision to do so until 2006. Fluroquinolones have been banned from extra-label use in food animals in the USA since 2007. However, they remain widely used in companion and exotic animals.
During 2007, two federal bills (S. 549 and H.R. 962) aimed at phasing out "nontherapeutic" antibiotics in U.S. food animal production. The Senate bill, introduced by Sen. Edward "Ted" Kennedy, died. The House bill, introduced by Rep. Louise Slaughter, died after being referred to Committee.
In March 2012, the United States District Court for the Southern District of New York, ruling in an action brought by the Natural Resources Defense Council and others, ordered the FDA to revoke approvals for the use of antibiotics in livestock that violated FDA regulations. On April 11, 2012 the FDA announced a voluntary program to phase out unsupervised use of drugs as feed additives and convert approved over-the-counter uses for antibiotics to prescription use only, requiring veterinarian supervision of their use and a prescription. In December 2013, the FDA announced the commencement of these steps to phase out the use of antibiotics for the purposes of promoting livestock growth.
Growing U.S. consumer concern about using antibiotics in animal feed has led to a niche market of "antibiotic-free" animal products, but this small market is unlikely to change entrenched, industry-wide practices.
Applications.
Antibiotic resistance is an important tool for genetic engineering. By constructing a plasmid that contains an antibiotic-resistance gene as well as the gene being engineered or expressed, a researcher can ensure that, when bacteria replicate, only the copies that carry the plasmid survive. This ensures that the gene being manipulated passes along when the bacteria replicates.
In general, the most commonly used antibiotics in genetic engineering are "older" antibiotics that have largely fallen out of use in clinical practice. These include:
In industry, the use of antibiotic resistance is disfavored, since maintaining bacterial cultures would require feeding them large quantities of antibiotics. Instead, the use of auxotrophic bacterial strains (and function-replacement plasmids) is preferred.

</doc>
<doc id="1915" url="http://en.wikipedia.org/wiki?curid=1915" title="Antigen">
Antigen

In immunology, an antigen (Ag), abbreviation of antibody generator, is any structural substance which serves as a target for the receptors of an adaptive immune response, TCR or BCR or its secreted form antibody, respectively. Each antibody is specifically selected after binding to a certain antigen because of random somatic diversification in the antibody complementarity determining regions (a common analogy used to describe this is the fit between a lock and a key). Paul Ehrlich coined the term antibody (in German "Antikörper") in his side-chain theory at the end of 19th century. In summary, an antigen is a molecule that binds to Ag-specific receptors but cannot induce an immune response in the body by itself. Antigen was originally a structural molecule that binds specifically to the antibody, but the term now also refers to any molecule or a linear fragment that can be recognized by highly variable antigen receptors (B-cell receptor or T-cell receptor) of the adaptive immune system.
The antigen may originate from within the body ("self") or from the external environment ("non-self"). The immune system is usually non-reactive against "self" antigens under normal homeostatic conditions due to negative selection of T cells in the thymus and is supposed to identify and attack only "non-self" invaders from the outside world or modified/harmful substances present in the body under distressed conditions.
Antigen presenting cells present the antigen structures in the form of processed antigenic peptides to the T cells of the adaptive immune system via a histocompatibility molecule. Depending on the antigen presented and the type of the histocompatibility molecule, several types of T cells can become activated. For T-Cell Receptor (TCR) recognition, it must be processed into small fragments inside the cell and presented to a T-cell receptor by major histocompatibility complex (MHC). Antigen by itself is not capable to elicit the immune response without the help of an Immunologic adjuvant. The essential role of the adjuvant component of vaccines in the activation of innate immune system is so-called immunologist's dirty little secret as originally described by Charles Janeway.
An immunogen is an analogy to the antigen a substance (or a mixture of substances) that is able to provoke an immune response if injected to the body. An immunogen is able to initiate an indispensable innate immune response first, later leading to the activation of the adaptive immune response, whereas an antigen is able to bind the highly variable immunoreceptor products (B-cell receptor or T-cell receptor) once these have been generated previously. Therefore, the overlapping concepts of immunogenicity and antigenicity are clearly different. According to current textbook notions:
Immunogenicity is the ability to induce a humoral and/or cell-mediated immune response
Antigenicity is the ability to combine specifically with the final products of the immune response (i.e. secreted antibodies and/or surface receptors on T-cells). Although all immunogenic molecules are also antigenic, the reverse is not true.
At the molecular level, an antigen can be characterized by its ability to be bound by the variable Fab region of an antibody. Note also that different antibodies have the potential to discriminate between specific epitopes present on the surface of the antigen (as illustrated in the Figure). Hapten is a small molecule that changes the structure of an antigenic epitope. In order to induce an immune response, it has to be attached to a large carrier molecule such as protein. 
Antigens are usually proteins and polysaccharides, less frequently also lipids. This includes parts (coats, capsules, cell walls, flagella, fimbrae, and toxins) of bacteria, viruses, and other microorganisms. Lipids and nucleic acids are antigenic only when combined with proteins and polysaccharides. Non-microbial exogenous (non-self) antigens can include pollen, egg white, and proteins from transplanted tissues and organs or on the surface of transfused blood cells. Vaccines are examples of antigens in an immunogenic form, which are to be intentionally administered to induce the memory function of adaptive immune system toward the antigens of the pathogen invading the recipient.
Origin of the term antigen.
In 1899, Ladislas Deutsch (Laszlo Detre) (1874–1939) named the hypothetical substances halfway between bacterial constituents and antibodies "substances immunogenes ou antigenes" (antigenic or immunogenic substances). He originally believed those substances to be precursors of antibodies, just as zymogen is a precursor of an enzyme. But, by 1903, he understood that an antigen induces the production of immune bodies (antibodies) and wrote that the word "antigen" is a contraction of Antisomatogen(= "Immunkörperbildner"). The Oxford English Dictionary indicates that the logical construction should be "anti(body)-gen".
Origin of antigens.
Antigens can be classified in order of their class.
Exogenous antigens.
Exogenous antigens are antigens that have entered the body from the outside, for example by inhalation, ingestion, or injection. The immune system's response to exogenous antigens is often subclinical. By endocytosis or phagocytosis, exogenous antigens are taken into the antigen-presenting cells (APCs) and processed into fragments. APCs then present the fragments to T helper cells (CD4+) by the use of class II histocompatibility molecules on their surface. Some T cells are specific for the peptide:MHC complex. They become activated and start to secrete cytokines. Cytokines are substances that can activate cytotoxic T lymphocytes (CTL), antibody-secreting B cells, macrophages, and other particles.
Some antigens start out as exogenontigens, and later become endogenous (for example, intracellular viruses). Intracellular antigens can again be released back into circulation upon the destruction of the infected cell.
Endogenous antigens.
Endogenous antigens are antigens that have been generated within previously normal cells as a result of normal cell metabolism, or because of viral or intracellular bacterial infection. The fragments are then presented on the cell surface in the complex with MHC class I molecules. If activated cytotoxic CD8+ T cells recognize them, the T cells begin to secrete various toxins that cause the lysis or apoptosis of the infected cell. In order to keep the cytotoxic cells from killing cells just for presenting self-proteins, self-reactive T cells are deleted from the repertoire as a result of tolerance (also known as negative selection). Endogenous antigens include xenogenic (heterologous), autologous and idiotypic or allogenic (homologous) antigens.
Autoantigens.
An autoantigen is usually a normal protein or complex of proteins (and sometimes DNA or RNA) that is recognized by the immune system of patients suffering from a specific autoimmune disease. These antigens should not be, under normal conditions, the target of the immune system, but, due mainly to genetic and environmental factors, the normal immunological tolerance for such an antigen has been lost in these patients.
Tumor antigens.
"Tumor antigens" or "neoantigens" are those antigens that are presented by MHC I or MHC II molecules on the surface of tumor cells. These antigens can sometimes be presented by tumor cells and never by the normal ones. In this case, they are called tumor-specific antigens (TSAs) and, in general, result from a tumor-specific mutation. More common are antigens that are presented by tumor cells and normal cells, and they are called tumor-associated antigens (TAAs). Cytotoxic T lymphocytes that recognize these antigens may be able to destroy the tumor cells before they proliferate or metastasize.
Tumor antigens can also be on the surface of the tumor in the form of, for example, a mutated receptor, in which case they will be recognized by B cells.
Nativity.
A native antigen is an antigen that is not yet processed by an APC to smaller parts. T cells cannot bind native antigens, but require that they be processed by APCs, whereas B cells can be activated by native ones.
Antigenic specificity.
Antigen(ic) specificity is the ability of the host cells to recognize an antigen specifically as a unique molecular entity and distinguish it from another with exquisite precision. Antigen specificity is due primarily to the side-chain conformations of the antigen. It is a measurement, although the degree of specificity may not be easy to measure, and need not be linear or of the nature of a rate-limited step or equation.

</doc>
<doc id="1916" url="http://en.wikipedia.org/wiki?curid=1916" title="Autosome">
Autosome

An autosome is a chromosome that is not an allosome (i.e., not a sex chromosome). Autosomes appear in pairs whose members have the same form but differ from other pairs in a diploid cell, whereas members of an allosome pair may differ from one another and thereby determine sex. The DNA in autosomes is collectively known as atDNA or auDNA.
For example, humans have a diploid genome that usually contains 22 pairs of autosomes and one allosome pair (46 chromosomes total). The autosome pairs are labeled with numbers (1-22 in humans) roughly in order of their sizes in base pairs, while allosomes are labeled with their letters. By contrast, the allosome pair consists of two X chromosomes in females or one X and one Y chromosome in males. (Unusual combinations of XYY, XXY, XXX, XXXX, XXXXX or XXYY, among other allosome combinations, are known to occur and usually cause developmental abnormalities.) 

</doc>
<doc id="1919" url="http://en.wikipedia.org/wiki?curid=1919" title="Antwerp (disambiguation)">
Antwerp (disambiguation)

Antwerp is a city in Belgium and capital of the Antwerp province.
Antwerp may also refer to:

</doc>
<doc id="1920" url="http://en.wikipedia.org/wiki?curid=1920" title="Aquila">
Aquila

Aquila is the Latin and Romance languages word for "eagle". Specifically, it may refer to:

</doc>
<doc id="1921" url="http://en.wikipedia.org/wiki?curid=1921" title="Al-Qaeda">
Al-Qaeda

al-Qaeda ( or ; Arabic: القاعدة‎ "al-qāʿidah", ], translation: "The Base", "The Foundation" or "The Fundament" and alternatively spelled al-Qaida and sometimes al-Qa'ida) is a global militant Islamist organization founded by Osama bin Laden, Abdullah Azzam, and several other militants, at some point between August 1988 and late 1989, with origins traceable to the Soviet war in Afghanistan. It operates as a network comprising both a multinational, stateless army and an Islamist, extremist, wahhabi, jihadist group. It has been designated as a terrorist organization by the United Nations Security Council, the North Atlantic Treaty Organization (NATO), the European Union, the United States, Russia, India, and various other countries (see below). Al-Qaeda has carried out many attacks on targets it considers "kafir". During the Syrian civil war, al-Qaeda factions started fighting each other, as well as the Kurds and the Syrian government.
Al-Qaeda has mounted attacks on civilian and military targets in various countries, including the 1998 U.S. embassy bombings, the September 11 attacks, and the 2002 Bali bombings. The U.S. government responded to the September 11 attacks by launching the "War on Terror". With the loss of key leaders, culminating in the death of Osama bin Laden, al-Qaeda's operations have devolved from actions that were controlled from the top down, to actions by franchise associated groups and lone-wolf operators.
Characteristic techniques employed by al-Qaeda include suicide attacks and the simultaneous bombing of different targets. Activities ascribed to it may involve members of the movement who have made a pledge of loyalty to Osama bin Laden, or the much more numerous "al-Qaeda-linked" individuals who have undergone training in one of its camps in Afghanistan, Pakistan, Iraq or Sudan who have not. Al-Qaeda ideologues envision a complete break from all foreign influences in Muslim countries, and the creation of a new worldwide Islamic caliphate. 
Among the beliefs ascribed to al-Qaeda members is the conviction that a Christian–Jewish alliance is conspiring to destroy Islam. As Salafist jihadists, they believe that the killing of non-combatants is religiously sanctioned, and they ignore any aspect of religious scripture which might be interpreted as forbidding the murder of non-combatants and internecine fighting. Al-Qaeda also opposes what it regards as man-made laws, and wants to replace them with a strict form of sharia law.
Al-Qaeda is also responsible for instigating sectarian violence among Muslims. Al-Qaeda leaders regard liberal Muslims, Shias, Sufis and other sects as heretics and have attacked their mosques and gatherings. Examples of sectarian attacks include the Yazidi community bombings, the Sadr City bombings, the Ashoura Massacre and the April 2007 Baghdad bombings. Since the death of Osama bin Laden in 2011 the group has been led by Egyptian Ayman al-Zawahiri.
Organization.
Al-Qaeda's management philosophy has been described as "centralization of decision and decentralization of execution." It is thought that al-Qaeda's leadership, following the War on Terror, has "become geographically isolated," leading to the "emergence of decentralized leadership" of regional groups using the al-Qaeda "brand".
Many terrorism experts do not believe that the global jihadist movement is driven at every level by al-Qaeda's leadership. Although bin Laden still held considerable ideological sway over some Muslim extremists before his death, experts argue that al-Qaeda has fragmented over the years into a variety of regional movements that have little connection with one another. Marc Sageman, a psychiatrist and former Central Intelligence Agency (CIA) officer, said that al-Qaeda is now just a "loose label for a movement that seems to target the West." "There is no umbrella organisation. We like to create a mythical entity called [al-Qaeda] in our minds, but that is not the reality we are dealing with."
This view mirrors the account given by Osama bin Laden in his October 2001 interview with Tayseer Allouni:
"... this matter isn't about any specific person and... is not about the al-Qa'idah Organization. We are the children of an Islamic Nation, with Prophet Muhammad as its leader, our Lord is one... and all the true believers [mu'mineen] are brothers. So the situation isn't like the West portrays it, that there is an 'organization' with a specific name (such as 'al-Qa'idah') and so on. That particular name is very old. It was born without any intention from us. Brother Abu Ubaida... created a military base to train the young men to fight against the vicious, arrogant, brutal, terrorizing Soviet empire... So this place was called 'The Base' ['Al-Qa'idah'], as in a training base, so this name grew and became. We aren't separated from this nation. We are the children of a nation, and we are an inseparable part of it, and from those public *** which spread from the far east, from the Philippines, to Indonesia, to Malaysia, to India, to Pakistan, reaching Mauritania... and so we discuss the conscience of this nation."
Others, however, see al-Qaeda as an integrated network that is strongly led from the Pakistani tribal areas and has a powerful strategic purpose. Bruce Hoffman, a terrorism expert at Georgetown University, said "It amazes me that people don't think there is a clear adversary out there, and that our adversary does not have a strategic approach."
Al-Qaeda has the following direct affiliates:
Al-Qaeda has the following indirect affiliates:
Leadership.
Information mostly acquired from Jamal al-Fadl provided American authorities with a rough picture of how the group was organized. While the veracity of the information provided by al-Fadl and the motivation for his cooperation are both disputed, American authorities base much of their current knowledge of al-Qaeda on his testimony.
Osama bin Laden was the most historically notable emir, or commander, and Senior Operations Chief of al-Qaeda prior to his assassination on May 1, 2011 by US forces. Ayman al-Zawahiri, al-Qaeda's Deputy Operations Chief prior to bin Laden's death, assumed the role of commander, according to an announcement by al-Qaeda on June 16, 2011. He replaced Saif al-Adel, who had served as interim commander.
Bin Laden was advised by a Shura Council, which consists of senior al-Qaeda members, estimated by Western officials to consist of 20–30 people.
Atiyah Abd al-Rahman was alleged to be second in command prior to his death on August 22, 2011.
On June 5, 2012, Pakistan intelligence officials announced that al-Rahman's alleged successor Abu Yahya al-Libi had been killed in Pakistan.
Al-Qaeda's network was built from scratch as a conspiratorial network that draws on leaders of all its regional nodes "as and when necessary to serve as an integral part of its high command."
Command structure.
When asked about the possibility of al-Qaeda's connection to the July 7, 2005 London bombings in 2005, Metropolitan Police Commissioner Sir Ian Blair said: "Al-Qaeda is not an organization. Al-Qaeda is a way of working... but this has the hallmark of that approach... al-Qaeda clearly has the ability to provide training... to provide expertise... and I think that is what has occurred here."
On August 13, 2005, however, "The Independent" newspaper, quoting police and MI5 investigations, reported that the July 7 bombers had acted independently of an al-Qaeda terror mastermind someplace abroad.
What exactly al-Qaeda is, or was, remains in dispute. Certainly, it has been obliged to evolve and adapt in the aftermath of 9/11 and the launch of the 'war on terror'.
Nasser al-Bahri, who was Osama bin Laden's bodyguard for four years in the run-up to 9/11 gives a highly detailed description of how the organization functioned at that time in his memoir. He describes its formal administrative structure and vast arsenal, as well as day-to-day life as a member.
However, author and journalist Adam Curtis argues that the idea of al-Qaeda as a formal organization is primarily an American invention. Curtis contends the name "al-Qaeda" was first brought to the attention of the public in the 2001 trial of bin Laden and the four men accused of the 1998 US embassy bombings in East Africa:
The reality was that bin Laden and Ayman al-Zawahiri had become the focus of a loose association of disillusioned Islamist militants who were attracted by the new strategy. But there was no organization. These were militants who mostly planned their own operations and looked to bin Laden for funding and assistance. He was not their commander. There is also no evidence that bin Laden used the term "al-Qaeda" to refer to the name of a group until after September 11 attacks, when he realized that this was the term the Americans had given it.
As a matter of law, the US Department of Justice needed to show that bin Laden was the leader of a criminal organization in order to charge him "in absentia" under the Racketeer Influenced and Corrupt Organizations Act, also known as the RICO statutes. The name of the organization and details of its structure were provided in the testimony of Jamal al-Fadl, who said he was a founding member of the organization and a former employee of bin Laden. Questions about the reliability of al-Fadl's testimony have been raised by a number of sources because of his history of dishonesty, and because he was delivering it as part of a plea bargain agreement after being convicted of conspiring to attack U.S. military establishments. Sam Schmidt, one of his defense lawyers, said:
There were selective portions of al-Fadl's testimony that I believe was false, to help support the picture that he helped the Americans join together. I think he lied in a number of specific testimony about a unified image of what this organization was. It made al-Qaeda the new Mafia or the new Communists. It made them identifiable as a group and therefore made it easier to prosecute any person associated with al-Qaeda for any acts or statements made by bin Laden.
Field operatives.
The number of individuals in the organization who have undergone proper military training, and are capable of commanding insurgent forces, is largely unknown. Documents captured in the raid on bin Laden compound in 2011, show that the core al-Qaeda membership in 2002 was 170. In 2006, it was estimated that al-Qaeda had several thousand commanders embedded in 40 different countries. s of 2009[ [update]], it was believed that no more than 200–300 members were still active commanders.
According to the award-winning 2004 BBC documentary "The Power of Nightmares", al-Qaeda was so weakly linked together that it was hard to say it existed apart from bin Laden and a small clique of close associates. The lack of any significant numbers of convicted al-Qaeda members, despite a large number of arrests on terrorism charges, was cited by the documentary as a reason to doubt whether a widespread entity that met the description of al-Qaeda existed.
Insurgent forces.
According to Robert Cassidy, al-Qaeda controls two separate forces deployed alongside insurgents in Iraq and Pakistan. The first, numbering in the tens of thousands, was "organized, trained, and equipped as insurgent combat forces" in the Soviet-Afghan war. It was made up primarily of foreign "mujahideen" from Saudi Arabia and Yemen. Many went on to fight in Bosnia and Somalia for global "jihad". Another group, approximately 10,000 strong, live in Western states and have received rudimentary combat training.
Other analysts have described al-Qaeda's rank and file as being "predominantly Arab," in its first years of operation, and now also includes "other peoples" as of 2007[ [update]]. It has been estimated that 62% of al-Qaeda members have university education.
Financing.
Some financing for al-Qaeda in the 1990s came from the personal wealth of Osama bin Laden. By 2001 Afghanistan had become politically complex and mired. With many financial sources for al-Qaeda, bin Laden's financing role may have become comparatively minor. Sources in 2001 could also have included Jamaa Al-Islamiyya and Islamic Jihad, both associated with Afghan-based Egyptians. Other sources of income in 2001 included the heroin trade and donations from supporters in Kuwait, Saudi Arabia and other Islamic countries. A WikiLeaks released memo from the United States Secretary of State sent in 2009 asserted that the primary source of funding of Sunni terrorist groups worldwide was Saudi Arabia.
Strategy.
On March 11, 2005, "Al-Quds Al-Arabi" published extracts from Saif al-Adel's document "Al Qaeda's Strategy to the Year 2020". Abdel Bari Atwan summarizes this strategy as comprising five stages to rid the Ummah from all forms of oppression:
Atwan also noted, regarding the collapse of the U.S., "If this sounds far-fetched, it is sobering to consider that this virtually describes the downfall of the Soviet Union."
According to Fouad Hussein, a Jordanian journalist and author who has spent time in prison with Al-Zarqawi, Al Qaeda's strategy plan consists of 7 phases and is similar to the plan described in Al Qaeda's Strategy to the year 2020:
Name.
In Arabic, "al-Qaeda" has four syllables ("al-qāʿidah", ] or ]). However, since two of the Arabic consonants in the name (the voiceless uvular plosive [q] and the voiced pharyngeal fricative [ʕ]) are not phones found in the English language, the closest naturalized English pronunciations include , and . Al-Qaeda's name can also be transliterated as "al-Qaida", "al-Qa'ida", or "el-Qaida".
The name comes from the Arabic noun "qā'idah", which means "foundation" or "basis", and can also refer to a military base. The initial "al-" is the Arabic definite article "the", hence "the base".
Bin Laden explained the origin of the term in a videotaped interview with Al Jazeera journalist Tayseer Alouni in October 2001:
The name 'al-Qaeda' was established a long time ago by mere chance. The late Abu Ebeida El-Banashiri established the training camps for our "mujahedeen" against Russia's terrorism. We used to call the training camp al-Qaeda. The name stayed.
It has been argued that two documents seized from the Sarajevo office of the Benevolence International Foundation prove that the name was not simply adopted by the "mujahid" movement and that a group called al-Qaeda was established in August 1988. Both of these documents contain minutes of meetings held to establish a new military group, and contain the term "al-Qaeda".
Former British Foreign Secretary Robin Cook wrote that the word al-Qaeda should be translated as "the database", and originally referred to the computer file of the thousands of "mujahideen" militants who were recruited and trained with CIA help to defeat the Russians. In April 2002, the group assumed the name "Qa'idat al-Jihad", which means "the base of Jihad". According to Diaa Rashwan, this was "apparently as a result of the merger of the overseas branch of Egypt's al-Jihad (Egyptian Islamist Jihad, or EIJ) group, led by Ayman al-Zawahiri, with the groups Bin Laden brought under his control after his return to Afghanistan in the mid-1990s."
Ideology.
The radical Islamist movement in general and al-Qaeda in particular developed during the Islamic revival and Islamist movement of the last three decades of the 20th century, along with less extreme movements. The Afghan jihad against the pro-Soviet government (December 1979 to February 1989) developed the Salafist Jihadist movement of which Al-Qaeda was the most prominent example.
Some have argued that "without the writings" of Islamic author and thinker Sayyid Qutb, "al-Qaeda would not have existed." Qutb preached that because of the lack of "sharia" law, the Muslim world was no longer Muslim, having reverted to pre-Islamic ignorance known as "jahiliyyah".
To restore Islam, he said a vanguard movement of righteous Muslims was needed to establish "true Islamic states", implement "sharia", and rid the Muslim world of any non-Muslim influences, such as concepts like socialism and nationalism. Enemies of Islam in Qutb's view included "treacherous Orientalists" and "world Jewry," who plotted "conspiracies" and "wicked[ly]" opposed Islam.
In the words of Mohammed Jamal Khalifa, a close college friend of bin Laden: Islam is different from any other religion; it's a way of life. We [Khalifa and bin Laden] were trying to understand what Islam has to say about how we eat, who we marry, how we talk. We read Sayyid Qutb. He was the one who most affected our generation.
Qutb had an even greater influence on bin Laden's mentor and another leading member of al-Qaeda, Ayman al-Zawahiri. Zawahiri's uncle and maternal family patriarch, Mafouz Azzam, was Qutb's student, then protégé, then personal lawyer, and finally executor of his estate—one of the last people to see Qutb before his execution. "Young Ayman al-Zawahiri heard again and again from his beloved uncle Mahfouz about the purity of Qutb's character and the torment he had endured in prison." Zawahiri paid homage to Qutb in his work "Knights under the Prophet's Banner."
One of the most powerful of Qutb's ideas was that many who said they were Muslims were not. Rather, they were apostates. That not only gave jihadists "a legal loophole around the prohibition of killing another Muslim," but made "it a religious obligation to execute" these self-professed Muslims. These alleged apostates included leaders of Muslim countries, since they failed to enforce "sharia" law.
Religious compatibility.
Abdel Bari Atwan writes that:
While the leadership's own theological platform is essentially Salafi, the organization's umbrella is sufficiently wide to encompass various schools of thought and political leanings. Al-Qaeda counts among its members and supporters people associated with Wahhabism, Shafi'ism, Malikism, and Hanafism. There are even some whose beliefs and practices are directly at odds with Salafism, such as Yunis Khalis, one of the leaders of the Afghan mujahedin. He is a mystic who visits tombs of saints and seeks their blessings—practices inimical to bin Laden's Wahhabi-Salafi school of thought. The only exception to this pan-Islamic policy is Shi'ism. Al-Qaeda seems implacably opposed to it, as it holds Shi'ism to be heresy. In Iraq it has openly declared war on the Badr Brigades, who have fully cooperated with the US, and now considers even Shi'i civilians to be legitimate targets for acts of violence.
History.
"The Guardian" has described five distinct phases in the development of al-Qaeda: beginnings in the late 1980s, a "wilderness" period in 1990–96, its "heyday" in 1996–2001, a network period from 2001 to 2005, and a period of fragmentation from 2005 to today.
Jihad in Afghanistan.
The origins of al-Qaeda as a network inspiring terrorism around the world and training operatives can be traced to the Soviet War in Afghanistan (December 1979 – February 1989). The US viewed the conflict in Afghanistan, with the Afghan Marxists and allied Soviet troops on one side and the native Afghan "mujahideen", some of whom were radical Islamic militants, on the other, as a blatant case of Soviet expansionism and aggression. A CIA program called Operation Cyclone channeled funds through Pakistan's Inter-Services Intelligence agency to the Afghan Mujahideen who were fighting the Soviet occupation.
At the same time, a growing number of Arab "mujahideen" joined the "jihad" against the Afghan Marxist regime, facilitated by international Muslim organizations, particularly the Maktab al-Khidamat, which was funded by the Saudi Arabia government as well as by individual Muslims (particularly Saudi businessmen who were approached by bin Laden). Together, these sources donated some $600 million a year to jihad.
In 1984, Maktab al-Khidamat (MAK), or the "Services Office", a Muslim organization founded to raise and channel funds and recruit foreign "mujahideen" for the war against the Soviets in Afghanistan, was established in Peshawar, Pakistan, by bin Laden and Abdullah Yusuf Azzam, a Palestinian Islamic scholar and member of the Muslim Brotherhood. MAK organized guest houses in Peshawar, near the Afghan border, and gathered supplies for the construction of paramilitary training camps to prepare foreign recruits for the Afghan war front. Bin Laden became a "major financier" of the "mujahideen", spending his own money and using his connections with "the Saudi royal family and the petro-billionaires of the Gulf" to influence public opinion about the war and raise additional funds.
From 1986, MAK began to set up a network of recruiting offices in the US, the hub of which was the Al Kifah Refugee Center at the Farouq Mosque on Brooklyn's Atlantic Avenue. Among notable figures at the Brooklyn center were "double agent" Ali Mohamed, whom FBI special agent Jack Cloonan called "bin Laden's first trainer," and "Blind Sheikh" Omar Abdel-Rahman, a leading recruiter of "mujahideen" for Afghanistan. Al-Qaeda evolved from MAK.
Azzam and bin Laden began to establish camps in Afghanistan in 1987.
US government financial support for the Afghan Islamic militants was substantial. Aid to Gulbuddin Hekmatyar, an Afghan "mujahideen" leader and founder and leader of the Hezb-e Islami radical Islamic militant faction, alone amounted "by the most conservative estimates" to $600 million. Later, in the early 1990s, after the US had withdrawn support, Hekmatyar "worked closely" with bin Laden. In addition to receiving hundreds of millions of dollars in American aid, Hekmatyar was the recipient of the lion's share of Saudi aid. There is evidence that the CIA supported Hekmatyar's drug trade activities by giving him immunity for his opium trafficking, which financed the operation of his militant faction.
MAK and foreign "mujahideen" volunteers, or "Afghan Arabs," did not play a major role in the war. While over 250,000 Afghan "mujahideen" fought the Soviets and the communist Afghan government, it is estimated that were never more than 2,000 foreign "mujahideen" in the field at any one time. Nonetheless, foreign "mujahideen" volunteers came from 43 countries, and the total number that participated in the Afghan movement between 1982 and 1992 is reported to have been 35,000. Bin Laden played a central role in organizing training camps for the foreign Muslim volunteers.
The Soviet Union finally withdrew from Afghanistan in 1989. To the surprise of many, Mohammad Najibullah's communist Afghan government hung on for three more years, before being overrun by elements of the "mujahideen". With "mujahideen" leaders unable to agree on a structure for governance, chaos ensued, with constantly reorganizing alliances fighting for control of ill-defined territories, leaving the country devastated.
Expanding operations.
Toward the end of the Soviet military mission in Afghanistan, some "mujahideen" wanted to expand their operations to include Islamist struggles in other parts of the world, such as Israel and Kashmir. A number of overlapping and interrelated organizations were formed, to further those aspirations.
One of these was the organization that would eventually be called al-Qaeda, formed by bin Laden with an initial meeting held on August 11, 1988.
Notes of a meeting of bin Laden and others on August 20, 1988, indicate al-Qaeda was a formal group by that time: "basically an organized Islamic faction, its goal is to lift the word of God, to make His religion victorious." A list of requirements for membership itemized the following: listening ability, good manners, obedience, and making a pledge ("bayat") to follow one's superiors.
In his memoir, bin Laden's former bodyguard, Nasser al-Bahri, gives the only publicly available description of the ritual of giving "bayat" when he swore his allegiance to the al-Qaeda chief.
According to Wright, the group's real name wasn't used in public pronouncements because "its existence was still a closely held secret." His research suggests that al-Qaeda was formed at an August 11, 1988, meeting between "several senior leaders" of Egyptian Islamic Jihad, Abdullah Azzam, and bin Laden, where it was agreed to join bin Laden's money with the expertise of the Islamic Jihad organization and take up the jihadist cause elsewhere after the Soviets withdrew from Afghanistan.
Bin Laden wished to establish non-military operations in other parts of the world; Azzam, in contrast, wanted to remain focused on military campaigns. After Azzam was assassinated in 1989, the MAK split, with a significant number joining bin Laden's organization.
In November 1989, Ali Mohamed, a former special forces Sergeant stationed at Fort Bragg, North Carolina, left military service and moved to California. He traveled to Afghanistan and Pakistan and became "deeply involved with bin Laden's plans."
A year later, on November 8, 1990, the FBI raided the New Jersey home of Ali Mohammed's associate El Sayyid Nosair, discovering a great deal of evidence of terrorist plots, including plans to blow up New York City skyscrapers. Nosair was eventually convicted in connection to the 1993 World Trade Center bombing. In 1991, Ali Mohammed is said to have helped orchestrate bin Laden's relocation to Sudan.
Gulf War and the start of US enmity.
Following the Soviet Union's withdrawal from Afghanistan in February 1989, bin Laden returned to Saudi Arabia. The Iraqi invasion of Kuwait in August 1990 had put the Kingdom and its ruling House of Saud at risk. The world's most valuable oil fields were within easy striking distance of Iraqi forces in Kuwait, and Saddam's call to pan-Arab/Islamism could potentially rally internal dissent.
In the face of a seemingly massive Iraqi military presence, Saudi Arabia's own forces were well armed but far outnumbered. Bin Laden offered the services of his "mujahideen" to King Fahd to protect Saudi Arabia from the Iraqi army. The Saudi monarch refused bin Laden's offer, opting instead to allow US and allied forces to deploy troops into Saudi territory.
The deployment angered bin Laden, as he believed the presence of foreign troops in the "land of the two mosques" (Mecca and Medina) profaned sacred soil. After speaking publicly against the Saudi government for harboring American troops, he was banished and forced to live in exile in Sudan.
Sudan.
From around 1992 to 1996, al-Qaeda and bin Laden based themselves in Sudan at the invitation of Islamist theoretician Hassan al-Turabi. The move followed an Islamist coup d'état in Sudan, led by Colonel Omar al-Bashir, who professed a commitment to reordering Muslim political values. During this time, bin Laden assisted the Sudanese government, bought or set up various business enterprises, and established camps where insurgents trained.
A key turning point for bin Laden, further pitting him against the Sauds, occurred in 1993 when Saudi Arabia gave support for the Oslo Accords, which set a path for peace between Israel and Palestinians.
Zawahiri and the EIJ, who served as the core of al-Qaeda but also engaged in separate operations against the Egyptian government, had bad luck in Sudan. In 1993, a young schoolgirl was killed in an unsuccessful EIJ attempt on the life of the Egyptian prime minister, Atef Sedki. Egyptian public opinion turned against Islamist bombings, and the police arrested 280 of al-Jihad's members and executed 6.
Due to bin Laden's continuous verbal assault on King Fahd of Saudi Arabia, on March 5, 1994 Fahd sent an emissary to Sudan demanding bin Laden's passport; bin Laden's Saudi citizenship was also revoked. His family was persuaded to cut off his monthly stipend, $7 million ($ today) a year, and his Saudi assets were frozen. His family publicly disowned him. There is controversy over whether and to what extent he continued to garner support from members of his family and/or the Saudi government.
In June 1995, an even more ill-fated attempt to assassinate Egyptian president Mubarak led to the expulsion of EIJ, and in May 1996, of bin Laden, by the Sudanese government.
According to Pakistani-American businessman Mansoor Ijaz, the Sudanese government offered the Clinton Administration numerous opportunities to arrest bin Laden. Those opportunities were met positively by Secretary of State Madeleine Albright, but spurned when Susan Rice and counter-terrorism czar Richard Clarke persuaded National Security Advisor Sandy Berger to overrule Albright. Ijaz's claims appeared in numerous Op-Ed pieces, including one in the "Los Angeles Times" and one in "The Washington Post" co-written with former Ambassador to Sudan Timothy M. Carney. Similar allegations have been made by "Vanity Fair" contributing editor David Rose, and Richard Miniter, author of "Losing bin Laden", in a November 2003 interview with "World".
Several sources dispute Ijaz's claim, including the National Commission on Terrorist Attacks on the US (the 9–11 Commission), which concluded in part: Sudan's minister of defense, Fatih Erwa, has claimed that Sudan offered to hand Bin Ladin over to the US The Commission has found no credible evidence that this was so. Ambassador Carney had instructions only to push the Sudanese to expel Bin Ladin. Ambassador Carney had no legal basis to ask for more from the Sudanese since, at the time, there was no indictment out-standing.
Refuge in Afghanistan.
After the Soviet withdrawal, Afghanistan was effectively ungoverned for seven years and plagued by constant infighting between former allies and various "mujahideen" groups.
Throughout the 1990s, a new force began to emerge. The origins of the Taliban (literally "students") lay in the children of Afghanistan, many of them orphaned by the war, and many of whom had been educated in the rapidly expanding network of Islamic schools (madrassas) either in Kandahar or in the refugee camps on the Afghan-Pakistani border.
According to Ahmed Rashid, five leaders of the Taliban were graduates of Darul Uloom Haqqania, a madrassa in the small town of Akora Khattak. The town is situated near Peshawar in Pakistan, but largely attended by Afghan refugees. This institution reflected Salafi beliefs in its teachings, and much of its funding came from private donations from wealthy Arabs. Bin Laden's contacts were still laundering most of these donations, using "unscrupulous" Islamic banks to transfer the money to an "array" of charities which serve as front groups for al-Qaeda, or transporting cash-filled suitcases straight into Pakistan. Another four of the Taliban's leaders attended a similarly funded and influenced madrassa in Kandahar.
Many of the "mujahideen" who later joined the Taliban fought alongside Afghan warlord Mohammad Nabi Mohammadi's Harkat i Inqilabi group at the time of the Russian invasion. This group also enjoyed the loyalty of most Afghan Arab fighters.
The continuing internecine strife between various factions, and accompanying lawlessness following the Soviet withdrawal, enabled the growing and well-disciplined Taliban to expand their control over territory in Afghanistan, and it came to establish an enclave which it called the Islamic Emirate of Afghanistan. In 1994, it captured the regional center of Kandahar, and after making rapid territorial gains thereafter, conquered the capital city Kabul in September 1996.
After the Sudanese made it clear, in May 1996, that bin Laden would never be welcome to return, Taliban-controlled Afghanistan—with previously established connections between the groups, administered with a shared militancy, and largely isolated from American political influence and military power—provided a perfect location for al-Qaeda to relocate its headquarters. Al-Qaeda enjoyed the Taliban's protection and a measure of legitimacy as part of their Ministry of Defense, although only Pakistan, Saudi Arabia, and the United Arab Emirates recognized the Taliban as the legitimate government of Afghanistan.
While in Afghanistan, the Taliban government tasked al-Qaeda with the training of Brigade 055, an elite part of the Taliban's army from 1997–2001. The Brigade was made up of mostly foreign fighters, many veterans from the Soviet Invasion, and all under the same basic ideology of the mujahideen. In November 2001, as Operation Enduring Freedom had toppled the Taliban government, many Brigade 055 fighters were captured or killed, and those that survived were thought to head into Pakistan along with bin Laden.
By the end of 2008, some sources reported that the Taliban had severed any remaining ties with al-Qaeda, while others cast doubt on this. According to senior US military intelligence officials, there were fewer than 100 members of al-Qaeda remaining in Afghanistan in 2009.
Call for global jihad.
Around 1994, the Salafi groups waging "jihad" in Bosnia entered into a seemingly irreversible decline. As they grew less and less aggressive, groups such as EIJ began to drift away from the Salafi cause in Europe. Al-Qaeda decided to step in and assumed control of around 80% of the terrorist cells in Bosnia in late 1995.
At the same time, al-Qaeda ideologues instructed the network's recruiters to look for "Jihadi international" Muslims who believed that "jihad" must be fought on a global level. The concept of a "global Salafi "jihad"" had been around since at least the early 1980s. Several groups had formed for the explicit purpose of driving non-Muslims out of every Muslim land at the same time and with maximum carnage. This was, however, a fundamentally defensive strategy.
Al-Qaeda sought to open the "offensive phase" of the global Salafi "jihad". Bosnian Islamists in 2006 called for "solidarity with Islamic causes around the world", supporting the insurgents in Kashmir and Iraq as well as the groups fighting for a Palestinian state.
Fatwas.
In 1996, al-Qaeda announced its "jihad" to expel foreign troops and interests from what they considered Islamic lands. Bin Laden issued a "fatwa" (binding religious edict), which amounted to a public declaration of war against the US and its allies, and began to refocus al-Qaeda's resources on large-scale, propagandist strikes.
On February 23, 1998, bin Laden and Ayman al-Zawahiri, a leader of Egyptian Islamic Jihad, along with three other Islamist leaders, co-signed and issued a "fatwa" calling on Muslims to kill Americans and their allies where they can, when they can. Under the banner of the World Islamic Front for Combat Against the Jews and Crusaders, they declared:
[T]he ruling to kill the Americans and their allies—civilians and military—is an individual duty for every Muslim who can do it in any country in which it is possible to do it, in order to liberate the al-Aqsa Mosque [in Jerusalem] and the holy mosque [in Mecca] from their grip, and in order for their armies to move out of all the lands of Islam, defeated and unable to threaten any Muslim. This is in accordance with the words of Almighty Allah, 'and fight the pagans all together as they fight you all together,' and 'fight them until there is no more tumult or oppression, and there prevail justice and faith in Allah'.
Neither bin Laden nor al-Zawahiri possessed the traditional Islamic scholarly qualifications to issue a "fatwa". However, they rejected the authority of the contemporary "ulema" (which they saw as the paid servants of "jahiliyya" rulers), and took it upon themselves. Former Russian FSB agent Alexander Litvinenko, who was later killed, said that the FSB trained al-Zawahiri in a camp in Dagestan eight months before the 1998 "fatwa".
Iraq.
Al-Qaeda is Sunni, and often attacked the Iraqi Shia majority in an attempt to incite sectarian violence and greater chaos in the country. Al-Zarqawi purportedly declared an all-out war on Shiites while claiming responsibility for Shiite mosque bombings. The same month, a statement claiming to be by AQI rejected as "fake" a letter allegedly written by al-Zawahiri, in which he appears to question the insurgents' tactic of indiscriminately attacking Shiites in Iraq. In a December 2007 video, al-Zawahiri defended the Islamic State in Iraq, but distanced himself from the attacks against civilians committed by "hypocrites and traitors existing among the ranks".
US and Iraqi officials accused AQI of trying to slide Iraq into a full-scale civil war between Iraq's majority Shiites and minority Sunni Arabs, with an orchestrated campaign of civilian massacres and a number of provocative attacks against high-profile religious targets. With attacks such as the 2003 Imam Ali Mosque bombing, the 2004 Day of Ashura and Karbala and Najaf bombings, the 2006 first al-Askari Mosque bombing in Samarra, the deadly single-day series of bombings in which at least 215 people were killed in Baghdad's Shiite district of Sadr City, and the second al-Askari bombing in 2007, they provoked Shiite militias to unleash a wave of retaliatory attacks, resulting in death squad-style killings and spiraling further sectarian violence which escalated in 2006 and brought Iraq to the brink of violent anarchy in 2007. In 2008, sectarian bombings blamed on al-Qaeda in Iraq killed at least 42 people at the Imam Husayn Shrine in Karbala in March, and at least 51 people at a bus stop in Baghdad in June.
In February 2014, after a prolonged dispute with al-Qaeda in Iraq's successor organisation, the Islamic State of Iraq and the Levant (ISIS), al-Qaeda publicly announced it was cutting all ties with the group, reportedly for its brutality and "notorious intractability".
Somalia and Yemen.
In Somalia, al-Qaeda agents had been collaborating closely with its Somali wing, which was created from the al-Shabaab group. In February 2012, al-Shabaab officially joined al-Qaeda, declaring loyalty in a joint video. The Somalian al-Qaeda actively recruit children for suicide-bomber training, and export young people to participate in military actions against Americans at the AfPak border.
The percentage of terrorist attacks in the West originating from the Afghanistan-Pakistan (AfPak) border declined considerably from almost 100% to 75% in 2007, and to 50% in 2010, as al-Qaeda shifted to Somalia and Yemen. While al-Qaeda leaders are hiding in the tribal areas along the AfPak border, the middle-tier of the movement display heightened activity in Somalia and Yemen.
"We know that South Asia is no longer their primary base," a US defense agency source said. "They are looking for a hide-out in other parts of the world, and continue to expand their organization."
In January 2009, al-Qaeda's division in Saudi Arabia merged with its Yemeni wing to form al-Qaeda in the Arabian Peninsula. Centered in Yemen, the group takes advantage of the country's poor economy, demography and domestic security. In August 2009, they made the first assassination attempt against a member of the Saudi royal dynasty in decades. President Obama asked his Yemen counterpart Ali Abdullah Saleh to ensure closer cooperation with the US in the struggle against the growing activity of al-Qaeda in Yemen, and promised to send additional aid. Because of the wars in Iraq and Afghanistan, the US was unable to pay sufficient attention to Somalia and Yemen, which could cause problems in the near future. In December 2011, US Secretary of Defense Leon Panetta said that the US operations against al-Qaeda "are now concentrating on key groups in Yemen, Somalia and North Africa." Al-Qaeda in the Arabian Peninsula claimed responsibility for the 2009 bombing attack on Northwest Airlines Flight 253 by Umar Farouk Abdulmutallab. The group released photos of Abdulmutallab smiling in a white shirt and white Islamic skullcap, with the al-Qaeda in Arabian Peninsula banner in the background.
United States operations.
In December 1998, the Director of the CIA Counterterrorism Center reported to the president that al-Qaeda was preparing for attacks in the USA, including the training of personnel to hijack aircraft. On September 11, 2001, al-Qaeda attacked the United States, hijacking four airliners within the country and deliberately crashing two into the twin towers of the World Trade Center in New York City, New York, and the third into the western side of the Pentagon in Arlington County, Virginia. The fourth, however, failed to reach its intended target — either the United States Capital or the White House, both located in Washington D.C., Maryland — due to the rebellion by the passengers to retake the airliner, and instead crashed into the field in Shanksville, Pennsylvania. In total, the attackers killed 2,977 victims, including 2,507 civilians, 72 law enforcement officers, 343 firefighters, and 55 military personnel.
U.S. officials called Anwar al-Awlaki an "example of al-Qaeda reach into" the U.S. in 2008 after probes into his ties to the September 11 attacks hijackers. A former FBI agent identifies Awlaki as a known "senior recruiter for al-Qaeda", and a spiritual motivator. Awlaki's sermons in the U.S. were attended by three of the 9/11 hijackers, as well as accused Fort Hood shooter Nidal Malik Hasan. U.S. intelligence intercepted emails from Hasan to Awlaki between December 2008 and early 2009. On his website, Awlaki has praised Hasan's actions in the Fort Hood shooting.
An unnamed official claimed there was good reason to believe Awlaki "has been involved in very serious terrorist activities since leaving the U.S. [after 9/11], including plotting attacks against America and our allies." In addition, "Christmas Day bomber" Umar Farouk Abdulmutallab said al-Awlaki was one of his al-Qaeda trainers, meeting with him and involved in planning or preparing the attack, and provided religious justification for it, according to unnamed U.S. intelligence officials. In March 2010, al‑Awlaki said in a videotape delivered to CNN that jihad against America was binding upon himself and every other able Muslim.
US President Barack Obama approved the targeted killing of al-Awlaki by April 2010, making al-Awlaki the first US citizen ever placed on the CIA target list. That required the consent of the U.S. National Security Council, and officials said it was appropriate for an individual who posed an imminent danger to national security. In May 2010, Faisal Shahzad, who pleaded guilty to the 2010 Times Square car bombing attempt, told interrogators he was "inspired by" al-Awlaki, and sources said Shahzad had made contact with al-Awlaki over the internet. Representative Jane Harman called him "terrorist number one", and "Investor's Business Daily" called him "the world's most dangerous man". In July 2010, the US Treasury Department added him to its list of Specially Designated Global Terrorists, and the UN added him to its list of individuals associated with al-Qaeda. In August 2010, al-Awlaki's father initiated a lawsuit against the U.S. government with the American Civil Liberties Union, challenging its order to kill al-Awlaki. In October 2010, U.S. and U.K. officials linked al-Awlaki to the 2010 cargo plane bomb plot. In September 2011, he was killed in a targeted killing drone attack in Yemen. It was reported on March 16, 2012 that Osama bin Laden plotted to kill United States President Barack Obama.
Death of Osama bin Laden.
On May 1, 2011 in Washington, D.C. (May 2, Pakistan Standard Time), U.S. President Barack Obama announced that Osama bin Laden had been killed by "a small team of Americans" acting under Obama's direct orders, in a covert operation in Abbottabad, Pakistan, about 50 km north of Islamabad. According to U.S. officials a team of 20–25 US Navy SEALs under the command of the Joint Special Operations Command and working with the CIA stormed bin Laden's compound in two helicopters. Bin Laden and those with him were killed during a firefight in which U.S. forces experienced no injuries or casualties. According to one US official the attack was carried out without the knowledge or consent of the Pakistani authorities. In Pakistan some people were reported to be shocked at the unauthorized incursion by US armed forces. The site is a few miles from the Pakistan Military Academy in Kakul. In his broadcast announcement President Obama said that U.S. forces "took care to avoid civilian casualties."
Details soon emerged that three men and a woman were killed along with bin Laden, the woman being killed when she was "used as a shield by a male combatant". DNA from bin Laden's body, compared with DNA samples on record from his dead sister, confirmed bin Laden's identity. The body was recovered by the US military and was in its custody until, according to one US official, his body was buried at sea according to Islamic traditions. One U.S. official stated that "finding a country willing to accept the remains of the world's most wanted terrorist would have been difficult." U.S State Department issued a "Worldwide caution" for Americans following bin Laden's death and U.S Diplomatic facilities everywhere were placed on high alert, a senior U.S official said. Crowds gathered outside the White House and in New York City's Times Square to celebrate bin Laden's death.
Syria.
In 2003, President Bashar al-Assad revealed in an interview with a Kuwaiti newspaper that he doubted the organization of al-Qaeda even existed. He was quoted as saying, "Is there really an entity called al-Qaeda? Was it in Afghanistan? Does it exist now?" He went on further to remark about bin Laden commenting, he "cannot talk on the phone or use the Internet, but he can direct communications to the four corners of the world? This is illogical."
Following the mass protests that took place later in 2011 demanding the resignation of al-Assad, al-Qaeda affiliated organizations and Sunni sympathizers soon began to constitute the most effective fighting force in the Syrian opposition. Until then, al-Qaeda's presence in Syria was not worth mentioning, but its growth thereafter was rapid. Groups such as the al-Nusra Front and the Islamic State of Iraq and the Levant (ISIS; sometimes ISIL) have recruited many foreign Mujahideen to train and fight in what has gradually become a highly sectarian war. Ideologically, the Syrian Civil War has served the interests of al-Qaeda as it pits a mainly Sunni opposition against a Shia backed Alawite regime. Viewing Shia Islam as heretical, al-Qaeda and other fundamentalist Sunni militant groups have invested heavily in the civil conflict, actively backing and supporting the Syrian Opposition despite its clashes with moderate opposition groups such as the Free Syrian Army (FSA).
On February 2, 2014, al-Qaeda distanced itself from ISIS and its actions in Syria.
India.
In September 2014 al-Zawahiri announced al-Qaeda was establishing a front in India to "wage jihad against its enemies, to liberate its land, to restore its sovereignty, and to revive its Caliphate." He nominated India as a beachhead for regional jihad taking in neighboring countries such as Myanmar and Bangladesh. The motivation for the video was questioned in some quarters where it was seen the militant group was struggling to remain relevant in light of the emerging prominence of ISIS. Reaction amongst Muslims in India to the formation of the new wing, to be known as "Qaedat al-Jihad fi'shibhi al-qarrat al-Hindiya" or al-Qaida in the Indian Subcontinent [AQIS], was one of fury. Leaders of several Indian Muslim organizations rejected al-Zawahiri's pronouncement, saying they could see no good coming from it, and viewed it as a threat to Muslim youth in the country.
US intelligence analyst accused the Pakistan military of 'stage-managing' the terror outfit's latest advance into India. Bruce Riedel, a former CIA analyst and National Security Council official for South Asia, also said that Pakistan should be warned that it will be placed on the list of states sponsoring terrorism. Riedel also said that "Zawahiri made the tape in his hideout in Pakistan, no doubt, and many Indians suspect the ISI (Inter Services Intelligence) is helping to protect him," he wrote.
Attacks.
Al-Qaeda has carried out a total of six major terrorist attacks, four of them in its jihad against America. In each case the leadership planned the attack years in advance, arranging for the shipment of weapons and explosives and using its privatized businesses to provide operatives with safehouses and false identities.
Al-Qaeda usually does not disburse funds for attacks, and very rarely makes wire transfers.
1992.
On December 29, 1992, al-Qaeda's first terrorist attack took place as two bombs were detonated in Aden, Yemen. The first target was the Movenpick Hotel and the second was the parking lot of the Goldmohur Hotel.
The bombings were an attempt to eliminate American soldiers on their way to Somalia to take part in the international famine relief effort, Operation Restore Hope. Internally, al-Qaeda considered the bombing a victory that frightened the Americans away, but in the US, the attack was barely noticed.
No Americans were killed because the soldiers were staying in a different hotel altogether, and they went on to Somalia as scheduled. However, two people were killed in the bombing, an Australian tourist and a Yemeni hotel worker. Seven others, mostly Yemenis, were severely injured. Two fatwas are said to have been appointed by the most theologically knowledgeable of al-Qaeda's members, Mamdouh Mahmud Salim, to justify the killings according to Islamic law. Salim referred to a famous fatwa appointed by Ibn Taymiyyah, a 13th-century scholar much admired by Wahhabis, which sanctioned resistance by any means during the Mongol invasions.
1993 World Trade Center bombing.
On February 26, 1993, Ramzi Yousef detonated a truck bomb inside a parking garage of Tower One of the World Trade Center in New York City. His hope was to destroy the foundation of Tower One, knocking it into Tower Two, and resulting in the collapse of the towers, with the goal of causing the deaths of tens of thousands of people. The towers shook and swayed but the complex held and he succeeded in killing only six civilians, injuring 919 others (including an EMS worker), 88 firefighters, and 35 police officers, and causing nearly $300 million in property damage.
After the attack, Yousef fled to Pakistan and later moved to Manila. There he began developing the Bojinka plot plans to implode a dozen American airliners simultaneously, to assassinate Pope John Paul II and President Bill Clinton, and to crash a private plane into CIA headquarters. He was later captured in Pakistan.
None of the U.S. government's indictments against bin Laden have suggested that he had any connection with this bombing, but Ramzi Yousef is known to have attended a terrorist training camp in Afghanistan. After his capture, Yousef declared that his primary justification for the attack was to punish the U.S. for its support for the Israeli occupation of Palestinian territories and made no mention of any religious motivations. A follow-up attack was planned by Omar Abdel-Rahman – the New York City landmark bomb plot. However, the plot was foiled by the authorities.
Late 1990s.
In 1996, bin Laden personally engineered a plot to assassinate Clinton while the president was in Manila for the Asia-Pacific Economic Cooperation. However, intelligence agents intercepted a message just minutes before the motorcade was to leave, and alerted the U.S. Secret Service. Agents later discovered a bomb planted under a bridge.
On August 7, 1998, al-Qaeda bombed the U.S. embassies in East Africa, killing 224 people, including 12 Americans. In retaliation, a barrage of cruise missiles launched by the U.S. military devastated an al-Qaeda base in Khost, Afghanistan, but the network's capacity was unharmed. In late 1999/2000, Al-Qaeda planned attacks to coincide with the millennium, masterminded by Abu Zubaydah and involving Abu Qatada, which would include the bombing Christian holy sites in Jordan, the bombing of Los Angeles International Airport by Ahmed Ressam, and the bombing of the .
On October 12, 2000, al-Qaeda militants in Yemen bombed the missile destroyer "USS Cole" in a suicide attack, killing 17 U.S. servicemen and damaging the vessel while it lay offshore. Inspired by the success of such a brazen attack, al-Qaeda's command core began to prepare for an attack on the U.S. itself.
September 11 attacks.
The September 11, 2001 attacks were the most devastating terrorist acts in American history, killing 2,977 victims, including 2,507 civilians, 72 law enforcement officers, 343 firefighters, and 55 military personnel. Two commercial airliners were deliberately flown into the twin towers of the World Trade Center, a third into the Pentagon, and a fourth, originally intended to target either the United States Capitol or the White House, crashed in a field in Stonycreek Township near Shanksville, Pennsylvania. The event was also the deadliest foreign attack on American soil since the Japanese surprise attack on Pearl Harbor on December 7, 1941.
The attacks were conducted by al-Qaeda, acting in accord with the 1998 "fatwa" issued against the U.S. and its allies by persons under the command of bin Laden, al-Zawahiri, and others. Evidence points to suicide squads led by al-Qaeda military commander Mohamed Atta as the culprits of the attacks, with bin Laden, Ayman al-Zawahiri, Khalid Sheikh Mohammed, and Hambali as the key planners and part of the political and military command.
Messages issued by bin Laden after September 11, 2001, praised the attacks, and explained their motivation while denying any involvement. Bin Laden legitimized the attacks by identifying grievances felt by both mainstream and Islamist Muslims, such as the general perception that the U.S. was actively oppressing Muslims.
Bin Laden asserted that America was massacring Muslims in 'Palestine, Chechnya, Kashmir and Iraq' and that Muslims should retain the 'right to attack in reprisal'. He also claimed the 9/11 attacks were not targeted at people, but 'America's icons of military and economic power', despite the fact he planned to attack in the morning where most of the people in the intended targets were present and thus generating massive amount of human casualties.
Evidence has since come to light that the original targets for the attack may have been nuclear power stations on the east coast of the U.S. The targets were later altered by al-Qaeda, as it was feared that such an attack "might get out of hand".
Designation as terrorist organization.
Al-Qaeda has been designated a "terrorist organization" by the following countries and international organizations:
War on Terrorism.
In the immediate aftermath of the attacks, the US government decided to respond militarily, and began to prepare its armed forces to overthrow the Taliban regime it believed was harboring al-Qaeda. Before the US attacked, it offered Taliban leader Mullah Omar a chance to surrender bin Laden and his top associates. The first forces to be inserted into Afghanistan were Paramilitary Officers from the CIA's elite Special Activities Division (SAD).
The Taliban offered to turn over bin Laden to a neutral country for trial if the US would provide evidence of bin Laden's complicity in the attacks. US President George W. Bush responded by saying: "We know he's guilty. Turn him over", and British Prime Minister Tony Blair warned the Taliban regime: "Surrender bin Laden, or surrender power".
Soon thereafter the US and its allies invaded Afghanistan, and together with the Afghan Northern Alliance removed the Taliban government in the war in Afghanistan.
As a result of the US using its special forces and providing air support for the Northern Alliance ground forces, both Taliban and al-Qaeda training camps were destroyed, and much of the operating structure of al-Qaeda is believed to have been disrupted. After being driven from their key positions in the Tora Bora area of Afghanistan, many al-Qaeda fighters tried to regroup in the rugged Gardez region of the nation.
Again, under the cover of intense aerial bombardment, US infantry and local Afghan forces attacked, shattering the al-Qaeda position and killing or capturing many of the militants. By early 2002, al-Qaeda had been dealt a serious blow to its operational capacity, and the Afghan invasion appeared an initial success. Nevertheless, a significant Taliban insurgency remains in Afghanistan, and al-Qaeda's top two leaders, bin Laden and al-Zawahiri, evaded capture.
Debate raged about the exact nature of al-Qaeda's role in the 9/11 attacks, and after the US invasion began, the US State Department also released a videotape showing bin Laden speaking with a small group of associates somewhere in Afghanistan shortly before the Taliban was removed from power. Although its authenticity has been questioned by some, the tape appears to implicate bin Laden and al-Qaeda in the September 11 attacks and was aired on many television channels all over the world, with an accompanying English translation provided by the U.S. Defense Department.
In September 2004, the US government 9/11 Commission investigating the September 11 attacks officially concluded that the attacks were conceived and implemented by al-Qaeda operatives. In October 2004, bin Laden appeared to claim responsibility for the attacks in a videotape released through Al Jazeera, saying he was inspired by Israeli attacks on high-rises in the 1982 invasion of Lebanon: "As I looked at those demolished towers in Lebanon, it entered my mind that we should punish the oppressor in kind and that we should destroy towers in America in order that they taste some of what we tasted and so that they be deterred from killing our women and children."
By the end of 2004, the U.S. government proclaimed that two-thirds of the most senior al-Qaeda figures from 2001 had been captured and interrogated by the CIA: Abu Zubaydah, Ramzi bin al-Shibh and Abd al-Rahim al-Nashiri in 2002; Khalid Sheikh Mohammed in 2003; and Saif al Islam el Masry in 2004. Mohammed Atef and several others were killed. The West was criticised for not being able to comprehend or deal with Al-Qaida despite more than a decade of the war. This also meant no progress has been made in global state security.
Activities.
Africa.
Al-Qaeda involvement in Africa has included a number of bombing attacks in North Africa, as well as supporting parties in civil wars in Eritrea and Somalia. From 1991 to 1996, bin Laden and other al-Qaeda leaders were based in Sudan.
Islamist rebels in the Sahara calling themselves al-Qaeda in the Islamic Maghreb have stepped up their violence in recent years. French officials say the rebels have no real links to the al-Qaeda leadership, but this is a matter of some dispute in the international press and amongst security analysts. It seems likely that bin Laden approved the group's name in late 2006, and the rebels "took on the al Qaeda franchise label", almost a year before the violence began to escalate.
In Mali, the Ansar Dine faction was also reported as an ally of Al-Qaeda in 2013. The Ansar al Dine faction aligned themselves with the AQIM.
Following the Libyan Civil War, the removal of Gaddafi and the ensuing period of post-civil war violence in Libya allowed various Islamist militant organizations affiliated with Al-Qaeda to expand their operations in the region. The 2012 Benghazi attack, which resulted in the death of US Ambassador J. Christopher Stevens and 3 other Americans, is suspected of having been carried out by various Jihadist networks, such as Al-Qaeda in the Islamic Maghreb, Ansar al-Sharia and several other Al-Qaeda affiliated groups. The capture of Nazih Abdul-Hamed al-Ruqai, a senior Al-Qaeda operative wanted by the United States for his involvement in the 1998 United States embassy bombings, on October 5, 2013 by US Navy Seals, FBI and CIA agents illustrates the importance the US and other Western allies have placed on North Africa.
Europe.
Prior to the September 11 attacks, Al-Qaeda was present in Bosnia and Herzegovina, and its members were mostly former veterans of the El Mudžahid detachment of the Bosnian Muslim Army of the Republic of Bosnia and Herzegovina. In 1997 three members of al Qaeda carried out the terrorist attack in Mostar. They were closely linked to and financed by the Saudi High Commission for Relief of Bosnia and Herzegovina founed by then-prince King Salman of Saudi Arabia.
Before the 9/11 attacks and the US invasion of Afghanistan, recruits at Al-Qaeda training camps who had Western backgrounds were especially sought after by Al-Qaeda's military wing for conducting operations overseas. Language skills and knowledge of Western culture were generally found among recruits from Europe, such was the case with Mohamed Atta, an Egyptian national studying in Germany at the time of his training, and other members of the Hamburg Cell. Osama bin Laden and Mohammed Atef would later designate Atta as the ringleader of the 9/11 hijackers. Following the attacks, Western intelligence agencies determined that Al-Qaeda cells operating in Europe had aided the hijackers with financing and communications with the central leadership based in Afghanistan.
In 2003, Islamists carried out a series of bombings in Istanbul killing fifty-seven people and injuring seven hundred. Seventy-four people were charged by the Turkish authorities. Some had previously met bin Laden, and though they specifically declined to pledge allegiance to al-Qaeda they asked for its blessing and help.
In 2009, three Londoners, Tanvir Hussain, Assad Sarwar and Ahmed Abdullah Ali, were convicted of conspiring to detonate bombs disguised as soft drinks on seven airplanes bound for Canada and the U.S. The massively complex police and MI5 investigation of the plot involved more than a year of surveillance work conducted by over two hundred officers. British and U.S. officials said the plan—unlike many recent homegrown European terrorist plots—was directly linked to al-Qaeda and guided by senior Islamic militants in Pakistan.
In 2012, Russian Intelligence indicated that al-Qaeda had given a call for "forest jihad" and has been starting massive forest fires as part of a strategy of "thousand cuts".
Arab world.
Following Yemeni unification in 1990, Wahhabi networks began moving missionaries into the country in an effort to subvert the capitalist north. Although it is unlikely bin Laden or Saudi al-Qaeda were directly involved, the personal connections they made would be established over the next decade and used in the "USS Cole" bombing. Concerns grow over Al Qaeda's group in Yemen.
In Iraq, al-Qaeda forces loosely associated with the leadership were embedded in the Jama'at al-Tawhid wal-Jihad organization commanded by Abu Musab al-Zarqawi. Specializing in suicide operations, they have been a "key driver" of the Sunni insurgency. Although they played a small part in the overall insurgency, between 30% and 42% of all suicide bombings which took place in the early years were claimed by Zarqawi's organization. Reports have indicated that oversights such as the failure to control access to the Qa'qaa munitions factory in Yusufiyah have allowed large quantities of munitions to fall into the hands of al-Qaida. In November 2010, the Islamic State of Iraq militant group, which is linked to al-Qaeda in Iraq, threatened to "exterminate Iraqi Christians".
Significantly, it was not until the late 1990s that al-Qaeda began training Palestinians. This is not to suggest that resistance fighters are underrepresented in the network as a number of Palestinians, mostly coming from Jordan, wanted to join and have risen to serve high-profile roles in Afghanistan. Rather, large groups such as Hamas and Palestinian Islamic Jihad—which cooperate with al-Qaeda in many respects—have had difficulties accepting a strategic alliance, fearing that al-Qaeda will co-opt their smaller cells. This may have changed recently, as Israeli security and intelligence services believe al-Qaeda has managed to infiltrate operatives from the Occupied Territories into Israel, and is waiting for the right time to mount an attack.
Kashmir.
Bin Laden and Ayman al-Zawahiri consider India to be a part of the 'Crusader-Zionist-Hindu' conspiracy against the Islamic world. According to the 2005 report 'Al Qaeda: Profile and Threat Assessment' by Congressional Research Service, bin Laden was involved in training militants for Jihad in Kashmir while living in Sudan in the early nineties. By 2001, Kashmiri militant group Harkat-ul-Mujahideen had become a part of the al-Qaeda coalition. According to the United Nations High Commissioner for Refugees al-Qaeda was thought to have established bases in Pakistan-administered Kashmir (in Azad Kashmir, and to some extent in Gilgit–Baltistan) during the 1999 Kargil War and continued to operate there with tacit approval of Pakistan's Intelligence services.
Many of the militants active in Kashmir were trained in the same Madrasahs as Taliban and al-Qaeda. Fazlur Rehman Khalil of Kashmiri militant group Harkat-ul-Mujahideen was a signatory of al-Qaeda's 1998 declaration of Jihad against America and its allies. In a 'Letter to American People' written by bin Laden in 2002 he stated that one of the reasons he was fighting America is because of its support to India on the Kashmir issue. In November 2001, Kathmandu airport went on high alert after threats that bin Laden planned to hijack a plane from there and crash it into a target in New Delhi. In 2002, US Secretary of Defense Donald Rumsfeld, on a trip to Delhi, suggested that al-Qaeda was active in Kashmir though he did not have any hard evidence. He proposed hi tech ground sensors along the line of control to prevent militants from infiltrating into Indian administered Kashmir.
An investigation in 2002 unearthed evidence that al-Qaeda and its affiliates were prospering in Pakistan-administered Kashmir with tacit approval of Pakistan's National Intelligence agency Inter-Services Intelligence In 2002, a special team of Special Air Service and Delta Force was sent into Indian Administered Kashmir to hunt for bin Laden after reports that he was being sheltered by Kashmiri militant group Harkat-ul-Mujahideen which had previously been responsible for 1995 Kidnapping of western tourists in Kashmir. Britain's highest ranking al-Qaeda operative Rangzieb Ahmed had previously fought in Kashmir with the group Harkat-ul-Mujahideen and spent time in Indian prison after being captured in Kashmir.
US officials believe that al-Qaeda was helping organize a campaign of terror in Kashmir in order to provoke conflict between India and Pakistan. Their strategy was to force Pakistan to move its troops to the border with India, thereby relieving pressure on al-Qaeda elements hiding in northwestern Pakistan. In 2006 al-Qaeda claimed they had established a wing in Kashmir; this has worried the Indian government. However the Indian Army Lt. Gen. H.S. Panag, GOC-in-C Northern Command, said to reporters that the army has ruled out the presence of al-Qaeda in Indian-administered Jammu and Kashmir; furthermore he said that there is nothing that can verify reports from the media of al-Qaeda presence in the state. He however stated that al-Qaeda had strong ties with Kashmiri militant groups Lashkar-e-Taiba and Jaish-e-Mohammed based in Pakistan. It has been noted that Waziristan has now become the new battlefield for Kashmiri militants fighting NATO in support of al-Qaeda and Taliban. Dhiren Barot, who wrote the "Army of Madinah in Kashmir" and was an al-Qaeda operative convicted for involvement in the 2004 financial buildings plot, had received training in weapons and explosives at a militant training camp in Kashmir.
Maulana Masood Azhar, the founder of another Kashmiri group Jaish-e-Mohammed, is believed to have met bin Laden several times and received funding from him. In 2002, Jaish-e-Mohammed organized the kidnapping and murder of Daniel Pearl in an operation run in conjunction with al-Qaeda and funded by bin Laden. According to American counter-terrorism expert Bruce Riedel, al-Qaeda and Taliban were closely involved in the 1999 hijacking of Indian Airlines Flight 814 to Kandahar which led to the release of Maulana Masood Azhar & Ahmed Omar Saeed Sheikh from an Indian prison in exchange for the passengers. This hijacking, Riedel stated, was rightly described by then Indian Foreign minister Jaswant Singh as a 'dress rehearsal' for September 11 attacks. Bin Laden personally welcomed Azhar and threw a lavish party in his honor after his release, according to Abu Jandal, bodyguard of bin Laden. Ahmed Omar Saeed Sheikh, who had been in Indian prison for his role in 1994 kidnappings of Western tourists in India, went on to murder Daniel Pearl and was sentenced to death by Pakistan. Al-Qaeda operative Rashid Rauf, who was one of the accused in 2006 transatlantic aircraft plot, was related to Maulana Masood Azhar by marriage.
Lashkar-e-Taiba, a Kashmiri militant group which is thought to be behind 2008 Mumbai attacks, is also known to have strong ties to senior al-Qaeda leaders living in Pakistan. In Late 2002, top al-Qaeda operative Abu Zubaydah was arrested while being sheltered by Lashkar-e-Taiba in a safe house in Faisalabad. The FBI believes that al-Qaeda and Lashkar have been 'intertwined' for a long time while the CIA has said that al-Qaeda funds Lashkar-e-Taiba. French investigating magistrate Jean-Louis Bruguière, who was the top French counter-terrorism official, told Reuters in 2009 that 'Lashkar-e-Taiba is no longer a Pakistani movement with only a Kashmir political or military agenda. Lashkar-e-Taiba is a member of al-Qaeda.'
In a video released in 2008, senior al-Qaeda operative American-born Adam Yahiye Gadahn stated that "victory in Kashmir has been delayed for years; it is the liberation of the jihad there from this interference which, Allah willing, will be the first step towards victory over the Hindu occupiers of that Islam land."
In September 2009, a US drone strike reportedly killed Ilyas Kashmiri who was the chief of Harkat-ul-Jihad al-Islami, a Kashmiri militant group associated with al-Qaeda. Kashmiri was described by Bruce Riedel as a 'prominent' al-Qaeda member while others have described him as head of military operations for al-Qaeda. Kashmiri was also charged by the US in a plot against Jyllands-Posten, the Danish newspaper which was at the center of Jyllands-Posten Muhammad cartoons controversy. US officials also believe that Kashmiri was involved in the Camp Chapman attack against the CIA. In January 2010, Indian authorities notified Britain of an al-Qaeda plot to hijack an Indian airlines or Air India plane and crash it into a British city. This information was uncovered from interrogation of Amjad Khwaja, an operative of Harkat-ul-Jihad al-Islami, who had been arrested in India.
In January 2010, US Defense secretary Robert Gates, while on a visit to Pakistan, stated that al-Qaeda was seeking to destabilize the region and planning to provoke a nuclear war between India and Pakistan.
Internet.
Timothy L. Thomas claims that in the wake of its evacuation from Afghanistan, al-Qaeda and its successors have migrated online to escape detection in an atmosphere of increased international vigilance. As a result, the organization's use of the Internet has grown more sophisticated, encompassing financing, recruitment, networking, mobilization, publicity, as well as information dissemination, gathering and sharing.
Abu Ayyub al-Masri's al-Qaeda movement in Iraq regularly releases short videos glorifying the activity of jihadist suicide bombers. In addition, both before and after the death of Abu Musab al-Zarqawi (the former leader of al-Qaeda in Iraq), the umbrella organization to which al-Qaeda in Iraq belongs, the Mujahideen Shura Council, has a regular presence on the Web.
The range of multimedia content includes guerrilla training clips, stills of victims about to be murdered, testimonials of suicide bombers, and videos that show participation in jihad through stylized portraits of mosques and musical scores. A website associated with al-Qaeda posted a video of captured American entrepreneur Nick Berg being decapitated in Iraq. Other decapitation videos and pictures, including those of Paul Johnson, Kim Sun-il, and Daniel Pearl, were first posted on jihadist websites.
In December 2004 an audio message claiming to be from bin Laden was posted directly to a website, rather than sending a copy to al Jazeera as he had done in the past.
Al-Qaeda turned to the Internet for release of its videos in order to be certain it would be available unedited, rather than risk the possibility of al Jazeera editors editing the videos and cutting out anything critical of the Saudi royal family. Bin Laden's December 2004 message was much more vehement than usual in this speech, lasting over an hour.
In the past, Alneda.com and Jehad.net were perhaps the most significant al-Qaeda websites. Alneda was initially taken down by American Jon Messner, but the operators resisted by shifting the site to various servers and strategically shifting content.
The US is currently attempting to extradite a British information technology specialist, Babar Ahmad, on charges of operating a network of English-language al-Qaeda websites, such as Azzam.com. Ahmad's extradition is opposed by various British Muslim organizations, such as the Muslim Association of Britain.
Online Communications.
In 2007, al-Qaeda released Mujahedeen Secrets, encryption software used for online and cellular communications. A later version, Mujahideen Secrets 2, was released in 2008.
Aviation network.
Al-Qaeda is believed to be operating a clandestine aviation network including "several Boeing 727 aircraft", turboprops and executive jets, according to a Reuters story. Based on a U.S. Department of Homeland Security report, the story said that al-Qaeda is possibly using aircraft to transport drugs and weapons from South America to various unstable countries in West Africa. A Boeing 727 can carry up to 10 tons of cargo. The drugs eventually are smuggled to Europe for distribution and sale, and the weapons are used in conflicts in Africa and possibly elsewhere. Gunmen with links to al-Qaeda have been increasingly kidnapping some Europeans for ransom. The profits from the drug and weapon sales, and kidnappings can, in turn, fund more militant activities.
Involvement in military conflicts.
The following is a list of military conflicts in which Al-Qaeda and its direct affiliates have taken part militarily.
Alleged CIA involvement.
 Experts debate whether or not the al-Qaeda attacks were blowback from the American CIA's "Operation Cyclone" program to help the Afghan mujahideen. Robin Cook, British Foreign Secretary from 1997 to 2001, has written that al-Qaeda and bin Laden were "a product of a monumental miscalculation by western security agencies", and that "Al-Qaida, literally 'the database', was originally the computer file of the thousands of mujahideen who were recruited and trained with help from the CIA to defeat the Russians."
Munir Akram, Permanent Representative of Pakistan to the United Nations from 2002 to 2008, wrote in a letter published in the New York Times on January 19, 2008:
The strategy to support the Afghans against Soviet military intervention was evolved by several intelligence agencies, including the C.I.A. and Inter-Services Intelligence, or ISI. After the Soviet withdrawal, the Western powers walked away from the region, leaving behind 40,000 militants imported from several countries to wage the anti-Soviet jihad. Pakistan was left to face the blowback of extremism, drugs and guns.
A variety of sources—CNN journalist Peter Bergen, Pakistani ISI Brigadier Mohammad Yousaf, and CIA operatives involved in the Afghan program, such as Vincent Cannistraro—deny that the CIA or other American officials had contact with the foreign mujahideen or bin Laden, let alone armed, trained, coached or indoctrinated them.
Bergen and others argue that there was no need to recruit foreigners unfamiliar with the local language, customs or lay of the land since there were a quarter of a million local Afghans willing to fight; that foreign mujahideen themselves had no need for American funds since they received several hundred million dollars a year from non-American, Muslim sources; that Americans could not have trained mujahideen because Pakistani officials would not allow more than a handful of them to operate in Pakistan and none in Afghanistan; and that the Afghan Arabs were almost invariably militant Islamists reflexively hostile to Westerners whether or not the Westerners were helping the Muslim Afghans.
According to Bergen, known for conducting the first television interview with bin Laden in 1997, the idea that "the CIA funded bin Laden or trained bin Laden... [is] a folk myth. There's no evidence of this... Bin Laden had his own money, he was anti-American and he was operating secretly and independently... The real story here is the CIA didn't really have a clue about who this guy was until 1996 when they set up a unit to really start tracking him." But Bergen conceded that, in one "strange incident," the CIA appeared to give visa help to mujahideen-recruiter Omar Abdel-Rahman.
In his widely praised account of al-Qaeda, English journalist Jason Burke wrote:It is often said that bin Laden was funded by the CIA. This is not true and, indeed, would have been impossible given the structure of funding that General Zia ul-Haq, who had taken power in Pakistan in 1977, had set up. A condition of Zia's cooperation with the American plan to turn Afghanistan into the Soviets' 'Vietnam' was that all American funding to the Afghan resistance had to be channelled through the Pakistani government, which in effect meant the Afghan bureau of the Inter Services Intelligence (ISI), the military spy agency. The American funding, which went exclusively to the Afghan mujahideen groups, not the Arab volunteers, was supplemented by Saudi government money and huge funds raised from mosques, non-governmental charitable institutions and private donors throughout the Islamic world.
Broader influence.
Anders Behring Breivik, the perpetrator of the 2011 Norway attacks, was inspired by al-Qaeda, calling it "the most successful revolutionary movement in the world." While admitting different aims, he sought to "create a European version of al-Qaida."
Criticism.
According to a number of sources there has been a "wave of revulsion" against al-Qaeda and its affiliates by "religious scholars, former fighters and militants" alarmed by al-Qaeda's takfir and killing of Muslims in Muslim countries, especially Iraq.
Noman Benotman, a former Afghan Arab and militant of the Libyan Islamic Fighting Group, went public with an open letter of criticism to Ayman al-Zawahiri in November 2007 after persuading imprisoned senior leadership of his former group to enter into peace negotiations with the Libyan regime. While Ayman al-Zawahiri announced the affiliation of the group with al-Qaeda in November 2007, the Libyan government released 90 members of the group from prison several months later after "they were said to have renounced violence."
In 2007, around the sixth anniversary of the September 11 attacks and a couple of months before "Rationalizing Jihad" first appeared in the newspapers, the Saudi sheikh Salman al-Ouda delivered a personal rebuke to bin Laden. Al-Ouda, a religious scholar and one of the fathers of the Sahwa, the fundamentalist awakening movement that swept through Saudi Arabia in the 1980s, is a widely respected critic of jihadism. Al-Ouda addressed al-Qaeda's leader on television asking him
My brother Osama, how much blood has been spilt? How many innocent people, children, elderly, and women have been killed... in the name of al-Qaeda? Will you be happy to meet God Almighty carrying the burden of these hundreds of thousands or millions [of victims] on your back?
According to Pew polls, support for al-Qaeda has been slightly dropped for parts of the Muslim world in the years before 2008. The numbers supporting suicide bombings in Indonesia, Lebanon, and Bangladesh, for instance, have dropped by half or more in the last five years. In Saudi Arabia, only 10 percent now have a favorable view of al-Qaeda, according to a December poll by Terror Free Tomorrow, a Washington-based think tank.
In 2007, the imprisoned Sayyed Imam Al-Sharif, an influential Afghan Arab, "ideological godfather of al-Qaeda", and former supporter of takfir, sensationally withdrew his support from al-Qaeda with a book "Wathiqat Tarshid Al-'Aml Al-Jihadi fi Misr w'Al-'Alam" ("Rationalizing Jihad in Egypt and the World").
Although once associated with al-Qaeda, in September 2009 LIFG completed a new "code" for jihad, a 417-page religious document entitled "Corrective Studies". Given its credibility and the fact that several other prominent Jihadists in the Middle East have turned against al-Qaeda, the LIFG's about face may be an important step toward staunching al-Qaeda's recruitment.
See also.
Publications:
Further reading.
</dl>
</dl>
</dl>

</doc>
<doc id="1923" url="http://en.wikipedia.org/wiki?curid=1923" title="Alessandro Volta">
Alessandro Volta

Alessandro Giuseppe Antonio Anastasio Volta (18 February 1745 – 5 March 1827) was an Italian physicist and chemist, credited with the invention of the first electrical battery, the Voltaic pile, which he invented in 1799 and the results of which he reported in 1800 in a two part letter to the President of the Royal Society. With this invention Volta proved that electricity could be generated chemically and debased the prevalent theory that electricity was generated solely by living beings. Volta's invention sparked a great amount of scientific excitement and led others to conduct similar experiments which eventually led to the development of the field of electrochemistry.
Alessandro Volta also drew admiration from Napoleon Bonaparte for his invention, and was invited to the Institute of France to demonstrate his invention to the members of the Institute. Volta enjoyed a certain amount of closeness with the Emperor throughout his life and he was conferred numerous honours by him. Alessandro Volta held the chair of experimental physics at the University of Pavia for nearly 40 years and was widely idolised by his students.
Despite his professional success Volta tended to be a person inclined towards domestic life and this was more apparent in his later years. At this time he tended to live secluded from public life and more for the sake of his family until his eventual death in 1827 from a series of illnesses which began in 1823. The SI unit of electric potential is named in his honour as the volt.
Early life and works.
Volta was born in Como, a town in present-day northern Italy (near the Swiss border) on 18 February 1745. In 1794, Volta married an aristocratic lady also from Como, Teresa Peregrini, with whom he raised three sons: Zanino, Flaminio, and Luigi. His own father Filippo Volta was of noble lineage. His mother Donna Maddalena came from the family of the Inzaghis.
In 1774, he became a professor of physics at the Royal School in Como. A year later, he improved and popularised the electrophorus, a device that produced static electricity. His promotion of it was so extensive that he is often credited with its invention, even though a machine operating on the same principle was described in 1762 by the Swedish experimenter Johan Wilcke. In 1777, he travelled through Switzerland. There he befriended H. B. de Saussure.
In the years between 1776 and 1778, Volta studied the chemistry of gases. He researched and discovered methane after reading a paper by Benjamin Franklin of United States on "flammable air". In November 1776, he found methane at Lake Maggiore, and by 1778 he managed to isolate methane. He devised experiments such as the ignition of methane by an electric spark in a closed vessel. Volta also studied what we now call electrical capacitance, developing separate means to study both electrical potential ("V" ) and charge ("Q" ), and discovering that for a given object, they are proportional. This is called Volta's Law of Capacitance, and it was for this work the unit of electrical potential has been named the volt.
In 1779 he became a professor of experimental physics at the University of Pavia, a chair that he occupied for almost 40 years.
Volta and Galvani.
Luigi Galvani, an Italian physicist, discovered something he named "animal electricity" when two different metals were connected in series with a frog's leg and to one another. Volta realised that the frog's leg served as both a conductor of electricity (what we would now call an electrolyte) and as a detector of electricity. He replaced the frog's leg with brine-soaked paper, and detected the flow of electricity by other means familiar to him from his previous studies.
In this way he discovered the electrochemical series, and the law that the electromotive force (emf) of a galvanic cell, consisting of a pair of metal electrodes separated by electrolyte, is the difference between their two electrode potentials (thus, two identical electrodes and a common electrolyte give zero net emf). This may be called Volta's Law of the electrochemical series.
In 1800, as the result of a professional disagreement over the galvanic response advocated by Galvani, Volta invented the voltaic pile, an early electric battery, which produced a steady electric current. Volta had determined that the most effective pair of dissimilar metals to produce electricity was zinc and silver. Initially he experimented with individual cells in series, each cell being a wine goblet filled with brine into which the two dissimilar electrodes were dipped. The voltaic pile replaced the goblets with cardboard soaked in brine.
First battery.
In announcing his discovery of the voltaic pile, Volta paid tribute to the influences of William Nicholson, Tiberius Cavallo, and Abraham Bennet.
The battery made by Volta is credited as the first electrochemical cell. It consists of two electrodes: one made of zinc, the other of copper. The electrolyte is either sulfuric acid mixed with water or a form of saltwater brine. The electrolyte exists in the form 2H+ and SO42−. The zinc, which is higher in the electrochemical series than both copper and hydrogen, reacts with the negatively charged sulfate (SO42−). The positively charged hydrogen ions (protons) capture electrons from the copper, forming bubbles of hydrogen gas, H2. This makes the zinc rod the negative electrode and the copper rod the positive electrode.
Thus, there are two terminals, and an electric current will flow if they are connected. The chemical reactions in this voltaic cell are as follows:
The copper does not react, but rather it functions as an electrode for the electric current.
However, this cell also has some disadvantages. It is unsafe to handle, since sulfuric acid, even if diluted, can be hazardous. Also, the power of the cell diminishes over time because the hydrogen gas is not released. Instead, it accumulates on the surface of the zinc electrode and forms a barrier between the metal and the electrolyte solution.
Last years and retirement.
In honour of his work, Volta was made a count by Napoleon Bonaparte in 1810. His image was depicted on the Italian 10,000 lira note along with a sketch of his voltaic pile.
Volta retired in 1819 to his estate in Camnago, a frazione of Como, Italy, now named "Camnago Volta" in his honour. He died there on 5 March 1827, just after his 82nd birthday. Volta's remains were buried in Camnago Volta.
Volta's legacy is celebrated by the Tempio Voltiano memorial located in the public gardens by the lake. There is also a museum which has been built in his honour, which exhibits some of the equipment that Volta used to conduct experiments. Nearby stands the Villa Olmo, which houses the Voltian Foundation, an organization promoting scientific activities. Volta carried out his experimental studies and produced his first inventions near Como.
Religious beliefs.
Volta was raised as a Catholic and for all of his life continued to maintain his belief. Because he was not ordained a clergyman as his family expected, he was sometimes accused of being irreligious and some people have speculated about his possible unbelief, stressing that "he did not join the Church", or that he virtually "ignored the church's call". Nevertheless, he cast out doubts in a declaration of faith in which he said:
I do not understand how anyone can doubt the sincerity and constancy of my attachment to the religion which I profess, the Roman, Catholic and Apostolic religion in which I was born and brought up, and of which I have always made confession, externally and internally. I have, indeed, and only too often, failed in the performance of those good works which are the mark of a Catholic Christian, and I have been guilty of many sins: but through the special mercy of God I have never, as far as I know, wavered in my faith... In this faith I recognise a pure gift of God, a supernatural grace; but I have not neglected those human means which confirm belief, and overthrow the doubts which at times arise. I studied attentively the grounds and basis of religion, the works of apologists and assailants, the reasons for and against, and I can say that the result of such study is to clothe religion with such a degree of probability, even for the merely natural reason, that every spirit unperverted by sin and passion, every naturally noble spirit must love and accept it. May this confession which has been asked from me and which I willingly give, written and subscribed by my own hand, with authority to show it to whomsoever you will, for I am not ashamed of the Gospel, may it produce some good fruit!

</doc>
<doc id="1924" url="http://en.wikipedia.org/wiki?curid=1924" title="Argo Navis">
Argo Navis

Argo Navis (or simply Argo) was a large constellation in the southern sky that has since been divided into three constellations. It represented the "Argo", the ship used by Jason and the Argonauts in Greek mythology. The abbreviation was "Arg" and the genitive was "Argus Navis".
Due to precession, the stars of Argo have been shifted farther south since Classical times, and far fewer of its stars are visible today from the latitude of the Mediterranean.
The original constellation was found low near the southern horizon of the Mediterranean sky. The ship became visible in springtime and sailed westward, skimming along the southern horizon. The ancient Greeks identified it with the ship sailed by Jason and the Argonauts in search of the Golden Fleece.
Argo Navis is the only one of the 48 constellations listed by the 2nd century astronomer Ptolemy that is no longer officially recognized as a constellation. It was unwieldy due to its enormous size: were it still considered a single constellation, it would be the largest of all. In 1752, the French astronomer Nicolas Louis de Lacaille subdivided it into Carina (the keel, or the hull, of the ship), Puppis (the poop deck, or stern), and Vela (the sails). When Argo Navis was split, Lacaille did not retain Bayer's designations (which bore scant relationship to the actual positions of the stars), but like Bayer he did use a single Greek-letter sequence for the three parts: Carina has the α, β and ε, Vela has γ and δ, Puppis has ζ, and so on.:82 (For the dimmer stars, however, Lacaille used a separate Latin-letter sequence for each part.):82
The constellation Pyxis (the mariner's compass) occupies an area which in antiquity was considered part of Argo's mast. Some authors state that Pyxis was part of the Greek conception of Argo Navis, but magnetic compasses were unknown in ancient Greek times. Lacaille considered it a separate constellation, representing one of the modern scientific instruments he placed among the constellations (like Microscopium and Telescopium); he assigned it Bayer designations separate from those of Carina, Puppis and Vela, and his illustration shows an isolated instrument not related to the figure of Argo.:262 In 1844 John Herschel suggested formalizing the mast as a new constellation, Malus, to replace Lacaille's Pyxis, but the idea did not catch on.
The Māori had several names for what was the constellation Argo, including "Te Waka-o-Tamarereti", "Te Kohi-a-Autahi", and "Te Kohi".

</doc>
