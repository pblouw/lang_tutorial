<doc id="1925" url="http://en.wikipedia.org/wiki?curid=1925" title="Andromeda (mythology)">
Andromeda (mythology)

In Greek mythology, Andromeda is the daughter of the Aethiopian king Cepheus and his wife Cassiopeia. When Cassiopeia's hubris leads her to boast that Andromeda is more beautiful than the Nereids, Poseidon, influenced by Hades, sends a sea monster, Cetus, to ravage Aethiopia as divine punishment. Andromeda is stripped and chained naked to a rock as a sacrifice to sate the monster, but is saved from death by Perseus.
Her name is the Latinized form of the Greek Ἀνδρομέδα ("Androméda") or Ἀνδρομέδη ("Andromédē"): "ruler of men", from ἀνήρ, ἀνδρός ("anēr, andrós") "man", and "medon", "ruler".
As a subject, Andromeda has been popular in art since classical times; it is one of several Greek myths of a Greek hero's rescue of the intended victim of an archaic "hieros gamos" (sacred marriage), giving rise to the "princess and dragon" motif. From the Renaissance, interest revived in the original story, typically as derived from Ovid's account.
Mythology.
In Greek mythology, Andromeda was the daughter of Cepheus and Cassiopeia, king and queen of the North African kingdom of Aethiopia.
Her mother Cassiopeia boasted that her daughter was more beautiful than the Nereids, the nymph-daughters of the sea god Nereus and often seen accompanying Poseidon. To punish the queen for her arrogance, Poseidon, brother to Zeus and god of the sea, sent a sea monster named Cetus to ravage the coast of Aethiopia including the kingdom of the vain queen. The desperate king consulted the Oracle of Apollo, who announced that no respite would be found until the king sacrificed his daughter, Andromeda, to the monster. Stripped naked, she was chained to a rock on the coast.
Perseus was returning from having slain the Gorgon Medusa. After he happened upon the chained Andromeda, he approached Cetus while invisible (for he was wearing Hades's helm), and killed the sea monster. He set Andromeda free, and married her in spite of her having been previously promised to her uncle Phineus. At the wedding a quarrel took place between the rivals and Phineus was turned to stone by the sight of the Gorgon's head.
Andromeda followed her husband, first to his native island of Serifos, where he rescued his mother Danaë, and then to Tiryns in Argos. Together, they became the ancestors of the family of the "Perseidae" through the line of their son Perses. Perseus and Andromeda had seven sons: Perses, Alcaeus, Heleus, Mestor, Sthenelus, Electryon, and Cynurus as well as two daughters, Autochthe and Gorgophone. Their descendants ruled Mycenae from Electryon down to Eurystheus, after whom Atreus attained the kingdom, and would also include the great hero Heracles. According to this mythology, Perseus is the ancestor of the Persians.
At the port city of Jaffa (today part of Tel Aviv) an outcrop of rocks near the harbour has been associated with the place of Andromeda's chaining and rescue by the traveler Pausanias, the geographer Strabo and the historian of the Jews Josephus.
After Andromeda's death, as Euripides had promised Athena at the end of his "Andromeda", produced in 412 BCE, the goddess placed her among the constellations in the northern sky, near Perseus and Cassiopeia; the constellation Andromeda, so known since antiquity, is named after her.
Constellations.
Andromeda is represented in the northern sky by the constellation Andromeda, which contains the Andromeda Galaxy.
Four constellations are associated with the myth. Viewing the fainter stars visible to the naked eye, the constellations are rendered as:
Other constellations related to the story are:
Perseus and Andromeda in art.
Sophocles and Euripides (and in more modern times, Corneille) made the story the subject of tragedies, and its incidents were represented in numerous ancient works of art, including Greek vases. Jean-Baptiste Lully's opera, "Persée", also dramatizes the myth.
Andromeda has been the subject of numerous ancient and modern works of art, which typically show the moment of rescue, with Andromeda usually still chained, and often naked or nearly so. Examples include: one of Titian's "poesies" (Wallace Collection), and compositions by Joachim Wtewael (Louvre), Veronese (Rennes), many versions by Rubens, Ingres, and Gustave Moreau. From the Renaissance onward the chained nude figure of Andromeda typically was the centre of interest. Rembrandt's "Andromeda Chained to the Rocks" is unusual in showing her alone, fearfully awaiting the monster.
<poem>
If by dull rhymes our English must be chain’d,
 And, like Andromeda, the Sonnet sweet
 Fetter’d, in spite of pained loveliness;
 Let us find out, if we must be constrain’d,
 Sandals more interwoven and complete
 To fit the naked foot of poesy;
 Let us inspect the lyre, and weigh the stress
 Of every chord, and see what may be gain’d
 By ear industrious, and attention meet:
 Misers of sound and syllable, no less
 Than Midas of his coinage, let us be
 Jealous of dead leaves in the bay wreath crown;
 So, if we may not let the Muse be free,
 She will be bound with garlands of her own.
</poem>
”
If By Dull Rhymes Our English Must Be Chain’d <br> by "John Keats",
The Italian composer Salvatore Sciarrino composed an hour-long operatic drama called Perseo e Andromeda in 2000.
Film.
In 1973, an animated film called "Perseus" (20 minutes) was made in the Soviet Union as part of Soviet Union animated film collection called "Legends and mуths of Ancient Greece".
The 1981 film "Clash of the Titans" retells the story of Perseus, Andromeda, and Cassiopeia, but makes a few changes (notably Cassiopeia boasts that her daughter is more beautiful than Thetis as opposed to the Nereids as a group). Thetis was a Nereid, but also the future mother of Achilles. Andromeda and Perseus meet and fall in love after he saves her soul from the enslavement of Thetis' son, Calibos, whereas in the myth, they simply meet as Perseus returns home from having slain Medusa. In the film, the monster is called a kraken, although it is depicted as a lizard-like creature rather than a squid; and combining two elements of the myth, Perseus defeats the sea monster by showing it Medusa's face, turning the monster into stone. Andromeda is depicted as being strong-willed and independent, whereas in the stories she is only really mentioned as being the princess whom Perseus saves from the sea monster. Andromeda was portrayed by Judi Bowker in this film.
Andromeda also features in the 2010 film "Clash of the Titans", a remake of the 1981 version. Several changes were made in regard to the myth, most notably that Perseus did not marry Andromeda after he rescued her from the sea monster. Andromeda was portrayed by Alexa Davalos. The character was played by Rosamund Pike in the sequel "Wrath of the Titans", the second of a planned trilogy. In the end of the sequel, Perseus and Andromeda begin a relationship.
In the Japanese anime "Saint Seiya" the character, Shun, represents the Andromeda constellation using chains as his main weapons, reminiscent of Andromeda being chained before she was saved by Perseus. In order to attain the Andromeda Cloth, he was chained between two large pillars of rock and he had to overcome the chains before the tide came in and killed him, also reminiscent of this myth.
Andromeda appears in Disney's "" as a new student of "Prometheus Academy" which Hercules and other characters from Greek mythology attend. In "The Sea of Monsters", the second book in the "Percy Jackson & the Olympians" series, a cruise ship which serves as living space for Kronos's army called "The Princess Andromeda" is named after her.

</doc>
<doc id="1926" url="http://en.wikipedia.org/wiki?curid=1926" title="Antlia">
Antlia

Antlia (; from Ancient Greek "ἀντλία") is a constellation in the southern sky. Its name means "pump" and it specifically represents an air pump. The constellation was created in the 18th century from an undesignated region of sky, so the stars comprising Antlia are faint. Antlia is bordered by Hydra the sea snake, Pyxis the compass, Vela the sails, and Centaurus the centaur. This group of constellations is prominent in the southern sky in late winter and spring. NGC 2997, a spiral galaxy, and the Antlia Dwarf Galaxy lie within Antlia's borders.
History.
Antlia was created in 1756 by the French astronomer Abbé Nicolas Louis de Lacaille, who created fourteen constellations for the southern sky to fill some faint regions. Though Antlia was technically visible to ancient Greek astronomers, its stars were too faint to have been included in any constellations. Because of this, its main stars have no particular pattern and it is devoid of bright deep-sky objects. It was originally named "Antlia pneumatica" ("Machine Pneumatique" in French) to commemorate the air pump invented by the French physicist Denis Papin.
Lacaille and Johann Bode each depicted Antlia differently, as either the single-cylinder vacuum pump used in Papin's initial experiments, or the more advanced double-cylinder version. The International Astronomical Union subsequently adopted it as one of the 88 modern constellations. There is no mythology attached to Antlia as Lacaille discontinued the tradition of giving names from mythology to constellations and instead chose names mostly from scientific instruments.
According to some, the most prominent stars that now comprise Antlia were once included within the ancient constellation Argo Navis, the Ship of the Argonauts, which due to its immense size was split into several smaller constellations by Lacaille in 1763. However, given the faintness and obscurity of its stars, most authorities do not believe that the ancient Greeks included Antlia as part of their classical depiction of Argo Navis.
In non-Western astronomy.
Chinese astronomers were able to view what is modern Antlia from their latitudes, and incorporated its stars into two different constellations. Several stars in the southern part of Antlia were a portion of "Dong'ou", which represented an area in southern China. Furthermore, Epsilon, Eta, and Theta Antliae were incorporated into the celestial temple, which also contained stars from modern Pyxis.
Notable features.
Stars.
Lacaille gave nine stars Bayer designations, labelling them Alpha through to Theta, including two stars next to each other as Zeta. Gould later added a tenth, Iota Antliae. Beta and Gamma Antliae (now HR 4339 and HD 90156) ended up in the neighbouring constellation Hydra once the constellation boundaries were delineated in 1930.
The constellation's brightest star, Alpha Antliae is an orange giant of spectral type K4III that is a suspected variable star, ranging between apparent magnitudes 4.22 and 4.29. It is 
located 366 light-years away from Earth. Estimated to be shining with around 480 to 555 times the luminosity of the Sun, it is most likely an ageing star that is brightening and on its way to becoming a Mira variable star, having converted all its core fuel into carbon. Located near Alpha is Delta Antliae, a binary star around 481 light years distant. The primary is a blue-white main sequence star of spectral type B9.5V and magnitude 5.6 and the secondary is an yellow-white main sequence star of spectral type F9Ve and magnitude 9.6.
Zeta Antliae is a wide double star 373 light years away. The primary (Zeta1 Antliae) is of magnitude 5.8, though it is a double star with a primary of magnitude 6.2 and a secondary of magnitude 7.0. The secondary (Zeta2 Antliae) is of magnitude 5.9. Eta Antliae is another double composed of an F-type main sequence star of spectral type and magnitude 5.22, with a companion of magnitude 11.3. Theta Antliae is likewise double, composed of an A-type main sequence star and yellow giant.
Epsilon Antliae is an evolved orange giant star of spectral type K3 IIIa, with a diameter around 69 times that of the Sun. It is slightly variable. At the other end of Antlia, Iota Antliae is likewise an orange giant of spectral type K1 III.
T Antliae is a yellow-white supergiant of spectral type F6Iab and Classical Cepheid variable ranging between magnitude 8.88 and 9.82 over 5.9 days. U Antliae is a red C-type carbon star and is an irregular variable that ranges between magnitudes 5.27 and 6.04.
HR 4049, also known as AG Antliae, is an unusual hot variable ageing star of spectral type B9.5Ib-II. It is undergoing intense mass-loss and is a unique variable, ranging between magnitudes 5.29 and 5.83 with a period of 429 days UX Antliae is an R Coronae Borealis variable with a baseline apparent magnitude of around 11.85, with irregular dimmings down to below magnitude 18.0. A luminous and remote star, it is a supergiant with a spectrum resembling that of a yellow-white F-type star but it has almost no hydrogen.
DEN 1048-3956 is a brown dwarf located around 13 light-years distant from Earth. At magnitude 17 it is much too faint to be seen with the unaided eye.
Deep-sky objects.
Because it occupies a part of the celestial sphere that faces away from the Milky Way, Antlia contains very few deep-sky objects. It contains no globular clusters, no planetary nebulae, and no open clusters. However, it does contain several galaxies.
NGC 2997 is a loose face-on spiral galaxy of type Sc. It is the brightest galaxy in Antlia at an integrated magnitude of 10.6. Though nondescript in most amateur telescopes, it presents bright clusters of young stars and many dark dust lanes in photographs.
The Antlia Dwarf, a 14.8m dwarf spheroidal galaxy that belongs to our Local Group of galaxies. It was discovered only as recently as 1997.
The Antlia Cluster, also known as Abell S0636, is a cluster of galaxies located in the Hydra-Centaurus Supercluster. It is the third nearest to our Local Group after the Virgo Cluster and Fornax Cluster. The cluster's distance from earth is 40.5 Mpc to 40.9 Mpc
References.
</dl>
Coordinates: 

</doc>
<doc id="1927" url="http://en.wikipedia.org/wiki?curid=1927" title="Ara (constellation)">
Ara (constellation)

Ara is a southern constellation situated between Scorpius and Triangulum Australe. Its name is Latin for "altar". Ara was one of the 48 Greek constellations described by the 2nd century astronomer Ptolemy, and it remains one of the 88 modern constellations defined by the International Astronomical Union.
Notable features.
Stars.
Ara contains part of the Milky Way to the south of Scorpius and thus has rich star fields.
The constellation's stars have no names in Western culture, but the Chinese call α Arae "Choo" ("club" or "staff"), and ε Arae "Tso Kang", meaning 'left guard'.
Deep-sky objects.
The northwest corner of Ara is crossed by the Milky Way and contains several open clusters (notably NGC 6200) and diffuse nebulae (including the bright cluster/nebula pair NGC 6188 and NGC 6193). The brightest of the globular clusters, sixth magnitude NGC 6397, lies at a distance of just 6500 ly, making it one of the closest globular cluster to the solar system.
Ara also contains Westerlund 1, a super star cluster that contains the red supergiant Westerlund 1-26, one of the largest stars known.
Although Ara lies close to the heart of the Milky Way, two spiral galaxies (NGC 6215 and NGC 6221) are visible near star η Arae.
Planetary Nebulae.
The Stingray Nebula (Hen 3-1357), the youngest known planetary nebula as of 2010, formed in Ara; the light from its formation was first observable around 1987.
Last, but not least; there is also NGC 6326. A planetary nebula that might have a binary system at its center.
Illustrations.
In illustrations, Ara is usually depicted as an altar with its smoke 'rising' southward. However, depictions of Ara often vary in their details. In the early days of printing, a 1482 woodcut of Gaius Julius Hyginus's classic "Poeticon Astronomicon" depicts the altar as surrounded by demons. Johann Bayer in 1603 depicted Ara as an altar with burning incense; the flames rise southward as in most atlases. Hyginus also depicted Ara as an altar with burning incense, though his Ara featured devils on either side of the flames. However, Willem Blaeu, a Dutch uranographer active in the 16th and 17th centuries, drew Ara as an altar designed for sacrifice, with a burning animal offering. Unlike most depictions, the smoke from Blaeu's Ara rises northward, represented by Alpha Arae. A more unusual depiction of Ara comes from Aratus, a Greek uranographer, in 270 BCE. He drew Ara as a lighthouse, where Alpha. Beta, Epsilon, and Zeta Arae represent the base, and Eta Arae represents the flames at the lighthouse's light.
Mythology.
In ancient Greek mythology, Ara was identified as the altar where the gods first made offerings and formed an alliance before defeating the Titans. The nearby Milky Way represents the smoke rising from the offerings on the altar.
Equivalents.
In Chinese astronomy, the stars of the constellation Ara lie within "The Azure Dragon of the East" (東方青龍, "Dōng Fāng Qīng Lóng").
Namesakes.
USS Ara (AK-136) was a United States Navy Crater class cargo ship named after the constellation.
Bibliography.
Online sources
External links.
Coordinates: 

</doc>
<doc id="1928" url="http://en.wikipedia.org/wiki?curid=1928" title="Auriga">
Auriga

Auriga can refer to:

</doc>
<doc id="1930" url="http://en.wikipedia.org/wiki?curid=1930" title="Arkansas">
Arkansas

Arkansas ( ) is a state located in the Southern region of the United States. Its name is of Siouan derivation, denoting the Quapaw Indians. The state's diverse geography ranges from the mountainous regions of the Ozark and the Ouachita Mountains, which make up the U.S. Interior Highlands, to the densely forested land in the south known as the Arkansas Timberlands, to the eastern lowlands along the Mississippi River and the Arkansas Delta. Known as "the Natural State or Diamond State", the diverse regions of Arkansas offer residents and tourists a variety of opportunities for outdoor recreation.
Arkansas is the 29th largest in square miles and the 32nd most populous of the 50 United States. The capital and most populous city is Little Rock, located in the central portion of the state, a hub for transportation, business, culture, and government. The northwestern corner of the state, including the Fayetteville–Springdale–Rogers Metropolitan Area and Fort Smith metropolitan area, is also an important population, education, and economic center. The largest city in the eastern part of the state is Jonesboro.
The Territory of Arkansas was admitted to the Union as the 25th state on June 15, 1836. Arkansas withdrew from the United States and joined the Confederate States of America during the Civil War. Upon returning to the Union, the state would continue to suffer due to its earlier reliance on slavery and the plantation economy, causing the state to fall behind economically and socially. White rural interests continued to dominate the state's politics until the Civil Rights movement in the mid-20th century. Arkansas began to diversify its economy following World War II and now relies on its service industry as well as aircraft, poultry, steel and tourism in addition to cotton and rice.
The culture of Arkansas is observable in museums, theaters, novels, television shows, restaurants and athletic venues across the state. Despite a plethora of cultural, economic, and recreational opportunities, Arkansas is often stereotyped as a "poor, banjo-picking hillbilly" state, a reputation dating back to early accounts of the territory by frontiersmen in the early 1800s. Arkansas's enduring image has earned the state "a special place in the American consciousness", but it has in reality produced such prominent figures as politician and educational advocate William Fulbright, former President Bill Clinton, former NATO Supreme Allied Commander, General Wesley Clark, Walmart magnate Sam Walton and singer-songwriter Johnny Cash.
Etymology.
The name Arkansas derives from the same root as the name for the state of Kansas. The Kansa tribe of Native Americans are closely associated with the Sioux tribes of the Great Plains. The word "Arkansas" itself is a French pronunciation ("Arcansas") of a Quapaw (a related "Kaw" tribe) word, "akakaze", meaning "land of downriver people" or the Sioux word "akakaze" meaning "people of the south wind".
In 1881, the pronunciation of Arkansas with the final "s" being silent was made official by an act of the state legislature after a dispute arose between Arkansas's then-two U.S. senators as one favored the pronunciation as while the other favored .
In 2007, the state legislature passed a non-binding resolution declaring the possessive form of the state's name to be "Arkansas's" which has been followed increasingly by the state government.
Geography.
Boundaries.
Arkansas borders Louisiana to the south, Texas to the southwest, Oklahoma to the west, Missouri to the north, as well as Tennessee and Mississippi on the east. The United States Census Bureau classifies Arkansas as a southern state, sub-categorized among the West South Central States. The Mississippi River forms most of Arkansas's eastern border, except in Clay and Greene, counties where the St. Francis River forms the western boundary of the Missouri Bootheel, and in many places where the current channel of the Mississippi has meandered from the location of its original legal designation. The state line along the Mississippi River is indeterminate along much of the eastern border with Mississippi due to these meanders.
Terrain.
Arkansas can generally be split into two halves, the highlands in the northwest half and the lowlands of the southeastern half. The highlands are part of the Southern Interior Highlands, including The Ozarks and the Ouachita Mountains. The southern lowlands include the Gulf Coastal Plain and the Arkansas Delta. This dual split is somewhat simplistic, however, and thus usually yields to general regions named northwest, southwest, northeast, southeast, or central Arkansas. These directionally named regions are also not defined along county lines and are also broad. Arkansas has seven distinct natural regions: the Ozark Mountains, Ouachita Mountains, Arkansas River Valley, Gulf Coastal Plain, Crowley's Ridge, and the Arkansas Delta, with Central Arkansas sometimes included as a blend of multiple regions.
The southeastern part of Arkansas along the Mississippi Alluvial Plain is sometimes called the Arkansas Delta. This region is a flat landscape of rich alluvial soils formed by repeated flooding of the adjacent Mississippi. Farther away from the river, in the southeast portion of the state, the Grand Prairie consists of a more undulating landscape. Both are fertile agricultural areas. The Delta region is bisected by an unusual geological formation known as Crowley's Ridge. A narrow band of rolling hills, Crowley's Ridge rises from 250 to above the surrounding alluvial plain and underlies many of the major towns of eastern Arkansas.
Northwest Arkansas is part of the Ozark Plateau including the Ozark Mountains, to the south are the Ouachita Mountains, and these regions are divided by the Arkansas River; the southern and eastern parts of Arkansas are called the Lowlands. These mountain ranges are part of the U.S. Interior Highlands region, the only major mountainous region between the Rocky Mountains and the Appalachian Mountains. The highest point in the state is Mount Magazine in the Ouachita Mountains; it rises to 2753 ft above sea level.
Hydrology.
Arkansas has many rivers, lakes, and reservoirs within or along its borders. Major tributaries of the Mississippi River include the Arkansas River, White River, and St. Francis River. The Arkansas is fed by the Mulberry River, and Fourche LaFave River in the Arkansas River Valley, which is also home to Lake Dardanelle. The Buffalo River, Little Red River, Black River and Cache River all serve as tributaries to the White River, which also empties into the Mississippi. The Saline River, Little Missouri River, Bayou Bartholomew, and the Caddo River all serve as tributaries to the Ouachita River in south Arkansas, which eventually empties into the Mississippi in Louisiana. The Red River briefly serves as the state's boundary with Texas. Arkansas has few natural lakes but many major reservoirs, including Bull Shoals Lake, Lake Ouachita, Greers Ferry Lake, Millwood Lake, Beaver Lake, Norfork Lake, DeGray Lake, and Lake Conway.
Arkansas is home to many caves, such as Blanchard Springs Caverns. More than 43,000 Native American living, hunting and tool making sites, many of them Pre-Columbian burial mounds and rock shelters, have been cataloged by the State Archeologist. Crater of Diamonds State Park near Murfreesboro is the world's only diamond-bearing site accessible to the public for digging. Arkansas is home to a dozen Wilderness Areas totaling 158444 acre. These areas are set aside for outdoor recreation and are open to hunting, fishing, hiking, and primitive camping. No mechanized vehicles nor developed campgrounds are allowed in these areas.
Flora and fauna.
Arkansas is divided into three broad ecoregions, the "Ozark, Ouachita-Appalachian Forests", "Mississippi Alluvial and Southeast USA Coastal Plains", and the "Southeastern USA Plains". The state is further divided into seven subregions: the Arkansas Valley, Boston Mountains, Mississippi Alluvial Plain, Mississippi Valley Loess Plain, Ozark Highlands, Ouachita Mountains, and the South Central Plains. A 2010 United States Forest Service survey determined 18720000 acre of Arkansas's land is forestland, or 56% of the state's total area. Dominant species in Arkansas's forests include "Quercus" (oak), "Carya" (hickory), "Pinus echinata" (shortleaf pine) and "Pinus taeda" (Loblolly pine).
Arkansas's plant life varies with its climate and elevation. The pine belt stretching from the Arkansas delta to Texas consists of dense oak-hickory-pine growth. Lumbering and paper milling activity is active throughout the region. In eastern Arkansas, one can find "Taxodium " (cypress), "Quercus nigra" (water oaks), and hickories with their roots submerged in the Mississippi Valley bayous indicative of the deep south. Nearby Crowley's Ridge is only home of the tulip tree in the state, and generally hosts more northeastern plant life such as the beech tree. The northwestern highlands are covered in an oak-hickory mixture, with Ozark white cedars, "cornus" (dogwoods), and "Cercis canadensis" (redbuds) also present. The higher peaks in the Arkansas River Valley play host to scores of ferns, including the "Woodsia scopulina" and "Adiantum" (maidenhair fern) on Mount Magazine.
Climate.
Arkansas generally has a humid subtropical climate, which borders on humid continental in some northern highland areas. While not bordering the Gulf of Mexico, Arkansas is still close enough to this warm, large body of water for it to influence the weather in the state. Generally, Arkansas has hot, humid summers and cold, slightly drier winters. In Little Rock, the daily high temperatures average around 93 °F with lows around 73 °F in July. In January highs average around 51 °F and lows around 32 °F. In Siloam Springs in the northwest part of the state, the average high and low temperatures in July are 89 and and in January the average high and lows are 44 and. Annual precipitation throughout the state averages between about 40 and; somewhat wetter in the south and drier in the northern part of the state. Snowfall is infrequent but most common in the northern half of the state. The half of the state south of Little Rock is more apt to see ice storms. Arkansas' all-time record high is 120 F at Ozark on August 10, 1936; the all-time record low is -29 F at Gravette, on February 13, 1905.
Arkansas is known for extreme weather and many storms. A typical year will see thunderstorms, tornadoes, hail, snow and ice storms. Between both the Great Plains and the Gulf States, Arkansas receives around 60 days of thunderstorms. A few of the most destructive tornadoes in U.S. history have struck the state. While being sufficiently away from the coast to be safe from a direct hit from a hurricane, Arkansas can often get the remnants of a tropical system which dumps tremendous amounts of rain in a short time and often spawns smaller tornadoes.
History.
Early Arkansas through territorial period.
Prior to European settlement of North America, Arkansas was inhabited by indigenous peoples for thousands of years. The Caddo, Osage, and Quapaw peoples encountered European explorers. These included Spanish explorer Hernando de Soto in 1541, the French Jacques Marquette and Louis Jolliet in 1673, and Frenchmen Robert La Salle and Henri de Tonti in 1681. De Tonti established Arkansas Post at a Quapaw village in 1686, making it the first European settlement in the territory. The early Spanish or French explorers of the state gave it its name, which is probably a phonetic spelling of the Illinois tribe's name for the Quapaw people, who lived downriver from them.
Settlers, including fur trappers, moved to Arkansas in the early 18th century. These people used Arkansas Post as a home base and entrepôt. During the colonial period, Arkansas changed hands between France and Spain following the Seven Years' War, although neither showed interest in the remote settlement of Arkansas Post. In April 1783, Arkansas saw its only battle of the American Revolutionary War, a brief siege of the post by British Captain James Colbert with the assistance of the Choctaw and Chickasaw. 
Napoleon Bonaparte sold French Louisiana to the United States in 1803, including all of Arkansas, in a transaction known today as the Louisiana Purchase. French soldiers remained as a garrison at Arkansas Post. Following the purchase, the balanced give-and-take relationship between settlers and Native Americans began to change all along the frontier, including in Arkansas. Following a controversy over allowing slavery in the territory, the Territory of Arkansas was organized on July 4, 1819. Gradual emancipation in Arkansas was struck down by one vote, the Speaker of the House Henry Clay, allowing Arkansas to organize as a slave territory.
Slavery became a wedge issue in Arkansas, forming a geographic divide that remained for decades. The owners and operators of the cotton plantation economy in southeast Arkansas firmly supported slavery, as slave labor was perceived by them to be the best or "only" economically viable method of harvesting their cotton commodity crops. The "hill country" of northwest Arkansas was unable to grow cotton and relied on a cash-scarce, subsistence farming economy.
As European Americans settled throughout the East Coast and into the Midwest, in the 1830s the United States government forced the removal of many Native American tribes to Arkansas and Indian Territory west of the Mississippi River.
Additional Native American removals began in earnest during the territorial period, with final Quapaw removal complete by 1833 as they were pushed into Indian Territory. The capital was relocated from Arkansas Post to Little Rock in 1821, during the territorial period.
Statehood, Civil War and Reconstruction.
When Arkansas applied for statehood, the slavery issue was again raised in Washington DC. Congress eventually approved the Arkansas Constitution after a 25-hour session, admitting Arkansas on June 15, 1836 as the 25th state and the 13th slave state, having a population of about 60,000. Arkansas struggled with taxation to support its new state government, a problem made worse by a state banking scandal and worse yet by the Panic of 1837.
In early antebellum Arkansas, the southeast Arkansas economy developed rapidly on the backs of slaves. On the eve of the Civil War in 1860, enslaved African Americans numbered 111,115 people, just over 25% of the state's population. However, plantation agriculture would ultimately set the state and region behind the nation for decades. The wealth developed among planters of southeast Arkansas caused a political rift to form between the northwest and southeast.
Many politicians were elected to office from the Family, the Southern rights political force in antebellum Arkansas. Residents generally wanted to avoid a civil war. When the Gulf states seceded in early 1861, Arkansas voted to remain in the Union. Arkansas did not secede until Abraham Lincoln demanded Arkansas troops be sent to Fort Sumter to quell the rebellion there. The following month a state convention voted to terminate Arkansas's membership in the Union and join the Confederate States of America.
Arkansas held a very important position for the Rebels, maintaining control of the Mississippi River and surrounding Southern states. The bloody Battle of Wilson's Creek just across the border in Missouri shocked many Arkansans who thought the war would be a quick and decisive Southern victory. Battles early in the war took place in northwest Arkansas, including the Battle of Cane Hill, Battle of Pea Ridge, and Battle of Prairie Grove. Union General Samuel Curtis swept across the state to Helena in the Delta in 1862. Little Rock was captured the following year. The government shifted the state Confederate capital to Hot Springs, and then again to Washington from 1863-1865, for the remainder of the war. Throughout the state, guerrilla warfare ravaged the countryside and destroyed cities. Passion for the Confederate cause waned after implementation of unpopular programs such as the draft, high taxes, and martial law.
Under the Military Reconstruction Act, Congress declared Arkansas restored to the Union in June 1868. The Republican-controlled reconstruction legislature established universal male suffrage (though temporarily disfranchising all former Confederates, who were mostly Democrats), a public education system, and passed general issues to improve the state and help more of the population. The state soon came under almost exclusive control of the Radical Republicans, (those who moved from the North being derided as "carpetbaggers" based on allegations of corruption), and led by Governor Powell Clayton, they presided over a time of great upheaval and racial violence in the state between Republican state militia and the Ku Klux Klan.
In 1874, the Brooks-Baxter War, a political struggle between factions of the Republican Party shook Little Rock and the state governorship. It was settled only when President Ulysses S. Grant ordered Joseph Brooks to disperse his militant supporters.
Following the Brooks-Baxter War, a new state constitution was ratified, re-enfranchising former Confederates.
In 1881, the Arkansas state legislature enacted a bill that adopted an official pronunciation of the state's name, to combat a controversy then simmering. (See Law and Government below.)
After Reconstruction, the state began to receive more immigrants and migrants. Chinese, Italian, and Syrian men were recruited for farm labor in the developing Delta region. None of these nationalities stayed long at farm labor; the Chinese especially quickly became small merchants in towns around the Delta. Many Chinese became such successful merchants in small towns that they were able to educate their children at college.
Some early 20th-century immigration included people from eastern Europe. Together, these immigrants made the Delta more diverse than the rest of the state. In the same years, some black migrants moved into the area because of opportunities to develop the bottomlands and own their own property. 
Construction of railroads enabled more farmers to get their products to market. It also brought new development into different parts of the state, including the Ozarks, where some areas were developed as resorts. In a few years at the end of the 19th century, for instance, Eureka Springs in Carroll County grew to 10,000 people, rapidly becoming a tourist destination and the fourth-largest city of the state. It featured newly constructed, elegant resort hotels and spas planned around its natural springs, considered to have healthful properties. The town's attractions included horse racing and other entertainment. It appealed to a wide variety of classes, becoming almost as popular as Hot Springs.
In the late 1880s, the worsening agricultural depression catalyzed Populist and third party movements, leading to interracial coalitions. Struggling to stay in power, in the 1890s the Democrats in Arkansas followed other Southern states in passing legislation and constitutional amendments that disfranchised blacks and poor whites. Democrats wanted to prevent their alliance. In 1891 state legislators passed a requirement for a literacy test, knowing that many blacks and whites would be excluded, at a time when more than 25% of the population could neither read nor write. In 1892 they amended the state constitution to require a poll tax and more complex residency requirements, both of which adversely affected poor people and sharecroppers, forcing most blacks and many poor whites from voter rolls.
By 1900 the Democratic Party expanded use of the white primary in county and state elections, further denying blacks a part in the political process. Only in the primary was there any competition among candidates, as Democrats held all the power. The state was a Democratic one-party state for decades, until after passage of the federal Civil Rights Act of 1964 and Voting Rights Act of 1965 to enforce constitutional rights.
Between 1905 and 1911, Arkansas began to receive a small immigration of German, Slovak, and Scots-Irish from Europe. The German and Slovak peoples settled in the eastern part of the state known as the Prairie, and the Irish founded small communities in the southeast part of the state. The Germans were mostly Lutheran and the Slovaks were primarily Catholic. The Irish were mostly Protestant from Ulster, of Scots and Northern Borders descent.
After the Supreme Court's decision in "Brown "v." Board of Education of Topeka, Kansas" in 1954 that segregation in public schools was unconstitutional, some students worked to integrate schools in the state. The Little Rock Nine brought Arkansas to national attention in 1957 when the Federal government had to intervene to protect African-American students trying to integrate a high school in the Arkansas capital. Governor Orval Faubus had ordered the Arkansas National Guard to aid segregationists in preventing nine African-American students from enrolling at Little Rock's Central High School. After attempting three times to contact Faubus, President Dwight D. Eisenhower sent 1000 troops from the active-duty 101st Airborne Division to escort and protect the African-American students as they entered school on September 25, 1957. In defiance of federal court orders to integrate, the governor and city of Little Rock decided to close the high schools for the remainder of the school year. By the fall of 1959, however, the Little Rock high schools were completely integrated.
Bill Clinton, the 42nd President of the United States, was born in Hope, Arkansas. Before his presidency, Clinton served as the 40th and 42nd Governor of Arkansas, a total of nearly 12 years.
Cities and towns.
Little Rock has been Arkansas's capital city since 1821 when it replaced Arkansas Post as the capital of the Territory of Arkansas. The state capitol was moved to Hot Springs and later Washington during the Civil War when the Union armies threatened the city in 1862, and state government did not return to Little Rock until after the war ended. Today, the Little Rock–North Little Rock–Conway metropolitan area is the largest in the state, with a population of 724,385 in 2013.
The Fayetteville–Springdale–Rogers Metropolitan Area is the second-largest metropolitan area in Arkansas, growing at the fastest rate due to the influx of businesses and the growth of the University of Arkansas and Walmart.
The state has nine cities with populations above 50,000 (based on 2010 census). In descending order of size, they are: Little Rock, Fort Smith, Fayetteville, Springdale, Jonesboro, North Little Rock, Conway, Rogers, and Pine Bluff. Of these, only Fort Smith and Jonesboro are outside the two largest metropolitan areas. Other notable cities include Hot Springs, Bentonville, Texarkana, Sherwood, Jacksonville, Russellville, Bella Vista, West Memphis, Paragould, Cabot, Searcy, Van Buren, El Dorado, Blytheville, Harrison, and Mountain Home.
Demographics.
Population.
The United States Census Bureau estimates that the population of Arkansas was 2,966,369 on July 1, 2014, a 1.73% increase since the 2010 United States Census.
As of 2014, Arkansas has an estimated population of 2,966,369. From fewer than 15,000 in 1820, Arkansas's population grew to 52,240 during a special census in 1835, far exceeding the 40,000 required to apply for statehood. Following statehood in 1836, the population doubled each decade until the 1870 Census conducted following the Civil War. The state recorded growth in each successive decade, although it gradually slowed in the 20th century.
It recorded population losses in the 1950 and 1960 Censuses. This outmigration was a result of multiple factors, including farm mechanization, decreasing labor demand, and young educated people leaving the state due to a lack of non-farming industry in the state. Arkansas again began to grow, recording positive growth rates ever since and exceeding the 2 million mark during the 1980 Census. Arkansas's current rate of change, age distributions, and gender distributions mirror national averages. Minority group data also approximates national averages. There are fewer people in Arkansas of Hispanic or Latino origin than the national average. The center of population of Arkansas for 2000 was located in Perry County, near Nogal.
Race and ancestry.
In terms of race and ethnicity, the state was 80.1% White (74.2% non-Hispanic White), 15.6% Black or African American, 0.9% American Indian and Alaska Native, 1.3% Asian, and 1.8% from Two or More Races. Hispanics or Latinos of any race made up 6.6% of the population.
As of 2011, 39.0% of Arkansas's population younger than age 1 were minorities.
European Americans have a strong presence in the northwestern Ozarks and the central part of the state. African Americans live mainly in the southern and eastern parts of the state. Arkansans of Irish, English and German ancestry are mostly found in the far northwestern Ozarks near the Missouri border. Ancestors of the Irish in the Ozarks were chiefly Scots-Irish, Protestants from Northern Ireland, the Scottish lowlands and northern England part of the largest group of immigrants from Great Britain and Ireland before the American Revolution. English and Scots-Irish immigrants settled throughout the backcountry of the South and in the more mountainous areas. Americans of English stock are found throughout the state.
The principal ancestries of Arkansas's residents in 2010 were surveyed to be the following:
Most of the people identifying as American are of English descent and/or Scots-Irish descent. Their families have been in the state so long, in many cases since before statehood, that they choose to identify simply as having American ancestry or do not in fact know their own ancestry. Their ancestry primarily goes back to the original 13 colonies and for this reason many of them today simply claim American ancestry. Many people who identify themselves as Irish descent are in fact of Scots-Irish descent.
According to the 2006–2008 American Community Survey, 93.8% of Arkansas' population (over the age of five) spoke only English at home. About 4.5% of the state's population spoke Spanish at home. About 0.7% of the state's population spoke any other Indo-European languages. About 0.8% of the state's population spoke an Asian language, and 0.2% spoke other languages.
Religion.
Arkansas, like most other Southern states, is part of the Bible Belt and is predominantly Protestant. The largest denominations by number of adherents in 2010 were the Southern Baptist Convention with 661,382; the United Methodist Church with 158,574; non-denominational Evangelical Protestants with 129,638; and the Catholic Church with 122,662. However, there are some residents of the state (approximately 1,301,561) who live by other religions such as Wiccan, Pagan, Islam, Hinduism, Buddhism or who prefer no religious denomination.
Economy.
Once a state with a cashless society in the uplands and plantation agriculture in the lowlands, Arkansas's economy has evolved and diversified to meet the needs of today's consumer. The state's gross domestic product (GDP) was $105 billion in 2010. Six Fortune 500 companies are based in Arkansas, including the world's #1 retailer, Walmart. The per capita personal income in 2010 was $36,027, ranking forty-fifth in the nation. The three-year median household income from 2009-11 was $39,806, ranking forty-ninth in the nation. The state's agriculture outputs are poultry and eggs, soybeans, sorghum, cattle, cotton, rice, hogs, and milk. Its industrial outputs are food processing, electric equipment, fabricated metal products, machinery, and paper products. Mines in Arkansas produce natural gas, oil, crushed stone, bromine, and vanadium. According to CNBC, Arkansas currently ranks as the 20th best state for business, with the 2nd-lowest cost of doing business, 5th-lowest cost of living, 11th best workforce, 20th-best economic climate, 28th-best educated workforce, 31st-best infrastructure and the 32nd-friendliest regulatory environment. Arkansas gained twelve spots in the best state for business rankings since 2011. As of 2014, Arkansas was found to be the most affordable US state to live in.
As of April 2013 the state's unemployment rate is 7.5%
Industry and commerce.
Arkansas's earliest industries were fur trading and agriculture, with development of cotton plantations in the areas near the Mississippi River. They were dependent on slave labor through the American Civil War.
Today only approximately 3% of the population is employed in the agricultural sector, it remains a major part of the state's economy, ranking 13th in the nation in the value of products sold. The state is the U.S.'s largest producer of rice, broilers, and turkeys, and ranks in the top three for cotton, pullets, and aquaculture (catfish). Forestry remains strong in the Arkansas Timberlands, and the state ranks fourth nationally and first in the South in softwood lumber production. In recent years, automobile parts manufacturers have opened factories in eastern Arkansas to support auto plants in other states. Bauxite was formerly a large part of the state's economy, mined mostly around Saline County.
Tourism is also very important to the Arkansas economy; the official state nickname "The Natural State" was created for state tourism advertising in the 1970s, and is still used to this day. The state maintains 52 state parks and the National Park Service maintains seven properties in Arkansas. The completion of the William Jefferson Clinton Presidential Library in Little Rock has drawn many visitors to the city and revitalized the nearby River Market District. Many cities also hold festivals which draw tourists to the culture of Arkansas, such as King Biscuit Blues Festival, Ozark Folk Festival, Toad Suck Daze, and Tontitown Grape Festival.
Culture.
The culture of Arkansas is available to all in various forms, whether it be architecture, literature, or fine and performing arts. The state's culture also includes distinct cuisine, dialect, and traditional festivals. Sports are also very important to the culture of Arkansas, ranging from football, baseball, and basketball to hunting and fishing. Perhaps the best-known piece of Arkansas's culture is the stereotype of its citizens as shiftless hillbillies. The reputation began when the state was characterized by early explorers as a savage wilderness full of outlaws and thieves. The most enduring icon of Arkansas's hillbilly reputation is "The Arkansas Traveller", a painted depiction of a folk tale from the 1840s. Although intended to represent the divide between rich southeastern plantation Arkansas planters and the poor northwestern hill country, the meaning was twisted to represent a Northerner lost in the Ozarks on a white horse asking a backwoods Arkansan for directions. The state also suffers from the racial stigma common to former Confederate states, with historical events such as the Little Rock Nine adding to Arkansas's enduring image.
Art and history museums display pieces of cultural value for Arkansans and tourists to enjoy. Crystal Bridges Museum of American Art in Bentonville is the most popular with 604,000 visitors in 2012, its first year. The museum includes walking trails and educational opportunities in addition to displaying over 450 works covering five centuries of American art. Several historic town sites have been restored as Arkansas state parks, including Historic Washington State Park, Powhatan Historic State Park, and Davidsonville Historic State Park.
Arkansas features a variety of native music across the state, ranging from the blues heritage of West Memphis, Pine Bluff, Helena-West Helena to rockabilly, bluegrass, and folk music from the Ozarks. Festivals such as the King Biscuit Blues Festival and Bikes, Blues, and BBQ pay homage to the history of blues in the state. The Ozark Folk Festival in Mountain View is a celebration of Ozark culture and often features folk and bluegrass musicians. Literature set in Arkansas such as "I Know Why the Caged Bird Sings" by Maya Angelou and "A Painted House" by John Grisham describe the culture at various time periods.
Sports and recreation.
Sports have become an integral part of the culture of Arkansas, and her residents enjoy participating in and spectating various events throughout the year.
Team sports and especially collegiate football have been important to Arkansans. College football in Arkansas began from humble beginnings. The University of Arkansas first fielded a team in 1894 when football was a very dangerous game. Recent studies of the damage to team members from the concussions common in football make it clear that the danger persists.
"Calling the Hogs" is a cheer that shows support for the Razorbacks, one of the two FBS teams in the state. High school football also began to grow in Arkansas in the early 20th century. Over the years, many Arkansans have looked to the Razorbacks football team as the public image of the state. Following the Little Rock Nine integration crisis at Little Rock Central High School, Arkansans looked to the successful Razorback teams in the following years to repair the state's reputation. Although the University of Arkansas is based in Fayetteville, the Razorbacks have always played at least two games per season at War Memorial Stadium in Little Rock in an effort to keep fan support in central and south Arkansas. Arkansas State University joined the University of Arkansas in the Football Bowl Subdivision in 1992 after playing in lower divisions for nearly two decades. However, the two schools have never played each other, due to the University of Arkansas' policy of not playing intrastate games. Six of Arkansas' smaller colleges play in the Great American Conference, with University of Arkansas at Pine Bluff playing in the Southwestern Athletic Conference and University of Central Arkansas competing in the Southland Conference.
Baseball runs deep in Arkansas and has been popular since before the state hosted Major League Baseball (MLB) spring training in Hot Springs from 1886-1920s. Today, two minor league teams are based in the state. The Arkansas Travelers play at Dickey-Stephens Park in North Little Rock, and the Northwest Arkansas Naturals play in Arvest Ballpark in Springdale. Both teams compete in the Texas League.
Related to the state's frontier past, hunting continues in the state. The state created the Arkansas Game and Fish Commission in 1915 to regulate and enforce hunting. Today a significant portion of Arkansas's population participates in hunting duck in the Mississippi flyway and deer across the state. Millions of acres of public land are available for both bow and modern gun hunters.
Fishing has always been popular in Arkansas, and the sport and the state have benefited from the creation of reservoirs across the state. Following the completion of Norfork Dam, the Norfork Tailwater and the White River have become a destination for trout fishers. Several smaller retirement communities such as Bull Shoals, Hot Springs Village, and Fairfield Bay have flourished due to their position on a fishing lake. The Buffalo National River has been preserved in its natural state by the National Park Service and is frequented by fly fishers annually.
Health.
Arkansans, as with many Southern states, have a high incidence of premature death, infant mortality, cardiovascular deaths, and occupational fatalities compared to the rest of the United States. The state is tied for 43rd with New York in percentage of adults who regularly exercise. Arkansas is usually ranked as one of the least healthy states due to high obesity, smoking, and sedentary lifestyle rates. In contrast though a Gallup poll demonstrates that Arkansas made the most immediate progress in reducing its number of uninsured residents following the passage of the Affordable Care Act. The percentage of uninsured in Arkansas dropped from 22.5 percent in 2013 to 12.4 percent in August 2014.
The Arkansas Clean Indoor Air Act went into effect in 2006, a statewide smoking ban excluding bars and some restaurants.
Healthcare in Arkansas is provided by a network of hospitals as members of the Arkansas Hospital Association. Major institutions with multiple branches include Baptist Health, Community Health Systems, and HealthSouth. The University of Arkansas for Medical Sciences (UAMS) in Little Rock operates the UAMS Medical Center, a teaching hospital ranked as high performing nationally in cancer and nephrology. The pediatric division of UAMS Medical Center is known as Arkansas Children's Hospital, nationally ranked in pediatric cardiology and heart surgery. Together, these two institutions are the state's only Level I trauma centers.
Education.
Arkansas ranks as the 32nd smartest state on the Morgan Quitno Smartest State Award, 44th in percentage of residents with at least a high school diploma, and 48th in percentage of bachelor's degree attainment. However, Arkansas has been making major strides recently in education reform. "Education Week" has praised the state, ranking Arkansas in the top 10 of their Quality Counts Education Rankings every year since 2009 while scoring it in the top 5 during 2012 and 2013. Arkansas specifically received an A in Transition and Policy Making for progress in this area consisting of early-childhood education, college readiness, and career readiness. Governor Mike Beebe has made improving education a major issue through his attempts to spend more on education. Through reforms, the state is now a leader in requiring curricula designed to prepare students for postsecondary education, rewarding teachers for student achievement, and providing incentives for principals who work in lower-tier schools.
In 2010 Arkansas students earned an average score of 20.3 on the ACT exam, just below the national average of 21. These results were expected due to the large increase in the number of students taking the exam since the establishment of the Academic Challenge Scholarship. Top high schools receiving recognition from the U.S. News & World Report are spread across the state, including Haas Hall Academy in Fayetteville, KIPP Delta Collegiate in Helena-West Helena, Bentonville, Rogers, Rogers Heritage, Valley Springs, Searcy, and McCrory. A total of 81 Arkansas high schools were ranked by the U.S. News & World Report in 2012.
The state supports a network of public universities and colleges, including two major university systems: Arkansas State University System and University of Arkansas System. The University of Arkansas, flagship campus of the University of Arkansas System in Fayetteville was ranked #63 among public schools in the nation by "U.S. News & World Report". Other public institutions include Arkansas Tech University, Henderson State University, Southern Arkansas University, and University of Central Arkansas across the state. It is also home to 11 private colleges and universities including Hendrix College, one of the nation's top 100 liberal arts colleges, according to U.S. News & World Report.
Transportation.
Transportation in Arkansas is overseen by the Arkansas State Highway and Transportation Department (AHTD), headquartered in Little Rock. Several main corridors pass through Little Rock, including Interstate 30 (I-30) and I-40 (the nation's 3rd-busiest trucking corridor). In northeast Arkansas, I-55 travels north from Memphis to Missouri, with a new spur to Jonesboro (I-555). Northwest Arkansas is served by I-540 from Fort Smith to Bella Vista, which is a segment of future I-49. The state also has the 13th largest state highway system in the nation.
Arkansas is served by 2750 mi of railroad track divided among twenty-six railroad companies including three Class I railroads. Freight railroads are concentrated in southeast Arkansas to serve the industries in the region. The Texas Eagle, an Amtrak passenger train, serves five stations in the state Walnut Ridge, Little Rock, Malvern, Arkadelphia, and Texarkana.
Arkansas also benefits from the use of its rivers for commerce. The Mississippi River and Arkansas River are both major rivers. The United States Army Corps of Engineers maintains the McClellan-Kerr Arkansas River Navigation System, allowing barge traffic up the Arkansas River to the Port of Catoosa in Tulsa, Oklahoma.
There are four airports with commercial service: Clinton National Airport, Northwest Arkansas Regional Airport, Fort Smith Regional Airport, and Texarkana Regional Airport, with dozens of smaller airports in the state.
Public transit and community transport services for the elderly or those with developmental disabilities are provided by agencies such as the Central Arkansas Transit Authority and the Ozark Regional Transit, organizations that are part of the Arkansas Transit Association.
Law and government.
As with the federal government of the United States, political power in Arkansas is divided into three branches: executive, legislative, and judicial. Each officer's term is four years long. Office holders are term-limited to two full terms plus any partial terms before the first full term.
Executive.
The current Governor of Arkansas is Asa Hutchinson, a Republican, who was inaugurated on January 13, 2015. The six other elected executive positions in Arkansas are lieutenant governor, secretary of state, attorney general, treasurer, auditor, and land commissioner. The governor also appoints qualified individuals to lead various state boards, committees, and departments. Arkansas governors served two-year terms until a referendum lengthened the term to four years, effective with the 1986 general election.
In Arkansas, the lieutenant governor is elected separately from the governor and thus can be from a different political party.
Legislative.
The Arkansas General Assembly is the state's bicameral bodies of legislators, composed of the Senate and House of Representatives. The Senate contains 35 members from districts of approximately equal population. These districts are redrawn decennially with each US census, and in election years ending in "2", the entire body is put up for reelection. Following the election, half of the seats are designated as two-year seats and will be up for reelection again in two years, these "half-terms" do not count against a legislator's term limits. The remaining half serve a full four-year term. This staggers elections such that half the body is up for re-election every two years and allows for complete body turnover following redistricting. Arkansas voters selected a 21-14 Republican majority in the Senate in 2012. Arkansas House members can serve a maximum of three two-year terms. House districts are redistricted by the Arkansas Board of Apportionment. Following the 2012 elections, Republicans gained a 51-49 majority in the House of Representatives.
The Republican Party majority status in the Arkansas State House of Representatives following the 2012 elections is the party's first since 1874. Arkansas was the last state of the old Confederacy to never have Republicans control either chamber of its house since the Civil War.
Following the term limits changes, studies have shown that lobbyists have become less influential in state politics, but legislative staff, not subject to term limits, have acquired additional power and influence due to the high rate of elected official turnover.
Judicial.
Arkansas's judicial branch has five court systems: Arkansas Supreme Court, Arkansas Court of Appeals, Circuit Courts, District Courts and City Courts.
Most cases begin in district court, which is subdivided into state district court and local district court. State district courts exercise district-wide jurisdiction over the districts created by the General Assembly, and local district courts are presided over by part-time judges who may privately practice law. There are currently 25 state district court judges presiding over 15 districts, with more districts to be created in 2013 and 2017. There are 28 judicial circuits of Circuit Court, with each contains five subdivisions: criminal, civil, probate, domestic relations, and juvenile court. The jurisdiction of the Arkansas Court of Appeals is determined by the Arkansas Supreme Court, and there is no right of appeal from the Court of Appeals to the high court. However, the Arkansas Supreme Court can review Court of Appeals cases upon application by either a party to the litigation, upon request by the Court of Appeals, or if the Arkansas Supreme Court feels the case should have been initially assigned to it. The twelve judges of the Arkansas Court of Appeals are elected from judicial districts to renewable six-year terms.
The Arkansas Supreme Court is the court of last resort in the state, composed of seven justices elected to eight-year terms. Established by the Arkansas Constitution in 1836, the court's decisions can be appealed to only the Supreme Court of the United States.
Federal.
Both of Arkansas's U.S. Senators, John Boozman and Tom Cotton, are Republicans. The state has four seats in U.S. House of Representatives. All four seats are held by Republicans: Rick Crawford (1st district), French Hill (2nd district), Steve Womack (3rd district), and Bruce Westerman (4th district).
Politics.
Arkansas Governor Bill Clinton brought national attention to the state with a long speech at the 1988 Democratic National Convention endorsing Michael Dukakis. Pundits suggested the speech would ruin Clinton's political career, but instead, Clinton won the Democratic nomination for President the following cycle. Presenting himself as a "New Democrat" and using incumbent George H. W. Bush's against him, Clinton won the 1992 presidential election (43.0% of the vote) against Republican Bush (37.4% of the vote) and billionaire populist Ross Perot, who ran as an independent (18.9% of the vote).
Most Republican strength lies mainly in the northwestern part of the state, particularly Fort Smith and Bentonville, as well as North Central Arkansas around the Mountain Home area. In the latter area, Republicans have been known to get 90 percent or more of the vote. The rest of the state is more Democratic. Arkansas has only elected three Republicans to the U.S. Senate since Reconstruction, Tim Hutchinson, who was defeated after one term by Mark Pryor; John Boozman, who defeated incumbent Blanche Lincoln; and Tom Cotton, who defeated Mark Pryor in the 2014 elections. Prior to 2013, the General Assembly had not been controlled by the Republican Party since Reconstruction with the GOP holding a 51-seat majority in the state House and a 21-seat (of 35) in the state Senate following victories in 2012. Arkansas was one of just three states among the states of the former Confederacy that sent two Democrats to the U.S. Senate (the others being Florida and Virginia) during the first decade of the 21st century.
In 2010, Republicans captured three of the state's four seats in the U.S. House of Representatives. In 2012, Republicans won election for all four House seats. Arkansas holds the distinction of having a U.S. House delegation composed of military veterans (Rick Crawford - Army; Tim Griffin - Army Reserve; Steve Womack - Army National Guard, Tom Cotton- Army). In 2014, the last Democrat in Arkansas' Congressional Delegation, Mark Pryor, was defeated in campaign to win a third term in the U.S. Senate.
Reflecting the state's large evangelical population, the state has a strong social conservative bent. Under the Arkansas Constitution Arkansas is a right to work state, its voters passed a ban on same-sex marriage with 75% voting yes, and the state is one of a handful with legislation on its books banning abortion in the event "Roe v. Wade" is ever overturned.
Attractions.
Arkansas is home to many areas protected by the National Park System. These include:
Further reading.
</dl>

</doc>
<doc id="1931" url="http://en.wikipedia.org/wiki?curid=1931" title="Atmosphere (disambiguation)">
Atmosphere (disambiguation)

An atmosphere is a gas layer around a celestial body.
Atmosphere may also refer to:

</doc>
<doc id="1933" url="http://en.wikipedia.org/wiki?curid=1933" title="Apus">
Apus

Apus is a faint constellation in the southern sky, first defined in the late 16th century. Its name means "no feet" in Greek, and it represents a bird-of-paradise (which were once believed to lack feet). It is bordered by Triangulum Australe, Circinus, Musca, Chamaeleon, Octans, Pavo and Ara. Its genitive is "Apodis".
History.
Apus was one of twelve constellations created by Petrus Plancius from the observations of Pieter Dirkszoon Keyser and Frederick de Houtman. It first appeared on a 35 cm diameter celestial globe published in 1597 or 1598 in Amsterdam by Plancius with Jodocus Hondius. Plancius called the constellation "Paradysvogel Apis Indica"; the first word is Dutch for "bird of paradise", of genus Pteridophora, but the others are Latin for "Indian Bee". "Apis" (Latin for "bee") is presumably an error for "avis" ("bird").
The name "Apus" is derived from the Greek "apous", meaning "without feet". This referred to the Western misconception that the bird-of-paradise had no feet, which arose because the only specimens available in the West had their feet and wings removed. Such specimens began to arrive in Europe in 1522, when the survivors of Ferdinand Magellan's expedition brought them home.
After its introduction on Plancius's globe, the constellation's first known appearance in a celestial atlas was in Johann Bayer's "Uranometria" of 1603, where it was called "Apis Indica".
Richard Allen reports Semler's assertion that de Houtman, who observed the southern constellations from the island of Sumatra, took his ideas for the formation of Apus (as well as Phoenix and Indus) from the Chinese, who knew the stars of Apus as the "Little Wonder Bird", and that Semler's assertion was disputed by Ideler (though Ideler acknowledged the Chinese constellations).
Notable features.
The most prominent deep-sky objects in Apus include the globular clusters NGC 6101 and IC 4499 as well as the spiral galaxy IC 4633.
Stars.
Alpha Apodis is an orange giant 411 light years away, with a magnitude of 3.8. Beta Apodis is an orange giant 158 light years away, with a magnitude of 4.2. Gamma Apodis is an orange giant 160 light years away, with a magnitude of 3.9. Delta Apodis is a double star with a separation of 103 arcseconds. δ1 is a red giant star located 765 light years away, with a magnitude of 4.7. δ2 is an orange giant star located 663 light years away, with a magnitude of 5.3. The separate components can be resolved with binoculars, a telescope, or the naked eye. Theta Apodis is a variable red giant at a distance of 328 light years with a period of approximately 4 months, or 109 days. It has a maximum magnitude of 4.8 and a minimum magnitude of 6.1.
NO Apodis is a red giant of spectral type M3III that varies between magnitudes 5.71 and 5.95. Located around 883 light-years distant, it shines with a luminosity approximately 2059 times that of the Sun and has a surface temperature of 3568 K.
Equivalents.
When the Ming Dynasty Chinese astronomer Xu Guangqi adapted the European southern hemisphere constellations to the Chinese system in "The Southern Asterisms", he combined Apus with some of the stars in Octans to form the "Exotic Bird" (異雀, "Yìquè").
External links.
Coordinates: 

</doc>
<doc id="1934" url="http://en.wikipedia.org/wiki?curid=1934" title="Abadan, Iran">
Abadan, Iran

Abadan (Persian: آبادان‎ "Ābādān") is a city in and the capital of Abadan County, Khuzestan Province which in located in central west of Iran. It lies on Abadan Island (68 km long, 3–19 km or 2–12 miles wide, the island is bounded in the west by the Arvand waterway and to the east by the Bahmanshir outlet of the Karun River (the Shatt al-Arab), 53 km from the Persian Gulf, near the Iraqi-Iran border.
Etymology.
The earliest mention of the island of Abadan, if not the port itself is found in works of the geographer Marcian, who renders the name "Apphadana". Earlier, the classical geographer, Ptolemy notes "Apphana" as an island off the mouth of the Tigris (which is, where the modern Island of Abadan is located). An etymology for this name is presented by 'B. Farahvashi" to be derived from the Persian word "ab" (water) and the root "pā" (guard, watch) thus "coastguard station").
In the Islamic times, a pseudo-etymology was produced by the historian Ahmad ibn Yahya al-Baladhuri (d.892) quoting a folk story that the town was presumably founded by one ""Abbad bin Hosayn" from the Arabian Tribe of Banu Tamim", who established a garrison there during the governorship of "Hajjaj" in the Ummayad period.
In the subsequent centuries, the Persian version of the name had begun to come into general use before it was adopted by official decree in 1935.
Population.
The civilian population of the city dropped close to zero during the eight years of the Iran–Iraq War (1980–88). The 1986 census recorded only 6 people. In 1991, 84,774 had returned to live in the city. By 2001, the population had jumped to 206,073, and it was 217,988, in 48,061 families, according to 2006 census. Abadan Refinery is one of the largest in the world.
Only 9% of managers (of the oil company) were from Khuzestan. The proportion of natives of Tehran, the Caspian, Azarbaijan and Kurdistan rose from 4% of blue collar workers to 22% of white collar workers to 45% of managers, thus Minority Arabic-speakers were concentrated on the lower rungs of the work force, managers tended to be brought in from some distance.
History.
Abadan is thought to have been further developed into a major port city under the Abbasids' rule. In this time period, it was a commercial source of salt and woven mats. The siltation of the river delta forced the town further away from water; In the 14th century, however, Ibn Battutah described Abadan just as a small port in a flat salty plain. Politically, Abadan was often the subject of dispute between the nearby states; in 1847, Persia acquired it from Turkey, in which state Abadan has remained since. From the 17th century onward, the island of Abadan was part of the lands of the Arab "Ka'ab" (Bani Kaab) tribe. One section of this tribe, "Mohaysen", had its headquarters at "Mohammara"(present-day Khorramshahr), until the removal of Shaikh Khaz'al Khan in 1924.
It was not until the 20th century that rich oil fields were discovered in the area. On 16 July 1909, after secret negotiation with the British consul (Percy Cox assisted by Arnold Wilson), Sheik Khaz'al agreed to a rental agreement for the island including Abadan. The Sheik continued to administer the island until 1924. The Anglo-Persian Oil Company built their first pipeline terminus oil refinery in Abadan, starting in 1909 and completing it in 1912, with oil flowing by August 1912 (see Abadan Refinery). Refinery throughput numbers rose from 33,000 tons in 1912-1913 to 4,338,000 tons in 1931. By 1938, it was the largest in the world.
During World War II, Abadan was a major logistics center for Lend-Lease aircraft being sent to the Soviet Union by the United States.
In 1951, Iran nationalized all oil properties and refining ground to a stop on the island. Rioting broke out in Abadan, after the government had decided to nationalize the oil facilities, and 3 British workers were killed. It wasn't until 1954, that a settlement was reached, which allows a group of international oil companies to manage the production and refining on the island. This continued until 1973, when the NIOC took over all facilities. After total nationalization, Iran focused on supplying oil domestically and built a pipeline from Abadan to Tehran.
Whereas Abadan was not a major cultural or religious center, it did play an important role in the Islamic Revolution. On 19 August 1978—the anniversary of the US backed pro-Shah coup d'état which overthrew the nationalists and popular Iranian prime minister, Dr. Mohammed Mossadegh— the Cinema Rex, a movie theatre in Abadan, Iran, was set ablaze. The Cinema Rex Fire was the site of 430 deaths, but more importantly, it was another event that kept the Islamic Revolution moving ahead. At the time there was a lot of confusion and misinformation about the incident; however the public blamed the local police chief and also the Shah and SAVAK. The reformist Sobhe Emrooz newspaper in one of its editorials revealed that the Cinema Rex was burned down by the radical Islamists. The newspaper was shut down immediately after. Over time, the true culprits, radical Islamists, were apprehended and the logic behind this act was revealed, as they were trying both to foment the general public to distrust the government even more, and also as they perceived cinema as a link to the Americans. This fire was one of four during a short period in August, with other fires in Mashhad, Rizaiya, and Shiraz.
In September 1980, Abadan was almost overrun during a surprise attack on Khuzestan by Iraq, marking the beginning of the Iran–Iraq War. For 12 months Abadan was besieged, but never captured, by Iraqi forces, and in September 1981, the Iranians broke the siege of Abadan. Much of the city, including the oil refinery which was the world's largest refinery with capacity of 628,000 barrels per day, was badly damaged or destroyed by the siege and by bombing. Previous to the war, the city's civilian population was about 300,000, but before it was over, most of the populace had sought refuge elsewhere in Iran.
After the war, the biggest concern was the rebuilding of Abadan's oil refinery, as they were operating at 10% of capacity due to damage. In 1993, the refinery began limited operation & and the port reopened. By 1997, the refinery reached the same rate of production it was at before the war. Recently, Abadan has been the site of major labor activity as workers at the oil refineries in the city have staged walkouts and strikes to protest non-payment of wages and the political situation in the country.
Recent events.
To honor the 100th anniversary of the refining of oil in Abadan, city officials are planning an oil museum. The Abadan oil refinery was featured on the reverse side of Iran's 100-rial banknotes printed in 1965 and from 1971 to 1973.
Climate.
The climate in Abadan is arid (Köppen climate classification "BWh") and similar to Baghdad's, but slightly hotter due to Abadan's lower latitude. Summers are dry and extremely hot, with temperatures above 45 °C almost daily. Winters are mildly wet and spring-like, though subject to cold spells. Winter temperatures are around 16-20 °C. The world's highest unconfirmed temperature was a temperature flare up during a heat burst in June 1967, with a temperature of 87 °C. Reliable measurements in the city range from -5 to.
Places of interest.
The Abadan Institute of Technology was established in Abadan in 1939. The school specialized in engineering and petroleum chemistry, and was designed to train staff for the refinery in town. The school's name has since changed several times, but since 1989 has been considered a branch campus of the Petroleum University of Technology, centered in Tehran.
There is an international airport in Abadan. It is represented by the IATA airport code ABD.
Main sights.
Mosques.
Rangoonis Mosque
References.
 

</doc>
<doc id="1935" url="http://en.wikipedia.org/wiki?curid=1935" title="Attorney">
Attorney

Attorney may refer to:

</doc>
<doc id="1937" url="http://en.wikipedia.org/wiki?curid=1937" title="Alexander Fleming">
Alexander Fleming

Sir Alexander Fleming, FRSE, FRS, FRCS(Eng) (6 August 1881 – 11 March 1955) was a Scottish biologist, pharmacologist and botanist. He wrote many articles on bacteriology, immunology, and chemotherapy. His best-known discoveries are the enzyme lysozyme in 1923 and the antibiotic substance benzylpenicillin (Penicillin G) from the mould "Penicillium notatum" in 1928, for which he shared the Nobel Prize in Physiology or Medicine in 1945 with Howard Florey and Ernst Boris Chain.
Early life and education.
Fleming was born on 6 August 1881 at Lochfield farm near Darvel, in Ayrshire, Scotland. He was the third of the four children of farmer Hugh Fleming (1816–1888) from his second marriage to Grace Stirling Morton (1848–1928), the daughter of a neighbouring farmer. Hugh Fleming had four surviving children from his first marriage. He was 59 at the time of his second marriage, and died when Alexander (known as Alec) was seven.
Fleming went to Loudoun Moor School and Darvel School, and earned a two-year scholarship to Kilmarnock Academy before moving to London, where he attended the Royal Polytechnic Institution. After working in a shipping office for four years, the twenty-year-old Fleming inherited some money from an uncle, John Fleming. His elder brother, Tom, was already a physician and suggested to his younger sibling that he should follow the same career, and so in 1903, the younger Alexander enrolled at St Mary's Hospital Medical School in Paddington; he qualified with an MBBS degree from the school with distinction in 1906. 
Fleming had been a private in the London Scottish Regiment of the Volunteer Force since 1900, and had been a member of the rifle club at the medical school. The captain of the club, wishing to retain Fleming in the team suggested that he join the research department at St Mary's, where he became assistant bacteriologist to Sir Almroth Wright, a pioneer in vaccine therapy and immunology. In 1908, he gained a BSc degree with Gold Medal in Bacteriology, and became a lecturer at St Mary's until 1914.
Fleming served throughout World War I as a captain in the Royal Army Medical Corps, and was Mentioned in Dispatches. He and many of his colleagues worked in battlefield hospitals at the Western Front in France. In 1918 he returned to St Mary's Hospital, where he was elected Professor of Bacteriology of the University of London in 1928. In 1951 he was elected the Rector of the University of Edinburgh for a term of 3 years.
Research.
Work before penicillin.
Following World War I, Fleming actively searched for anti-bacterial agents, having witnessed the death of many soldiers from sepsis resulting from infected wounds. Antiseptics killed the patients' immunological defences more effectively than they killed the invading bacteria. In an article he submitted for the medical journal "The Lancet" during World War I, Fleming described an ingenious experiment, which he was able to conduct as a result of his own glass blowing skills, in which he explained why antiseptics were killing more soldiers than infection itself during World War I. Antiseptics worked well on the surface, but deep wounds tended to shelter anaerobic bacteria from the antiseptic agent, and antiseptics seemed to remove beneficial agents produced that protected the patients in these cases at least as well as they removed bacteria, and did nothing to remove the bacteria that were out of reach. Sir Almroth Wright strongly supported Fleming's findings, but despite this, most army physicians over the course of the war continued to use antiseptics even in cases where this worsened the condition of the patients.
Accidental discovery.
"When I woke up just after dawn on September 28, 1928, I certainly didn't plan to revolutionise all medicine by discovering the world's first antibiotic, or bacteria killer," Fleming would later say, "But I suppose that was exactly what I did."
By 1927, Fleming had been investigating the properties of staphylococci. He was already well-known from his earlier work, and had developed a reputation as a brilliant researcher, but his laboratory was often untidy. On 3 September 1928, Fleming returned to his laboratory having spent August on holiday with his family. Before leaving, he had stacked all his cultures of staphylococci on a bench in a corner of his laboratory. On returning, Fleming noticed that one culture was contaminated with a fungus, and that the colonies of staphylococci immediately surrounding the fungus had been destroyed, whereas other staphylococci colonies farther away were normal, famously remarking "That's funny". Fleming showed the contaminated culture to his former assistant Merlin Price, who reminded him, "That's how you discovered lysozyme." Fleming grew the mould in a pure culture and found that it produced a substance that killed a number of disease-causing bacteria. He identified the mould as being from the "Penicillium" genus, and, after some months of calling it "mould juice", named the substance it released "penicillin" on 7 March 1929. The laboratory in which Fleming discovered and tested penicillin is preserved as the Alexander Fleming Laboratory Museum in St. Mary's Hospital, Paddington.
He investigated its positive anti-bacterial effect on many organisms, and noticed that it affected bacteria such as staphylococci and many other Gram-positive pathogens that cause scarlet fever, pneumonia, meningitis and diphtheria, but not typhoid fever or paratyphoid fever, which are caused by Gram-negative bacteria, for which he was seeking a cure at the time. It also affected "Neisseria gonorrhoeae," which causes gonorrhoea although this bacterium is Gram-negative.
Fleming published his discovery in 1929, in the British "Journal of Experimental Pathology," but little attention was paid to his article. Fleming continued his investigations, but found that cultivating "penicillium" was quite difficult, and that after having grown the mould, it was even more difficult to isolate the antibiotic agent. Fleming's impression was that because of the problem of producing it in quantity, and because its action appeared to be rather slow, penicillin would not be important in treating infection. Fleming also became convinced that penicillin would not last long enough in the human body ("in vivo") to kill bacteria effectively. Many clinical tests were inconclusive, probably because it had been used as a surface antiseptic. In the 1930s, Fleming’s trials occasionally showed more promise, and he continued, until 1940, to try to interest a chemist skilled enough to further refine usable penicillin. Fleming finally abandoned penicillin, and not long after he did, Howard Florey and Ernst Boris Chain at the Radcliffe Infirmary in Oxford took up researching and mass-producing it, with funds from the U.S. and British governments. They started mass production after the bombing of Pearl Harbor. By D-Day in 1944, enough penicillin had been produced to treat all the wounded with the Allied forces.
Purification and stabilisation.
In Oxford, Ernst Boris Chain and Edward Abraham discovered how to isolate and concentrate penicillin. Abraham was the first to propose the correct structure of penicillin. Shortly after the team published its first results in 1940, Fleming telephoned Howard Florey, Chain's head of department, to say that he would be visiting within the next few days. When Chain heard that Fleming was coming, he remarked "Good God! I thought he was dead."
Norman Heatley suggested transferring the active ingredient of penicillin back into water by changing its acidity. This produced enough of the drug to begin testing on animals. There were many more people involved in the Oxford team, and at one point the entire Dunn School was involved in its production.
After the team had developed a method of purifying penicillin to an effective first stable form in 1940, several clinical trials ensued, and their amazing success inspired the team to develop methods for mass production and mass distribution in 1945.
Fleming was modest about his part in the development of penicillin, describing his fame as the "Fleming Myth" and he praised Florey and Chain for transforming the laboratory curiosity into a practical drug. Fleming was the first to discover the properties of the active substance, giving him the privilege of naming it: penicillin. He also kept, grew, and distributed the original mould for twelve years, and continued until 1940 to try to get help from any chemist who had enough skill to make penicillin. But Sir Henry Harris said in 1998: "Without Fleming, no Chain; without Chain, no Florey; without Florey, no Heatley; without Heatley, no penicillin."
Antibiotics.
Fleming's accidental discovery and isolation of penicillin in September 1928 marks the start of modern antibiotics. Before that, several scientists had published or pointed out that mould or "penicillium sp." were able to inhibit bacterial growth, and even to cure bacterial infections in animals. Ernest Duchesne in 1897 in his thesis "Contribution to the study of vital competition in micro-organisms: antagonism between moulds and microbes", or also Clodomiro Picado Twight whose work at Institut Pasteur in 1923 on the inhibiting action of fungi of the "Penicillin sp" genre in the growth of staphylococci drew little interest from the direction of the Institut at the time. Fleming was the first to push these studies further by isolating the penicillin, and by being motivated enough to promote his discovery at a larger scale. Fleming also discovered very early that bacteria developed antibiotic resistance whenever too little penicillin was used or when it was used for too short a period. Almroth Wright had predicted antibiotic resistance even before it was noticed during experiments. Fleming cautioned about the use of penicillin in his many speeches around the world. He cautioned not to use penicillin unless there was a properly diagnosed reason for it to be used, and that if it were used, never to use too little, or for too short a period, since these are the circumstances under which bacterial resistance to antibiotics develops.
Myths.
The popular story of Winston Churchill's father paying for Fleming's education after Fleming's father saved young Winston from death is false. According to the biography, "Penicillin Man: Alexander Fleming and the Antibiotic Revolution" by Kevin Brown, Alexander Fleming, in a letter to his friend and colleague Andre Gratia, described this as "A wondrous fable." Nor did he save Winston Churchill himself during World War II. Churchill was saved by Lord Moran, using sulphonamides, since he had no experience with penicillin, when Churchill fell ill in Carthage in Tunisia in 1943. The "Daily Telegraph" and the "Morning Post" on 21 December 1943 wrote that he had been saved by penicillin. He was saved by the new sulphonamide drug Sulphapyridine, known at the time under the research code M&B 693, discovered and produced by May & Baker Ltd, Dagenham, Essex – a subsidiary of the French group Rhône-Poulenc. In a subsequent radio broadcast, Churchill referred to the new drug as "This admirable M&B." It is highly probable that the correct information about the sulphonamide did not reach the newspapers because, since the original sulphonamide antibacterial, Prontosil, had been a discovery by the German laboratory Bayer, and as Britain was at war with Germany at the time, it was thought better to raise British morale by associating Churchill's cure with a British discovery, penicillin.
Personal life.
On 24 December 1915, Fleming married a trained nurse, Sarah Marion McElroy of Killala, County Mayo, Ireland. Their only child, Robert Fleming, (b. 1924) became a general medical practitioner. After Sarah's death in 1949, Fleming married Dr. Amalia Koutsouri-Vourekas, a Greek colleague at St. Mary's, on 9 April 1953; she died in 1986.
Death.
On 11 March 1955, Fleming died at his home in London of a heart attack. He was buried in St Paul's Cathedral.
Honours, awards and achievements.
His discovery of penicillin had changed the world of modern medicine by introducing the age of useful antibiotics; penicillin has saved, and is still saving, millions of people around the world.
The laboratory at St Mary's Hospital where Fleming discovered penicillin is home to the Fleming Museum, a popular London attraction. His alma mater, St Mary's Hospital Medical School, merged with Imperial College London in 1988. The Sir Alexander Fleming Building on the South Kensington campus was opened in 1998 and is now one of the main preclinical teaching sites of the Imperial College School of Medicine.
His other alma mater, the Royal Polytechnic Institution (now the University of Westminster) has named one of its student halls of residence "Alexander Fleming House", which is near to Old Street.
It was a discovery that would change the course of history. The active ingredient in that mould, which Fleming named penicillin, turned out to be an infection-fighting agent of enormous potency. When it was finally recognized for what it was, the most efficacious life-saving drug in the world, penicillin would alter forever the treatment of bacterial infections. By the middle of the century, Fleming's discovery had spawned a huge pharmaceutical industry, churning out synthetic penicillins that would conquer some of mankind's most ancient scourges, including syphilis, gangrene and tuberculosis.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="1938" url="http://en.wikipedia.org/wiki?curid=1938" title="Andrew Carnegie">
Andrew Carnegie

Andrew Carnegie ( , but commonly or ; November 25, 1835 – August 11, 1919) was a Scottish American industrialist who led the enormous expansion of the American steel industry in the late 19th century. He built a leadership role as a philanthropist for America and the British Empire. He gave away to charities and foundations about $350 million (in 2015, $) – almost 90 percent of his fortune. His 1889 article proclaiming "The Gospel of Wealth" called on the rich to use their wealth to improve society, and it stimulated a wave of philanthropy.
Carnegie was born in Dunfermline, Scotland, and immigrated to the United States with his very poor parents in 1848. Carnegie started work as a telegrapher and by the 1860s had investments in railroads, railroad sleeping cars, bridges and oil derricks. He accumulated further wealth as a bond salesman raising money for American enterprise in Europe. He built Pittsburgh's Carnegie Steel Company, which he sold to J.P. Morgan in 1901 for $480 million (in 2015, $), creating the U.S. Steel Corporation. Carnegie devoted the remainder of his life to large-scale philanthropy, with special emphasis on local libraries, world peace, education and scientific research. With the fortune he made from business, he built Carnegie Hall and he founded the Carnegie Corporation of New York, Carnegie Endowment for International Peace, Carnegie Institution for Science, Carnegie Trust for the Universities of Scotland, Carnegie Hero Fund, Carnegie Mellon University and the Carnegie Museums of Pittsburgh, among others. 
Biography.
Early life.
Andrew Carnegie was born in Dunfermline, Scotland, in a typical weaver's cottage with only one main room, consisting of half the ground floor which was shared with the neighboring weaver's family. The main room served as a living room, dining room and bedroom. He was named after his legal grandfather. In 1836, the family moved to a larger house in Edgar Street (opposite Reid's Park), following the demand for more heavy damask from which his father, William Carnegie, benefited. His uncle, George Lauder, whom he referred to as "Dod", introduced him to the writings of Robert Burns and historical Scottish heroes such as Robert the Bruce, William Wallace, and Rob Roy. Falling on very hard times as a handloom weaver and with the country in starvation, William Carnegie decided to move with his family to Allegheny, Pennsylvania, in the United States in 1848 for the prospect of a better life. Andrew's family had to borrow money in order to migrate. Allegheny was a growing industrial area that produced many products to include wool and cotton cloth, and the "Made in Allegheny" label used on these and other diversified products was becoming more and more popular. His first job at age 13 in 1848 was as a bobbin boy, changing spools of thread in a cotton mill 12 hours a day, 6 days a week in a Pittsburgh cotton factory. His starting wage was $1.20 per week. Andrew's father, William Carnegie, started off working in a cotton mill but then would earn money weaving and peddling linens. His mother, Margaret Morrison Carnegie, earned money by binding shoes.
Railroads.
In 1850, Carnegie became a telegraph messenger boy in the Pittsburgh Office of the Ohio Telegraph Company, at $2.50 per week, following the recommendation of his uncle. His new job gave him many benefits including free admission to the local theater. This made him appreciate Shakespeare's work.
He was a very hard worker and would memorize all of the locations of Pittsburgh's businesses and the faces of important men. He made many connections this way. He also paid close attention to his work, and quickly learned to distinguish the differing sounds the incoming telegraph signals produced. He developed the ability to translate signals by ear, without using the paper slip, and within a year was promoted as an operator. Carnegie's education and passion for reading was given a great boost by Colonel James Anderson, who opened his personal library of 400 volumes to working boys each Saturday night. Carnegie was a consistent borrower and a "self-made man" in both his economic development and his intellectual and cultural development. His capacity, his willingness for hard work, his perseverance, and his alertness soon brought forth opportunities.
Starting in 1853, Thomas A. Scott of the Pennsylvania Railroad Company employed Carnegie as a secretary/telegraph operator at a salary of $4.00 per week. At age 18, the precocious youth began a rapid advance through the company, becoming the superintendent of the Pittsburgh Division. His employment by the Pennsylvania Railroad Company would be vital to his later success. The railroads were the first big businesses in America, and the Pennsylvania was one of the largest of them all. Carnegie learned much about management and cost control during these years, and from Scott in particular.
Scott also helped him with his first investments. Many of these were part of the corruption indulged in by Scott and the Pennsylvania's president, J. Edgar Thomson, which consisted of inside trading in companies that the railroad did business with, or payoffs made by contracting parties "as part of a quid pro quo". In 1855, Scott made it possible for Carnegie to invest $500 in the Adams Express, which contracted with the Pennsylvania to carry its messengers. The money was secured by his mother's placing a $500 mortgage on the family's $700 home, but the opportunity was available only because of Carnegie's close relationship with Scott. A few years later, he received a few shares in T.T. Woodruff's sleeping car company, as a reward for holding shares that Woodruff had given to Scott and Thomson, as a payoff. Reinvesting his returns in such inside investments in railroad-related industries: (iron, bridges, and rails), Carnegie slowly accumulated capital, the basis for his later success. Throughout his later career, he made use of his close connections to Thomson and Scott, as he established businesses that supplied rails and bridges to the railroad, offering the two men a stake in his enterprises.
1860–1865: The Civil War.
Before the Civil War, Carnegie arranged a merger between Woodruff's company and that of George Pullman, the inventor of a sleeping car for first class travel which facilitated business travel at distances over 500 mi. The investment proved a great success and a source of profit for Woodruff and Carnegie. The young Carnegie continued to work for the Pennsylvania's Tom Scott, and introduced several improvements in the service.
In spring 1861, Carnegie was appointed by Scott, who was now Assistant Secretary of War in charge of military transportation, as Superintendent of the Military Railways and the Union Government's telegraph lines in the East. Carnegie helped open the rail lines into Washington D.C. that the rebels had cut; he rode the locomotive pulling the first brigade of Union troops to reach Washington D.C. Following the defeat of Union forces at Bull Run, he personally supervised the transportation of the defeated forces. Under his organization, the telegraph service rendered efficient service to the Union cause and significantly assisted in the eventual victory. Carnegie later joked that he was "the first casualty of the war" when he gained a scar on his cheek from freeing a trapped telegraph wire.
Defeat of the Confederacy required vast supplies of munitions, as well as railroads (and telegraph lines) to deliver the goods. The war demonstrated how integral the industries were to American success.
Keystone Bridge Company.
In 1864, Carnegie invested $40,000 in Story Farm on Oil Creek in Venango County, Pennsylvania. In one year, the farm yielded over $1,000,000 in cash dividends, and petroleum from oil wells on the property sold profitably. The demand for iron products, such as armor for gunboats, cannon, and shells, as well as a hundred other industrial products, made Pittsburgh a center of wartime production. Carnegie worked with others in establishing a steel rolling mill, and steel production and control of industry became the source of his fortune. Carnegie had some investments in the iron industry before the war.
After the war, Carnegie left the railroads to devote all his energies to the ironworks trade. Carnegie worked to develop several iron works, eventually forming The Keystone Bridge Works and the Union Ironworks, in Pittsburgh. Although he had left the Pennsylvania Railroad Company, he remained closely connected to its management, namely Thomas A. Scott and J. Edgar Thomson. He used his connection to the two men to acquire contracts for his Keystone Bridge Company and the rails produced by his ironworks. He also gave stock to Scott and Thomson in his businesses, and the Pennsylvania was his best customer. When he built his first steel plant, he made a point of naming it after Thomson. As well as having good business sense, Carnegie possessed charm and literary knowledge. He was invited to many important social functions—functions that Carnegie exploited to his own advantage.
Carnegie believed in using his fortune for others and doing more than making money. He wrote: I propose to take an income no greater than $50,000 per annum! Beyond this I need ever earn, make no effort to increase my fortune, but spend the surplus each year for benevolent purposes! Let us cast aside business forever, except for others. Let us settle in Oxford and I shall get a thorough education, making the acquaintance of literary men. I figure that this will take three years active work. I shall pay especial attention to speaking in public. We can settle in London and I can purchase a controlling interest in some newspaper or live review and give the general management of it attention, taking part in public matters, especially those connected with education and improvement of the poorer classes. Man must have no idol and the amassing of wealth is one of the worst species of idolatry! No idol is more debasing than the worship of money! Whatever I engage in I must push inordinately; therefore should I be careful to choose that life which will be the most elevating in its character. To continue much longer overwhelmed by business cares and with most of my thoughts wholly upon the way to make more money in the shortest time, must degrade me beyond hope of permanent recovery. I will resign business at thirty-five, but during these ensuing two years I wish to spend the afternoons in receiving instruction and in reading systematically!
Industrialist.
1885–1900: Steel empire.
Carnegie did not want to marry during his mother's lifetime, instead choosing to take care of her in her illness towards the end of her life. After she died in 1886, Carnegie married Louise Whitfield, who was more than 20 years his junior. In 1897, the couple had their only child, a daughter, whom they named after Carnegie's mother, Margaret.
Carnegie made his fortune in the steel industry, controlling the most extensive integrated iron and steel operations ever owned by an individual in the United States. One of his two great innovations was in the cheap and efficient mass production of steel by adopting and adapting the Bessemer process for steel making. Sir Henry Bessemer had invented the furnace which allowed the high carbon content of pig iron to be burnt away in a controlled and rapid way. The steel price dropped as a direct result, and Bessemer steel was rapidly adopted for railway lines and girders for buildings and bridges.
The second was in his vertical integration of all suppliers of raw materials. In the late 1880s, Carnegie Steel was the largest manufacturer of pig iron, steel rails, and coke in the world, with a capacity to produce approximately 2,000 tons of pig metal per day. In 1883, Carnegie bought the rival Homestead Steel Works, which included an extensive plant served by tributary coal and iron fields, a 425-mile (685 km) long railway, and a line of lake steamships. Carnegie combined his assets and those of his associates in 1892 with the launching of the Carnegie Steel Company.
By 1889, the U.S. output of steel exceeded that of the UK, and Carnegie owned a large part of it. Carnegie's empire grew to include the J. Edgar Thomson Steel Works, (named for John Edgar Thomson, Carnegie's former boss and president of the Pennsylvania Railroad), Pittsburgh Bessemer Steel Works, the Lucy Furnaces, the Union Iron Mills, the Union Mill (Wilson, Walker & County), the Keystone Bridge Works, the Hartman Steel Works, the Frick Coke Company, and the Scotia ore mines. Carnegie, through Keystone, supplied the steel for and owned shares in the landmark Eads Bridge project across the Mississippi River at St. Louis, Missouri (completed 1874). This project was an important proof-of-concept for steel technology, which marked the opening of a new steel market.
1901: U.S. Steel.
In 1901, Carnegie was 66 years of age and considering retirement. He reformed his enterprises into conventional joint stock corporations as preparation to this end. John Pierpont Morgan was a banker and perhaps America's most important financial deal maker. He had observed how efficiently Carnegie produced profit. He envisioned an integrated steel industry that would cut costs, lower prices to consumers, produce in greater quantities and raise wages to workers. To this end, he needed to buy out Carnegie and several other major producers and integrate them into one company, thereby eliminating duplication and waste. He concluded negotiations on March 2, 1901, and formed the United States Steel Corporation. It was the first corporation in the world with a market capitalization over $1 billion.
The buyout, secretly negotiated by Charles M. Schwab (no relation to Charles R. Schwab), was the largest such industrial takeover in United States history to date. The holdings were incorporated in the United States Steel Corporation, a trust organized by Morgan, and Carnegie retired from business. His steel enterprises were bought out at a figure equivalent to 12 times their annual earnings—$480 million (in 2015, $) which at the time was the largest ever personal commercial transaction.
Carnegie's share of this amounted to $225,639,000 (in 2015, $), which was paid to Carnegie in the form of 5%, 50-year gold bonds. The letter agreeing to sell his share was signed on February 26, 1901. On March 2, the circular formally filing the organization and capitalization (at $1,400,000,000—4% of U.S. national wealth at the time) of the United States Steel Corporation actually completed the contract. The bonds were to be delivered within two weeks to the Hudson Trust Company of Hoboken, New Jersey, in trust to Robert A. Franks, Carnegie's business secretary. There, a special vault was built to house the physical bulk of nearly $230,000,000 worth of bonds. It was said that "...Carnegie never wanted to see or touch these bonds that represented the fruition of his business career. It was as if he feared that if he looked upon them they might vanish like the gossamer gold of the leprechaun. Let them lie safe in a vault in New Jersey, safe from the New York tax assessors, until he was ready to dispose of them..."
Scholar and activist.
1880–1900.
Carnegie continued his business career; some of his literary intentions were fulfilled. He befriended English poet Matthew Arnold, English philosopher Herbert Spencer, and American humorist Mark Twain, as well as being in correspondence and acquaintance with most of the U.S. Presidents, statesmen, and notable writers.
Carnegie constructed commodious swimming-baths for the people of his hometown in Dunfermline in 1879. In the following year, Carnegie gave $40,000 for the establishment of a free library in Dunfermline. In 1884, he gave $50,000 to Bellevue Hospital Medical College (now part of New York University Medical Center) to found a histological laboratory, now called the Carnegie Laboratory.
In 1881, Carnegie took his family, including his 70 year-old mother, on a trip to the United Kingdom. They toured Scotland by coach, and enjoyed several receptions en route. The highlight for them all was a triumphal return to Dunfermline, where Carnegie's mother laid the foundation stone of a Carnegie library for which he donated the money. Carnegie's criticism of British society did not mean dislike; on the contrary, one of Carnegie's ambitions was to act as a catalyst for a close association between the English-speaking peoples. To this end, in the early 1880s in partnership with Samuel Storey, he purchased numerous newspapers in England, all of which were to advocate the abolition of the monarchy and the establishment of "the British Republic". Carnegie's charm aided by his great wealth meant that he had many British friends, including Prime Minister William Ewart Gladstone.
In 1886, Carnegie's younger brother Thomas died at age 43. Success in the business continued, however. While owning steel works, Carnegie had purchased at low cost the most valuable of the iron ore fields around Lake Superior. The same year Carnegie became a figure of controversy. Following his tour of the UK, he wrote about his experiences in a book entitled "An American Four-in-hand in Britain". Although still actively involved in running his many businesses, Carnegie had become a regular contributor to numerous magazines, most notably "The Nineteenth Century", under the editorship of James Knowles, and the influential "North American Review", led by editor Lloyd Bryce.
In 1886, Carnegie wrote his most radical work to date, entitled "Triumphant Democracy". Liberal in its use of statistics to make its arguments, the book argued his view that the American republican system of government was superior to the British monarchical system. It gave a highly favorable and idealized view of American progress and criticized the British royal family. The cover depicted an upended royal crown and a broken scepter. The book created considerable controversy in the UK. The book made many Americans appreciate their country's economic progress and sold over 40,000 copies, mostly in the U.S.
In 1889, Carnegie published "Wealth" in the June issue of the "North American Review". After reading it, Gladstone requested its publication in England, where it appeared as "The Gospel of Wealth" in the "Pall Mall Gazette". The article was the subject of much discussion. Carnegie argued that the life of a wealthy industrialist should comprise two parts. The first part was the gathering and the accumulation of wealth. The second part was for the subsequent distribution of this wealth to benevolent causes. The philanthropy was key to making the life worthwhile.
Carnegie was a well-regarded writer. He published three books on travel.
Anti-imperialism.
While Carnegie did not comment on British imperialism, he very strongly opposed the idea of American colonies. He strongly opposed the annexation of the Philippines, almost to the point of supporting William Jennings Bryan against McKinley in 1900. In 1898, Carnegie tried to arrange for independence for the Philippines. As the end of the Spanish American War neared, the United States bought the Philippines from Spain for $20 million. To counter what he perceived as imperialism on the part of the United States, Carnegie personally offered $20 million to the Philippines so that the Filipino people could buy their independence from the United States. However, nothing came of the offer. In 1898 Carnegie joined the American Anti-Imperialist League, in opposition to the U.S. annexation of the Philippines. Its membership included former presidents of the United States Grover Cleveland and Benjamin Harrison and literary figures like Mark Twain.
1901–1919: Philanthropist.
Carnegie spent his last years as a philanthropist. From 1901 forward, public attention was turned from the shrewd business acumen which had enabled Carnegie to accumulate such a fortune, to the public-spirited way in which he devoted himself to utilizing it on philanthropic projects. He had written about his views on social subjects and the responsibilities of great wealth in "Triumphant Democracy" (1886) and "Gospel of Wealth" (1889). Carnegie bought Skibo Castle in Scotland, and made his home partly there and partly in New York. He then devoted his life to providing the capital for purposes of public interest and social and educational advancement.
He was a powerful supporter of the movement for spelling reform as a means of promoting the spread of the English language.
Among his many philanthropic efforts, the establishment of public libraries throughout the United States, Britain, Canada and other English-speaking countries was especially prominent. In this special driving interest and project of his he was inspired by a visit and tour he made with Mr. Enoch Pratt (1808–1896), formerly of Massachusetts but who made his fortune in Baltimore and ran his various mercantile and financial businesses very thriftily. Pratt in turn had been inspired and helped by his friend and fellow Bay Stater, George Peabody, (1795–1869) who also had made his fortune in the "Monumental City" of Baltimore before moving to New York and London to expand his empire as the richest man in America before the Civil War. Later he too endowed several institutions, schools, libraries and foundations in his home commonwealth, and also in Baltimore with his Peabody Institute in 1857, completed in 1866, with added library wings a decade later and several educational foundations throughout the Old South. Several decades later, Carnegie's visit with Mr. Pratt for several days; resting and dining in his city mansion, then touring, visiting and talking with staff and ordinary citizen patrons of the newly established Enoch Pratt Free Library (1886) impressed the Scotsman deeply and years later he was always heard to proclaim that "Pratt was my guide and inspiration". The first Carnegie library opened in 1883 in Dunfermline. His method was to build and equip, but only on condition that the local authority matched that by providing the land and a budget for operation and maintenance. To secure local interest, in 1885, he gave $500,000 to Pittsburgh for a public library, and in 1886, he gave $250,000 to Allegheny City for a music hall and library; and $250,000 to Edinburgh for a free library. In total Carnegie funded some 3,000 libraries, located in 47 US states, and also in Canada, the United Kingdom, what is now the Republic of Ireland, Australia, New Zealand, the West Indies, and Fiji. He also donated £50,000 to help set up the University of Birmingham in 1899. In the early 20th Century, a decade after Mr. Pratt's death, when expansion and city revenues grew tight, Carnegie returned the favor and endowed a large sum to permit the building of many Carnegie Libraries in the Enoch Pratt system in Baltimore and enabled EPFL to expand through the next quarter-century to meet the needs of the growing city and supply neighborhood branches for its annexed suburbs.
As Van Slyck (1991) showed, the last years of the 19th century saw acceptance of the idea that free libraries should be available to the American public. But the design of the idealized free library was the subject of prolonged and heated debate. On one hand, the library profession called for designs that supported efficiency in administration and operation; on the other, wealthy philanthropists favored buildings that reinforced the paternalistic metaphor and enhanced civic pride. Between 1886 and 1917, Carnegie reformed both library philanthropy and library design, encouraging a closer correspondence between the two.
He gave $2 million in 1900 to start the Carnegie Institute of Technology (CIT) at Pittsburgh and the same amount in 1902 to found the Carnegie Institution at Washington, D.C. He later contributed more to these and other schools. CIT is now known as Carnegie Mellon University after it merged with the Mellon Institute of Industrial Research. Carnegie also served on the Board of Cornell University.
In 1911, Carnegie became a sympathetic benefactor to George Ellery Hale, who was trying to build the 100 in Hooker Telescope at Mount Wilson, and donated an additional ten million dollars to the Carnegie Institution with the following suggestion to expedite the construction of the telescope: "I hope the work at Mount Wilson will be vigorously pushed, because I am so anxious to hear the expected results from it. I should like to be satisfied before I depart, that we are going to repay to the old land some part of the debt we owe them by revealing more clearly than ever to them the new heavens." The telescope saw first light on November 2, 1917, with Carnegie still alive.
In Scotland, he gave $10 million in 1901 to establish the Carnegie Trust for the Universities of Scotland. It was created by a deed which he signed on June 7, 1901, and it was incorporated by Royal Charter on August 21, 1902. The Trust was funded by a gift of $10 million (a then unprecedented sum: at the time, total government assistance to all four Scottish universities was about £50,000 a year) and its aim was to improve and extend the opportunities for scientific research in the Scottish universities and to enable the deserving and qualified youth of Scotland to attend a university. He was subsequently elected Lord Rector of University of St. Andrews in December 1901. He also donated large sums of money to Dunfermline, the place of his birth. In addition to a library, Carnegie also bought the private estate which became Pittencrieff Park and opened it to all members of the public, establishing the Carnegie Dunfermline Trust to benefit the people of Dunfermline. A statue of him stands there today. He gave a further $10 million in 1913 to endow the Carnegie United Kingdom Trust, a grant-making foundation.
Carnegie also established large pension funds in 1901 for his former employees at Homestead and, in 1905, for American college professors. The latter fund evolved into TIAA-CREF. One critical requirement was that church-related schools had to sever their religious connections to get his money.
His interest in music led him to fund construction of 7,000 church organs. He built and owned Carnegie Hall in New York City.
Carnegie was a large benefactor of the Tuskegee Institute under Booker T. Washington for African-American education. He helped Washington create the National Negro Business League.
He founded the Carnegie Hero Fund for the United States and Canada in 1904 (a few years later also established in the United Kingdom, Switzerland, Norway, Sweden, France, Italy, the Netherlands, Belgium, Denmark, and Germany) for the recognition of deeds of heroism. Carnegie contributed $1,500,000 in 1903 for the erection of the Peace Palace at The Hague; and he donated $150,000 for a Pan-American Palace in Washington as a home for the International Bureau of American Republics.
Carnegie was honored for his philanthropy and support of the arts by initiation as an honorary member of Phi Mu Alpha Sinfonia Fraternity on October 14, 1917, at the New England Conservatory of Music in Boston, Massachusetts. The fraternity's mission reflects Carnegie's values by developing young men to share their talents to create harmony in the world.
By the standards of 19th century tycoons, Carnegie was not a particularly ruthless man but a humanitarian with enough acquisitiveness to go in the ruthless pursuit of money. "Maybe with the giving away of his money," commented biographer Joseph Wall, "he would justify what he had done to get that money."
To some, Carnegie represents the idea of the American dream. He was an immigrant from Scotland who came to America and became successful. He is not only known for his successes but his enormous amounts of philanthropist works, not only to charities but also to promote democracy and independence to colonized countries.
Death.
Carnegie died on August 11, 1919, in Lenox, Massachusetts at his Shadow Brook estate, of bronchial pneumonia. He had already given away $350,695,653 (approximately $4.75 billion, adjusted to 2015 figures) of his wealth.After his death, his last $30,000,000 was given to foundations, charities, and to pensioners. He was buried at the Sleepy Hollow Cemetery in North Tarrytown, New York. The grave site is located on the Arcadia Hebron plot of land at the corner of Summit Avenue and Dingle Road. Carnegie is buried only a few yards away from union organizer Samuel Gompers, another important figure of industry in the Gilded Age.
Controversies.
1889: Johnstown Flood.
Carnegie was one of more than 50 members of the South Fork Fishing and Hunting Club, which has been blamed for the Johnstown Flood that killed 2,209 people in 1889.
At the suggestion of his friend Benjamin Ruff, Carnegie's partner Henry Clay Frick had formed the exclusive South Fork Fishing and Hunting Club high above Johnstown, Pennsylvania. The sixty-odd club members were the leading business tycoons of Western Pennsylvania and included among their number Frick's best friend, Andrew Mellon, his attorneys Philander Knox and James Hay Reed, as well as Frick's business partner, Carnegie. High above the city, near the small town of South Fork, the South Fork Dam was originally built between 1838 and 1853 by the Commonwealth of Pennsylvania as part of a canal system to be used as a reservoir for a canal basin in Johnstown. With the coming-of-age of railroads superseding canal barge transport, the lake was abandoned by the Commonwealth, sold to the Pennsylvania Railroad, and sold again to private interests and eventually came to be owned by the South Fork Fishing and Hunting Club in 1881. Prior to the flood, speculators had purchased the abandoned reservoir, made less than well-engineered repairs to the old dam, raised the lake level, built cottages and a clubhouse, and created the South Fork Fishing and Hunting Club. Less than 20 miles downstream from the dam sat the city of Johnstown.
The dam was 72 ft high and 931 ft long. Between 1881 when the club was opened, and 1889, the dam frequently sprang leaks and was patched, mostly with mud and straw. Additionally, a previous owner removed and sold for scrap the 3 cast iron discharge pipes that previously allowed a controlled release of water. There had been some speculation as to the dam's integrity, and concerns had been raised by the head of the Cambria Iron Works downstream in Johnstown. Such repair work, a reduction in height, and unusually high snowmelt and heavy spring rains combined to cause the dam to give way on May 31, 1889 resulting in twenty million tons of water sweeping down the valley causing the Johnstown Flood. When word of the dam's failure was telegraphed to Pittsburgh, Frick and other members of the South Fork Fishing and Hunting Club gathered to form the Pittsburgh Relief Committee for assistance to the flood victims as well as determining never to speak publicly about the club or the flood. This strategy was a success, and Knox and Reed were able to fend off all lawsuits that would have placed blame upon the club's members.
Although Cambria Iron and Steel's facilities were heavily damaged by the flood, they returned to full production within a year. After the flood, Carnegie built Johnstown a new library to replace the one built by Cambria's chief legal counsel Cyrus Elder, which was destroyed in the flood. The Carnegie-donated library is now owned by the Johnstown Area Heritage Association, and houses the Flood Museum.
1892: Homestead Strike.
The Homestead Strike was a bloody labor confrontation lasting 143 days in 1892, one of the most serious in U.S. history. The conflict was centered on Carnegie Steel's main plant in Homestead, Pennsylvania, and grew out of a dispute between the National Amalgamated Association of Iron and Steel Workers of the United States and the Carnegie Steel Company.
Carnegie left on a trip to Scotland before the unrest peaked. In doing so, Carnegie left mediation of the dispute in the hands of his associate and partner Henry Clay Frick. Frick was well known in industrial circles for maintaining staunch anti-union sensibilities.
After a recent increase in profits by 60%, the company refused to raise workers' pay by more than 30%. When some of the workers demanded the full 60%, management locked the union out. Workers considered the stoppage a "lockout" by management and not a "strike" by workers. As such, the workers would have been well within their rights to protest, and subsequent government action would have been a set of criminal procedures designed to crush what was seen as a pivotal demonstration of the growing labor rights movement, strongly opposed by management. Frick brought in thousands of strikebreakers to work the steel mills and Pinkerton agents to safeguard them.
On July 6, the arrival of a force of 300 Pinkerton agents from New York City and Chicago resulted in a fight in which 10 men—seven strikers and three Pinkertons—were killed and hundreds were injured. Pennsylvania Governor Robert Pattison ordered two brigades of state militia to the strike site. Then, allegedly in response to the fight between the striking workers and the Pinkertons, anarchist Alexander Berkman shot at Frick in an attempted assassination, wounding Frick. While not directly connected to the strike, Berkman was tied in for the assassination attempt. According to Berkman, "...with the elimination of Frick, responsibility for Homestead conditions would rest with Carnegie." Afterwards, the company successfully resumed operations with non-union immigrant employees in place of the Homestead plant workers, and Carnegie returned to the United States. However, Carnegie's reputation was permanently damaged by the Homestead events.
Philosophy.
Andrew Carnegie Dictum.
In his final days, Carnegie suffered from bronchial pneumonia. Before his death on August 11, 1919, Carnegie had donated $350,695,654 for various causes. The "Andrew Carnegie Dictum" was:
Carnegie was involved in philanthropic causes, but he kept himself away from religious circles. He wanted to be identified by the world as a "positivist". He was highly influenced in public life by John Bright.
On wealth.
As early as 1868, at age 33, he drafted a memo to himself. He wrote: "...The amassing of wealth is one of the worse species of idolatry. No idol more debasing than the worship of money."
In order to avoid degrading himself, he wrote in the same memo he would retire at age 35 to pursue the practice of philanthropic giving for "...the man who dies thus rich dies disgraced." However, he did not begin his philanthropic work in all earnest until 1881, with the gift of a library to his hometown of Dunfermline, Scotland.
Carnegie wrote "The Gospel of Wealth", an article in which he stated his belief that the rich should use their wealth to help enrich society.
The following is taken from one of Carnegie's memos to himself: Man does not live by bread alone. I have known millionaires starving for lack of the nutriment which alone can sustain all that is human in man, and I know workmen, and many so-called poor men, who revel in luxuries beyond the power of those millionaires to reach. It is the mind that makes the body rich. There is no class so pitiably wretched as that which possesses money and nothing else. Money can only be the useful drudge of things immeasurably higher than itself. Exalted beyond this, as it sometimes is, it remains Caliban still and still plays the beast. My aspirations take a higher flight. Mine be it to have contributed to the enlightenment and the joys of the mind, to the things of the spirit, to all that tends to bring into the lives of the toilers of Pittsburgh sweetness and light. I hold this the noblest possible use of wealth.
Intellectual influences.
Carnegie claimed to be a champion of evolutionary thought particularly the work of Herbert Spencer, even declaring Spencer his teacher. Though Carnegie claims to be a disciple of Spencer many of his actions went against the ideas espoused by Spencer.
Spencerian evolution was for individual rights and against government interference. Furthermore, Spencerian evolution held that those unfit to sustain themselves must be allowed to perish. Spencer believed that just as there were many varieties of beetles, respectively modified to existence in a particular place in nature, so too had human society “spontaneously fallen into division of labour”. Individuals who survived to this, the latest and highest stage of evolutionary progress would be “those in whom the power of self-preservation is the greatest—are the select of their generation.” Moreover, Spencer perceived governmental authority as borrowed from the people to perform the transitory aims of establishing social cohesion, insurance of rights, and security. Spencerian ‘survival of the fittest’ firmly credits any provisions made to assist the weak, unskilled, poor and distressed to be an imprudent disservice to evolution. Spencer insisted people should resist for the benefit of collective humanity as these severe fate singles out the weak, debauched, and disabled.
Andrew Carnegie’s political and economic focus of during the late nineteenth and early twentieth century was the defense of laissez faire economics. Carnegie emphatically resisted government intrusion in commerce, as well as government-sponsored charities. Carnegie believed the concentration of capital was essential for societal progress and should be encouraged. Carnegie was an ardent supporter of commercial “survival of the fittest” and sought to attain immunity from business challenges by dominating all phases of the steel manufacturing procedure. Carnegie’s determination to lower costs included cutting labor expenses as well. In a notably Spencerian manner, Carnegie argued that unions impeded the natural reduction of prices by pushing up costs, which blocked evolutionary progress. Carnegie felt that unions represented the narrow interest of the few while his actions benefited the entire community.
On the surface, Andrew Carnegie appears to be a strict laissez-faire capitalist and follower of Herbert Spencer, often referring to himself as a disciple of Spencer. Conversely, Carnegie a titan of industry seems to embody all of the qualities of Spencerian survival of the fittest. The two men enjoyed a mutual respect for one another and maintained correspondence until Spencer’s death in 1903. There are however, some major discrepancies between Spencer’s capitalist evolutionary conceptions and Andrew Carnegie’s capitalist practices.
Spencer wrote that in production the advantages of the superior individual is comparatively minor, and thus acceptable, yet the benefit that dominance provides those who control a large segment of production might be hazardous to competition. Spencer feared that an absence of “sympathetic self-restraint” of those with too much power could lead to the ruin of his competitors. He did not think free market competition necessitated competitive warfare. Furthermore, Spencer argued that individuals with superior resources who deliberately used investment schemes to put competitor out of business were committing acts of “commercial murder”. Carnegie built his wealth in the steel industry by maintaining an extensively integrated operating system. Carnegie also bought out some regional competitors, and merged with others, usually maintaining the majority shares in the companies. Over the course of twenty years, Carnegie’s steel properties grew to include the Edgar Thomson Steel Works, the Lucy Furnace Works, the Union Iron Mills, the Homestead Works, the Keystone Bridge Works, the Hartman Steel Works, the Frick Coke Company, and the Scotia ore mines among many other industry related assets. Furthermore, Carnegie’s success was due to his convenient relationship with the railroad industries, which not only relied on steel for track, but were also making money from steel transport. The steel and railroad barons worked closely to negotiate prices instead of free market competition determinations.
Besides Carnegie’s market manipulation, United States trade tariffs were also working in favor of the steel industry. Carnegie spent energy and resources lobbying congress for a continuation of favorable tariffs from which he earned millions of dollars a year. Carnegie tried to keep this information concealed, but legal document released in 1900, during proceeding with the ex-chairman of Carnegie Steel Henry Clay Frick revealed how favorable the tariffs had been. Herbert Spencer absolutely was against government interference in business in the form of regulatory limitation, taxes, and tariffs as well. Spencer saw tariffs as a form of taxation that levied against the majority in service to “the benefit of a small minority of manufacturers and artisans”.
Despite Carnegie's personal dedication to Herbert Spencer as a friend, his adherence to Spencer’s political and economic ideas is more contentious. In particular, it appears Carnegie either misunderstood or intentionally misrepresented some of Spencer's principal arguments. Spencer remarked upon his first visit to Carnegie's steel mills in Pittsburgh, which Carnegie saw as the manifestation of Spencer's philosophy, "Six months' residence here would justify suicide."
The conditions of human society create for this an imperious demand; the concentration of capital is a necessity for meeting the demands of our day, and as such should not be looked at askance, but be encouraged. There is nothing detrimental to human society in it, but much that is, or is bound soon to become, beneficial. It is an evolution from the heterogeneous to the homogeneous, and is clearly another step in the upward path of development.—Carnegie, Andrew 1901 The Gospel of Wealth and Other Timely Essays
On the subject of charity Andrew Carnegie's actions diverged in the most significant and complex manner from Herbert Spencer's philosophies. In his 1854 essay Manners and Fashion, Spencer referred to public education as “Old schemes”. He went on to declare that public schools and colleges, fill the heads of students with inept useless knowledge, which excludes useful knowledge. Spencer stated that he trusted no organization of any kind, “political, religious, literary, philanthropic”, and believed that as they expanded in influence so too did its regulations expand. In addition Spencer thought that as all institutions grow they become evermore corrupted by the influence of power and money. The institution eventually loses its “original spirit, and sinks into a lifeless mechanism”. Spencer insisted that all forms of philanthropy uplift the poor and downtrodden were reckless and incompetent. Spencer thought any attempt to prevent “the really salutary sufferings” of the less fortunate “bequeath to posterity a continually increasing curse”. Carnegie, a self-proclaimed devotee of Spencer, testified to Congress on February 5, 1915: "My business is to do as much good in the world as I can; I have retired from all other business."
Carnegie held that societal progress relied on individuals who maintained moral obligations to themselves and to society. Furthermore, he believed that charity supplied the means for those who wish to improve themselves to achieve their goals. Carnegie urged other wealthy people to contribute to society in the form of parks, works of art, libraries and other endeavors that improve the community and contribute to the “lasting good.” Carnegie also held a strong opinion against inherited wealth. Carnegie believed that the sons of prosperous businesspersons were rarely as talented as their fathers. By leaving large sums of money to their children, wealthy business leaders were wasting resources that could be used to benefit society. Most notably, Carnegie believed that the future leaders of society would rise from the ranks the poor. Carnegie strongly believed in this because he had risen from the bottom. He believed the poor possessed an advantage over the wealthy because they receive greater attention from their parents and are taught better work ethics.
Religion and world view.
Witnessing sectarianism and strife in 19th century Scotland regarding religion and philosophy, Carnegie kept his distance from organized religion and theism. Carnegie instead preferred to see things through naturalistic and scientific terms stating, "Not only had I got rid of the theology and the supernatural, but I had found the truth of evolution."
Later in life, Carnegie's firm opposition to religion softened. For many years he was a member of Madison Avenue Presbyterian Church, pastored from 1905 to 1926 by Social Gospel exponent Henry Sloane Coffin, while his wife and daughter belonged to the Brick Presbyterian Church. He also prepared (but did not deliver) an address in which he professed a belief in "an Infinite and Eternal Energy from which all things proceed".
Records exist of a short period of correspondence around 1912–1913 between Carnegie and `Abdu'l-Bahá, the eldest son of Bahá'u'lláh, founder of the Bahá'í Faith. In these letters, one of which was published in the "New York Times" in full text, Carnegie is extolled as a "lover of the world of humanity and one of the founders of Universal Peace".
World peace.
Influenced by his "favorite living hero in public life", the British liberal, John Bright, Carnegie started his efforts in pursuit of world peace at a young age. His motto, "All is well since all grows better", served not only as a good rationalization of his successful business career but also in his view of international relations.
Despite his efforts towards international peace, Carnegie faced many dilemmas on his quest. These dilemmas are often regarded as conflicts between his view on international relations and his other loyalties. Throughout the 1880s and 1890s, for example, Carnegie allowed his steel works to fill large orders of armor plate for the building of an enlarged and modernized United States Navy; while he opposed American oversea expansion.
On the matter of American colonial expansion, Carnegie had always thought it is an unwise gesture for the United States. He did not oppose the annexation of the Hawaiian islands or Puerto Rico, but he opposed the annexation of the Philippines. Carnegie believed that it involved a denial of the fundamental democratic principle, and he also urged William McKinley to withdraw American troops and allow the Filipinos to live with their independence. This act well impressed the other American anti-imperialists, who soon elected him vice-president of the Anti-Imperialist League.
After he sold his steel company in 1901, Carnegie was able to get fully involved into the acts for the peace cause, both financially and personally. He gave away much of his fortunes to various peace-keeping agencies in order to keep them growing. When his friend, the British writer William T. Stead, asked him to create a new organization for the goal of a peace and arbitration society, his reply was as such:
I do not see that it is wise to devote our efforts to creating another organization. Of course I may be wrong in believing that, but I am certainly not wrong that if it were dependent on any millionaire's money it would begin as an object of pity and end as one of derision. I wonder that you do not see this. There is nothing that robs a righteous cause of its strength more than a millionaire's money. Its life is tainted thereby.
Carnegie believed that it is the effort and will of the people, that maintains the peace in international relations. Money is just a push for the act. If world peace depended solely on financial support, it would not seem a goal, but more like an act of pity.
Like Stead, he believed that the United States and the British Empire would merge into one nation, telling him "We are heading straight to the Re-United States". Carnegie believed that the combined country's power would maintain world peace and disarmament. The creation of the Carnegie Endowment for International Peace in 1910 was regarded as a milestone on the road to the ultimate goal of abolition of war. Beyond a gift of $10 million for peace promotion, Carnegie also encouraged the "scientific" investigation of the various causes of war, and the adoption of judicial methods that should eventually eliminate them. He believed that the Endowment exists to promote information on the nations' rights and responsibilities under existing international law and to encourage other conferences to codify this law.
In 1914, on the eve of the First World War, Carnegie founded the Church Peace Union (CPU), a group of leaders in religion, academia, and politics. Through the CPU, Carnegie hoped to mobilize the world's churches, religious organizations, and other spiritual and moral resources to join in promoting moral leadership to put an end to war forever. For its inaugural international event, the CPU sponsored a conference to be held on August 1, 1914, on the shores of Lake Constance in southern Germany. As the delegates made their way to the conference by train, Germany was invading Belgium.
Despite its inauspicious beginning, the CPU thrived. Today its focus is on ethics and it is known as the Carnegie Council for Ethics in International Affairs, an independent, nonpartisan, nonprofit organization, whose mission is to be the voice for ethics in international affairs.
The outbreak of the First World War was clearly a shock to Carnegie and his optimistic view on world peace. Although his promotion of anti-imperialism and world peace had all failed, and the Carnegie Endowment had not fulfilled his expectations, his beliefs and ideas on international relations had helped build the foundation of the League of Nations after his death, which took world peace to another level.
Writings.
Carnegie was a frequent contributor to periodicals on labor issues. In addition to "Triumphant Democracy" (1886), and "The Gospel of Wealth" (1889), he also wrote "Our Coaching Trip, Brighton to Inverness" (1882), "An American Four-in-hand in Britain" (1883), "Round the World" (1884), "The Empire of Business" (1902), "The Secret of Business is the Management of Men" (1903), "James Watt" (1905) in the Famous Scots Series, "Problems of Today" (1907), and his posthumously published "Autobiography of Andrew Carnegie" (1920).
Legacy and honors.
Carnegie received the honorary Doctor of Laws (DLL) from the University of Glasgow in June 1901, and received the Freedom of the City of Glasgow "in recognition of his munificence" later the same year.
Carnegie's personal papers reside at the Library of Congress Manuscript Division.
The Carnegie Collections of the Columbia University Rare Book and Manuscript Library consist of the archives of the following organizations founded by Carnegie: The Carnegie Corporation of New York (CCNY); The Carnegie Endowment for International Peace (CEIP); the Carnegie Foundation for the Advancement of Teaching (CFAT);The Carnegie Council on Ethics and International Affairs (CCEIA). These collections deal primarily with Carnegie philanthropy and have very little personal material related to Carnegie. Carnegie Mellon University and the Carnegie Library of Pittsburgh jointly administer the Andrew Carnegie Collection of digitized archives on Carnegie's life.
Further reading.
Listen to this article ()
This audio file was created from a revision of the "Andrew Carnegie" article dated 2009-03-10, and does not reflect subsequent edits to the article. ()
More spoken articles
Listen to this article ()
This audio file was created from a revision of the "Andrew Carnegie" article dated 2009-03-10, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="1939" url="http://en.wikipedia.org/wiki?curid=1939" title="Approximant consonant">
Approximant consonant

Approximants are speech sounds that involve the articulators approaching each other but not narrowly enough nor with enough articulatory precision to create turbulent airflow. Therefore, approximants fall between fricatives, which do produce a turbulent airstream, and vowels, which produce no turbulence. This class of sounds includes lateral approximants like [l] (as in "less"), non-lateral approximants like [ɹ] (as in "rest"), and semivowels like [j] and [w] (as in "yes" and "west", respectively).
Before Peter Ladefoged coined the term "approximant" in the 1960s the term "frictionless continuant" referred to non-lateral approximants.
Semivowels.
Some approximants resemble vowels in acoustic and articulatory properties and the terms "semivowel" and "glide" are often used for these non-syllabic vowel-like segments. The correlation between semivowels and vowels is strong enough that cross-language differences between semivowels correspond with the differences between their related vowels.
Vowels and their corresponding semivowels alternate in many languages depending on the phonological environment, or for grammatical reasons, as is the case with Indo-European ablaut. Similarly, languages often avoid configurations where a semivowel precedes its corresponding vowel. A number of phoneticians distinguish between semivowels and approximants by their location in a syllable. Although he uses the terms interchangeably, remarks that, for example, the final glides of English "par" and "buy" differ from French "par" ('through') and "baille" ('tub') in that, in the latter pair, the approximants appear in the syllable coda, whereas, in the former, they appear in the syllable nucleus. This means that opaque (if not minimal) contrasts can occur in languages like Italian (with the i-like sound of "piede" 'foot', appearing in the nucleus: [ˈpi̯eˑde], and that of "piano" 'slow', appearing in the syllable onset: [ˈpjaˑno]) and Spanish (with a near minimal pair being "abyecto" [aβˈjekto] 'abject' and "abierto" [aˈβi̯erto] 'opened').
In articulation and often diachronically, palatal approximants correspond to front vowels, velar approximants to back vowels, and labialized approximants to rounded vowels. In American English, the rhotic approximant corresponds to the rhotic vowel. This can create alternations (as shown in the above table).
In addition to alternations, glides can be inserted to the left or the right of their corresponding vowels when occurring next to a hiatus. For example, in Ukrainian, medial /i/ triggers the formation of an inserted [j] that acts as a syllable onset so that when the affix /-ist/ is added to футбол ('football') to make футболіст 'football player', it's pronounced [futˈbo̞list] but маоїст ('Maoist'), with the same affix, is pronounced [ˈmao̞jist] with a glide. Dutch has a similar process that extends to mid vowels:
Similarly, vowels can be inserted next to their corresponding glide in certain phonetic environments. Sievers' law describes this behaviour for Germanic.
Non-high semivowels also occur. In colloquial Nepali speech, a process of glide-formation occurs, wherein one of two adjacent vowels becomes non-syllabic; this process includes mid vowels so that [dʱo̯a] ('cause to wish') features a non-syllabic mid vowel. Spanish features a similar process and even nonsyllabic /a/ can occur so that "ahorita" ('right away') is pronounced [a̯o̞ˈɾita]. It is not often clear, however, whether such sequences involve a semivowel (a consonant) or a diphthong (a vowel), and in many cases that may not be a meaningful distinction.
Although many languages have central vowels [ɨ, ʉ], which lie between back/velar [ɯ, u] and front/palatal [i, y], there are few cases of a corresponding approximant [ ȷ̈]. One is in the Korean diphthong [ ȷ̈i] or [ɨ̯i], though this is more frequently analyzed as velar (as in the table above), and Mapudungun may be another: It has three high vowel sounds, /i/, /u/, /ɨ/ and three corresponding consonants, /j/, and /w/, and a third one is often described as a voiced unrounded velar fricative; some texts note a correspondence between this approximant and /ɨ/ that is parallel to /j/–/i/ and /w/–/u/. An example is "liq" /ˈliɣ/ ([ˈliɨ̯]?) ('white').
Approximants versus fricatives.
In addition to less turbulence, approximants also differ from fricatives in the precision required to produce them. 
When emphasized, approximants may be slightly fricated (that is, the airstream may become slightly turbulent), which is reminiscent of fricatives. For example, the Spanish word "ayuda" ('help') features a palatal approximant that is pronounced as a fricative in emphatic speech. However, such frication is generally slight and intermittent, unlike the strong turbulence of fricative consonants. 
Because voicelessness has comparatively reduced resistance to air flow from the lungs, the increased air flow creates more turbulence, making acoustic distinctions between voiceless approximants (which are extremely rare cross-linguistically) and voiceless fricatives difficult. This is why, for example, the voiceless labialized velar approximant [w̥] (also transcribed with the special letter ⟨ʍ⟩) has traditionally been labeled a fricative, and no language is known to contrast it with a voiceless labialized velar fricative [xʷ]. Similarly, Standard Tibetan has a voiceless lateral approximant, [l̥], and Welsh has a voiceless lateral fricative [ɬ], but the distinction is not always clear from descriptions of these languages. Again, no language is known to contrast the two. Iaai is reported to have an unusually large number of voiceless approximants, with /l̥ ɥ̊ w̥/.
For places of articulation further back in the mouth, languages do not contrast voiced fricatives and approximants. Therefore the IPA allows the symbols for the voiced fricatives to double for the approximants, with or without a lowering diacritic. 
Occasionally, the glottal "fricatives" are called approximants, since [h] typically has no more frication than voiceless approximants, but they are often phonations of the glottis without any accompanying manner or place of articulation.
Lateral approximants.
In lateral approximants, the center of tongue makes solid contact with the roof of the mouth. However, the defining location is the side of the tongue, which only approaches the teeth. 
Voiceless approximants.
Voiceless approximants are rarely distinguished from voiceless fricatives. Some of them are:
Nasal approximants.
Examples are:
In Portuguese, the nasal glides [j̃] and [w̃] historically became /ɲ/ and /m/ in some words. In Bini, the nasalized allophones of the approximants /j/ and /w/ are nasal occlusives, [ɲ] and [ŋʷ].
What are transcribed as nasal approximants may include non-syllabic elements of nasal vowels/diphthongs.

</doc>
<doc id="1940" url="http://en.wikipedia.org/wiki?curid=1940" title="Astronomer Royal">
Astronomer Royal

Astronomer Royal is a senior post in the Royal Households of the United Kingdom. There are two officers, the senior being the Astronomer Royal dating from 22 June 1675; the second is the Astronomer Royal for Scotland dating from 1834.
King Charles II, who founded the Royal Observatory Greenwich in 1675 instructed the first Astronomer Royal John Flamsteed "forthwith to apply himself withthe most exact care and diligence to the rectifying the tables of the motions of the heavens, and the places of the fixed stars, so as to find out the so-much desired longitude of places, for the perfecting the art of navigation."
From that time until 1972, the Astronomer Royal was Director of the Royal Observatory Greenwich. The Astronomer Royal receives a stipend of 100 GBP per year and is a member of the Royal Household, under the general authority of the Lord Chamberlain. After the separation of the two offices, the position of Astronomer Royal has been largely honorary, though he remains available to advise the Sovereign on astronomical and related scientific matters, and the office is of great prestige.
There was also formerly a Royal Astronomer of Ireland.

</doc>
<doc id="1941" url="http://en.wikipedia.org/wiki?curid=1941" title="Aeon">
Aeon

The word aeon , also spelled eon, originally meant "life" or "being", though it then tended to mean "age" in the sense of "time", "ages", "forever" or "for eternity". It is a Latin transliteration from the koine Greek word ὁ αἰών ("ho aion"), from the archaic αἰϝών ("aiwon"). In Homer it typically refers to life or lifespan. Its latest meaning is more or less similar to the Sanskrit word "kalpa" and Hebrew word "olam". A cognate Latin word "aevum" or "aeuum" (cf. αἰϝών) for "age" is present in words such as "longevity" and "mediaeval".
Although the term aeon may be used in reference to a period of a billion years (especially in geology, cosmology or astronomy), its more common usage is for any long, indefinite, period. Aeon can also refer to the four aeons on the Geologic Time Scale that make up the Earth's history, the Hadean, Archean, Proterozoic, and the current aeon Phanerozoic.
Astronomy and cosmology.
In astronomy an aeon is defined as a billion years (109 years, abbreviated AE).
Roger Penrose uses the word "aeon" to describe the period between successive and cyclic big bangs within the context of conformal cyclic cosmology.
Eternity or age.
The Bible translation is a treatment of the Hebrew word "olam" and the Greek word "aion". Both these words have similar meaning, and Young's Literal Translation renders them and their derivatives as “age” or “age-during”. Other English versions most often translate them to indicate eternity, being translated as eternal, everlasting, forever, etc. However, there are notable exceptions to this in all major translations, such as : “…I am with you always, to the end of the age” (NRSV), the word “age” being a translation of "aion". Rendering "aion" to indicate eternality in this verse would result in the contradictory phrase “end of eternity”, so the question arises whether it should ever be so. Proponents of Universal Reconciliation point out that this has significant implications for the problem of hell. Contrast in well-known English translations with its rendering in Young's Literal Translation:
And these shall go away to punishment age-during, but the righteous to life age-during. (YLT)
Then they will go away to eternal punishment, but the righteous to eternal life. (NIV)
These will go away into eternal punishment, but the righteous into eternal life. (NASB)
And these shall go away into everlasting punishment, but the righteous into eternal life. (KJV)
And these will depart into everlasting cutting-off, but the righteous ones into everlasting life. (NWT)
Philosophy and mysticism.
Plato used the word "aeon" to denote the eternal world of ideas, which he conceived was "behind" the perceived world, as demonstrated in his famous allegory of the cave.
Christianity's idea of "eternal life" comes from the word for life, "zoe", and a form of "aeon", which could mean life in the next aeon, the Kingdom of God, or Heaven, just as much as immortality, as in .
According to the Christian doctrine of Universal Reconciliation, the Greek New Testament scriptures use the word "eon" to mean a long period (perhaps 1000 years) and the word "eonian" to mean "during a long period"; Thus there was a time before the eons, and the eonian period is finite. After each man's mortal life ends, he is judged worthy of eonian life or eonian punishment. That is, after the period of the eons, all punishment will cease and death is overcome and then God becomes the all in each one (). This contrasts with the conventional Christian belief in eternal life and eternal punishment.
Occultists of the Thelema and O.T.O. traditions sometimes speak of a "magical Aeon" that may last for far less time, perhaps as little as 2,000 years.
Aeon may also be an archaic name for omnipotent beings, such as gods.
Gnosticism.
In many Gnostic systems, the various emanations of God, who is also known by such names as the One, the Monad, "Aion teleos" (αἰών τέλεος "The Broadest Aeon"), Bythos ("depth or profundity", Greek βυθός), "Proarkhe" ("before the beginning", Greek προαρχή), the "Arkhe" ("the beginning", Greek ἀρχή), "Sophia" (wisdom), Christos (the Anointed One) are called "Aeons". In the different systems these emanations are differently named, classified, and described, but the emanation theory itself is common to all forms of Gnosticism. 
In the Basilidian Gnosis they are called sonships (υἱότητες "huiotetes"; sing.: υἱότης "huiotes"); according to Marcus, they are numbers and sounds; in Valentinianism they form male/female pairs called "syzygies" (Greek συζυγίαι, from σύζυγοι "syzygoi").
Similarly, in the Greek Magical Papyri, the term "Aion" is often used to denote the All, or the supreme aspect of God. 
References.
 incorporates text from a publication now in the public domain: 

</doc>
<doc id="1942" url="http://en.wikipedia.org/wiki?curid=1942" title="Airline">
Airline

An airline is a company that provides air transport services for traveling passengers and freight. Airlines lease or own their aircraft with which to supply these services and may form partnerships or alliances with other airlines for mutual benefit. Generally, airline companies are recognized with an air operating certificate or license issued by a governmental aviation body.
Airlines vary from those with a single aircraft carrying mail or cargo, through full-service international airlines operating hundreds of aircraft. Airline services can be categorized as being intercontinental, intra-continental, domestic, regional, or international, and may be operated as scheduled services or charters.
History.
The first airlines.
DELAG, "Deutsche Luftschiffahrts-Aktiengesellschaft" was the world's first airline. It was founded on November 16, 1909 with government assistance, and operated airships manufactured by The Zeppelin Corporation. Its headquarters were in Frankfurt. The first fixed wing scheduled air service was started on January 1, 1914 from St. Petersburg, Florida to Tampa, Florida. The four oldest non-dirigible airlines that still exist are Netherlands' KLM, Colombia's Avianca, Australia's Qantas, and the Czech Republic's Czech Airlines. KLM first flew in May 1920, while Qantas (which stands for "Queensland and Northern Territory Aerial Services Limited") was founded in Queensland, Australia, in late 1920.
European airline industry.
Beginnings.
The earliest fixed wing airline was the Aircraft Transport and Travel, formed by George Holt Thomas in 1916. Using a fleet of former military Airco DH.4A biplanes that had been modified to carry two passengers in the fuselage, it operated relief flights between Folkestone and Ghent. On 15 July 1919, the company flew a proving flight across the English Channel, despite a lack of support from the British government. Flown by Lt. H Shaw in an Airco DH.9 between RAF Hendon and Paris - Le Bourget Airport, the flight took 2 hours and 30 minutes at £21 per passenger.
On 25 August 1919, the company used DH.16s to pioneer a regular service from Hounslow Heath Aerodrome to Le Bourget, the first regular international service in the world. The airline soon gained a reputation for reliability, despite problems with bad weather and began to attract European competition. In November 1919, it won the first British civil airmail contract. Six Royal Air Force Airco DH.9A aircraft were lent to the company, to operate the airmail service between Hawkinge and Cologne. In 1920, they were returned to the Royal Air Force.
Other British competitors were quick to follow - Handley Page Transport was established in 1919 and used the company's converted wartime Type O/400 bombers with a capacity for 19 passengers, to run a London-Paris passenger service.
The first French airlines were also established and began to offer competition for the same route. The Société Générale des Transports Aériens was created in late 1919, by the Farman brothers and the Farman F.60 Goliath plane flew scheduled services from Toussus-le-Noble to Kenley, near Croydon. Another early French airline was the Compagnie des Messageries Aériennes, established in 1919 by Louis-Charles Breguet, offering a mail and freight service between Le Bourget Airport, Paris and Lesquin Airport, Lille.
The Dutch airline KLM made its first flight in 1920, and is the oldest continuously operating airline in the world. Established by aviator Albert Plesman, it was immediately awarded a "Royal" predicate from Queen Wilhelmina Its first flight was from Croydon Airport, London to Amsterdam, using a leased Aircraft Transport and Travel DH-16, and carrying two British journalists and a number of newspapers. In 1921 KLM started scheduled services.
In Finland, the charter establishing Aero O/Y (now Finnair) was signed in the city of Helsinki on September 12, 1923. Junkers F.13 D-335 became the first aircraft of the company, when Aero took delivery of it on March 14, 1924. The first flight was between Helsinki and Tallinn, capital of Estonia, and it took place on March 20, 1924, one week later.
In the Soviet Union, the Chief Administration of the Civil Air Fleet was established in 1921. One of its first acts was to help found Deutsch-Russische Luftverkehrs A.G. (Deruluft), a German-Russian joint venture to provide air transport from Russia to the West. Domestic air service began around the same time, when Dobrolyot started operations on 15 July 1923 between Moscow and Nizhni Novgorod. Since 1932 all operations had been carried under the name Aeroflot.
Early European airlines tended to favour comfort - the passenger cabins were often spacious with luxury interiors - over speed and efficiency. The relatively basic navigational capabilities of pilots at the time also meant that delays due to the weather, especially during the winter in the south of England, were commonplace.
Rationalization.
By the early 1920s, small airlines were struggling to compete, and there was a movement towards increased rationalization and consolidation. In 1924, Imperial Airways was formed from the merger of Instone Air Line Company, British Marine Air Navigation, Daimler Airway and Handley Page Transport Co Ltd., to allow British airlines to compete with stiff competition from French and German airlines that were enjoying heavy government subsidies. The airline was a pioneer in surveying and opening up air routes across the world to serve far-flung parts of the British Empire and to enhance trade and integration.
The first new airliner ordered by Imperial Airways, was the Handley Page W8f "City of Washington", delivered on 3 November 1924. In the first year of operation the company carried 11,395 passengers and 212,380 letters. In April 1925, the film "The Lost World" became the first film to be screened for passengers on a scheduled airliner flight when it was shown on the London-Paris route.
Two French airlines also merged to form Air Union on 1 January 1923. This later merged with four other French airlines to become Air France, the country's flagship carrier to this day, on 7 October 1933.
Germany's Deutsche Luft Hansa was created in 1926 by merger of two airlines, one of them Junkers Luftverkehr. Luft Hansa, due to the Junkers heritage and unlike most other airlines at the time, became a major investor in airlines outside of Europe, providing capital to Varig and Avianca. German airliners built by Junkers, Dornier, and Fokker were among the most advanced in the world at the time.
Global expansion.
In 1926, Alan Cobham surveyed a flight route from the UK to Cape Town, South Africa, following this up with another proving flight to Melbourne, Australia. Other routes to British India and the Far East were also charted and demonstrated at this time. Regular services to Cairo and Basra began in 1927 and was extended to Karachi in 1929. The London-Australia service was inaugurated in 1932 with the Handley Page HP 42 airliners. Further services were opened up to Calcutta, Rangoon, Singapore, Brisbane and Hong Kong passengers departed London on 14 March 1936 following the establishment of a branch from Penang to Hong Kong.
Imperial's aircraft were small, most seating fewer than twenty passengers, and catered for the rich - only about 50,000 passengers used Imperial Airways in the 1930s. Most passengers on intercontinental routes or on services within and between British colonies were men doing colonial administration, business or research.
Like Imperial Airways, Air France and KLM's early growth depended heavily on the needs to service links with far-flung colonial possessions (North Africa and Indochina for the French and the East Indies for the Dutch). France began an air mail service to Morocco in 1919 that was bought out in 1927, renamed Aéropostale, and injected with capital to become a major international carrier. In 1933, Aéropostale went bankrupt, was nationalized and merged into Air France.
Although Germany lacked colonies, it also began expanding its services globally. In 1931, the airship Graf Zeppelin began offering regular scheduled passenger service between Germany and South America, usually every two weeks, which continued until 1937. In 1936, the airship Hindenburg entered passenger service and successfully crossed the Atlantic 36 times before crashing at Lakehurst, New Jersey on May 6, 1937.
By the end of the 1930s Aeroflot had become the world's largest airline, employing more than 4,000 pilots and 60,000 other service personnel and operating around 3,000 aircraft (of which 75% were considered obsolete by its own standards). During the Soviet era Aeroflot was synonymous with Russian civil aviation, as it was the only air carrier. It became the first airline in the world to operate sustained regular jet services on 15 September 1956 with the Tupolev Tu-104.
EU airline deregulation.
Deregulation of the European Union airspace in the early 1990s has had substantial effect on the structure of the industry there. The shift towards 'budget' airlines on shorter routes has been significant. Airlines such as EasyJet and Ryanair have often grown at the expense of the traditional national airlines.
There has also been a trend for these national airlines themselves to be privatized such as has occurred for Aer Lingus and British Airways. Other national airlines, including Italy's Alitalia, have suffered - particularly with the rapid increase of oil prices in early 2008.
U.S. airline industry.
Early development.
Tony Jannus conducted the United States' first scheduled commercial airline flight on 1 January 1914 for the St. Petersburg-Tampa Airboat Line. The 23-minute flight traveled between St. Petersburg, Florida and Tampa, Florida, passing some 50 ft above Tampa Bay in Jannus' Benoist XIV wood and muslin biplane flying boat. His passenger was a former mayor of St. Petersburg, who paid $400 for the privilege of sitting on a wooden bench in the open cockpit. The Airboat line operated for about four months, carrying more than 1,200 passengers who paid $5 each. Chalk's International Airlines began service between Miami and Bimini in the Bahamas in February 1919. Based in Ft. Lauderdale, Chalk's claimed to be the oldest continuously operating airline in the United States until its closure in 2008.
Following World War I, the United States found itself swamped with aviators. Many decided to take their war-surplus aircraft on barnstorming campaigns, performing aerobatic maneuvers to woo crowds. In 1918, the United States Postal Service won the financial backing of Congress to begin experimenting with air mail service, initially using Curtiss Jenny aircraft that had been procured by the United States Army Air Service. Private operators were the first to fly the mail but due to numerous accidents the US Army was tasked with mail delivery. During the Army's involvement they proved to be too unreliable and lost their air mail duties. By the mid-1920s, the Postal Service had developed its own air mail network, based on a transcontinental backbone between New York City and San Francisco. To supplant this service, they offered twelve contracts for spur routes to independent bidders. Some of the carriers that won these routes would, through time and mergers, evolve into Pan Am, Delta Air Lines, Braniff Airways, American Airlines, United Airlines (originally a division of Boeing), Trans World Airlines, Northwest Airlines, and Eastern Air Lines.
Service during the early 1920s was sporadic: most airlines at the time were focused on carrying bags of mail. In 1925, however, the Ford Motor Company bought out the Stout Aircraft Company and began construction of the all-metal Ford Trimotor, which became the first successful American airliner. With a 12-passenger capacity, the Trimotor made passenger service potentially profitable. Air service was seen as a supplement to rail service in the American transportation network.
At the same time, Juan Trippe began a crusade to create an air network that would link America to the world, and he achieved this goal through his airline, Pan American World Airways, with a fleet of flying boats that linked Los Angeles to Shanghai and Boston to London. Pan Am and Northwest Airways (which began flights to Canada in the 1920s) were the only U.S. airlines to go international before the 1940s.
With the introduction of the Boeing 247 and Douglas DC-3 in the 1930s, the U.S. airline industry was generally profitable, even during the Great Depression. This trend continued until the beginning of World War II.
Development since 1945.
As governments met to set the standards and scope for an emergent civil air industry toward the end of the war, the U.S. took a position of maximum operating freedom; U.S. airline companies were not as hard-hit as European and the few Asian ones had been. This preference for "open skies" operating regimes continues, with limitations, to this day.
World War II, like World War I, brought new life to the airline industry. Many airlines in the Allied countries were flush from lease contracts to the military, and foresaw a future explosive demand for civil air transport, for both passengers and cargo. They were eager to invest in the newly emerging flagships of air travel such as the Boeing Stratocruiser, Lockheed Constellation, and Douglas DC-6. Most of these new aircraft were based on American bombers such as the B-29, which had spearheaded research into new technologies such as pressurization. Most offered increased efficiency from both added speed and greater payload.
In the 1950s, the De Havilland Comet, Boeing 707, Douglas DC-8, and Sud Aviation Caravelle became the first flagships of the Jet Age in the West, while the Eastern bloc had Tupolev Tu-104 and Tupolev Tu-124 in the fleets of state-owned carriers such as Czechoslovak ČSA, Soviet Aeroflot and East-German Interflug. The Vickers Viscount and Lockheed L-188 Electra inaugurated turboprop transport.
The next big boost for the airlines would come in the 1970s, when the Boeing 747, McDonnell Douglas DC-10, and Lockheed L-1011 inaugurated widebody ("jumbo jet") service, which is still the standard in international travel. The Tupolev Tu-144 and its Western counterpart, Concorde, made supersonic travel a reality. Concorde first flew in 1969 and operated through 2003. In 1972, Airbus began producing Europe's most commercially successful line of airliners to date. The added efficiencies for these aircraft were often not in speed, but in passenger capacity, payload, and range. Airbus also features modern electronic cockpits that were common across their aircraft to enable pilots to fly multiple models with minimal cross-training.
US airline deregulation.
1970 U.S. airline industry deregulation lowered federally controlled barriers for new airlines just as a downturn in the nation's economy occurred. New start-ups entered during the downturn, during which time they found aircraft and funding, contracted hangar and maintenance services, trained new employees, and recruited laid off staff from other airlines.
Major airlines dominated their routes through aggressive pricing and additional capacity offerings, often swamping new start-ups. In the place of high barriers to entry imposed by regulation, the major airlines implemented an equally high barrier called loss leader pricing. In this strategy an already established and dominant airline stomps out its competition by lowering airfares on specific routes, below the cost of operating on it, choking out any chance a start-up airline may have. The industry side effect is an overall drop in revenue and service quality. Since deregulation in 1978 the average domestic ticket price has dropped by 40%. So has airline employee pay. By incurring massive losses, the airlines of the USA now rely upon a scourge of cyclical Chapter 11 bankruptcy proceedings to continue doing business. America West Airlines (which has since merged with US Airways) remained a significant survivor from this new entrant era, as dozens, even hundreds, have gone under.
In many ways, the biggest winner in the deregulated environment was the air passenger. Although not exclusively attributable to deregulation, indeed the U.S. witnessed an explosive growth in demand for air travel. Many millions who had never or rarely flown before became regular fliers, even joining frequent flyer loyalty programs and receiving free flights and other benefits from their flying. New services and higher frequencies meant that business fliers could fly to another city, do business, and return the same day, from almost any point in the country. Air travel's advantages put long distance intercity railroad travel and bus lines under pressure, with most of the latter having withered away, whilst the former is still protected under nationalization through the continuing existence of Amtrak.
By the 1980s, almost half of the total flying in the world took place in the U.S., and today the domestic industry operates over 10,000 daily departures nationwide.
Toward the end of the century, a new style of low cost airline emerged, offering a no-frills product at a lower price. Southwest Airlines, JetBlue, AirTran Airways, Skybus Airlines and other low-cost carriers began to represent a serious challenge to the so-called "legacy airlines", as did their low-cost counterparts in many other countries. Their commercial viability represented a serious competitive threat to the legacy carriers. However, of these, ATA and Skybus have since ceased operations.
Increasingly since 1978, US airlines have been reincorporated and spun off by newly created and internally led manangement companies, and thus becoming nothing more than operating units and subsidiaries with limited financially decisive control. Among some of these holding companies and parent companies which are relatively well known, are the UAL Corporation, along with the AMR Corporation, among a long list of airline holding companies sometime recognized worldwide. Less recognized are the private equity firms which often seize managerial, financial, and board of directors control of distressed airline companies by temporarily investing large sums of capital in air carriers, to rescheme an airlines assets into a profitable organization or liquidating an air carrier of their profitable and worthwhile routes and business operations.
Thus the last 50 years of the airline industry have varied from reasonably profitable, to devastatingly depressed. As the first major market to deregulate the industry in 1978, U.S. airlines have experienced more turbulence than almost any other country or region. In fact, no U.S. legacy carrier survived bankruptcy-free. Amongst the outspoken critics of deregulation, former CEO of American Airlines, Robert Crandall has publicly stated:
"Chapter 11 bankruptcy protection filing shows airline industry deregulation was a mistake."
The airline industry bailout.
Congress passed the (P.L. 107-42) in response to a severe liquidity crisis facing the already-troubled airline industry in the aftermath of the September 11th terrorist attacks. Congress sought to provide cash infusions to carriers for both the cost of the four-day federal shutdown of the airlines and the incremental losses incurred through December 31, 2001 as a result of the terrorist attacks. This resulted in the first government bailout of the 21st century. Between 2000 and 2005 US airlines lost $30 billion with wage cuts of over $15 billion and 100,000 employees laid off.
In recognition of the essential national economic role of a healthy aviation system, Congress authorized partial compensation of up to $5 billion in cash subject to review by the Department of Transportation and up to $10 billion in loan guarantees subject to review by a newly created Air Transportation Stabilization Board (ATSB). The applications to DOT for reimbursements were subjected to rigorous multi-year reviews not only by DOT program personnel but also by the Government Accountability Office and the DOT Inspector General.
Ultimately, the federal government provided $4.6 billion in one-time, subject-to-income-tax cash payments to 427 U.S. air carriers, with no provision for repayment, essentially a gift from the taxpayers. (Passenger carriers operating scheduled service received approximately $4 billion, subject to tax.) In addition, the ATSB approved loan guarantees to six airlines totaling approximately $1.6 billion. Data from the US Treasury Department show that the government recouped the $1.6 billion and a profit of $339 million from the fees, interest and purchase of discounted airline stock associated with loan guarantees.
Asian airline industry.
Although Philippine Airlines (PAL) was officially founded on February 26, 1941, its license to operate as an airliner was derived from merged Philippine Aerial Taxi Company (PATCO) established by mining magnate Emmanuel N. Bachrach on December 3, 1930, making it Asia's oldest scheduled carrier still in operation. Commercial air service commenced three weeks later from Manila to Baguio, making it Asia's first airline route. Bachrach's death in 1937 paved the way for its eventual merger with Philippine Airlines in March 1941 and made it Asia's oldest airline. It is also the oldest airline in Asia still operating under its current name. Bachrach's majority share in PATCO was bought by beer magnate Andres R. Soriano in 1939 upon the advice of General Douglas MacArthur and later merged with newly formed Philippine Airlines with PAL as the surviving entity. Soriano has controlling interest in both airlines before the merger. PAL restarted service on March 15, 1941 with a single Beech Model 18 NPC-54 aircraft, which started its daily services between Manila (from Nielson Field) and Baguio, later to expand with larger aircraft such as the DC-3 and Vickers Viscount.
India was also one of the first countries to embrace civil aviation. One of the first West Asian airline companies was Air India, which had its beginning as Tata Airlines in 1932, a division of Tata Sons Ltd. (now Tata Group). The airline was founded by India's leading industrialist, JRD Tata. On October 15, 1932, J. R. D. Tata himself flew a single engined De Havilland Puss Moth carrying air mail (postal mail of Imperial Airways) from Karachi to Bombay via Ahmedabad. The aircraft continued to Madras via Bellary piloted by Royal Air Force pilot Nevill Vintcent. Tata Airlines was also one of the world's first major airlines which began its operations without any support from the Government.
With the outbreak of World War II, the airline presence in Asia came to a relative halt, with many new flag carriers donating their aircraft for military aid and other uses. Following the end of the war in 1945, regular commercial service was restored in India and Tata Airlines became a public limited company on July 29, 1946 under the name Air India. After the independence of India, 49% of the airline was acquired by the Government of India. In return, the airline was granted status to operate international services from India as the designated flag carrier under the name Air India International.
On July 31, 1946, a chartered Philippine Airlines (PAL) DC-4 ferried 40 American servicemen to Oakland, California, from Nielson Airport in Makati City with stops in Guam, Wake Island, Johnston Atoll and Honolulu, Hawaii, making PAL the first Asian airline to cross the Pacific Ocean. A regular service between Manila and San Francisco was started in December. It was during this year that the airline was designated as the flag carrier of Philippines.
During the era of decolonization, newly born Asian countries started to embrace air transport. Among the first Asian carriers during the era were Cathay Pacific of Hong Kong (founded in September 1946 ), Orient Airways (later Pakistan International Airlines; founded in October 1946), Air Ceylon (later SriLankan Airlines; founded in 1947)Malayan Airways Limited in 1947 (later Singapore and Malaysia Airlines), El Al in Israel in 1948, Garuda Indonesia in 1948, Japan Airlines in 1951, Thai Airways International in 1960, and Korean National Airlines in 1947.
Latin American airline industry.
Among the first countries to have regular airlines in Latin America were Bolivia with Lloyd Aéreo Boliviano, Cuba with Cubana de Aviación, Colombia with Avianca, Argentina with Aerolineas Argentinas, Chile with LAN Chile (today LAN Airlines), Brazil with Varig, Dominican Republic with Dominicana de Aviación, Mexico with Mexicana de Aviación, Trinidad and Tobago with BWIA West Indies Airways (today Caribbean Airlines), Venezuela with Aeropostal, and TACA based in El Salvador and representing several airlines of Central America (Costa Rica, Guatemala, Honduras and Nicaragua). All the previous airlines started regular operations well before World War II.
The air travel market has evolved rapidly over recent years in Latin America. Some industry estimates indicate that over 2,000 new aircraft will begin service over the next five years in this region.
These airlines serve domestic flights within their countries, as well as connections within Latin America and also overseas flights to North America, Europe, Australia, and Asia.
Only three airlines: LAN, OceanAir and TAM Airlines have international subsidiaries and cover many destinations within the Americas as well as major hubs in other continents. LAN with Chile as the central operation along with Peru, Ecuador, Colombia and Argentina and some operations in the Dominican Republic. The recently formed AviancaTACA group has control of Avianca Brazil, VIP Ecuador and a strategic alliance with AeroGal. And TAM with its Mercosur base in Asuncion, Paraguay. As of 2010, talks of uniting LAN and TAM have strongly developed to create a joint airline named LATAM.
Regulatory considerations.
National.
Many countries have national airlines that the government owns and operates. Fully private airlines are subject to a great deal of government regulation for economic, political, and safety concerns. For instance, governments often intervene to halt airline labor actions to protect the free flow of people, communications, and goods between different regions without compromising safety.
The United States, Australia, and to a lesser extent Brazil, Mexico, India, the United Kingdom, and Japan have "deregulated" their airlines. In the past, these governments dictated airfares, route networks, and other operational requirements for each airline. Since deregulation, airlines have been largely free to negotiate their own operating arrangements with different airports, enter and exit routes easily, and to levy airfares and supply flights according to market demand.
The entry barriers for new airlines are lower in a deregulated market, and so the U.S. has seen hundreds of airlines start up (sometimes for only a brief operating period). This has produced far greater competition than before deregulation in most markets. The added competition, together with pricing freedom, means that new entrants often take market share with highly reduced rates that, to a limited degree, full service airlines must match. This is a major constraint on profitability for established carriers, which tend to have a higher cost base.
As a result, profitability in a deregulated market is uneven for most airlines. These forces have caused some major airlines to go out of business, in addition to most of the poorly established new entrants.
International.
Groups such as the International Civil Aviation Organization establish worldwide standards for safety and other vital concerns. Most international air traffic is regulated by bilateral agreements between countries, which designate specific carriers to operate on specific routes. The model of such an agreement was the Bermuda Agreement between the US and UK following World War II, which designated airports to be used for transatlantic flights and gave each government the authority to nominate carriers to operate routes.
Bilateral agreements are based on the "freedoms of the air", a group of generalized traffic rights ranging from the freedom to overfly a country to the freedom to provide domestic flights within a country (a very rarely granted right known as cabotage). Most agreements permit airlines to fly from their home country to designated airports in the other country: some also extend the freedom to provide continuing service to a third country, or to another destination in the other country while carrying passengers from overseas.
In the 1990s, "open skies" agreements became more common. These agreements take many of these regulatory powers from state governments and open up international routes to further competition. Open skies agreements have met some criticism, particularly within the European Union, whose airlines would be at a comparative disadvantage with the United States' because of cabotage restrictions.
Economic considerations.
Historically, air travel has survived largely through state support, whether in the form of equity or subsidies. The airline industry as a whole has made a cumulative loss during its 100-year history, once the costs include subsidies for aircraft development and airport construction.
One argument is that positive externalities, such as higher growth due to global mobility, outweigh the microeconomic losses and justify continuing government intervention. A historically high level of government intervention in the airline industry can be seen as part of a wider political consensus on strategic forms of transport, such as highways and railways, both of which receive public funding in most parts of the world.
Although many countries continue to operate state-owned or parastatal airlines, many large airlines today are privately owned and are therefore governed by microeconomic principles to maximize shareholder profit.
Top airline groups by revenue.
for 2010, source : Airline Business August 2011, Flightglobal Data Research
Ticket revenue.
Airlines assign prices to their services in an attempt to maximize profitability. The pricing of airline tickets has become increasingly complicated over the years and is now largely determined by computerized yield management systems.
Because of the complications in scheduling flights and maintaining profitability, airlines have many loopholes that can be used by the knowledgeable traveler. Many of these airfare secrets are becoming more and more known to the general public, so airlines are forced to make constant adjustments.
Most airlines use differentiated pricing, a form of price discrimination, to sell air services at varying prices simultaneously to different segments. Factors influencing the price include the days remaining until departure, the booked load factor, the forecast of total demand by price point, competitive pricing in force, and variations by day of week of departure and by time of day. Carriers often accomplish this by dividing each cabin of the aircraft (first, business and economy) into a number of travel classes for pricing purposes.
A complicating factor is that of origin-destination control ("O&D control"). Someone purchasing a ticket from Melbourne to Sydney (as an example) for A$200 is competing with someone else who wants to fly Melbourne to Los Angeles through Sydney on the same flight, and who is willing to pay A$1400. Should the airline prefer the $1400 passenger, or the $200 passenger plus a possible Sydney-Los Angeles passenger willing to pay $1300? Airlines have to make hundreds of thousands of similar pricing decisions daily.
The advent of advanced computerized reservations systems in the late 1970s, most notably Sabre, allowed airlines to easily perform cost-benefit analyses on different pricing structures, leading to almost perfect price discrimination in some cases (that is, filling each seat on an aircraft at the highest price that can be charged without driving the consumer elsewhere).
The intense nature of airfare pricing has led to the term "fare war" to describe efforts by airlines to undercut other airlines on competitive routes. Through computers, new airfares can be published quickly and efficiently to the airlines' sales channels. For this purpose the airlines use the Airline Tariff Publishing Company (ATPCO), who distribute latest fares for more than 500 airlines to Computer Reservation Systems across the world.
The extent of these pricing phenomena is strongest in "legacy" carriers. In contrast, low fare carriers usually offer preannounced and simplified price structure, and sometimes quote prices for each leg of a trip separately.
Computers also allow airlines to predict, with some accuracy, how many passengers will actually fly after making a reservation to fly. This allows airlines to overbook their flights enough to fill the aircraft while accounting for "no-shows," but not enough (in most cases) to force paying passengers off the aircraft for lack of seats, stimulative pricing for low demand flights coupled with overbooking on high demand flights can help reduce this figure. This is especially crucial during tough economic times as airlines undertake massive cuts to ticket prices to retain demand.
Operating costs.
Full-service airlines have a high level of fixed and operating costs to establish and maintain air services: labor, fuel, airplanes, engines, spares and parts, IT services and networks, airport equipment, airport handling services, sales distribution, catering, training, aviation insurance and other costs. Thus all but a small percentage of the income from ticket sales is paid out to a wide variety of external providers or internal cost centers.
Moreover, the industry is structured so that airlines often act as tax collectors. Airline fuel is untaxed because of a series of treaties existing between countries. Ticket prices include a number of fees, taxes and surcharges beyond the control of airlines. Airlines are also responsible for enforcing government regulations. If airlines carry passengers without proper documentation on an international flight, they are responsible for returning them back to the original country.
Analysis of the 1992–1996 period shows that every player in the air transport chain is far more profitable than the airlines, who collect and pass through fees and revenues to them from ticket sales. While airlines as a whole earned 6% return on capital employed (2-3.5% less than the cost of capital), airports earned 10%, catering companies 10-13%, handling companies 11-14%, aircraft lessors 15%, aircraft manufacturers 16%, and global distribution companies more than 30%. (Source: Spinetta, 2000, quoted in Doganis, 2002)
The widespread entrance of a new breed of low cost airlines beginning at the turn of the century has accelerated the demand that full service carriers control costs. Many of these low cost companies emulate Southwest Airlines in various respects, and like Southwest, they can eke out a consistent profit throughout all phases of the business cycle.
As a result, a shakeout of airlines is occurring in the U.S. and elsewhere. American Airlines, United Airlines, Continental Airlines (twice), US Airways (twice), Delta Air Lines, and Northwest Airlines have all declared Chapter 11 bankruptcy. Some argue that it would be far better for the industry as a whole if a wave of actual closures were to reduce the number of "undead" airlines competing with healthy airlines while being artificially protected from creditors via bankruptcy law. On the other hand, some have pointed out that the reduction in capacity would be short lived given that there would be large quantities of relatively new aircraft that bankruptcies would want to get rid of and would re-enter the market either as increased fleets for the survivors or the basis of cheap planes for new startups.
Where an airline has established an engineering base at an airport, then there may be considerable economic advantages in using that same airport as a preferred focus (or "hub") for its scheduled flights.
Assets and financing.
Airline financing is quite complex, since airlines are highly leveraged operations. Not only must they purchase (or lease) new airliner bodies and engines regularly, they must make major long-term fleet decisions with the goal of meeting the demands of their markets while producing a fleet that is relatively economical to operate and maintain. Compare Southwest Airlines and their reliance on a single airplane type (the Boeing 737 and derivatives), with the now defunct Eastern Air Lines which operated 17 different aircraft types, each with varying pilot, engine, maintenance, and support needs.
A second financial issue is that of hedging oil and fuel purchases, which are usually second only to labor in its relative cost to the company. However, with the current high fuel prices it has become the largest cost to an airline. Legacy airlines, compared with new entrants, have been hit harder by rising fuel prices partly due to the running of older, less fuel efficient aircraft. While hedging instruments can be expensive, they can easily pay for themselves many times over in periods of increasing fuel costs, such as in the 2000–2005 period.
In view of the congestion apparent at many international airports, the ownership of slots at certain airports (the right to take-off or land an aircraft at a particular time of day or night) has become a significant tradable asset for many airlines. Clearly take-off slots at popular times of the day can be critical in attracting the more profitable business traveler to a given airline's flight and in establishing a competitive advantage against a competing airline.
If a particular city has two or more airports, market forces will tend to attract the less profitable routes, or those on which competition is weakest, to the less congested airport, where slots are likely to be more available and therefore cheaper. For example, Reagan National Airport attracts profitable routes due partly to its congestion, leaving less-profitable routes to Baltimore-Washington International Airport and Dulles International Airport.
Other factors, such as surface transport facilities and onward connections, will also affect the relative appeal of different airports and some long distance flights may need to operate from the one with the longest runway. For example, LaGuardia Airport is the preferred airport for most of Manhattan due to its proximity, while long-distance routes must use John F. Kennedy International Airport's longer runways.
Airline partnerships.
Codesharing is the most common type of airline partnership; it involves one airline selling tickets for another airline's flights under its own airline code. An early example of this was Japan Airlines' (JAL) codesharing partnership with Aeroflot in the 1960s on Tokyo–Moscow flights; Aeroflot operated the flights using Aeroflot aircraft, but JAL sold tickets for the flights as if they were JAL flights. This practice allows airlines to expand their operations, at least on paper, into parts of the world where they cannot afford to establish bases or purchase aircraft. Another example was the Austrian– Sabena partnership on the Vienna–Brussels–New York/JFK route during the late '60s, using a Sabena Boeing 707 with Austrian livery.
Since airline reservation requests are often made by city-pair (such as "show me flights from Chicago to Düsseldorf"), an airline that can codeshare with another airline for a variety of routes might be able to be listed as indeed offering a Chicago–Düsseldorf flight. The passenger is advised however, that airline no. 1 operates the flight from say Chicago to Amsterdam, and airline no. 2 operates the continuing flight (on a different airplane, sometimes from another terminal) to Düsseldorf. Thus the primary rationale for code sharing is to expand one's service offerings in city-pair terms to increase sales.
A more recent development is the airline alliance, which became prevalent in late 1990s. These alliances can act as virtual mergers to get around government restrictions. Alliances of airlines such as Star Alliance, Oneworld, and SkyTeam coordinate their passenger service programs (such as lounges and frequent-flyer programs), offer special interline tickets, and often engage in extensive codesharing (sometimes systemwide). These are increasingly integrated business combinations—sometimes including cross-equity arrangements—in which products, service standards, schedules, and airport facilities are standardized and combined for higher efficiency. One of the first airlines to start an alliance with another airline was KLM, who partnered with Northwest Airlines. Both airlines later entered the SkyTeam alliance after the fusion of KLM and Air France in 2004.
Often the companies combine IT operations, or purchase fuel and aircraft as a bloc to achieve higher bargaining power. However, the alliances have been most successful at purchasing invisible supplies and services, such as fuel. Airlines usually prefer to purchase items visible to their passengers to differentiate themselves from local competitors. If an airline's main domestic competitor flies Boeing airliners, then the airline may prefer to use Airbus aircraft regardless of what the rest of the alliance chooses.
Fuel hedging.
Southwest is credited with maintaining strong business profits between 1999 and the early 2000s due to its fuel hedging policy. Looking at the annual reports, many other airlines are replicating Southwest's hedging policy to control their fuel costs.
Environmental impacts.
Aircraft engines emit noise pollution, gases and particulate emissions, and contribute to global dimming.
Growth of the industry in recent years raised a number of ecological questions.
Domestic air transport grew in China at 15.5 percent annually from 2001 to 2006. The rate of air travel globally increased at 3.7 percent per year over the same time. In the EU greenhouse gas emissions from aviation increased by 87% between 1990 and 2006. However it must be compared with the flights increase, only in UK, between 1990 and 2006 terminal passengers increased from 100 000 thousands to 250 000 thousands., according to AEA reports every year, 750 million passengers travel by European airlines, which also share 40% of merchandise value in and out of Europe. Without even pressure from "green activists", targeting lower ticket prices, generally, airlines do what is possible to cut the fuel consumption (and gas emissions connected therewith). Further, according to some reports, it can be concluded that the last piston-powered aircraft were as fuel-efficient as the average jet in 2005.
Despite continuing efficiency improvements from the major aircraft manufacturers, the expanding demand for global air travel has resulted in growing greenhouse gas (GHG) emissions. Currently, the aviation sector, including US domestic and global international travel, make approximately 1.6 percent of global anthropogenic GHG emissions per annum. North America accounts for nearly 40 percent of the world's GHG emissions from aviation fuel use.
CO2 emissions from the jet fuel burned per passenger on an average 3200 km airline flight is about 353 kilograms (776 pounds). Loss of natural habitat potential associated with the jet fuel burned per passenger on a 3200 km airline flight is estimated to be 250 square meters (2700 square feet).
In the context of climate change and peak oil, there is a debate about possible taxation of air travel and the inclusion of aviation in an emissions trading scheme, with a view to ensuring that the total external costs of aviation are taken into account.
The airline industry is responsible for about 11 percent of greenhouse gases emitted by the U.S. transportation sector. Boeing estimates that biofuels could reduce flight-related greenhouse-gas emissions by 60 to 80 percent. The solution would be blending algae fuels with existing jet fuel:
There are Electric aircraft projects, where some of them are fully operational planes as of 2013.
Call signs.
Each operator of a scheduled or charter flight uses an airline call sign when communicating with airports or air traffic control centres. Most of these call-signs are derived from the airline's trade name, but for reasons of history, marketing, or the need to reduce ambiguity in spoken English (so that pilots do not mistakenly make navigational decisions based on instructions issued to a different aircraft), some airlines and air forces use call-signs less obviously connected with their trading name. For example, British Airways uses a "Speedbird" call-sign, named after the logo of its predecessor, BOAC, while SkyEurope used "Relax".
Airline personnel.
The various types of airline personnel include:
Flight operations personnel including flight safety personnel.
Airlines follow a corporate structure where each broad area of operations (such as maintenance, flight operations(including flight safety),
and passenger service) is supervised by a vice president. Larger airlines often appoint vice presidents to oversee each of the
airline's hubs as well. Airlines employ lawyers to deal with regulatory procedures and other administrative tasks.
Industry trends.
The pattern of ownership has been privatized in the recent years, that is, the ownership has gradually changed from governments to private and individual sectors or organizations. This occurs as regulators permit greater freedom and non-government ownership, in steps that are usually decades apart. This pattern is not seen for all airlines in all regions. 
The overall trend of demand has been consistently increasing. In the 1950s and 1960s, annual growth rates of 15% or more were common. Annual growth of 5-6% persisted through the 1980s and 1990s. Growth rates are not consistent in all regions, but countries with a de-regulated airline industry have more competition and greater pricing freedom. This results in lower fares and sometimes dramatic spurts in traffic growth. The U.S., Australia, Canada, Japan, Brazil, India and other markets exhibit this trend. The industry has been observed to be cyclical in its financial performance. Four or five years of poor earnings precede five or six years of improvement. But profitability even in the good years is generally low, in the range of 2-3% net profit after interest and tax. In times of profit, airlines lease new generations of airplanes and upgrade services in response to higher demand. Since 1980, the industry has not earned back the cost of capital during the best of times. Conversely, in bad times losses can be dramatically worse. Warren Buffett once said that despite all the money that has been invested in all airlines, the net profit is less than zero. He believes it is one of the hardest businesses to manage.
As in many mature industries, consolidation is a trend. Airline groupings may consist of limited bilateral partnerships, long-term, multi-faceted alliances between carriers, equity arrangements, mergers, or takeovers. Since governments often restrict ownership and merger between companies in different countries, most consolidation takes place within a country. In the U.S., over 200 airlines have merged, been taken over, or gone out of business since deregulation in 1978. Many international airline managers are lobbying their governments to permit greater consolidation to achieve higher economy and efficiency.

</doc>
<doc id="1943" url="http://en.wikipedia.org/wiki?curid=1943" title="Australian Democrats">
Australian Democrats

The Australian Democrats is a centrist political party in Australia with a social-liberal ideology. The party was formed in 1977, a merger of the Australia Party and the New Liberal Movement, with former Liberal minister Don Chipp as its high-profile leader. Though never achieving a seat in the House of Representatives, the party had considerable influence in the Senate for the following thirty years. Its representation in the Parliament of Australia ended on 30 June 2008, after loss of its four remaining Senate seats at the 2007 general election. s of 2012[ [update]], the organisation had disintegrated and control was contested by two factions associated with two former parliamentarians. The party was deregistered by the Australian Electoral Commission on 16 April 2015 due to the party's failure to demonstrate requisite 500 members to maintain registration.
Even before its deregistration and since it became extinct as a parliamentary party anywhere in Australia, the party saw many of its prominent members including former federal party leader Andrew Bartlett and former NSW MLC Arthur Chesterfield-Evans defect to the Greens.
Overview.
The party was founded on principles of honesty, tolerance, compassion and direct democracy through postal ballots of all members, so that "there should be no hierarchical structure ... by which a carefully engineered elite could make decisions for the members.":p187 From the outset, members' participation was fiercely protected in national and divisional constitutions prescribing internal elections, regular meeting protocols, annual conferences—and monthly journals for open discussion and balloting. Dispute resolution procedures were established, with final recourse to a party ombudsman and membership ballot.
Policies determined by the unique participatory method promoted environmental awareness and sustainability, opposition to the primacy of economic rationalism (Australian neoliberalism), preventative approaches to human health and welfare, animal rights, rejection of nuclear technology and weapons.
The Australian Democrats were the first representatives of green politics at the federal level in Australia. They played a key role in the "cause célèbre" of the Franklin River Dam.
The party's centrist role made it subject to criticism from both the right and left of the political spectrum. In particular, Chipp's former conservative affiliation was frequently recalled by opponents on the left. This problem was to torment later leaders and strategists who, by 1991, were proclaiming "the electoral objective" as a higher priority than the rigorous participatory democracy espoused by the party's founders.
Over three decades, the Australian Democrats achieved representation in the legislatures of the ACT, South Australia, New South Wales, Western Australia and Tasmania as well as Senate seats in all six states. However, at the 2004 and 2007 federal elections, all seven of its Senate seats were lost. The last remaining State parliamentarian, David Winderlich, left the party and was defeated as an independent in 2010.
History.
1977–79.
On the evening of 29 April 1977, Don Chipp addressed an overflowing Perth Town Hall meeting which unanimously passed a resolution to form a Centre-Line Party, which Chipp was invited to lead:p185—but he firmly declined to reverse his avowed decision to quit politics, having resigned from the Liberal Party and been offered a lucrative position as a radio public affairs commentator. The Centre-Line Party was the provisional title of the Australian Democrats party.:p 185 The occasion was a meeting at the Perth Town Hall to which Don Chipp had been invited in the hope that he would accept the position of leader of the new party, which would be an amalgamation of the Australia Party and the New Liberal Movement. On that occasion, Chipp declined to commit himself but did so at a corresponding public meeting in Melbourne on 9 May 1977. Chipp received a standing ovation from over 3,000 people, including former Prime Minister John Gorton, and decided to commit himself to leading the new party which was already being constructed by a national steering committee.:p186 The new party was eventually renamed the Australian Democrats by a ballot of its membership. "Fifty-six suggestions produced by members were listed on the ballot paper, including Uniting Australia Party, Australian Centre Line Party, Dinkum Democrats, Practical Idealists of Australia and People for Sanity Party!! After the ballot, the suggestion of the Steering Committee, 'Australian Democrats', was overwhelmingly accepted.":p188 The name "Australian Democrats" was already in informal currency before this decision.
The first Australian Democrats (AD) federal parliamentarian was Senator Janine Haines who filled Steele Hall's casual Senate vacancy for South Australia in 1977. Surprisingly, she was not a candidate when the party contested the 1977 federal elections after Don Chipp had agreed to be leader and figurehead. Members and candidates were not lacking in electoral experience, since the Australia Party had been contesting all federal elections since 1969 and the Liberal Movement, in 1974 and 1975. The party's broad aim was to achieve a balance of power in one or more parliaments and to exercise it responsibly in line with policies determined by membership.
The grassroot support attracted by Chipp's leadership was measurable at the party's first electoral test at the 1977 federal election on 10 December, when 9.38 per cent of the total Lower House vote was polled and 11.13 per cent of the Senate vote. At that time, with five Senate seats being contested in each state, the required quota was a daunting 16.66 per cent. However, the first 6-year-term seats were won by Don Chipp (Vic) and Colin Mason (NSW).
1980–82.
The Australian Democrats' first national conference, on 16–17 February 1980, was opened by the distinguished nuclear physicist and former governor of South Australia, Sir Mark Oliphant, who said: "I was privileged to be in the chair at the public meeting in Melbourne when [Don Chipp] announced formation of a new party, dedicated to preserve what freedoms we still retain, and to increase them. A party in which dictatorship from the top was replaced by consensus. A party not ordered about by big business and the rich, or by union bosses. A party where a man could retain freedom of conscience and not thereby be faced with expulsion. A party to which the intelligent individual could belong without having to subscribe to a dogmatic creed. In other words, a democratic party." 
At a Melbourne media conference on 19 September 1980, in the midst of the 1980 election campaign, Chipp described his party's aim as to "keep the bastards honest"—the "bastards" being the major parties and/or politicians in general. This became a long-lived slogan for the Democrats.
At the October 1980 election, the Democrats polled 9.25 per cent of the Senate vote, electing Janine Haines (SA) and two new senators Michael Macklin (Qld) and John Siddons (Vic), bringing the party's strength to five Senate seats from 1 July 1981 .
A by-election in the South Australian state seat of Mitcham (now Waite) saw Heather Southcott retain the seat for the Democrats in 1982. Since 1955 it had been held by conservative lawyer Robin Millhouse whose New Liberal Movement merged into the Democrats in 1977, and who was resigning to take up a senior judicial appointment. Southcott was defeated later that year at the 1982 state election. Mitcham was the only single-member lower-house seat anywhere in Australia to be won by the Democrats.
1986–90.
Don Chipp resigned from the Senate on 18 August 1986, being succeeded as party leader by Janine Haines and replaced as a senator for Victoria by Janet Powell.
At the 1987 election following a double dissolution, the reduced quota of 7.7% necessary to win a seat assisted the election of three new senators. 6-year terms were won by Paul McLean (NSW) and incumbents Janine Haines (South Australia) and Janet Powell (Victoria). In South Australia, a second senator, John Coulter, was elected for a 3-year term, as were incumbent Michael Macklin (Queensland) and Jean Jenkins (Western Australia).
1990–91.
1990 saw the voluntary departure from the Senate of Janine Haines (a step with which not all Democrats agreed) and the failure of her strategic goal of winning the House of Representatives seat of Kingston.
The casual vacancy was filled by Meg Lees several months before the election of Cheryl Kernot in place of retired deputy leader Michael Macklin. The ambitious Kernot immediately contested the party's national parliamentary deputy leadership. Being unemployed at the time, she requested and obtained party funds to pay for her travel to address members in all seven divisions. In the event, Victorian Janet Powell was elected as leader and John Coulter was chosen as deputy leader.
Despite the loss of Haines and the WA Senate seat (through an inconsistent national preference agreement with the ALP), the 1990 federal election heralded something of a rebirth for the party, with a dramatic rise in primary vote. This was at the same time as an economic recession was building, and events such as the Gulf War in Kuwait were beginning to shepherd issues of globalisation and transnational trade on to national government agendas.
Virtually alone on the Australian political landscape, Janet Powell consistently attacked both the government and opposition which had closed ranks in support of the Gulf War. Whereas the House of Representatives was thus able to avoid any debate about the war and Australia's participation, the Democrats took full advantage of the opportunity to move for a debate in the Senate.
Possibly because of the party's opposition to the Gulf War, there was mass-media antipathy and negative publicity which some construed as poor media performance by Janet Powell, the party's standing having stalled at about 10%. Before 12 months of her leadership had passed, the South Australian and Queensland divisions were circulating the party's first-ever petition to criticise and oust the parliamentary leader. The explicit grounds related to Powell's alleged responsibility for poor AD ratings in Gallup and other media surveys of potential voting support. When this charge was deemed insufficient, interested party officers and senators reinforced it with negative media 'leaks' concerning her openly established relationship with Sid Spindler and exposure of administrative failings resulting in excessive overtime to a staff member. With National Executive blessing, the party room pre-empted the ballot by replacing the leader with deputy John Coulter. In the process, severe internal divisions were generated. One major collateral casualty was the party whip Paul McLean who resigned and quit the Senate in disgust at what he perceived as in-fighting between close friends. The casual NSW vacancy created by his resignation was filled by Karin Sowada. Powell duly left the party, along with many leading figures of the Victorian branch of the party, and unsuccessfully stood as an Independent candidate when her term expired. In later years, she campaigned for the Australian Greens.
Electoral fortunes.
The Australian Democrats' electoral fortunes have fluctuated throughout their history.
During the Hawke and Keating Labor Governments (1983–96), the Australian Democrats held a theoretical balance of power in the Senate: the numbers were such that they could team with Labor to pass legislation, or team with the Coalition to block legislation on occasions when the Coalition decided to oppose a government bill.
Their power was weakened in 1996 after the Howard Government was elected, and a Labor senator, Mal Colston, resigned from the Labor party. This meant that the Australian Democrats now shared the parliamentary balance of power with two Independent senators. As a result, the Coalition government could often bypass the Australian Democrats, and pass legislation by negotiating with Colston and Brian Harradine. Following the 1998 election the Australian Democrats again held the balance of power, until the Coalition gained a Senate majority at the 2004 election.
The Hawke and Keating governments pursued economic policies that drew on economic rationalist and neo-liberal thought, and the Australian Democrats positioned themselves to the left of the ALP government, and thus at the left end of mainstream Australian politics. Their appeal (and focus on issues beyond the usual "economic" ones that monopolised major party attention) was always greatest amongst tertiary-educated voters. However, the party's progressive politics also remained attractive to a sizeable section of mainly middle class ("wet") Liberal supporters – often female, and often disparagingly described on the right of the Liberal Party as "Soccer Mums" or "Doctor's Wives" – who were turned off by the Liberal party's social conservatism and "Reagonomic/Thatcherite" economic policies. Many Liberals saw their support of the Australian Democrats in the Senate as having "an each way bet", ameliorating the effect of their support for the Liberals in the House of Representatives – an attitude positively fostered, not unsurprisingly, by Democrat politicians and campaigners.
Kernot became leader in 1993. She had strong media appeal, which increased media and public awareness of herself and the party. She was known to have interests in industrial relations and was able to cultivate solid relationships with Labor government frontbenchers, which also added to her credibility in the press gallery.
Lack of clear direction other than, possibly, senators' common ambition to play a more productive role in government manifested itself in tensions over Kernot's policy on industrial relations (see the Workplace Relations Act 1996). Under Kernot, after negotiations and some compromises from the government, the Australian Democrats voted for the Howard Government's right-leaning industrial relations legislation which decreased union power and allowed a larger role for individual employer-employee contracts.
Kernot, however, remained broadly opposed to the Liberal government. This, together with her personal ambition for a role and contribution to strategy in government, led her to defect to the ALP in 1997. Her replacement as leader was by long-serving deputy, Meg Lees.
Under Lees' leadership, in the 1998 federal election, the Democrats' candidate John Schumann came within 2 per cent of taking Liberal Foreign Minister Alexander Downer's seat of Mayo in the Adelaide Hills under Australia's preferential voting system. The party's Senate representation increased to nine Senators.
Internal conflict and leadership tensions from 2000 to 2002, blamed on the party's support for the Government's Goods and Services Tax (GST), was damaging to the Democrats.
Opposed by the Labor Party, the Australian Greens and independent Senator Harradine, the GST required Democrat support to pass. In an election fought on tax, the Democrats publicly stated that they liked neither the Liberal (GST) tax package or the Labor package, but pledged to work with whichever party was elected to make their tax package better. They campaigned with the slogan "No GST on food".
In 1999, after negotiations with Prime Minister Howard, Meg Lees, Andrew Murray and the party room Senators agreed to support the A New Tax System (ANTS) legislation with exemptions from GST for most food and some medicines, as well as many environmental and social concessions. Five Australian Democrats senators voted in favour. However, two dissident senators on the party's left Natasha Stott Despoja and Andrew Bartlett voted against the GST.
In 2001, a leadership spill saw Meg Lees replaced as leader by Natasha Stott Despoja after a very public and bitter leadership battle. Despite criticism of Stott Despoja's youth and lack of experience, the 2001 election saw the Democrats receive similar media coverage to the previous election. Despite the internal divisions, the Australian Democrats' election result in 2001 was quite good. However, it was not enough to prevent the loss of Vicki Bourne's Senate seat in NSW.
Resulting tensions between Stott Despoja and Lees led to Meg Lees leaving the party in 2002, becoming an independent and forming the Australian Progressive Alliance. Stott Despoja stood down from the leadership following a loss of confidence by her party room colleagues. It led to a protracted leadership battle in 2002, which eventually led to the election of Senator Andrew Bartlett as leader. While the public fighting stopped, the public support for the party remained at record lows.
On 6 December 2003, Bartlett stepped aside temporarily as leader of the party, after an incident in which he swore at Liberal Senator Jeannie Ferris on the floor of Parliament while intoxicated. The party issued a statement stating that Deputy Leader Lyn Allison would serve as the Acting Leader of the party. Bartlett apologised to the Democrats, Jeannie Ferris and the Australian public for his behaviour and assured all concerned that it would never happen again. On 29 January 2004, after seeking medical treatment, Bartlett returned to the Australian Democrats leadership, vowing to abstain from alcohol.
2004.
Support for the Australian Democrats fell significantly at the 2004 federal election in which they achieved only 2.4 per cent of the national vote. Nowhere was this more noticeable than in their key support base of suburban Adelaide in South Australia, where they received between 7 and 31 per cent of the Lower House vote in 2001, and between 1 and 4 per cent in 2004. Three incumbent senators were defeated—Aden Ridgeway (NSW), Brian Greig (WA) and John Cherry (Qld). Following the loss, the customary post-election leadership ballot installed Lyn Allison as leader and Andrew Bartlett as her deputy.
From 1 July 2005 the Australian Democrats lost official parliamentary party status, being represented by only four senators while the governing Liberal-National Coalition gained a majority and potential control of the Senate—the first time this advantage had been enjoyed by any government since 1980.
2006.
On 5 January 2006, the ABC reported that the Tasmanian Electoral Commission had de-registered that division of the party for failing to provide a list containing the required number of members to be registered for Tasmanian state and local elections.
On 18 March 2006, at the 2006 South Australian state election, the Australian Democrats were reduced to 1.7 per cent of the Legislative Council (upper house) vote. Their sole councillor up for re-election, Kate Reynolds, was defeated.
After the election, South Australian senator Natasha Stott Despoja denied rumours that she was considering quitting the party.
In early July, Richard Pascoe, national and South Australian party president, resigned, citing slumping opinion polls and the poor result in the 2006 South Australian election as well as South Australian parliamentary leader Sandra Kanck's comments regarding the drug MDMA which he saw as damaging to the party.
On 5 July 2006, Australian Democrats senator for Western Australia Andrew Murray announced his intention not to contest the 2007 federal election, citing frustration arising from the Howard Government's control of both houses and his unwillingness to serve another six-year term. His term ended on 30 June 2008.
On 28 August 2006, the founder of the Australian Democrats, Don Chipp, died. Former prime minister Bob Hawke said: "... there is a coincidental timing almost between the passing of Don Chipp and what I think is the death throes of the Democrats. "
On 22 October 2006, Australian Democrats Senator Natasha Stott Despoja announced her intention not to seek re-election at the 2007 federal election due to health concerns. Her term ended on 30 June 2008.
In November 2006, the Australian Democrats fared very poorly in the Victorian state election, receiving a Legislative Council vote tally of only 0.83%, less than half of the party's result in 2002 (1.79 per cent).
2007.
In the New South Wales state election of March 2007, the Australian Democrats lost their last remaining NSW Upper House representative, Arthur Chesterfield-Evans. The party fared poorly, gaining only 1.8 per cent of the Legislative Council vote. A higher vote was achieved in some of the Legislative Assembly seats selectively contested as compared to 2003. However, the statewide vote share fell because the party was unable to field as many candidates as in 2003.
In the Victorian state by-election in Albert Park District the Australian Democrats stood candidate Paul Kavanagh, who polled 5.75 per cent of the primary vote, despite a large number of candidates, and all media attention focusing on the battle between Labor and Greens candidates.
On 13 September 2007, the ACT Democrats (Australian Capital Territory Division of the party) was deregistered by the ACT Electoral Commissioner, being unable to demonstrate a minimum membership of 100 electors.
The Democrats had no success at the 2007 federal election. Two incumbent senators, Lyn Allison (Victoria) and Andrew Bartlett (Queensland), were defeated, their seats both reverting to major parties. Their two remaining colleagues, Andrew Murray (WA) and Natasha Stott Despoja (SA), did not run for new terms. All four senators' terms expired on 30 June 2008—leaving the Australian Democrats with no federal representation for the first time since its founding in 1977. An ABC report noted that "on the Australian Electoral Commission (AEC) website the party is now referred to just as 'other'".
Post-2007.
The last of the party's state upper-house members, David Winderlich, resigned from the party in October 2009 and was defeated as an independent at the 2010 election.
In March 2012, the Australian Electoral Commission queried a Democrats submission of 550 names of purported members and proposed deregistering the party for having fewer than 500 members, the threshold needed for registration. The Commission later satisfied itself that the party had sufficient membership to continue its registration.
The Democrats did not nominate a single candidate in the 2014 South Australian election, in the party's state of origin.
On 16 April 2015, the Australian Electoral Commission deregistered the Australian Democrats as a political party for failure to demonstrate the requisite 500 members to maintain registration.
The Australian Democrats have said they will appeal the AEC decision, which under the legislation is reviewable.
Policy.
The party's original support base consisted of voters alienated by perceived unproductive adversarial conflict between the two mainstream parties and an emerging new constituency of people with a desire to participate more effectively in government and to promote concerns for environmental protection and social justice. The party aimed to combine liberal social policies with centrist, particularly neo-Keynesian economics and a progressive environmental platform.
The original agenda included interventionist economic policies, commitment to environmental causes, support for reconciliation with Australia's indigenous population through such mechanisms as formal treaties, pacifist approaches to international relations, open government, constitutional reform, progressive approaches to social issues such as sexuality and drugs, and strong support for human rights and civil liberties. Its membership largely comprised tertiary-educated and middle-class constituents. The party also appealed to voters opposed to untrammeled government power and wishing to have alternative views aired in parliaments and media.
The party has a platform of participatory democracy, with policies supporting proportional representation and citizen-initiated referenda. Many important internal issues (such as electoral preselection and leadership) are decided by direct postal ballot of the membership. Although policies are theoretically set in a similar fashion, Australian Democrats parliamentarians generally had extensive freedom in interpreting them.
However, by 1980, the Australian Democrats had employed the postal-ballot method at both national at state levels to develop an extensive body of written policy covering not only the political agendas of the day but also innovative and far-sighted policies for environmental and economic sustainability, water and energy conservation, e.g., through development of alternative energy sources, expanded public transport, etc. To the community's growing concerns about human rights, the Australian Democrats added finely detailed policies on animal welfare and species preservation. The material is available in election manifestos and copies of the party's journals, obtainable in major public libraries.
In a 2009 "rebuild" process, the party announced creation of a new policy process, attempts to improve internal communication, and envisaged development of a new party constitution.
Prior to the 2013 federal election, the party, though factionally divided into two separate organisations, was able to publish a comprehensive package of member-balloted policies.
Support.
Support for the Democrats historically tended to fluctuate between about 5 and 10 per cent of the population and was geographically concentrated around the wealthy dense CBD and inner-suburban neighbourhoods of the capital cities (especially Adelaide). Therefore, they never managed to win a House of Representatives seat. During the 1980s, 1990s and early 2000s they typically held one or two Senate seats in each state, as well as having some representatives in state parliaments.
Following the internal conflict over GST (1998–2001) and resultant leadership changes, a dramatic decline occurred in the Democrats' membership and voting support in all states. Simultaneously, an increase was recorded in support for the Australian Greens who, by 2004, were supplanting the Democrats as a substantial third party. The trend was noted that year by political scientists Dean Jaensch et al. Elsewhere, Jaensch later suggested it was possible the Democrats could make a political comeback in the federal arena.
Following Tony Abbott's displacement of Malcolm Turnbull as federal leader of the Liberal Party in 2009, the Democrats sought to attract the support of "those Liberals who no longer feel they can support their party".
Federal parliamentary leaders.
Of the party's nine elected federal parliamentary leaders, six were women. Aboriginal senator Aden Ridgeway was deputy leader under Natasha Stott Despoja.
Rideway was technically leader between Stott Despoja's resignation and the appointment of Brian Greig as interim leader.

</doc>
<doc id="1944" url="http://en.wikipedia.org/wiki?curid=1944" title="Australian Capital Territory">
Australian Capital Territory

Australian Capital Territory (ACT) (formerly, "The Territory for the Seat of Government" and, later, the "Federal Capital Territory") is a territory in the south east of Australia, enclaved within New South Wales. It is the smaller of the two self-governing internal territories in Australia. The only city and by far the most populous community is Canberra, the capital city of Australia.
The need for a national territory was flagged by colonial delegates during the Federation conventions of the late 19th century. Section 125 of the Australian Constitution provided that, following Federation in 1901, land would be ceded freely to the new Federal Government. The territory was transferred to the Commonwealth by the state of New South Wales in 1911, two years prior to the naming of Canberra as the national capital in 1913. The floral emblem of the ACT is the Royal Bluebell and the bird emblem is the Gang-gang Cockatoo.
Geography.
The ACT is bounded by the Goulburn-Cooma railway line in the east, the watershed of Naas Creek in the south, the watershed of the Cotter River in the west, and the watershed of the Molonglo River in the north-east. The ACT also has a small strip of territory around the southern end of the Beecroft Peninsula, which is the northern headland of Jervis Bay.
Apart from the city of Canberra, the Australian Capital Territory also contains agricultural land (sheep, dairy cattle, vineyards and small amounts of crops) and a large area of national park (Namadgi National Park), much of it mountainous and forested. Small townships and communities located within the ACT include Williamsdale, Naas, Uriarra, Tharwa and Hall.
Tidbinbilla is a locality to the south-west of Canberra that features the Tidbinbilla Nature Reserve and the Canberra Deep Space Communication Complex, operated by the United States' National Aeronautics and Space Administration (NASA) as part of its Deep Space Network.
There are a large range of mountains, rivers and creeks in the Namadgi National Park. These include the Naas and Murrumbidgee Rivers.
Climate.
Because of its elevation 650 m and distance from the coast, the Australian Capital Territory experiences four distinct seasons, unlike many other Australian cities whose climates are moderated by the sea. Canberra is noted for its warm to hot, dry summers, and cold winters with occasional fog and frequent frosts. Many of the higher mountains in the territory's south-west are snow-covered for at least part of the winter. Thunderstorms can occur between October and March, and annual rainfall is 623 mm, with rainfall highest in spring and summer and lowest in winter.
The highest maximum temperature recorded in the ACT was 42.8 °C at Acton on 11 January 1939. The lowest minimum temperature was −14.6 °C at Gudgenby on 11 July 1971.
Geology.
Notable geological formations in the Australian Capital Territory include the "Canberra Formation", the "Pittman Formation", "Black Mountain Sandstone" and "State Circle Shale".
In the 1840s fossils of brachiopods and trilobites from the Silurian period were discovered at Woolshed Creek near Duntroon. At the time, these were the oldest fossils discovered in Australia, though this record has now been far surpassed. Other specific geological places of interest include the State Circle cutting and the Deakin anticline.
The oldest rocks in the ACT date from the Ordovician around 480 million years ago. During this period the region along with most of Eastern Australia was part of the ocean floor; formations from this period include the "Black Mountain Sandstone" formation and the "Pittman Formation" consisting largely of quartz-rich sandstone, siltstone and shale. These formations became exposed when the ocean floor was raised by a major volcanic activity in the Devonian forming much of the east coast of Australia.
Governance.
The ACT has internal self-government, but Australia's Constitution does not afford the territory government the full legislative independence provided to Australian states. Laws are made in a 17-member Legislative Assembly that combines both state and local government functions.
Members of the Legislative Assembly are elected via the Hare Clarke system. The ACT Chief Minister (currently Andrew Barr, Australian Labor Party) is elected by members of the ACT Assembly. The ACT Government Chief Minister is a member of the Council of Australian Governments.
Unlike other self-governing Australian territories (for example, the Northern Territory), the ACT does not have an Administrator. The Crown is represented by the Australian Governor-General in the government of the ACT. Until 4 December 2011, the decisions of the assembly could be overruled by the Governor-General (effectively by the national government) under section 35 of the Australian Capital Territory (Self-Government) Act 1988, although the federal parliament voted in 2011 to abolish this veto power, instead requiring a majority of both houses of the federal parliament to override an enactment of the ACT. The Chief Minister performs many of the roles that a state governor normally holds in the context of a state; however, the Speaker of the Legislative Assembly gazettes the laws and summons meetings of the Assembly.
In Australia's Federal Parliament, the ACT is represented by four federal members: two members of the House of Representatives; the Division of Fraser and the Division of Canberra and is one of only two territories to be represented in the Senate, with two Senators (the other being the Northern Territory). The Member for Fraser and the ACT Senators also represent the constituents of the Jervis Bay Territory.
In 1915 the "Jervis Bay Territory Acceptance Act 1915" created the Jervis Bay Territory as an annexe to the Australian Capital Territory. In 1988, when the ACT gained self-government, Jervis Bay became a separate territory administered by the Australian Government Minister responsible for Territories, presently the Minister for Home Affairs.
The ACT retains a small area of territory on the coast on the Beecroft Peninsula, consisting of a strip of coastline around the northern headland of Jervis Bay (not to be confused with the Jervis Bay Territory, which is on the southern headland of the Bay). The ACT's land on the Beecroft Peninsula is an "exclave", that is, an area of territory not physically connected to the main part of the ACT. Interestingly, this ACT exclave surrounds a small exclave of NSW territory, namely the Point Perpendicular lighthouse which is at the southern tip of the Beecroft Peninsula. The lighthouse and its grounds are New South Wales territory, but cut off from the rest of the state by the strip of ACT land. This is a geographic curiosity: an exclave of NSW land enclosed by an exclave of ACT land.
Administration.
ACT Ministers implement their executive powers through the following government directorates:
Demographics.
In the 2011 census the population of the ACT was 357,222 of whom most lived in Canberra. The ACT median weekly income for people aged over 15 was in the range $600–$699 while that for the population living outside Canberra was at the national average of $400–$499. The average level of degree qualification in the ACT is higher than the national average. Within the ACT 4.5% of the population have a postgraduate degree compared to 1.8% across the whole of Australia.
Urban structure.
Canberra is a planned city that was originally designed by Walter Burley Griffin, a major 20th century American architect. Major roads follow a wheel-and-spoke pattern rather than a grid. The city centre is laid out on two perpendicular axes: a water axis stretching along Lake Burley Griffin, and a ceremonial land axis stretching from Parliament House on Capital Hill north-eastward along ANZAC Parade to the Australian War Memorial at the foot of Mount Ainslie.
The area known as the Parliamentary Triangle is formed by three of Burley Griffin's axes, stretching from Capital Hill along Commonwealth Avenue to the Civic Centre around City Hill, along Constitution Avenue to the Defence precinct on Russell Hill, and along Kings Avenue back to Capital Hill.
The larger scheme of Canberra's layout is based on the three peaks surrounding the city, Mount Ainslie, Black Mountain, and Red Hill. The main symmetrical axis of the city is along ANZAC Parade and roughly on the line between Mount Ainslie and Bimberi Peak. Bimberi Peak being the highest mountain in the ACT approximately 52 km south west of Canberra . The precise alignment of ANZAC parade is between Mount Ainslie and Capital Hill (formally Kurrajong Hill).
The Griffins assigned spiritual values to Mount Ainslie, Black Mountain, and Red Hill and originally planned to cover each of these in flowers. That way each hill would be covered with a single, primary color which represented its spiritual value. This part of their plan never came to fruition. In fact, WWI interrupted the construction and some conflicts after the war made it a difficult process for the Griffins. Nevertheless, Canberra stands as an exemplary city design and is located halfway between the ski slopes and the beach. It enjoys a natural cooling from geophysical factors.
The urban areas of Canberra are organised into a hierarchy of districts, town centres, group centres, local suburbs as well as other industrial areas and villages. There are seven districts (with an eighth currently under construction), each of which is divided into smaller suburbs, and most of which have a town centre which is the focus of commercial and social activities. The districts were settled in the following chronological order:
The North and South Canberra districts are substantially based on Walter Burley Griffin's designs. In 1967 the then National Capital Development Commission adopted the "Y Plan" which laid out future urban development in Canberra around a series of central shopping and commercial area known as the 'town centres' linked by freeways, the layout of which roughly resembled the shape of the letter Y, with Tuggeranong at the base of the Y and Belconnen and Gungahlin located at the ends of the arms of the Y.
Development in Canberra has been closely regulated by government, both through the town planning process, but also through the use of crown lease terms that have tightly limited the use of parcels of land. All land in the ACT is held on 99 year leases from the national government, although most leases are now administered by the Territory government.
Most suburbs have their own local shops, and are located close to a larger shopping centre serving a group of suburbs. Community facilities and schools are often also located near local shops or group shopping centres. Many of Canberra's suburbs are named after former Prime Ministers, famous Australians, early settlers, or use Aboriginal words for their title.
Street names typically follow a particular theme; for example, the streets of Duffy are named after Australian dams and reservoirs, the streets of Dunlop are named after Australian inventions, inventors and artists and the streets of Page are named after biologists and naturalists. Most diplomatic missions are located in the suburbs of Yarralumla, Deakin and O'Malley. There are three light industrial areas: the suburbs of Fyshwick, Mitchell and Hume.
Education.
Almost all educational institutions in the Australian Capital Territory are located within Canberra. The ACT public education system schooling is normally split up into Pre-School, Primary School (K-6), High School (7–10) and College (11–12) followed by studies at university or CIT (Canberra Institute of Technology). Many private high schools include years 11 and 12 and are referred to as colleges. Children are required to attend school until they turn 17 under the ACT Government's "Learn or Earn" policy.
In February 2004 there were 140 public and non-governmental schools in Canberra; 96 were operated by the Government and 44 are non-Government. In 2005 there were 60,275 students in the ACT school system. 59.3% of the students were enrolled in government schools with the remaining 40.7% in non-government schools. There were 30,995 students in primary school, 19,211 in high school, 9,429 in college and a further 340 in special schools.
As of May 2004, 30% of people in the ACT aged 15–64 had a level of educational attainment equal to at least a bachelor's degree, significantly higher than the national average of 19%. The two main tertiary institutions are the Australian National University (ANU) in Acton and the University of Canberra (UC) in Bruce. There are also two religious university campuses in Canberra: Signadou is a campus of the Australian Catholic University and St Mark's Theological College is a campus of Charles Sturt University. Tertiary level vocational education is also available through the multi-campus Canberra Institute of Technology.
The Australian Defence Force Academy (ADFA) and the Royal Military College, Duntroon (RMC) are in the suburb of Campbell in Canberra's inner northeast. ADFA teaches military undergraduates and postgraduates and is officially a campus of the University of New South Wales while Duntroon provides Australian Army Officer training.
The Academy of Interactive Entertainment (AIE) offers courses in computer game development and 3D animation.
References.
</dl>

</doc>
<doc id="1946" url="http://en.wikipedia.org/wiki?curid=1946" title="Unit of alcohol">
Unit of alcohol

Units of alcohol are used in the United Kingdom (UK) as a measure to quantify the actual alcoholic content within a given volume of an alcoholic beverage, in order to provide guidance on total alcohol consumption.
A number of other countries (including Australia, Canada, New Zealand, and the USA) use the concept of a "standard drink", the definition of which varies from country to country, for the same purpose. "Standard drinks" were referred to in the first UK guidelines (1984) that published "safe limits" for drinking, but these were replaced by references to "alcohol units" in the 1987 guidelines and the latter term has been used in all subsequent UK guidance.
One unit of alcohol (UK) is defined as 10 millilitres (8 grams) of pure alcohol. Typical drinks (i.e. typical quantities or servings of common alcoholic beverages) may contain 1–3 units of alcohol.
Containers of alcoholic beverages sold directly to UK consumers are normally labelled to indicate the number of units of alcohol in a typical serving of the beverage (optional) and in the full container (can or bottle), as well as information about responsible drinking. Additionally, the advent of smartphones has led to the creation of apps which report the number of units contained in an alcoholic drink.
As an approximate guideline, a typical healthy adult can metabolise (break down) about one unit of alcohol per hour, although this may vary depending on gender, age, weight, health, and many other factors.
Formula.
The number of UK units of alcohol in a drink can be determined by multiplying the volume of the drink (in millilitres) by its percentage ABV, and dividing by 1000.
For example, one imperial pint (568 ml) of beer at 4% alcohol by volume (ABV) contains:
formula_1
The formula uses ml ÷ 1000. This results in exactly one unit per percentage point per litre, of any alcoholic beverage.
You can cancel the /1000 by using the serving size in liters. Thus, a 750 ml bottle of wine at 12% ABV contains 0.75 * 12 = 9 units.
Labelling.
UK alcohol companies pledged in March 2011 to implement an innovative health labelling scheme to provide more information about responsible drinking on alcohol labels and containers. This voluntary scheme is the first of its kind in Europe and has been developed in conjunction with the UK Department of Health. The pledge stated:
At the end of 2014, 101 companies had committed to the pledge labelling scheme.
There are five elements included within the overall labelling scheme, the first three being mandatory, and the last two optional:
Drinks companies had pledged to display the three mandatory items on 80% of drinks containers on shelves in the UK off-trade by the end of December 2013. A report published in Nov 2014, confirmed that UK drinks producers had delivered on that pledge with a 79.3% compliance with the pledge elements as measured by products on shelf. Compared with labels from 2008 on a like-for-like basis, information on Unit alcohol content had increased by 46%; 91% of products displayed alcohol and pregnancy warnings (18% in 2008); and 75% showed the Chief Medical Officers’ lower risk daily guidelines (6% in 2008).
Quantities.
Spirits.
Most spirits sold in the United Kingdom have 40% ABV or slightly less. In England a single pub measure (25 ml) of a spirit contains one unit. However, a larger 35ml measure is increasingly used (and in particular is standard in Northern Ireland ), which contains 1.4 units of alcohol at 40% ABV. Sellers of spirits by the glass must state the capacity of their standard measure in ml.
Time to metabolise.
On average, it takes about one hour for the body to metabolise (break down) one unit of alcohol. However, this can vary with body weight, sex, age, personal metabolic rate, recent food intake, the type and strength of the alcohol, and
medications taken. Alcohol may be metabolised more slowly if liver function is impaired.
Recommended maximum.
From 1992 to 1995 the UK government advised that men should drink no more than 21 units per week, and women no more than 14. (The difference between the sexes was due to the typically lower weight and water-to-body-mass ratio of women.) The Times reported in October 2007 that these limits had been "plucked out of the air" and had no scientific basis.
This was changed after a government study showed that many people were in effect "saving up" their units and using them at the end of the week, a phenomenon referred to as binge drinking. Since 1995 the advice was that regular consumption of 3–4 units a day for men, or 2–3 units a day for women, would not pose significant health risks, but that consistently drinking four or more units a day (men), or three or more units a day (women), is not advisable.
An international study of about 6,000 men and 11,000 women for a total of 75,000 person-years found that people who reported that they drank more than a threshold value of 2 units of alcohol a day had a higher risk of fractures than non-drinkers. For example, those who drank over 3 units a day had nearly twice the risk of a hip fracture.

</doc>
<doc id="1947" url="http://en.wikipedia.org/wiki?curid=1947" title="Aotus">
Aotus

Aotus (the name is derived from the Ancient Greek words for "earless" in both cases: the monkey is missing external ears, and the pea is missing earlike bracteoles) may refer to:

</doc>
<doc id="1948" url="http://en.wikipedia.org/wiki?curid=1948" title="Ally McBeal">
Ally McBeal

Ally McBeal is an American legal comedy-drama television series, originally aired on Fox from September 8, 1997 to May 20, 2002. Created by David E. Kelley, the series stars Calista Flockhart in the title role as a young lawyer working in the fictional Boston law firm Cage and Fish, with other young lawyers whose lives and loves were eccentric, humorous and dramatic. The series placed #48 on "Entertainment Weekly"‍‍ '​‍s 2007 "New TV Classics" list.
Overview.
The series, set in the fictional Boston law firm Cage and Fish, begins with main character Allison Marie "Ally" McBeal joining the firm (co-owned by her law school classmate Richard Fish) after leaving her previous job due to sexual harassment. On her first day Ally is horrified to find that she will be working alongside her ex-boyfriend Billy Thomas—whom she has never gotten over—and to make things worse, Billy is now married to fellow lawyer Georgia, who also later joins Cage and Fish. The triangle among the three forms the basis for the main plot for the show's first three seasons.
Although ostensibly a legal drama, the main focus of the series was the romantic and personal lives of the main characters, often using legal proceedings as plot devices to contrast or reinforce a character's drama. For example, bitter divorce litigation of a client might provide a backdrop for Ally's decision to break up with a boyfriend. Legal arguments were also frequently used to explore multiple sides of various social issues.
Cage & Fish (which becomes Cage/Fish & McBeal or Cage, Fish, & Associates towards the end of the series), the fictional law firm where most of the characters work, is depicted as a highly sexualized environment, symbolized by its unisex restroom. Lawyers and secretaries in the firm routinely date, flirt with, or have a romantic history with each other, and frequently run into former or potential romantic interests in the courtroom or on the street outside.
The series had many offbeat and frequently surreal running gags and themes, such as Ally's tendency to immediately fall over whenever she met somebody she found attractive, or Richard Fish's wattle fetish and humorous mottos ("Fishisms" & "Bygones"), or John's gymnastic dismounts out of the office's unisex bathroom stalls, that ran through the series. The show used vivid, dramatic fantasy sequences for Ally's and other characters' wishful thinking; particularly notable is the dancing baby.
The series also featured regular visits to a local bar where singer Vonda Shepard regularly performed (though occasionally handing over the microphone to the characters). The series also took place in the same continuity as David E. Kelley's legal drama "The Practice" (which aired on ABC), as the two shows crossed over with one another on occasion, a very rare occurrence for two shows that aired on different networks.
Episodes.
In Australia, Ally McBeal was aired from the Seven Network from 1997 to 2002. In 2010, Ally McBeal was aired repeatedly with Network Ten.
Crossovers with "The Practice".
Seymore Walsh, a stern judge often exasperated by the eccentricities of the Cage & Fish lawyers and played by actor Albert Hall, was also a recurring character on "The Practice". In addition, Judge Jennifer 'Whipper' Cone appears on "The Practice" episode "Line of Duty" (S02E15), while Judge Roberta Kittelson, a recurring character on "The Practice", has a featured guest role in the "Ally McBeal" episode "Do you Wanna Dance?"
Most of the primary "Practice" cast members guest starred in the "Ally McBeal" episode "The Inmates" (S01E20), in a storyline that concluded with the "Practice" episode "Axe Murderer" (S02E26), featuring Calista Flockhart and Gil Bellows reprising their "Ally" characters; what's unique about this continuing storyline is that "Ally McBeal" and "The Practice" happened to air on different networks. Bobby Donnell, the main character of "The Practice" played by Dylan McDermott, was featured heavily in both this crossover and another "Ally McBeal" episode, "These are the Days."
Regular "Practice" cast members Lara Flynn Boyle and Michael Badalucco each had a cameo in "Ally McBeal" (Boyle as a woman who trades insults with Ally in the episode "Making Spirits Bright" and Badalucco as one of Ally's dates in the episode "I Know him by Heart") but it remains ambiguous whether they were playing the same characters they play on "The Practice".
Reception.
The show's ratings began to decline in the third season, but stabilized in the fourth season after Robert Downey, Jr. joined the regular cast as Ally's boyfriend Larry Paul, and a fresher aesthetic was created by new art director Matthew DeCoste. However, Downey's character was written out after the end of the season due to the actor's troubles with drug addiction.
Along with "Dharma & Greg", "Ally McBeal" was one of the last two surviving shows to debut during the 1997-98 season, one of the weakest in television history for new shows. (Only seven shows to debut would be picked up for a second season, and only "Dharma & Greg" and "Ally McBeal" would last longer than three seasons, each providing enough episodes for syndication.) Both shows ended at the end of the 2001-02 season, five years after their debut. Coincidentally, both shows were produced by 20th Century Fox Television.
Feminist criticism.
Despite its success, "Ally McBeal" received some negative criticism from TV critics and feminists who found the title character annoying and demeaning to women (specifically professional women) because of her perceived flightiness, lack of demonstrated legal knowledge, short skirts, and emotional instability. Perhaps the most notorious example of the debate sparked by the show was the June 29, 1998 cover story of "Time" magazine, which juxtaposed McBeal with three pioneering feminists (Susan B. Anthony, Betty Friedan, Gloria Steinem) and asked "Is Feminism Dead?" In episode 12 of the second season of the show, Ally talks to her co-worker John Cage about a dream she had, saying "You know, I had a dream that they put my face on the cover of "Time" magazine as 'the face of feminism'."
Music.
"Ally McBeal" was a heavily music-oriented show. Vonda Shepard, a virtually unknown musician at the time, was featured continually on the show. Her song "Searchin' My Soul" became the show's theme song. Many of the songs Shepard performed were established hits with lyrics that paralleled the events of the episode, including "Both Sides Now", "Hooked on a Feeling" and "Tell Him". Besides recording background music for the show, Shepard frequently appeared at the ends of episodes as a musician performing at a local piano bar frequented by the main characters. On rare occasions, her character would have conventional dialogue. A portion of "Searchin' My Soul" was played at the beginning of each episode, but remarkably the song was never played in its entirety.
Several of the characters had a musical leitmotif that played when they appeared. John Cage's was "My Everything", Ling Woo's was the Wicked Witch of the West theme from "The Wizard of Oz", and Ally McBeal herself picked "Tell Him", when told by a psychiatrist that she needed a theme.
Due to the popularity of the show and Shepard's music, a soundtrack titled "Songs from Ally McBeal" was released in 1998, as well as a successor soundtrack titled "Heart and Soul: New Songs From Ally McBeal" in 1999. Two compilation albums from the show featuring Shepard were also released in 2000 and 2001. A Christmas album was also released under the title "Ally McBeal: A Very Ally Christmas". The album received positive reviews, and Shephard’s version of Kay Starr’s Christmas song (Everybody's Waitin' For) The Man with the Bag, received considerable airplay during the holiday season.
Other artists featured on the show include Barry White, Al Green, Tina Turner, Anastacia, Elton John, Sting and Mariah Carey. Josh Groban played the role of Malcolm Wyatt in the May 2001 season finale, performing "You're Still You". The series creator, David E. Kelley, was impressed with Groban's performance at The Family Celebration event and based on the audience reaction to Groban's singing, Kelley created a character for him in that finale. The background score for the show was composed by Danny Lux.
DVD releases.
Due to music licensing issues, none of the seasons of "Ally McBeal" were available on DVD in the United States (only 6 random episodes can be found on the R1 edition) until 2009, though it has been available in Italy, Belgium, the Netherlands, Japan, Hong Kong, Portugal, Spain, France, Germany, the United Kingdom, Mexico, Taiwan, Australia, Brazil and the Czech Republic with all the show's music intact since 2005. In the UK, Ireland and Spain all seasons are available in a complete box set.
20th Century Fox released the complete first season on DVD in Region 1 on October 6, 2009. They also released a special complete series edition on the same day. Season 1 does not contain any special features, the complete series set however does contain several bonus features including featurettes, an all-new retrospective, the episode of The Practice in which Calista Flockhart guest starred and a bonus disc entitled "The Best of Ally McBeal Soundtrack". In addition, both releases contain all of the original music. Season 2 was released on April 6, 2010. Seasons 3, 4 and 5 were all released on October 5, 2010. Season 1 and 2 are also available on the US iTunes Store.
"Ally" (1999).
In 1999, at the height of the show's popularity, a half-hour version entitled "Ally" began airing in parallel with the main program. This version, designed in a sitcom format, used re-edited scenes from the main program, along with previously unseen footage. The intention was to further develop the plots in the comedy-drama in a sitcom style. It also focused only on Ally's personal life, cutting all the courtroom plots. The repackaged show was cancelled partway through its initial run. While 13 episodes of "Ally" were created, only 10 were actually broadcast.
In popular culture.
McBeal and 1990s young affluent professional women were parodied in the song "Ally McBeal" (tune of "Like a Rolling Stone" by Bob Dylan) by a cappella group Da Vinci's Notebook on their album "The Life and Times of Mike Fanning", released in 2000.
In episode 2, season 3 of the British comedy "The Adam and Joe Show", the show was parodied as 'Ally McSqeal' using soft toys.
A season 2 episode of "Futurama", "When Aliens Attack", featured a parody of the show entitled "Single Female Lawyer". The principal crux of the parody was that, effectively, "Single Female Lawyer" had no discernible plot other than the fact that the female lead was very attractive, wore a short skirt, and slept with her clients. The show has been broadcast into space for centuries, but the last episode was missing (due to Philip Fry's incompetence and time travel) and so a warlike alien race, who had become hooked on the show, demanded that Earth either play out the final episode for them or they would ignite the planet's atmosphere. Luckily, due to the nature of the show being little more than fan service, it was easy for Fry, Leela and the others to replicate it by simply putting Leela in a miniskirt and ad libbing the dialogue on the spot.

</doc>
<doc id="1949" url="http://en.wikipedia.org/wiki?curid=1949" title="Andreas Capellanus">
Andreas Capellanus

Andreas Capellanus ("Capellanus" meaning "chaplain"), also known as Andrew the Chaplain, and occasionally by a French translation of his name, André le Chapelain, was the 12th-century author of a treatise commonly known as "De amore" ("About Love"), and often known in English, somewhat misleadingly, as "The Art of Courtly Love", though its realistic, somewhat cynical tone suggests that it is in some measure an antidote to courtly love. Little is known of Andreas Capellanus's life, but he is presumed to have been a courtier of Marie de Champagne, and probably of French origin.
His work.
"De Amore" was written at the request of Marie de Champagne, daughter of King Louis VII of France and of Eleanor of Aquitaine. In it, the author informs a young pupil, Walter, of the pitfalls of love. A dismissive allusion in the text to the "wealth of Hungary" has suggested the hypothesis that it was written after 1184, at the time when Bela III of Hungary had sent to the French court a statement of his income and had proposed marriage to Marie's half-sister Marguerite of France, but before 1186, when his proposal was accepted.
"De Amore" is made up of three books. The first book covers the etymology and definition of love and is written in the manner of an academic lecture. The second book consists of sample dialogues between members of different social classes; it outlines how the romantic process between the classes should work. Book three is made of stories from actual courts of love presided over by noble women.
John Jay Parry, the editor of one modern edition of "De Amore", quotes critic Robert Bossuat as describing "De Amore" as "one of those capital works which reflect the thought of a great epoch, which explains the secret of a civilization". It may be viewed as didactic, mocking, or merely descriptive; in any event it preserves the attitudes and practices that were the foundation of a long and significant tradition in Western literature.
The social system of "courtly love", as gradually elaborated by the Provençal troubadours from the mid twelfth century, soon spread. One of the circles in which this poetry and its ethic were cultivated was the court of Eleanor of Aquitaine (herself the granddaughter of an early troubadour poet, William IX of Aquitaine). It has been claimed that "De Amore" codifies the social and sexual life of Eleanor's court at Poitiers between 1170 and 1174, though it was evidently written at least ten years later and, apparently, at Troyes. It deals with several specific themes that were the subject of poetical debate among late twelfth century troubadours and trobairitz.
The meaning of "De Amore" has been debated over the centuries. In the years immediately following its release many people took Andreas’ opinions concerning Courtly Love seriously. In more recent times, however, scholars have come to view the priest’s work as satirical. Many scholars now agree that Andreas was commenting on the materialistic, superficial nature of the nobles of the Middle Ages. Andreas seems to have been warning young Walter, his protege, about love in the Middle Ages.

</doc>
<doc id="1950" url="http://en.wikipedia.org/wiki?curid=1950" title="American Civil Liberties Union">
American Civil Liberties Union

The American Civil Liberties Union (ACLU) is a nonpartisan, non-profit organization whose stated mission is "to defend and preserve the individual rights and liberties guaranteed to every person in this country by the Constitution and laws of the United States." It works through litigation, lobbying, and community empowerment. Founded in 1920 by Roger Baldwin, Crystal Eastman, Walter Nelles, Morris Ernst, Albert DeSilver, Arthur Garfield Hays, Helen Keller, Jane Addams, Felix Frankfurter, and Elizabeth Gurley Flynn, the ACLU has over 500,000 members and has an annual budget of over $100 million. Local affiliates of the ACLU are active in all 50 states and Puerto Rico. The ACLU provides legal assistance in cases when it considers civil liberties to be at risk. Legal support from the ACLU can take the form of direct legal representation, or preparation of "amicus curiae" briefs expressing legal arguments (when another law firm is already providing representation).
When the ACLU was founded in 1920, its focus was on freedom of speech, primarily for anti-war protesters. During the 1920s, the ACLU expanded its scope to include protecting the free speech rights of artists and striking workers, and working with the National Association for the Advancement of Colored People (NAACP) to combat racism and discrimination. During the 1930s, the ACLU started to engage in work combating police misconduct and for Native American rights. Most of the ACLU's cases came from the Communist party and Jehovah's Witnesses. In 1940, the ACLU leadership was caught up in the Red Scare, and voted to exclude Communists from its leadership positions. During World War II, the ACLU defended Japanese-American citizens, unsuccessfully trying to prevent their forcible relocation to internment camps. During the Cold War, the ACLU headquarters was dominated by anti-communists, but many local affiliates defended members of the Communist Party.
By 1964, membership had risen to 80,000, and the ACLU participated in efforts to expand civil liberties. In the 1960s, the ACLU continued its decades-long effort to enforce separation of church and state. It defended several anti-war activists during the Vietnam War. The ACLU was involved in the "Miranda" case, which addressed misconduct by police during interrogations; and in the "New York Times" case, which established new protections for newspapers reporting on government activities. In the 1970s and 1980s, the ACLU ventured into new legal areas, defending homosexuals, students, prisoners, and the poor. In the twenty-first century, the ACLU has fought the teaching of creationism in public schools and challenged some provisions of anti-terrorism legislation as infringing on privacy and civil liberties.
In addition to representing persons and organizations in lawsuits, the ACLU lobbies for policies that have been established by its board of directors. Current positions of the ACLU include: opposing the death penalty; supporting same-sex marriage and the right of gays to adopt; supporting birth control and abortion rights; eliminating discrimination against women, minorities, and LGBT people; supporting the rights of prisoners and opposing torture; and opposing government preference for religion over non-religion, or for particular faiths over others.
Legally, the ACLU consists of two separate but closely affiliated nonprofit organizations: the American Civil Liberties Union, a 501(c)(4) social welfare group, and the ACLU Foundation, a 501(c)(3) public charity. Both organizations engage in civil rights litigation, advocacy, and education, but only donations to the 501(c)(3) foundation are tax deductible, and only the 501(c)(4) group can engage in unlimited political lobbying. The two organizations share office space and employees.
Organization.
Leadership.
The ACLU is led by a president and an executive director, Susan N. Herman and Anthony Romero, respectively, in 2015. The president acts as chairman of the ACLU's board of directors, leads fundraising, and facilitates policy-setting. The executive director manages the day-to-day operations of the organization. The board of directors consists of 80 persons, including representatives from each state affiliate, as well as at-large delegates. The organization has its headquarters in 125 Broad Street, a 40-story skyscraper located in Lower Manhattan, New York City.
The leadership of the ACLU does not always agree on policy decisions; differences of opinion within the ACLU leadership have sometimes grown into major debates. In 1937, an internal debate erupted over whether to defend Henry Ford's right to distribute anti-union literature. In 1939, a heated debate took place over whether to prohibit communists from serving in ACLU leadership roles. During the early 1950s the board was divided on whether to defend communists persecuted under McCarthyism. In 1968, a schism formed over whether to represent Dr. Spock's anti-war activism. In 1973, there was internal conflict over whether to call for the impeachment of Richard Nixon. In 2005, there was internal conflict about whether or not a gag rule should be imposed on ACLU employees to prevent publication of internal disputes.
Funding.
In the year ending March 31, 2014, the ACLU and the ACLU Foundation had a combined income from support and revenue of $100.4 million, originating from grants (50.0%), membership donations (25.4%), donated legal services (7.6%), bequests (16.2%), and revenue (.9%). Membership dues are treated as donations; members choose the amount they pay annually, averaging approximately $50 per member per year. In the year ending March 31, 2014, the combined expenses of the ACLU and ACLU Foundation were $133.4 million, spent on programs (86.2%), management (7.4%), and fundraising (8.2%). (After factoring in other changes in net assets of +$30.9 million, from sources such as investment income, the organization had an overall decrease in net assets of $2.1 million.) Over the period from 2011 to 2014 the ACLU Foundation, on the average, has accounted for roughly 70% of the combined budget, and the ACLU roughly 30%.
The ACLU solicits donations to its charitable foundation. The ACLU is accredited by the Better Business Bureau, and the Charity Navigator has ranked the ACLU with a four-star rating. The local affiliates solicit their own funding; however, some also receive funds from the national ACLU, with the distribution and amount of such assistance varying from state to state. At its discretion, the national organization provides subsidies to smaller affiliates that lack sufficient resources to be self-sustaining; for example, the Wyoming ACLU chapter received such subsidies until April 2015, when, as part of a round of layoffs at the national ACLU, the Wyoming office was closed.
In October 2004, the ACLU rejected $1.5 million from both the Ford Foundation and Rockefeller Foundation because the Foundations had adopted language from the USA PATRIOT Act in their donation agreements, including a clause stipulating that none of the money would go to "underwriting terrorism or other unacceptable activities." The ACLU views this clause, both in Federal law and in the donors' agreements, as a threat to civil liberties, saying it is overly broad and ambiguous.
Due to the nature of its legal work, the ACLU is often involved in litigation against governmental bodies, which are generally protected from adverse monetary judgments; a town, state or federal agency may be required to change its laws or behave differently, but not to pay monetary damages except by an explicit statutory waiver. In some cases, the law permits plaintiffs who successfully sue government agencies to collect money damages or other monetary relief. In particular, the Civil Rights Attorney's Fees Award Act of 1976 leaves the government liable in some civil rights cases. Fee awards under this civil rights statute are considered "equitable relief" rather than damages, and government entities are not immune from equitable relief. Under laws such as this, the ACLU and its state affiliates sometimes share in monetary judgments against government agencies. In 2006, the Public Expressions of Religion Protection Act sought to prevent monetary judgments in the particular case of violations of church-state separation.
The ACLU has received court awarded fees from opponents, for example, the Georgia affiliate was awarded $150,000 in fees after suing a county demanding the removal of a Ten Commandments display from its courthouse; a second Ten Commandments case in the State, in a different county, led to a $74,462 judgment. The State of Tennessee was required to pay $50,000, the State of Alabama $175,000, and the State of Kentucky $121,500, in similar Ten Commandments cases.
State affiliates.
Most of the organization's workload is performed by the 53 local affiliates. There is an affiliate in each state and in Puerto Rico. California has three affiliates. The affiliates operate autonomously from the national organization; each affiliate has its own staff, executive director, board of directors, and budget. Each affiliate consists of two non-profit corporations: a 501(c)(3) corporation that does not perform lobbying, and a 501(c)(4) corporation which is entitled to lobby.
ACLU affiliates are the basic unit of the ACLU's organization and engage in litigation, lobbying, and public education. For example, in a twenty-month period beginning January 2004, the ACLU's New Jersey chapter was involved in fifty-one cases according to their annual report—thirty-five cases in state courts, and sixteen in federal court. They provided legal representation in thirty-three of those cases, and served as amicus in the remaining eighteen. They listed forty-four volunteer attorneys who assisted them in those cases.
Positions.
The ACLU's official position statements, as of January 2012, included the following policies:
Support and opposition.
The ACLU is supported by a variety of persons and organizations. There were over 500,000 members in 2011, and the ACLU annually receives thousands of grants from hundreds of charitable foundations. Allies of the ACLU in legal actions have included the National Association for the Advancement of Colored People, the American Jewish Congress, People For the American Way, the National Rifle Association, the Electronic Frontier Foundation, Americans United for Separation of Church and State, and the National Organization for Women.
The ACLU has been criticized by liberals, such as when it excluded communists from its leadership ranks, when it defended Neo-Nazis, when it declined to defend Paul Robeson, or when it opposed the passage of the National Labor Relations Act. Conversely, it has been criticized by conservatives, such as when it argued against official prayer in public schools, or when it opposed the Patriot Act. The ACLU has supported conservative figures such as Rush Limbaugh, George Wallace, Henry Ford, and Oliver North; and it has supported liberal figures such as Dick Gregory, H. L. Mencken, Rockwell Kent, and Dr. Benjamin Spock.
A major source of criticism are legal cases in which the ACLU represents an individual or organization that promotes offensive or unpopular viewpoints, such as the Ku Klux Klan, Neo-Nazis, Nation of Islam, North American Man/Boy Love Association, or Westboro Baptist Church. The ACLU responded to these criticisms by stating "It is easy to defend freedom of speech when the message is something many people find at least reasonable. But the defense of freedom of speech is most critical when the message is one most people find repulsive."
Early years.
CLB era.
The ACLU developed from the National Civil Liberties Bureau (CLB), co-founded in 1917 during the Great War by Crystal Eastman, an attorney activist, and Roger Nash Baldwin. The focus of the CLB was on freedom of speech, primarily anti-war speech, and on supporting conscientious objectors who did not want to serve in World War I.
Three United States Supreme Court decisions in 1919 each upheld convictions under laws against certain kinds of anti-war speech. In 1919, the Court upheld the conviction of Socialist Party leader Charles Schenck for publishing anti-war literature. In "Debs v. United States," the court upheld the conviction of Eugene Debs. While the Court upheld a conviction a third time in "Abrams v. United States", Justice Oliver Wendell Holmes wrote an important dissent which has gradually been absorbed as an American principle: he urged the court to treat freedom of speech as a fundamental right, which should rarely be restricted.
In 1918 Crystal Eastman resigned from the organization due to health issues. After assuming sole leadership of the CLB, Baldwin insisted that the organization be reorganized. He wanted to change its focus from litigation to direct action and public education.
The CLB directors concurred, and on January 19, 1920, they formed an organization under a new name, the American Civil Liberties Union. Although a handful of other organizations in the United States at that time focused on civil rights, such as the National Association for the Advancement of Colored People (NAACP) and Anti-Defamation League (ADL), the ACLU was the first that did not represent a particular group of persons, or a single theme. Like the CLB, the NAACP pursued litigation to work on civil rights, including efforts to overturn the disfranchisement of African Americans in the South that had taken place since the turn of the century.
During the first decades of the ACLU, Baldwin continued as its leader. His charisma and energy attracted many supporters to the ACLU board and leadership ranks. Baldwin was ascetic, wearing hand-me-down clothes, pinching pennies, and living on a very small salary. The ACLU was directed by an executive committee, but it was not particularly democratic or egalitarian. The ACLU's base in New York resulted in its being dominated by people from the city and state. Most ACLU funding came from philanthropies, such as the Garland Fund.
Free speech era.
In the 1920s, government censorship was commonplace. Magazines were routinely confiscated under the anti-obscenity Comstock laws; permits for labor rallies were often denied; and virtually all anti-war or anti-government literature was outlawed. Right-wing conservatives wielded vast amounts of power, and activists that promoted unionization, socialism, or government reform were often denounced as un-American or unpatriotic. In one typical instance in 1923, author Upton Sinclair was arrested for trying to read the First Amendment during an Industrial Workers of the World rally.
ACLU leadership was divided on how to challenge the civil rights violations. One faction, including Baldwin, Arthur Garfield Hays and Norman Thomas, believed that direct, militant action was the best path. Hays was the first of many successful attorneys that relinquished their private practices to work for the ACLU. Another group, including Walter Nelles and Walter Pollak felt that lawsuits taken to the Supreme Court were the best way to achieve change. Both groups worked in tandem, but equally worshipped the Bill of Rights and the US Constitution.
During the 1920s, the ACLU's primary focus was on freedom of speech in general, and speech within the labor movement particularly. Because most of the ACLU's efforts were associated with the labor movement, the ACLU itself came under heavy attack from conservative groups, such as the American Legion, the National Civic Federation, and Industrial Defense Association and the Allied Patriotic Societies.
In addition to labor, the ACLU also led efforts in non-labor arenas, for example, promoting free speech in public schools. The ACLU itself was banned from speaking in New York public schools in 1921. The ACLU, working with the NAACP, also supported racial discrimination cases. The ACLU defended free speech regardless of the opinions being espoused. For example, the reactionary, anti-Catholic, anti-black Ku Klux Klan (KKK) was a frequent target of ACLU efforts, but the ACLU defended the KKK's right to hold meetings in 1923. There were some civil rights that the ACLU did not make an effort to defend in the 1920s, including censorship of the arts, government search and seizure issues, right to privacy, or wiretapping.
The Communist party of the United States was routinely harassed and oppressed by government officials, leading it to be the primary client of the ACLU. The Communists were very aggressive in their tactics, often engaging in illegal or unethical conduct, and this led to frequent conflicts between the Communists and ACLU. Communist leaders often attacked the ACLU, particularly when the ACLU defended the free speech rights of conservatives. This uneasy relationship between the two groups continued for decades.
Scopes trial.
When 1925 arrived – five years after the ACLU was formed – the organization had virtually no success to show for its efforts. That changed in 1925, when the ACLU persuaded John T. Scopes to defy Tennessee's anti-evolution law in a court test. Clarence Darrow, a member of the ACLU National Committee, headed Scopes' legal team. The prosecution, led by William Jennings Bryan, contended that the Bible should be interpreted literally in teaching creationism in school. The ACLU lost the case and Scopes was fined $100. The Tennessee Supreme Court later upheld the law but overturned the conviction on a technicality.
The Scopes trial was a phenomenal public relations success for the ACLU. The ACLU became well known across America, and the case led to the first endorsement of the ACLU by a major U.S. newspaper. The ACLU continued to fight for the separation of church and state in schoolrooms, decade after decade, including the 1982 case "McLean v. Arkansas" and the 2005 case "Kitzmiller v. Dover Area School District".
Baldwin himself was involved in an important free speech victory of the 1920s, after he was arrested for attempting to speak at a rally of striking mill workers in New Jersey. Although the decision was limited to the state of New Jersey, the appeals court's judgement in 1928 declared that constitutional guarantees of free speech must be given "liberal and comprehensive construction", and it marked a major turning point in the civil rights movement, signaling the shift of judicial opinion in favor of civil rights.
The most important ACLU case of the 1920s was "Gitlow v. New York", in which Benjamin Gitlow was arrested for violating a state law against inciting anarchy and violence, when he distributed literature promoting communism. Although the Supreme Court did not overturn Gitlow's conviction, it adopted the ACLU's stance (later termed the incorporation doctrine) that the First Amendment freedom of speech applied to state laws, as well as federal laws.
First victories.
Leaders of the ACLU were divided on the best tactics to use to promote civil liberties. Felix Frankfurter felt that legislation was the best long-term solution, because the Supreme Court could not (and – in his opinion – should not) mandate liberal interpretations of the Bill of Rights. But Walter Pollack, Morris Ernst, and other leaders felt that Supreme Court decisions were the best path to guarantee civil liberties. A series of Supreme Court decisions in the 1920s foretold a changing national atmosphere; anti-radical emotions were diminishing, and there was a growing willingness to protect freedom of speech and assembly via court decisions.
Free speech.
Censorship was commonplace in the early 20th century. State laws and city ordinances routinely outlawed speech deemed to be obscene or offensive, and prohibited meetings or literature that promoted unions or labor organization. Starting in 1926, the ACLU began to expand its free speech activities to encompass censorship of art and literature. In that year, H. L. Mencken deliberately broke Boston law by distributing copies of his banned "American Mercury" magazine; the ACLU defended him and won an acquittal. The ACLU went on to win additional victories, including the landmark case "United States v. One Book Called Ulysses" in 1933, which reversed a ban by the Customs Department against the book "Ulysses" by James Joyce. The ACLU only achieved mixed results in the early years, and it was not until 1966 that the Supreme Court finally clarified the obscenity laws in the "Roth v. United States" and "Memoirs v. Massachusetts" cases.
The Comstock laws banned distribution of sex education information, based on the premise that it was obscene and led to promiscuous behavior Mary Ware Dennett was fined $300 in 1928, for distributing a pamphlet containing sex education material. The ACLU, led by Morris Ernst, appealed her conviction and won a reversal, in which judge Learned Hand ruled that the pamphlet's main purpose was to "promote understanding".
The success prompted the ACLU to broaden their freedom of speech efforts beyond labor and political speech, to encompass movies, press, radio and literature. The ACLU formed the National Committee on Freedom from Censorship in 1931 to coordinate this effort. By the early 1930s, censorship in the United States was diminishing.
Two major victories in the 1930s cemented the ACLUs campaign to promote free speech. In "Stromberg v. California", decided in 1931, the Supreme Court sided with the ACLU and affirmed the right of a communist party member to salute a communist flag. The result was the first time the Supreme Court used the Due Process Clause of the 14th amendment to subject states to the requirements of the First Amendment. In "Near v. Minnesota", also decided in 1931, the Supreme Court ruled that states may not exercise prior restraint and prevent a newspaper from publishing, simply because the newspaper had a reputation for being scandalous.
1930s.
The late 1930s saw the emergence of a new era of tolerance in the United States. National leaders hailed the Bill of Rights, particularly as it protected minorities, as the essence of democracy. The 1939 Supreme Court decision in "Hague v. Committee for Industrial Organization" affirmed the right of communists to promote their cause. Even conservative elements, such as the American Bar Association began to campaign for civil liberties, which were long considered to be the domain of left-leaning organizations. By 1940, the ACLU had achieved many of the goals it set in the 1920s, and many of its policies were the law of the land.
Expansion.
In 1929, after the Scopes and Dennett victories, Baldwin perceived that there was vast, untapped support for civil liberties in the United States. Baldwin proposed an expansion program for the ACLU, focusing on police brutality, Native American rights, African American rights, censorship in the arts, and international civil liberties. The board of directors approved Baldwin's expansion plan, except for the international efforts.
The ACLU played a major role in passing the 1932 Norris–La Guardia Act, a federal law which prohibited employers from preventing employees from joining unions, and stopped the practice of outlawing strikes, unions, and labor organizing activities with the use of injunctions. The ACLU also played a key role in initiating a nationwide effort to reduce misconduct (such as extracting false confessions) within police departments, by publishing the report "Lawlessness in Law Enforcement" in 1931, under the auspices of Herbert Hoover's Wickersham Commission. In 1934, the ACLU lobbied for the passage of the Indian Reorganization Act, which restored some autonomy to Native American tribes, and established penalties for kidnapping native American children.
Although the ACLU deferred to the NAACP for litigation promoting civil liberties for African Americans, the ACLU did engage in educational efforts, and published "Black Justice" in 1931, a report which documented institutional racism throughout the South, including lack of voting rights, segregation, and discrimination in the justice system. Funded by the Garland Fund, the ACLU also participated in producing the influential Margold Report, which outlined a strategy to fight for civil rights for blacks. The ACLU's plan was to demonstrate that the "separate but equal" policies governing the Southern discrimination were illegal because blacks were never, in fact, treated equally.
Depression era and the New Deal.
In 1932 – twelve years after the ACLU was founded – it had achieved significant success; the Supreme Court had embraced the free speech principles espoused by the ACLU, and the general public was becoming more supportive of civil rights in general. But the Great Depression brought new assaults on civil liberties; the year 1930 saw a large increase in the number of free speech prosecutions, a doubling of the number of lynchings, and all meetings of unemployed persons were banned in Philadelphia.
The Franklin D. Roosevelt administration proposed the New Deal to combat the depression. ACLU leaders were of mixed opinions about the New Deal, since many felt that it represented an increase in government intervention into personal affairs, and because the National Recovery Administration suspended anti-trust legislation. Roosevelt was not personally interested in civil rights, but did appoint many civil libertarians to key positions, including Interior Secretary Harold Ickes, a member of the ACLU.
The economic policies of the New Deal leaders were often aligned with ACLU goals, but social goals were not. In particular, movies were subject to a barrage of local ordinances banning screenings that were deemed immoral or obscene. Even public health films portraying pregnancy and birth were banned; as was "Life" magazine's April 11, 1938 issue which included photos of the birth process. The ACLU fought these bans, but did not prevail.
The Catholic Church attained increasing political influence in the 1930s, and used its influence to promote censorship of movies, and to discourage publication of birth control information. This conflict between the ACLU and the Catholic Church led to the resignation of the last Catholic priest from ACLU leadership in 1934; a Catholic priest would not be represented there again until the 1970s.
The ACLU took no official position on president Franklin Delano Roosevelt's 1937 court-packing plan, which threatened to increase the number of Supreme Court justices, unless the Supreme Court reversed its course and began approving New Deal legislation. The Supreme Court responded by making a major shift in policy, and no longer applied strict constitutional limits to government programs, and also began to take a more active role in protecting civil liberties.
The first decision that marked the court's new direction was "De Jonge v. Oregon", in which a communist labor organizer was arrested for calling a meeting to discuss unionization. The ACLU attorney Osmond Fraenkel, working with International Labor Defense, defended De Jonge in 1937, and won a major victory when the Supreme Court ruled that "peaceable assembly for lawful discussion cannot be made a crime." The De Jonge case marked the start of an era lasting for a dozen years, during which Roosevelt appointees (led by Hugo Black, William O. Douglas, and Frank Murphy) established a body of civil liberties law. In 1938, Justice Harlan F. Stone wrote the famous "footnote four" in "United States v. Carolene Products Co." in which he suggested that state laws which impede civil liberties would – henceforth – require compelling justification.
Senator Robert F. Wagner proposed the National Labor Relations Act in 1935, which empowered workers to unionize. Ironically, the ACLU, after 15 years of fighting for workers rights, initially opposed the act (it later took no stand on the legislation) because some ACLU leaders feared the increased power the bill gave to the government. The newly formed National Labor Relations Board (NLRB) posed a dilemma for the ACLU, because in 1937 it issued an order to Henry Ford, prohibiting Ford from disseminating anti-union literature. Part of the ACLU leadership habitually took the side of labor, and that faction supported the NLRB's action. But part of the ACLU supported Ford's right to free speech. ACLU leader Arthur Garfield Hays proposed a compromise (supporting the auto workers union, yet also endorsing Ford's right to express personal opinions), but the schism highlighted a deeper divide that would become more prominent in the years to come.
The ACLU's support of the NRLB was a major development for the ACLU, because it marked the first time it accepted that a government agency could be responsible for upholding civil liberties. Until 1937, the ACLU felt that civil rights were best upheld by citizens and private organizations.
Some factions in the ACLU proposed new directions for the organization. In the late 1930s, some local affiliates proposed shifting their emphasis from civil liberties appellate actions, to becoming a legal aid society, centered on store front offices in low income neighborhoods. The ACLU directors rejected that proposal. Other ACLU members wanted the ACLU to shift focus into the political arena, and to be more willing to compromise their ideals in order to strike deals with politicians. This initiative was also rejected by the ACLU leadership.
Jehovah's Witnesses.
The ACLU's support of defendants with unpopular, sometimes extreme, viewpoints have produced many landmark court cases and established new civil liberties. One such defendant was the Jehovah's Witnesses, who were involved in a large number of Supreme Court cases. Cases that the ACLU supported included "Lovell v. City of Griffin" (which struck down a city ordinance that required a permit before a person could distribute "literature of any kind"); "Martin v. Struthers" (which struck down an ordinance prohibiting door-to-door canvassing); and "Cantwell v. Connecticut" (which reversed the conviction of a Witness who was reciting offensive speech on a street corner).
The most important cases involved statutes requiring flag salutes. The Jehovah's Witnesses felt that saluting a flag was contrary to their religious beliefs. Two children were convicted in 1938 of not saluting the flag. The ACLU supported their appeal to the Supreme Court, but the court affirmed the conviction, in 1940. But three years later, in "West Virginia State Board of Education v. Barnette", the Supreme court reversed itself and wrote "If there is any fixed star in our constitutional constellation, it is that no official, high or petty, can prescribe what shall be orthodox in politics, nationalism, religion, or other matters of opinion or force citizens to confess by word or act their faith therein." To underscore its decision, the Supreme Court announced it on Flag Day.
Communism and totalitarianism.
The rise of totalitarianism in Germany, Russia, and Italy during World War II had a tremendous impact on the civil liberties movement. On the one hand, the oppression of the totalitarian states put into sharp relief the virtue of freedom of speech and association in the United States; on the other hand, they prompted an anti-communist hysteria in America which eroded many civil liberties.
The ACLU leadership was divided over whether or not to defend pro-Nazi speech in the United States; pro-labor elements within the ACLU were hostile towards Nazism and fascism, and objected when the ACLU defended Nazis. Several states passed laws outlawing the hate speech directed at ethnic groups. The first person arrested under New Jersey's 1935 hate speech law was a Jehovah's Witness who was charged with disseminating anti-Catholic literature. The ACLU defended the Jehovah's Witnesses, and the charges were dropped. The ACLU proceeded to defend numerous pro-Nazi groups, defending their rights to free speech and free association.
In the late 1930s, the ACLU allied itself with the Popular Front, a coalition of liberal organizations coordinated by the United States Communist Party. The ACLU benefited because affiliates from the Popular Front could often fight local civil rights battles much more effectively than the New York-based ACLU. The association with the Communist Party led to accusations that the ACLU was a "communist front", particularly because Harry F. Ward was both chairman of the ACLU and chairman of the American League Against War and Fascism, a communist organization.
The House Unamerican Activities Committee (HUAC) was created in 1938 to uncover sedition and treason within the United States. When witnesses testified at its hearings, the ACLU was mentioned several times, leading the HUAC to mention the ACLU prominently in its 1939 report. This damaged the ACLU's reputation severely, even though the report said that it could not "definitely state whether or not" the ACLU was a communist organization.
While the ACLU rushed to defend its image against allegations of being a communist front, it also worked to protect witnesses who were being harassed by the HUAC. The ACLU was one of the few organizations to protest (unsuccessfully) against passage of the Smith Act in 1940, which would later be used to imprison many persons who supported Communism. The ACLU defended many persons who were prosecuted under the Smith Act, including labor leader Harry Bridges.
ACLU leadership was split on whether to purge its leadership of communists. Norman Thomas, John Haynes Holmes, and Morris Ernst were anti-communists who wanted to distance the ACLU from communism; opposing them were Harry Ward, Corliss Lamont and Elizabeth Flynn who rejected any political test for ACLU leadership. A bitter struggle ensued throughout 1939, and the anti-communists prevailed in February 1940, when the board voted to prohibit anyone who supported totalitarianism from ACLU leadership roles. Chairman Harry Ward immediately resigned, and – following a contentious six-hour debate – Elizabeth Flynn was voted off the ACLU's board. The 1940 resolution was a disaster for the ACLU, and considered by many to be a betrayal of its fundamental principles. The resolution was rescinded in 1968, and Flynn was posthumously reinstated to the ACLU in 1970.
Mid-century.
World War II.
When World War II engulfed the United States, the Bill of Rights was enshrined as a hallowed document, and numerous organizations defended civil liberties. Chicago and New York proclaimed "Civil Rights" weeks, and President Franklin Delano Roosevelt announced a national Bill of Rights day. Eleanor Roosevelt was the keynote speaker at the 1939 ACLU convention. In spite of this newfound respect for civil rights, Americans were becoming adamantly anti-communist, and believed that excluding communists from American society was an essential step to preserve democracy.
Contrasted with World War I, there was relatively little violation of civil liberties during World War II. President Roosevelt was a strong supporter of civil liberties, but – more importantly – there were few anti-war activists during World War II. The most significant exception was the internment of Japanese Americans. Two months after the Japanese attack on Pearl Harbor, Roosevelt authorized the creation of military "exclusion zones" with Executive Order 9066, paving the way for the detention of all West Coast Japanese Americans in inland camps. In addition to the non-citizen Issei (prohibited from naturalization as members of an "unassimilable" race), over two-thirds of those swept up were American-born citizens. The ACLU immediately protested to Roosevelt, comparing the evacuations to Nazi concentration camps. The ACLU was the only major organization to object to the internment plan, and their position was very unpopular, even within the organization. Not all ACLU leaders wanted to defend the Japanese Americans; Roosevelt loyalists such as Morris Ernst wanted to support Roosevelt's war effort, but pacifists such as Baldwin and Norman Thomas felt that Japanese Americans needed access to due process before they could be imprisoned. In a March 20, 1942 letter to Roosevelt, Baldwin called on the administration to allow Japanese Americans to prove their loyalty at individual hearings, describing the constitutionality of the planned removal "open to grave question." His suggestions went nowhere, and opinions within the organization became increasingly divided as the Army began the "evacuation" of the West Coast. In May, the two factions, one pushing to fight the exclusion orders then being issued, the other advocating support for the President's policy of removing citizens whose "presence may endanger national security," brought their opposing resolutions to a vote before the board and the ACLU's national leaders. They decided not to challenge the eviction of Japanese American citizens, and on June 22 instructions were sent to West Coast branches not to support cases that argued the government had no constitutional right to do so.
The ACLU offices on the West Coast had been more directly involved in addressing the tide of anti-Japanese prejudice from the start, as they were geographically closer to the issue, and were already working on cases challenging the exclusion by this time. The Seattle office, assisting in Gordon Hirabayashi's lawsuit, created an unaffiliated committee to continue the work the ACLU had started, while in Los Angeles, attorney A.L. Wirin continued to represent Ernest Kinzo Wakayama but without addressing the case's constitutional questions. (Wirin would lose private clients because of his defense of Wakayama and other Japanese Americans.) However, the San Francisco branch, led by Ernest Besig, refused to discontinue its support for Fred Korematsu, whose case had been taken on prior to the June 22 directive, and attorney Wayne Collins, with Besig's full support, centered his defense on the illegality of Korematsu's exclusion.
The West Coast offices had wanted a test case to take to court, but had a difficult time finding a Japanese American who was both willing to violate the internment orders and able to meet the ACLU's desired criteria of a sympathetic, Americanized plaintiff. Of the 120,000 Japanese Americans affected by the order, only 12 disobeyed, and Korematsu, Hirabayashi, and two others were the only resisters whose cases eventually made it to the Supreme Court. "Hirabayashi v. United States" came before the Court in May 1943, and the justices upheld the government's right to exclude Japanese Americans from the West Coast; although it had earlier forced its local office in L.A. to stop aiding Hirabayashi, the ACLU donated $1,000 to the case (over a third of the legal team's total budget) and submitted an "amicus" brief. Besig, dissatisfied with Osmond Fraenkel's tamer defense, filed an additional "amicus" brief that directly addressed Hirabayashi's constitutional rights. In the meantime, A.L. Wirin served as one of the attorneys in "Yasui v. United States" (decided the same day as the Hirabayashi case, and with the same results), but he kept his arguments within the perimeters established by the national office. The only case to receive a favorable ruling, "ex parte Endo", was also aided by two "amicus" briefs from the ACLU, one from the more conservative Fraenkel and another from the more putative Wayne Collins.
"Korematsu v. United States" proved to be the most controversial of these cases, as Besig and Collins refused to bow to national pressure to pursue the case without challenging the government's right to remove citizens from their homes. The ACLU board threatened to revoke the San Francisco branch's national affiliation, while Baldwin tried unsuccessfully to convince Collins to step down so he could replace him as lead attorney in the case. Eventually Collins agreed to present the case alongside Charles Horsky, although their arguments before the Supreme Court remained based in the unconstitutionality of the exclusion order Korematsu had disobeyed. The case was decided in December 1944, when the Court once again upheld the government's right to relocate Japanese Americans, although Korematsu's, Hirabayashi's and Yasui's convictions were later overturned in "coram nobis" proceedings in the 1980s.
Although the ACLU (somewhat unevenly) defended the Japanese Americans, it was more reluctant to defend anti-war protesters. A majority of the board passed a resolution in 1942 which declared the ACLU unwilling to defend anyone who interfered with the United States' war effort. Included in this group were the thousands of Nisei who renounced their U.S. citizenship during the war but later regretted the decision and tried to revoke their applications for "repatriation." (A significant number of those slated to "go back" to Japan had never actually been to the country and were in fact being deported rather than repatriated.) Ernest Besig had in 1944 visited the Tule Lake Segregation Center, where the majority of these "renunciants" were concentrated, and subsequently enlisted Wayne Collins' help to file a lawsuit on their behalf, arguing the renunciations had been given under duress. The national organization prohibited local branches from representing the renunciants, forcing Collins to pursue the case on his own, although Besig and the Northern California office provided some support.
When the war ended in 1945, the ACLU was 25 years old, and had accumulated an impressive set of legal victories. President Harry S. Truman sent a congratulatory telegram to the ACLU on the occasion of their 25th anniversary. American attitudes had changed since World War I, and dissent by minorities was tolerated with more willingness. The Bill of Rights was more respected, and minority rights were becoming more commonly championed. During their 1945 annual conference, the ACLU leaders composed a list of important civil rights issues to focus on in the future, and the list included racial discrimination and separation of church and state.
The ACLU supported the African-American defendants in "Shelley v. Kraemer", when they tried to occupy a house they had purchased in a neighborhood which had racially restrictive housing covenants. The African-American purchasers won the case in 1945.
Cold War era.
Anti-communist sentiment gripped the United States during the Cold War beginning in 1946. Federal investigations caused many persons with communist or left-leaning affiliations to lose their jobs, become blacklisted, or be jailed. During the Cold War, although the United States collectively ignored the civil rights of communists, other civil liberties—such as due process in law and separation of church and state—continued to be reinforced and even expanded.
The ACLU was internally divided when it purged communists from its leadership in 1940, and that ambivalence continued as it decided whether to defend alleged communists during the late 1940s. Some ACLU leaders were anti-communist, and felt that the ACLU should not defend any victims. Some ACLU leaders felt that communists were entitled to free speech protections, and the ACLU should defend them. Other ACLU leaders were uncertain about the threat posed by communists, and tried to establish a compromise between the two extremes. This ambivalent state of affairs would last until 1954, when the civil liberties faction prevailed, leading to the resignation of most of the anti-communist leaders.
In 1947, President Truman issued Executive Order 9835, which created the Federal Loyalty Program. This program authorized the Attorney General to create a list of organizations which were deemed to be subversive. Any association with these programs was ground for barring the person from employment. Listed organizations were not notified that they were being considered for the list, nor did they have an opportunity to present counterarguments; nor did the government divulge any factual basis for inclusion in the list. Although ACLU leadership was divided on whether to challenge the Federal Loyalty Program, some challenges were successfully made.
Also in 1947, the House Un-American Activities Committee (HUAC) subpoenaed ten Hollywood directors and writers, the "Hollywood Ten", intending to ask them to identify Communists, but the witnesses refused to testify. All were imprisoned for contempt of Congress. The ACLU supported the appeals of several of the artists, but lost on appeal. The Hollywood establishment panicked after the HUAC hearings, and created a blacklist which prohibited anyone with leftist associations from working. The ACLU supported legal challenges to the blacklist, but those challenges failed. The ACLU was more successful with an education effort; the 1952 report "The Judges and the Judged", prepared at the ACLU's direction in response to the blacklisting of actress Jean Muir, described the unfair and unethical actions behind the blacklisting process, and it helped gradually turn public opinion against McCarthyism.
The federal government took direct aim at the U.S. communist party in 1948 when it indicted its top twelve leaders in the Foley Square trial. The case hinged on whether or not mere membership in a totalitarian political party was sufficient to conclude that members advocated the overthrow of the United States government. The ACLU chose to not represent any of the defendants, and they were all found guilty and sentenced to three to five years in prison. Their defense attorneys were all cited for contempt, went to prison and were disbarred. When the government indicted additional party members, the defendants could not find attorneys to represent them. Communists protested outside the courthouse; a bill to outlaw picketing of courthouses was introduced in Congress, and the ACLU supported the anti-picketing law.
The ACLU, in a change of heart, supported the party leaders during their appeal process. The Supreme Court upheld the convictions in the "Dennis v. United States" decision by softening the free speech requirements from a "clear and present danger" test, to a "grave and probable" test. The ACLU issued a public condemnation of the "Dennis" decision, and resolved to fight it. One reason for the Supreme Court's support of cold war legislation was the 1949 deaths of Supreme Court justices Frank Murphy and Wiley Rutledge, leaving Hugo Black and William O. Douglas as the only remaining civil libertarians on the Court.
The "Dennis" decision paved the way for the prosecution of hundreds of other communist party members. The ACLU supported many of the communists during their appeals (although most of the initiative originated with local ACLU affiliates, not the national headquarters) but most convictions were upheld. The two California affiliates, in particular, felt the national ACLU headquarters was not supporting civil liberties strongly enough, and they initiated more cold war cases than the national headquarters did.
The ACLU also challenged many loyalty oath requirements across the country, but the courts upheld most of the loyalty oath laws. California ACLU affiliates successfully challenged the California state loyalty oath. The Supreme Court, until 1957, upheld nearly every law which restricted the liberties of communists.
The ACLU, even though it scaled back its defense of communists during the Cold War, still came under heavy criticism as a "front" for communism. Critics included the American Legion, Senator Joseph McCarthy, the HUAC, and the FBI. Several ACLU leaders were sympathetic to the FBI, and as a consequence, the ACLU rarely investigated any of the many complaints alleging abuse of power by the FBI during the Cold War.
Organizational change.
In 1950, the ACLU board of directors asked executive director Baldwin to resign, feeling that he lacked the organizational skills to lead the 9,000 (and growing) member organization. Baldwin objected, but a majority of the board elected to remove him from the position, and he was replaced by Patrick Murphy Malin. Under Malin's guidance, membership tripled to 30,000 by 1955 – the start of a 24-year period of continual growth leading to 275,000 members in 1974. Malin also presided over an expansion of local ACLU affiliates.
The ACLU, which had been controlled by an elite of a few dozen New Yorkers, became more democratic in the 1950s. In 1951, the ACLU amended its bylaws to permit the local affiliates to participate directly in voting on ACLU policy decisions. A bi-annual conference, open to the entire membership, was instituted in the same year, and in later decades it became a pulpit for activist members, who suggested new directions for the ACLU, including abortion rights, death penalty, and rights of the poor.
McCarthyism era.
During the early 1950s, the ACLU continued to steer a moderate course through the Cold War. When leftist singer Paul Robeson was denied a passport in 1950, even though he was not a communist and not accused of any illegal acts, the ACLU chose to not defend him. The ACLU later reversed their stance, and supported William Worthy and Rockwell Kent in their passport confiscation cases, which resulted in legal victories in the late 1950s.
In response to communist witch-hunts, many witnesses and employees chose to use the fifth amendment protection against self-incrimination to avoid divulging information about their political beliefs. Government agencies and private organizations, in response, established polices which inferred communist party membership for anyone who invoked the fifth amendment. The national ACLU was divided on whether to defend employees who had been fired merely for pleading the fifth amendment, but the New York affiliate successfully assisted teacher Harry Slochower in his Supreme Court case which reversed his termination.
The fifth amendment issue became the catalyst for a watershed event in 1954, which finally resolved the ACLU's ambivalence by ousting the anti-communists from ACLU leadership. In 1953, the anti-communists, led by Norman Thomas and James Fly, proposed a set of resolutions that inferred guilt of persons that invoked the fifth amendment. These resolutions were the first that fell under the ACLU's new organizational rules permitting local affiliates to participate in the vote; the affiliates outvoted the national headquarters, and rejected the anti-communist resolutions. Anti-communists leaders refused to accept the results of the vote, and brought the issue up for discussion again at the 1954 bi-annual convention. ACLU member Frank Graham, president of the University of North Carolina, attacked the anti-communists with a counter-proposal, which stated that the ACLU "stand[s] against guilt by association, judgment by accusation, the invasion of privacy of personal opinions and beliefs, and the confusion of dissent with disloyalty." The anti-communists continued to battle Graham's proposal, but were outnumbered by the affiliates. The anti-communists finally gave up and departed the board of directors in late 1954 and 1955, ending an eight-year reign of ambivalence within the ACLU leadership ranks. Thereafter, the ACLU proceeded with firmer resolve against Cold War anti-communist legislation. The period from the 1940 resolution (and the purge of Elizabeth Flynn) to the 1954 resignation of the anti-communist leaders is considered by many to be an era in which the ACLU abandoned its core principles.
McCarthyism declined in late 1954 after television journalist Edward R. Murrow and others publicly chastised McCarthy. The controversies over the Bill of Rights that were generated by the Cold War ushered in a new era in American Civil liberties. In 1954 in "Brown v. Board of Education", the Supreme Court unanimously overturned state-sanctioned school segregation, and thereafter a flood of civil rights victories dominated the legal landscape.
The Supreme Court handed the ACLU two key victories in 1957, in "Watkins v. United States" and "Yates v. United States", both of which undermined the Smith Act and marked the beginning of the end of communist party membership inquiries. In 1965, the Supreme Court produced some decisions, including "Lamont v. Postmaster General" (in which the plaintiff was Corliss Lamont, a former ACLU board member), which upheld fifth amendment protections and brought an end to restrictions on political activity.
1960s.
The decade from 1954 to 1964 was the most successful period in the ACLU's history. Membership rose from 30,000 to 80,000, and by 1965 it had affiliates in seventeen states. During the ACLU's bi-annual conference in Colorado in 1964, the Supreme Court issued rulings on eight cases in which the ACLU was involved; the ACLU prevailed on seven of the eight. The ACLU played a role in Supreme Court decisions reducing censorship of literature and arts, protecting freedom of association, prohibiting racial segregation, excluding religion from public schools, and providing due process protection to criminal suspects. The ACLU's success arose from changing public attitudes; the American populace was more educated, more tolerant, and more willing to accept unorthodox behavior.
Separation of church and state.
Legal battles concerning the separation of church and state originated in laws dating to 1938 which required religious instruction in school, or provided state funding for religious schools. The Catholic church was a leading proponent of such laws; and the primary opponents (the "separationists") were the ACLU, Americans United for Separation of Church and State, and the American Jewish Congress. The ACLU led the challenge in the 1947 "Everson v. Board of Education" case, in which Justice Hugo Black wrote "[t]he First Amendment has erected a wall between church and state…. That wall must be kept high and impregnable." It was not clear that the Bill of Rights forbid state governments from supporting religious education, and strong legal arguments were made by religious proponents, arguing that the Supreme Court should not act as a "national school board", and that the Constitution did not govern social issues. However, the ACLU and other advocates of church/state separation persuaded the Court to declare such activities unconstitutional. Historian Samuel Walker writes that the ACLU's "greatest impact on American life" was its role in persuading the Supreme Court to "constitutionalize" so many public controversies.
In 1948, the ACLU prevailed in the "McCollum v. Board of Education" case, which challenged public school religious classes taught by clergy paid for from private funds. The ACLU also won cases challenging schools in New Mexico which were taught by clergy and had crucifixes hanging in the classrooms. In the 1960s, the ACLU, in response to member insistence, turned its attention to in-class promotion of religion. In 1960, 42 percent of American schools included Bible reading. In 1962, the ACLU published a policy statement condemning in-school prayers, observation of religious holidays, and Bible reading. The Supreme Court concurred with the ACLU's position, when it prohibited New York's in-school prayers in the 1962 "Engel v. Vitale" decision. Religious factions across the country rebelled against the anti-prayer decisions, leading them to propose the School Prayer Constitutional Amendment, which declared in-school prayer legal. The ACLU participated in a lobbying effort against the amendment, and the 1966 congressional vote on the amendment failed to obtain the required two-thirds majority.
However, not all cases were victories; ACLU lost cases in 1949 and 1961 which challenged state laws requiring commercial businesses to close on Sunday, the Christian Sabbath. The Supreme court has never overturned such laws, although some states subsequently revoked many of the laws under pressure from commercial interests.
Freedom of expression.
During the 1940s and 1950s, the ACLU continued its battle against censorship of art and literature. In 1948, the New York affiliate of the ACLU received mixed results from the Supreme Court, winning the appeal of Carl Jacob Kunz, who was convicted for speaking without a police permit, but losing the appeal of Irving Feiner who was arrested to prevent a breach of the peace, based on his oration denouncing president Truman and the American Legion. The ACLU lost the case of Joseph Beahharnais, who was arrested for group libel when he distributed literature impugning the character of African Americans.
Cities across America routinely banned movies because they were deemed to be "harmful", "offensive", or "immoral" – censorship which was validated by the 1915 "Mutual v. Ohio" Supreme Court decision which held movies to be mere commerce, undeserving of first amendment protection. The film "The Miracle" was banned in New York in 1951, at the behest of the Catholic Church, but the ACLU supported the film's distributor in an appeal of the ban, and won a major victory in the 1952 decision "Joseph Burstyn, Inc. v. Wilson". The Catholic Church led efforts throughout the 1950s attempting to persuade local prosecutors to ban various books and movies, leading to conflict with the ACLU when the ACLU published it statement condemning the church's tactics. Further legal actions by the ACLU successfully defended films such as "M" and "la Ronde", leading the eventual dismantling of movie censorship. Hollywood continued employing self-censorship with its own Production Code, but in 1956 the ACLU called on Hollywood to abolish the Code.
The ACLU defended beat generation artists, including Allen Ginsburg who was prosecuted for his poem "Howl"; and – in an unorthodox case – the ACLU helped a coffee house regain its restaurant license which was revoked because its Beat customers were allegedly disturbing the peace and quiet of the neighborhood.
The ACLU lost an important press censorship case when, in 1957, the Supreme Court upheld the obscenity conviction of publisher Samuel Roth for distributing adult magazines. As late as 1953, books such as "Tropic of Cancer" and "From Here to Eternity" were still banned. But public standards rapidly became more liberal though the 1960s, and obscenity was notoriously difficult to define, so by 1971 prosecutions for obscenity had halted.
Racial discrimination.
A major aspect of civil liberties progress after World War II was the undoing centuries of racism in federal, state, and local governments – an effort generally known as the Civil Rights Movement. Several civil liberties organizations worked together for progress, including the National Association for the Advancement of Colored People (NAACP), the ACLU, and the American Jewish Congress. The NAACP took primary responsibility for Supreme Court cases (often led by lead NAACP attorney Thurgood Marshall), with the ACLU focusing on police misconduct, and supporting the NAACP with amicus briefs. The NAACP achieved a key victory in 1950 with the "Henderson v. United States" decision that ended segregation in interstate bus and rail transportation.
In 1954, the ACLU filed an amicus brief in the case of "Brown v. Board of Education", which led to the ban on racial segregation in U.S. public schools. Southern states instituted a McCarthyism-style witch-hunt against the NAACP, attempting it to disclose membership lists. The ACLU's fight against racism was not limited to segregation; in 1964 the ACLU provided key support to plaintiffs, primarily lower income urban residents, in "Reynolds v. Sims", which required states to establish the voting districts in accordance with the "one person, one vote" principle.
Police misconduct.
The ACLU regularly tackled police misconduct issues, starting with the 1932 case "Powell v. Alabama" (right to an attorney), and including 1942's "Betts v. Brady" (right to an attorney), and 1951's "Rochin v. California" (involuntary stomach pumping). In the late 1940s, several ACLU local affiliates established permanent committees to address policing issues. During the 1950s and 1960s, the ACLU was responsible for substantially advancing the legal protections against police misconduct. The Philadelphia affiliate was responsible for causing the City of Philadelphia, in 1958, to create the nation's first civilian police review board. In 1959, the Illinois affiliate published the first report in the nation, "Secret Detention by the Chicago Police", which documented unlawful detention by police.
Some of the most well known ACLU successes came in the 1960s, when the ACLU prevailed in a string of cases limiting the power of police to gather evidence; in 1961's "Mapp v. Ohio", the Supreme court required states to obtain a warrant before searching a person's home. The "Gideon v. Wainwright" decision in 1963 provided legal representation to indigents. In 1964, the ACLU persuaded the Court, in "Escobedo v. Illinois", to permit suspects to have an attorney present during questioning. And, in 1966, the "Miranda v. Arizona" decision required police to notify suspects of their constitutional rights. Although many law enforcement officials criticized the ACLU for expanding the rights of suspects, police officers themselves took advantage of the ACLU. For example when the ACLU represented New York policemen in their lawsuit which objected to searches of their workplace lockers. In the late 1960s, civilian review boards in New York and Philadelphia were abolished, over the ACLU's objection.
Civil liberties revolution of the 1960s.
The 1960s was a tumultuous era in the United States, and public interest in civil liberties underwent an explosive growth. Civil liberties actions in the 1960s were often led by young people, and often employed tactics such as sit ins and marches. Protests were often peaceful, but sometimes employed militant tactics. The ACLU played a central role in all major civil liberties debates of the 1960s, including new fields such as gay rights, prisoner's rights, abortion, rights of the poor, and the death penalty. Membership in the ACLU increased from 52,000 at the beginning of the decade, to 104,000 in 1970. In 1960, there were affiliates in seven states, and by 1974 there were affiliates in 46 states. During the 1960s, the ACLU underwent a major transformation tactics; it shifted emphasis from legal appeals (generally involving amicus briefs submitted to the Supreme Court) to direct representation of defendants when they were initially arrested. At the same time, the ACLU transformed its style from "disengaged and elitist" to "emotionally engaged". The ACLU published a breakthrough document in 1963, titled "How Americans Protest", which was borne of frustration with the slow progress in battling racism, and which endorsed aggressive, even militant protest techniques.
African-American protests in the South accelerated in the early 1960s, and the ACLU assisted at every step. After four African-American college students staged a sit-in in a segregated North Carolina department store, the sit-in movement gained momentum across the United States. During 1960-61, the ACLU defended black students arrested for demonstrating in North Carolina, Florida, and Louisiana. The ACLU also provided legal help for the Freedom Rides in 1961, the integration of the University of Mississippi, the 1963 protests in Birmingham, Alabama, and the 1964 Freedom Summer.
The NAACP was responsible for managing most sit-in related cases that made it to the Supreme Court, winning nearly every decision. But it fell to the ACLU and other legal volunteer efforts to provide legal representation to hundreds of protestors – white and black – who were arrested while protesting in the South. The ACLU joined with other civil liberties groups to form the Lawyers Constitutional Defense Committee (LCDC) which subsequently provided legal representation to many of the protesters. The ACLU provided the majority of the funding for the LCDC.
In 1964, the ACLU opened up a major office in Atlanta, Georgia, dedicated to serving Southern issues. Much of the ACLU's progress in the South was due to Charles Morgan, Jr., the charismatic leader of the Atlanta office. He was responsible for desegregating juries ("Whitus v. Georgia"), desegregating prisons ("Lee v. Washington"), and reforming election laws. The ACLU's southern office also defended African-American congressman Julian Bond in "Bond v. Floyd", when the Georgia congress refused to formally induct Bond into the legislature. Another widely publicized case defended by Morgan was that of Army doctor Howard Levy, who was convicted of refusing to train Green Berets. Despite raising the defense that the Green Berets were committing war crimes in Vietnam, Levy lost on appeal in "Parker v. Levy", 417 U.S. 733 (1974).
In 1969, the ACLU won a major victory for free speech, when it defended Dick Gregory after he was arrested for peacefully protesting against the mayor of Chicago. The court ruled in "Gregory v. Chicago" that a speaker cannot be arrested for disturbing the peace when the hostility is initiated by someone in the audience, as that would amount to a "heckler's veto".
Vietnam war.
The ACLU was at the center of several legal aspects of the Vietnam war: defending draft resisters, challenging the constitutionality of the war, the potential impeachment of Richard Nixon, and the use of national security concerns to preemptively censor newspapers.
David J. Miller was the first person prosecuted for burning his draft card. The New York affiliate of the ACLU appealed his 1965 conviction (367 F.2d 72: "United States of America v. David J. Miller", 1966), but the Supreme Court refused to hear the appeal. Two years later, the Massachusetts affiliate took the card-burning case of David O'Brien to the Supreme court, arguing that the act of burning was a form of symbolic speech, but the Supreme Court upheld the conviction in "United States v. O'Brien", 391 US 367 (1968). Thirteen-year-old Junior High student Mary Tinker wore a black armband to school in 1965 to object to the war, and was suspended from school. The ACLU appealed her case to the Supreme Court and won a victory in "Tinker v. Des Moines Independent Community School District". This critical case established that the government may not establish "enclaves" such as schools or prisons where all rights are forfeit.
The ACLU defended Sydney Street, who was arrested for burning an American flag to protest the reported assassination of civil rights leader James Meredith. In the "Street v. New York" decision, the court agreed with the ACLU that encouraging the country to abandon one of its national symbols was constitutionally protected form of expression. The ACLU successfully defended Paul Cohen, who was arrested for wearing a jacket with the words "fuck the draft" on its back, while he walked through the Los Angeles courthouse. The Supreme Court, in "Cohen v. California", held that the vulgarity of the wording was essential to convey the intensity of the message.
Non-war related free speech rights were also advanced during the Vietnam war era; in 1969, the ACLU defended a Ku Klux Klan member who advocated long-term violence against the government, and the Supreme Court concurred with the ACLU's argument in the landmark decision "Brandenburg v. Ohio", which held that only speech which advocated "imminent" violence could be outlawed.
A major crisis gripped the ACLU in 1968 when a debate erupted over whether to defend Benjamin Spock and the Boston Five against federal charges that they encouraged draftees to avoid the draft. The ACLU board was deeply split over whether to defend the activists; half the board harbored anti-war sentiments, and felt that the ACLU should lend its resources to the cause of the Boston Five. The other half of the board believed that civil liberties were not at stake, and the ACLU would be taking a political stance. Behind the debate was the longstanding ACLU tradition that it was politically impartial, and provided legal advice without regard to the political views of the defendants. The board finally agreed to a compromise solution that permitted the ACLU to defend the anti-war activists, without endorsing the activist's political views. Some critics of the ACLU suggest that the ACLU became a partisan political organization following the Spock case. After the Kent State shootings in 1970, ACLU leaders took another step towards politics by passing a resolution condemning the Vietnam war. The resolution was based in a variety of legal arguments, including civil liberties violations and a claim that the war was illegal.
Also in 1968, the ACLU held an internal symposium to discuss its dual roles: providing "direct" legal support (defense for accused in their initial trial, benefiting only the individual defendant), and appellate support (providing amicus briefs during the appeal process, to establish widespread legal precedent). Historically, the ACLU was known for its appellate work which led to landmark Supreme Court decisions, but by 1968, 90% of the ACLU's legal activities involved direct representation. The symposium concluded that both roles were valid for the ACLU.
1970s and 1980s.
Watergate era.
The ACLU supported "The New York Times" in its 1971 suit against the government, requesting permission to publish the Pentagon papers. The court upheld the "Times" and ACLU in the "New York Times Co. v. United States" ruling, which held that the government could not preemptively prohibit the publication of classified information and had to wait until after it was published to take action.
As the Watergate saga unfolded, the ACLU became the first national organization to call for Nixon's impeachment. This, following the resolution opposing the Vietnam war, was a second major decision that caused critics of the ACLU, particularly conservatives, to claim that the ACLU had evolved into a liberal political organization.
Enclaves and new civil liberties.
The decade from 1965 to 1975 saw an expansion of the field of civil liberties. Administratively, the ACLU responded by appointing Aryeh Neier to take over from Pemberton as Executive Director in 1970. Neier embarked on an ambitious program to expand the ACLU; he created the ACLU Foundation to raise funds, and he created several new programs to focus the ACLU's legal efforts. By 1974, ACLU membership had reached 275,000.
During those years, the ACLU led the way in expanding legal rights in three directions: new rights for persons within government-run "enclaves", new rights for victim groups, and privacy rights for mainstream citizens. At the same time, the organization grew substantially. The ACLU helped develop the field of constitutional law that governs "enclaves", which are groups of persons that live in conditions under government control. Enclaves include mental hospital patients, members of the military, and prisoners, and students (while at school). The term enclave originated with Supreme Court justice Abe Fortas's use of the phrase "schools may not be enclaves of totalitarianism" in the "Tinker v. Des Moines" decision.
The ACLU initiated the legal field of student's rights with the "Tinker v. Des Moines" case, and expanded it with cases such as "Goss v. Lopez" which required schools to provide students an opportunity to appeal suspensions.
As early as 1945, the ACLU had taken a stand to protect the rights of the mentally ill, when it drafted a model statute governing mental commitments. In the 1960s, the ACLU opposed involuntary commitments, unless it could be demonstrated that the person was a danger to himself or the community. In the landmark 1975 "O'Connor v. Donaldson" decision the ACLU represented a non-violent mental health patient who had been confined against his will for 15 years, and persuaded the Supreme Court to rule such involuntary confinements illegal. The ACLU has also defended the rights of mentally ill individuals who are not dangerous, but who create disturbances. The New York chapter of the ACLU defended Billie Boggs, a mentally ill woman who exposed herself and defecated and urinated in public.
Prior to 1960, prisoners had virtually no recourse to the court system, because courts considered prisoners to have no civil rights. That changed in the late 1950s, when the ACLU began representing prisoners that were subject to police brutality, or deprived of religious reading material. In 1968, the ACLU successfully sued to desegregate the Alabama prison system; and in 1969, the New York affiliate adopted a project to represent prisoners in New York prisons. Private attorney Phil Hirschkop discovered degrading conditions in Virginia prisons following the Virginia State Penitentiary strike, and won an important victory in 1971's "Landman v. Royster" which prohibited Virginia from treating prisoners in inhumane ways. In 1972, the ACLU consolidated several prison rights efforts across the nation and created the National Prison Project. The ACLU's efforts led to landmark cases such as "Ruiz v. Estelle" (requiring reform of the Texas prison system) and in 1996 U.S. Congress enacted the Prison Litigation Reform Act (PLRA) which codified prisoners' rights.
Victim groups.
The ACLU, during the 1960s and 1970s, expanded its scope to include what it referred to as "victim groups", namely women, the poor, and homosexuals. Heeding the call of female members, the ACLU endorsed the Equal Rights Amendment in 1970 and created the Women's Rights Project in 1971. The Women's Rights Project dominated the legal field, handling more than twice as many cases as the National Organization for Women, including breakthrough cases such as "Reed v. Reed", "Frontiero v. Richardson", and " Taylor v. Louisiana".
ACLU leader Harriet Pilpel raised the issue of the rights of homosexuals in 1964, and two years later the ACLU formally endorsed gay rights. In 1973 the ACLU created the Sexual Privacy Project (later the Gay and Lesbian Rights Project) which combated discrimination against homosexuals. This support continues even today. After then-Senator Larry Craig was arrested for soliciting sex in a public bathroom, the ACLU wrote an amicus brief for Craig, saying that sex between consenting adults in public places was protected under privacy rights.
Rights of the poor was another area that was expanded by the ACLU. In 1966 and again in 1968, activists within the ACLU encouraged the organization to adopt a policy overhauling the welfare system, and guaranteeing low-income families a baseline income; but the ACLU board did not approve the proposals. The ACLU played a key role in the 1968 "King v. Smith" decision, where the Supreme Court ruled that welfare benefits for children could not be denied by a state simply because the mother cohabited with a boyfriend.
Privacy.
The right to privacy is not explicitly identified in the U.S. Constitution, but the ACLU led the charge to establish such rights in the indecisive 1961 "Poe v. Ullman" case, which addressed a state statute outlawing contraception. The issue arose again in "Griswold v. Connecticut" (1965), and this time the Supreme Court adopted the ACLUs position, and formally declared a right to privacy. The New York affiliate of the ACLU pushed to eliminate anti-abortion laws starting in 1964, a year before "Griswold" was decided, and in 1967 the ACLU itself formally adopted the right to abortion as a policy. The ACLU led the defense in "United States v. Vuitch" which expanded the right of physicians to determine when abortions were necessary. These efforts culminated in one of the most controversial Supreme Court decisions of all time, "Roe v. Wade", which legalized abortion in the first three months of pregnancy. The ACLU successfully argued against state bans on interracial marriage, in the case of "Loving v. Virginia" (1967).
Related to privacy, the ACLU engaged in several battles to ensure that government records about individuals were kept private, and to give individuals the right to review their records. The ACLU supported several measures, including the 1970 Fair Credit Reporting Act required credit agencies to divulge credit information to individuals; the 1973 Family Educational Rights and Privacy Act, which provided students the right to access their records; and the 1974 Privacy Act which prevented the federal government from disclosing personal information without good cause.
Allegations of bias.
In the early 1970s, conservatives and libertarians began to criticize the ACLU for being too political and too liberal. Legal scholar Joseph W. Bishop wrote that the ACLU's trend to partisanship started with its defense of Dr. Spock's anti-war protests. Critics also blamed the ACLU for encouraging the Supreme Court to embrace judicial activism. Critics claimed that the ACLU's support of controversial decisions like "Roe v. Wade" and "Griswold v. Connecticut" violated the intention of the authors of the Bill of Rights. The ACLU became an issue in the 1988 presidential campaign, when Republican candidate George H. W. Bush accused Democratic candidate Michael Dukakis (a member of the ACLU) of being a "card carrying member of the ACLU".
The Skokie case.
It is the policy of the ACLU to support the civil liberties of defendants regardless of their ideological stance. The ACLU takes pride in defending individuals with unpopular or bigoted viewpoints, such as George Wallace, George Lincoln Rockwell, and KKK members. The ACLU has defended American Nazis many times, and their actions often brought protests, particularly from American Jews.
In 1977, a small group of American Nazis, led by Frank Collin, applied to the town of Skokie, Illinois for permission to hold a demonstration in the town park. Skokie at the time had a majority population of Jews, totaling 40,000 of 70,000 citizens, some of whom were survivors of Nazi concentration camps. Skokie refused to grant permission, and an Illinois judge supported Skokie and prohibited the demonstration. Skokie immediately passed three ordinances aimed at preventing the group from meeting in Skokie. The ACLU assisted Collin and appealed to federal court. The appeal dragged on for a year, and the ACLU eventually prevailed in "Smith v. Collin", 447 F.Supp. 676.
The Skokie case was heavily publicized across America, partially because Jewish groups such as the Jewish Defense League and Anti Defamation League strenuously objected to the demonstration, leading many members of the ACLU to cancel their memberships. The Illinois affiliate of the ACLU lost about 25% of its membership and nearly one-third of its budget. The financial strain from the controversy led to layoffs at local chapters. After the membership crisis died down, the ACLU sent out a fund-raising appeal which explained their rationale for the Skokie case, and raised over $500,000 ($ in 2015 dollars).
Reagan era.
The inauguration of Ronald Reagan as president in 1981, ushered in an eight-year period of conservative leadership in the U.S. government. Under his leadership, the government pushed a conservative social agenda, including outlawing abortion, inserting prayer in schools, banning pornography, and resisting gay rights.
Fifty years after the Scopes trial, the ACLU found itself fighting another classroom case, the Arkansas 1981 creationism statute, which required schools to teach the biblical account of creation as a scientific alternative to evolution. The ACLU won the case in the "McLean v. Arkansas" decision.
In 1982, the ACLU became involved in a case involving the distribution of child pornography ("New York v. Ferber"). In an amicus brief, the ACLU argued that child pornography that violates the three prong obscenity test should be outlawed, but that the law in question was overly restrictive because it outlawed artistic displays and otherwise non-obscene material. The court did not adopt the ACLU's position.
During the 1988 presidential election, Vice President George H. W. Bush noted that his opponent Massachusetts Governor Michael Dukakis had described himself as a "card-carrying member of the ACLU" and used that as evidence that Dukakis was "a strong, passionate liberal" and "out of the mainstream". The phrase subsequently was used by the organization in an advertising campaign.
In 1990 the ACLU defended Lieutenant Colonel Oliver North, whose conviction was tainted by coerced testimony – a violation of his fifth amendment rights – during the Iran–Contra affair, where Oliver North was involved in illegal weapons sales to Iran in order to illegally fund the Contra guerillas.
Modern era.
1990 to 2000.
In 1997, ruling unanimously in the case of "Reno v. American Civil Liberties Union", the Supreme Court voted down anti-indecency provisions of the Communications Decency Act (the CDA), finding they violated the freedom of speech provisions of the First Amendment. In their decision, the Supreme Court held that the CDA's "use of the undefined terms 'indecent' and 'patently offensive' will provoke uncertainty among speakers about how the two standards relate to each other and just what they mean."
The ACLU's position on spam is considered controversial by a broad cross-section of political points of view. In 2000, Marvin Johnson, a legislative counsel for the ACLU, stated that proposed anti-spam legislation infringed on free speech by denying anonymity and by forcing spam to be labeled as such, "Standardized labeling is compelled speech." He also stated, "It's relatively simple to click and delete." The debate found the ACLU joining with the Direct Marketing Association and the Center for Democracy and Technology in criticizing a bipartisan bill in the House of Representatives in 2000. As early as 1997 the ACLU had taken a strong position that nearly all spam legislation was improper, although it has supported "opt-out" requirements in some cases. The ACLU opposed the 2003 CAN-SPAM act suggesting that it could have a chilling effect on speech in cyberspace.
In November 2000, 15 African-American residents of Hearne, Texas, were indicted on drug charges after being arrested in a series of "drug sweeps". The ACLU filed a class action lawsuit, "Kelly v. Paschall", on their behalf, alleging that the arrests were unlawful. The ACLU contended that 15 percent of Hearne's male African American population aged 18 to 34 were arrested based on the "uncorroborated word of a single unreliable confidential informant coerced by police to make cases." On May 11, 2005, the ACLU and Robertson County announced a confidential settlement of the lawsuit, an outcome which "both sides stated that they were satisfied with." The District Attorney dismissed the charges against the plaintiffs of the suit. The 2009 film American Violet depicts this case.
In 2000, the ACLU's Massachusetts affiliate represented the North American Man Boy Love Association (NAMBLA), on first amendment grounds, in the "Curley v. NAMBLA" wrongful death civil suit that was based solely on the fact that a man who raped and murdered a child had visited the NAMBLA website. Also In 2000, the ACLU lost the "Boy Scouts of America v. Dale" case, which had asked the Supreme Court to require the Boy Scouts of America to drop their policy of prohibiting homosexuals from becoming Boy Scout leaders.
Twenty-first century.
In March 2004, the ACLU, along with Lambda Legal and the National Center for Lesbian Rights, sued the state of California on behalf of six same-sex couples who were denied marriage licenses. That case, "Woo v. Lockyer", was eventually consolidated into "In re Marriage Cases", the California Supreme Court case which led to same-sex marriage being available in that state from June 16, 2008 until Proposition 8 was passed on November 4, 2008.
During the 2004 trial regarding allegations of Rush Limbaugh's drug abuse, the ACLU argued that his privacy should not have been compromised by allowing law enforcement examination of his medical records. In June 2004, the school district in Dover, Pennsylvania, required that its high school biology students listen to a statement which asserted that the theory of evolution is not fact and mentioning intelligent design as an alternative theory. Several parents called the ACLU to complain, because they believed that the school was promoting a religious idea in the classroom and violating the Establishment Clause of the First Amendment. The ACLU, joined by Americans United for Separation of Church and State, represented the parents in a lawsuit against the school district. After a lengthy trial, Judge John E. Jones III ruled in favor of the parents in the "Kitzmiller v. Dover Area School District" decision, finding that intelligent design is not science and permanently forbidding the Dover school system from teaching intelligent design in science classes.
In April 2006, Edward Jones and the ACLU sued the City of Los Angeles, on behalf of Robert Lee Purrie and five other homeless people, for the city's violation of the 8th and 14th Amendments to the U.S. Constitution, and Article I, sections 7 and 17 of the California Constitution (supporting due process and equal protection, and prohibiting cruel and unusual punishment). The Court ruled in favor of the ACLU, stating that, "the LAPD cannot arrest people for sitting, lying, or sleeping on public sidewalks in Skid Row." Enforcement of section 41.18(d) 24 hours a day against persons who have nowhere else to sit, lie, or sleep, other than on public streets and sidewalks, is breaking these amendments. The Court said that the anti-camping ordinance is "one of the most restrictive municipal laws regulating public spaces in the United States". Jones and the ACLU wanted a compromise in which the LAPD is barred from enforcing section 41.18(d) (arrest, seizure, and imprisonment) in Skid Row between the hours of 9:00 p.m. and 6:30 a.m. The compromise plan permits the homeless to sleep on the sidewalk, provided they are not "within 10 feet of any business or residential entrance" and only between these hours. One of the motivations for the compromise is the shortage of space in the prison system. Downtown development business interests and the Central City Association (CCA) were against the compromise. Police Chief William Bratton said the case had slowed the police effort to fight crime and clean up Skid Row, and that when he was allowed to clean up Skid Row, real estate profited. On September 20, 2006, the Los Angeles City Council voted to reject the compromise. On October 3, 2006, police arrested Skid Row's transients for sleeping on the streets for the first time in months.
In 2006, the ACLU of Washington State joined with a pro-gun rights organization, the Second Amendment Foundation, and prevailed in a lawsuit against the North Central Regional Library District (NCRL) in Washington for its policy of refusing to disable restrictions upon an adult patron's request. Library patrons attempting to access pro-gun web sites were blocked, and the library refused to remove the blocks. In 2012, the ACLU sued the same library system for refusing to temporarily, at the request of an adult patron, disable Internet filters which blocked access to Google Images.
In 2006, the ACLU challenged a Missouri law that prohibited picketing outside of veterans' funerals. The suit was filed in support of the Westboro Baptist Church and Shirley Phelps-Roper, who were threatened with arrest. The Westboro Baptist Church is well known for their picket signs that contain messages such as, "God Hates Fags", "Thank God for Dead Soldiers" and "Thank God for 9/11". The ACLU issued a statement calling the legislation a "law that infringes on Shirley Phelps-Roper's rights to religious liberty and free speech". The ACLU prevailed in the lawsuit. In 2008, the ACLU was part of a consortium of legal advocates, including Lambda Legal and the National Center for Lesbian Rights, that challenged California's Proposition 8, which declared same-sex marriages illegal. The ACLU and its allies prevailed.
In light of the Supreme Court's "Heller" decision recognizing that the Constitution protects an individual right to bear arms, ACLU of Nevada took a position of supporting "the individual's right to bear arms subject to constitutionally permissible regulations" and pledged to "defend this right as it defends other constitutional rights". Since 2008, the ACLU has increasingly assisted gun owners recover firearms that have been seized illegally by law enforcement.
In 2009, the ACLU filed an amicus brief in "Citizens United v. FEC", arguing that the Bipartisan Campaign Reform Act of 2002 violated the First Amendment right to free speech by curtailing political speech. This stance on the landmark "Citizens United" case caused considerable disagreement within the organization, resulting in a discussion about its future stance during a quarterly board meeting in 2010. On March 27, 2012, the ACLU reaffirmed its stance in support of the Supreme Court's "Citizens United" ruling, at the same time voicing support for expanded public financing of election campaigns and stating the organization would firmly oppose any future constitutional amendment limiting free speech.
In 2010 the ACLU of Illinois was inducted into the Chicago Gay and Lesbian Hall of Fame as a Friend of the Community.
In 2011 the ACLU started its Don't Filter Me project, countering LGBT-related Internet censorship in public schools in the United States.
On January 7, 2013, the ACLU reached a settlement with the federal government in "Collins v. United States" that provided for the payment of full separation pay to servicemembers discharged under "don't ask, don't tell" since November 10, 2004, who had previously been granted only half that. Some 181 were expected to receive about $13,000 each.
Anti-terrorism issues.
After the September 11, 2001 attacks, the federal government instituted a broad range of new measures to combat terrorism, including the passage of the USA PATRIOT Act. The ACLU challenged many of the measures, claiming that they violated rights regarding due process, privacy, illegal searches, and cruel and unusual punishment. An ACLU policy statement states:Our way forward lies in decisively turning our backs on the policies and practices that violate our greatest strength: our Constitution and the commitment it embodies to the rule of law. Liberty and security do not compete in a zero-sum game; our freedoms are the very foundation of our strength and security. The ACLU's National Security Project advocates for national security policies that are consistent with the Constitution, the rule of law, and fundamental human rights. The Project litigates cases relating to detention, torture, discrimination, surveillance, censorship, and secrecy.
During the ensuing debate regarding the proper balance of civil liberties and security, the membership of the ACLU increased by 20%, bringing the group's total enrollment to 330,000. The growth continued, and by August 2008 ACLU membership was greater than 500,000. It remained at that level through 2011.
The ACLU has been a vocal opponent of the USA PATRIOT Act of 2001, the PATRIOT 2 Act of 2003, and associated legislation made in response to the threat of domestic terrorism. In response to a requirement of the USA PATRIOT Act, the ACLU withdrew from the Combined Federal Campaign charity drive. The campaign imposed a requirement that ACLU employees must be checked against a federal anti-terrorism watch list. The ACLU has stated that it would "reject $500,000 in contributions from private individuals rather than submit to a government 'blacklist' policy."
In 2004, the ACLU sued the federal government in "American Civil Liberties Union v. Ashcroft" on behalf of Nicholas Merrill, owner of an Internet service provider. Under the provisions of the Patriot Act, the government had issued national security letters to Merrill to compel him to provide private Internet access information from some of his customers. In addition, the government placed a gag order on Merrill, forbidding him from discussing the matter with anyone.
In January 2006, the ACLU filed a lawsuit, "ACLU v. NSA", in a federal district court in Michigan, challenging government spying in the NSA warrantless surveillance controversy. On August 17, 2006, that court ruled that the warrantless wiretapping program is unconstitutional and ordered it ended immediately. However, the order was stayed pending an appeal. The Bush administration did suspend the program while the appeal was being heard. In February 2008, the U.S. Supreme Court turned down an appeal from the ACLU to let it pursue a lawsuit against the program that began shortly after the September 11 terror attacks.
The ACLU and other organizations also filed separate lawsuits around the country against telecommunications companies. The ACLU filed a lawsuit in Illinois ("Terkel v. AT&T") which was dismissed because of the state secrets privilege and two others in California requesting injunctions against AT&T and Verizon. On August 10, 2006, the lawsuits against the telecommunications companies were transferred to a federal judge in San Francisco.
The ACLU represents a Muslim-American who was detained but never accused of a crime in "Ashcroft v. al-Kidd", a civil suit against former Attorney General John Ashcroft. In January 2010, the American military released the names of 645 detainees held at the Bagram Theater Internment Facility in Afghanistan, modifying its long-held position against publicizing such information. This list was prompted by a Freedom of Information Act lawsuit filed in September 2009 by the ACLU, whose lawyers had also requested detailed information about conditions, rules and regulations.
The ACLU has also criticized targeted killings of American citizens who fight against the United States. In 2011 the ACLU criticized the killing of radical Muslim cleric Anwar al-Awlaki on the basis that it was a violation of his Fifth Amendment right not to be deprived of life, liberty, or property without due process of law.

</doc>
<doc id="1955" url="http://en.wikipedia.org/wiki?curid=1955" title="Adobe Systems">
Adobe Systems

Adobe Systems Incorporated is an American multinational computer software company. The company is headquartered in San Jose, California, United States. Adobe has historically focused upon the creation of multimedia and creativity software products, with a more-recent foray towards rich Internet application software development. It is best known for Photoshop, the Portable Document Format (PDF) and Adobe Creative Suite, as well as its successor Adobe Creative Cloud.
Adobe was founded in February 1982 by John Warnock and Charles Geschke, who established the company after leaving Xerox PARC in order to develop and sell the PostScript page description language. In 1985, Apple Computer licensed PostScript for use in its LaserWriter printers, which helped spark the desktop publishing revolution.
s of 2015[ [update]], Adobe Systems has about 13,500 employees, about 40% of whom work in San Jose. Adobe also has major development operations in Newton, Massachusetts; New York City, New York; Orlando, Florida; Minneapolis, Minnesota; Lehi, Utah; Seattle, Washington; San Francisco and San Luis Obispo, California in the United States.
History.
The name of the company, "Adobe", comes from Adobe Creek in Los Altos, California, which ran behind the houses of both of the company's founders. Adobe's corporate logo features a stylized "A" and was designed by Marva Warnock, wife of John Warnock, who is also a graphic designer.
Adobe's first products after PostScript were digital fonts, which they released in a proprietary format called Type 1. Apple subsequently developed a competing standard, TrueType, which provided full scalability and precise control of the pixel pattern created by the font's outlines, and licensed it to Microsoft. Adobe responded by publishing the Type 1 specification and releasing Adobe Type Manager, software that allowed WYSIWYG scaling of Type 1 fonts on screen - like TrueType without the precise pixel-level control. But these moves were too late to stop the rise of TrueType. Although Type 1 remained the standard in the graphics/publishing market, TrueType became the standard for business and the average Windows user. In 1996, Adobe and Microsoft announced the OpenType font format, and in 2003 Adobe completed converting its Type 1 font library to OpenType.
In the mid-1980s, Adobe entered the consumer software market with Adobe Illustrator, a vector-based drawing program for the Apple Macintosh. Illustrator, which grew from the firm's in-house font-development software, helped popularize PostScript-enabled laser printers. Unlike MacDraw, the then standard Macintosh vector drawing program, Illustrator described shapes with more flexible Bézier curves, providing unprecedented accuracy. Font rendering in Illustrator, however, was left to the Macintosh's QuickDraw libraries and would not be superseded by a PostScript-like approach until Adobe released Adobe Type Manager.
Adobe Systems entered NASDAQ in 1986. Its revenue has grown from roughly $1 billion in 1999 to roughly $4 billion in 2012. Adobe's fiscal years run from December to November. For example, the 2007 fiscal year ended on November 30, 2007.
In 1989, Adobe introduced what was to become its flagship product, a graphics editing program for the Macintosh called Photoshop. Stable and full-featured, Photoshop 1.0 was ably marketed by Adobe and soon dominated the market.
In 1993, Adobe introduced PDF, the Portable Document Format, and its Adobe Acrobat and Reader software. PDF is now an International Standard: ISO 32000-1:2008. The technology is adopted worldwide as a common medium for electronic documents.
In December 1991, Adobe released Adobe Premiere, which Adobe rebranded to Adobe Premiere Pro in 2003. In 1994, Adobe acquired Aldus and added Adobe PageMaker and Adobe After Effects to its product line later in the year; it also controls the TIFF file format. In 1995, Adobe added Adobe FrameMaker, the long-document DTP application, to its product line after Adobe acquired Frame Technology Corp. In 1996, Adobe Systems Inc added Ares Software Corp. In 1999, Adobe introduced Adobe InCopy as a direct competitor to QuarkCopyDesk.
In 1992, Adobe acquired OCR Systems, Inc.; in 1994, the company acquired Aldus Corporation. On May 30, 1997, Adobe reincorporated in Delaware by merging with and into Adobe Systems (Delaware), which had incorporated on May 9, 1997. Adobe Systems Incorporated (Delaware), the surviving corporation, changed its name to Adobe Systems Incorporated concurrently with the merger.
The company acquired GoLive Systems, Inc. and released Adobe GoLive in 1999 and began shipping Adobe InDesign as a direct competitor to QuarkXPress and as an eventual replacement for PageMaker. In May 2003, Adobe acquired Syntrillium Software, adding Adobe Audition to its product line. In December 2004, French company OKYZ S.A., makers of 3D collaboration software, was acquired. This acquisition added 3D technology and expertise to the Adobe Intelligent Document Platform.
On December 12, 2005, Adobe acquired its main rival Macromedia in a stock swap valued at about $3.4 billion, adding Adobe ColdFusion, Adobe Contribute, Adobe Captivate, Adobe Acrobat Connect (formerly Macromedia Breeze), Adobe Director, Adobe Dreamweaver, Adobe Fireworks, Adobe Flash, FlashPaper, Adobe Flex, Macromedia FreeHand, Macromedia HomeSite, Macromedia JRun, Adobe Presenter, and Macromedia Authorware to Adobe's product line.
On November 12, 2007, CEO, Bruce Chizen resigned. Effective December 1, he was replaced by Shantanu Narayen, Adobe's current president and Chief Operating Officer. Bruce Chizen served out his term on Adobe's Board of Directors, and then continued in a strategic advisory role until the end of Adobe's 2008 fiscal year.
Adobe released Adobe Media Player in April 2008. On April 27, Adobe discontinued development and sales of its older HTML/web development software, GoLive in favor of Dreamweaver. Adobe offered a discount on Dreamweaver for GoLive users and supports those who still use GoLive with online tutorials and migration assistance. On June 1, Adobe launched Acrobat.com, a series of web applications geared for collaborative work. Creative Suite 4, which includes Design, Web, Production Premium and Master Collection came out in October 2008 in six configurations at prices from about USD $1,700 to $2,500 or by individual application. The Windows version of Photoshop includes 64-bit processing. On December 3, 2008, Adobe laid off 600 of its employees (8% of the worldwide staff) citing the weak economic environment.
Adobe announced two acquisitions in 2009: on August 29, it purchased Business Catalyst, and on September 15, it bought Omniture. On November 10, the company laid off 680 employees. Adobe announced it was investigating a "coordinated attack" against corporate network systems in China, managed by the company.
Adobe's 2010 was marked by continuing front-and-back arguments with Apple over the latter's non-support for Adobe Flash on its iPhone, iPad and other products. Apple CEO Steve Jobs claimed that Flash was not reliable or secure enough, while Adobe executives have argued that Apple wish to maintain control over the iOS platform. In April 2010, Steve Jobs published a post titled "Thoughts on Flash" where he outlined his thoughts on Adobe Flash and the rise of HTML 5.
In July 2010, Adobe bought Day Software integrating their line of CQ Products: WCM, DAM, SOCO, and Mobile
In January 2011, Adobe acquired DemDex, Inc with the intent of adding DemDex's audience-optimization software to its online marketing suite. At Photoshop World 2011, Adobe unveiled a new mobile photo service. Carousel is a new application for iPhone, iPad and Mac that uses Photoshop Lightroom technology for users to adjust and fine-tune images on all platforms. Carousel will also allow users to automatically sync, share and browse photos. The service was later renamed to "Adobe Revel". On November 9, 2011, Adobe announced that they would cease development of Flash for mobile devices following version 11.1. Instead they will be focusing on HTML 5 for mobile devices. On December 1, 2011, Adobe announced that it has entered into a definitive agreement to acquire privately held Efficient Frontier.
In December 2012, Adobe opened a new 280,000 square foot corporate campus in Lehi, UT.
In 2013 Adobe Systems endured a major security breach. Vast portions of the source code for the company's software were stolen and posted online and over 150 million records of Adobe's customers have been made readily available for download. In 2012, about 40 million sets of payment card information were compromised by a hack of Adobe.
A class-action lawsuit alleging that the company suppressed employee compensation was filed against Adobe, and three other Silicon Valley-based companies in a California federal district court in 2013. In May 2014 it was revealed the four companies, Adobe, Apple, Google, and Intel had reached agreement with the plaintiffs, 64,000 employees of the four companies, to pay a sum of $324.5 million to settle the suit.
Reception.
For years hackers have exploited vulnerabilities in Adobe programs, such as Adobe Reader, to gain unauthorized access to computers. Adobe's Flash Player has also been criticized for, among other things, suffering from performance, memory usage and security problems (see criticism of Flash Player). A report by security researchers from Kaspersky Lab criticized Adobe for producing the products having top 10 security vulnerabilities.
Observers noted that Adobe was spying on its customers by including spyware in the Creative Suite 3 software and quietly sending user data to a firm named Omniture. When users became aware, Adobe explained what the suspicious software did and admitted that they: "could and should do a better job taking security concerns into account". When a security flaw was later discovered in Photoshop CS5, Adobe sparked outrage by saying it would leave the flaw unpatched, so anyone who wanted to use the software securely would have to pay for an upgrade. Following a fierce backlash Adobe decided to provide the software patch.
Adobe has been criticized for pushing unwanted software - third-party browser toolbars and free virus scanners, usually as part of the Flash update process, and for pushing a third-party scareware program designed to scare users into paying for unneeded system repairs.
Awards.
Since 1995, "Fortune" has ranked Adobe as an outstanding place to work. Adobe was rated the 5th best U.S. company to work for in 2003, 6th in 2004, 31st in 2007, 40th in 2008, 11th in 2009, 42nd in 2010, 65th in 2011, 41st in 2012, and 83rd in 2013.
In May 2008, Adobe Systems India was ranked 19th of great places to work in India. In June 2014, it was ranked 6th of great places to work in India. In October 2008, Adobe Systems Canada Inc. was named one of "Canada's Top 100 Employers" by Mediacorp Canada Inc., and was featured in "Maclean's" newsmagazine.
Adobe ranked no. 5 on a list of "Internet’s 9 Most Hated Companies", based on a 2013 survey on Reddit.com. Adobe's Reader and Flash were listed on "The 10 most hated programs of all time" on TechRadar.com.
Pricing.
Adobe has been criticized for its pricing practices, with retail prices being as much as twice as high in non-US countries as in the US. As pointed out by many, it is significantly cheaper to pay for a return airfare ticket to the United States and purchase one particular collection of Adobe's software there than to buy it locally in Australia.
After Adobe revealed the pricing for the Creative Suite 3 Master Collection, which was £1,000 higher for European customers, a petition to protest over "unfair pricing" was published and signed by 10,000 users. In June 2009, Adobe further increased its prices in the UK by 10% in spite of weakening of the pound against the dollar, and UK users are not allowed to buy from the US store.
Source code and customer data breach.
On October 3, 2013, the company initially revealed that 2.9 million customers' sensitive and personal data was stolen in security breach which included encrypted credit card information. Adobe later admitted that 38 million active users have been affected and the attackers obtained access to their IDs and encrypted passwords, as well as to many inactive Adobe accounts. The company did not make it clear if all the personal information was encrypted, such as email addresses and physical addresses, though data privacy laws in 44 states require this information to be encrypted.
A 3.8 GB file stolen from Adobe and containing 152 million usernames, reversibly encrypted passwords and unencrypted password hints was posted on AnonNews.org. LastPass, a password security firm, said that Adobe failed to use best practices for securing the passwords and has not salted them. Another security firm, Sophos, showed that Adobe used a weak encryption method permitting the recovery of a lot of information with very little effort. According to an IT expert, Adobe has failed its customers and ‘should hang their heads in shame’.
Many of the credit cards were tied to the Creative Cloud software-by-subscription service. Adobe offered its affected US customers a free membership in a credit monitoring service, but no similar arrangements have been made for non-US customers. When a data breach occurs in the US, penalties depend on the state where the victim resides, not where the company is based.
After stealing the customers' data, cyber-thieves also accessed Adobe's source code repository, likely in mid-August 2013. Because hackers acquired copies of the source code of Adobe proprietary products, they could find and exploit any potential weaknesses in its security, computer experts warned. Security researcher Alex Holden, chief information security officer of Hold Security, characterized this Adobe breach, which affected Acrobat, ColdFusion and numerous other applications, as "one of the worst in US history". Adobe also announced that hackers stole parts of the source code of Photoshop, which according to commentators could allow programmers to copy its engineering techniques and would make it easier to pirate Adobe's expensive products.
On a server of a Russian-speaking hacker group, the "disclosure of encryption algorithms, other security schemes, and software vulnerabilities can be used to bypass protections for individual and corporate data" and may have opened the gateway to new generation zero-day attacks. Hackers already used ColdFusion exploits to make off with usernames and encrypted passwords of PR Newswire's customers, which has been tied to the Adobe security breach. They also used a ColdFusion exploit to breach Washington state court and expose up to 160,000 Social Security numbers.

</doc>
<doc id="1957" url="http://en.wikipedia.org/wiki?curid=1957" title="Alexander technique">
Alexander technique

The Alexander technique, named after Frederick Matthias Alexander, teaches people how to avoid unnecessary muscular and mental tension during their everyday activities. It is an educational process rather than a relaxation technique or form of exercise. Most other methods take it for granted that 'one's awareness of oneself' is accurate, whereas Alexander realized that a person who had been using himself wrongly for a long time could not trust his feelings (sensory appreciation) in carrying out any activity. Practitioners say that such problems are often caused by repeated misuse of the body over a long period of time, for example, by standing or sitting with one's weight unevenly distributed, holding one's head incorrectly, or walking or running inefficiently. The purpose of the Alexander technique is to help people unlearn maladaptive physical habits and return to a balanced state of rest and poise in which the body is well-aligned.
Alexander developed the technique's principles in the 1890s as a personal tool to alleviate breathing problems and hoarseness during public speaking. He credited the technique with allowing him to pursue his passion for Shakespearean acting.
History.
Frederick Matthias Alexander (1869-1955) was a Shakespearean orator who developed voice loss during his performances. After doctors found no physical cause, Alexander reasoned that he was doing something to himself while speaking to cause his problem. His self-observation in multiple mirrors revealed that he was contracting his whole body prior to phonation in preparation for all verbal response. He developed the hypothesis that this habitual pattern of pulling the head backwards and downwards needlessly disrupted the normal working of the total postural, breathing and vocal mechanisms. After experimenting to develop his ability to stop the unnecessary and habitual contracting in his neck, he found that his problem with recurrent voice loss was resolved. While on a recital tour in New Zealand (1895) he began to realise the wider significance of head carriage for overall physical functioning. Further, Alexander observed that many individuals commonly tightened the musculature of the upper torso as he had done, in anticipation of many other activities besides speech.
Alexander believed his work could be applied to improve individual health and well being. He further refined his technique of self-observation and re-training to teach his discoveries to others. He explained his reasoning in four books published in 1918, 1923, 1931 (1932 in the UK) and 1942. He also trained teachers to teach his work from 1930 until his death in 1955. Teacher training was interrupted during World War II between 1941 and 1943, when Alexander accompanied children and teachers of the Little School to Stow, Massachusetts to join his brother, A. R. Alexander, who also taught his brother's technique. In the 1960s, there was enough interest to start the first dedicated school, called The American Center for the Alexander Technique, in New York City.
Famous people who have studied the Alexander Technique include actors Michael Caine, John Cleese, Jamie Lee Curtis, Judi Dench, Suzanna Hamilton, William Hurt, Jeremy Irons, Hugh Jackman, Sir Ben Kingsley, Kevin Kline, Patti Lupone, Paul Newman, Mary Steenburgen, Hilary Swank, and Robin Williams; musicians Madonna, Paul McCartney, Yehudi Menuhin and Sting; playwright George Bernard Shaw; writers Roald Dahl, Robertson Davies, and Aldous Huxley; and Nobel Prize winner for medicine and physiology Nikolaas Tinbergen.
Process.
Alexander's approach emphasizes mindful action. The technique is applied dynamically to everyday movements, as well as actions selected by students.
Actions such as sitting, squatting, lunging or walking are often selected by the teacher. Other actions may be selected by the student, tailored to their interests or work activities such as hobbies, computer use, lifting, driving or performance in acting, sports, speech or music. Alexander teachers often use themselves as examples. They demonstrate, explain, and analyze a student's moment to moment responses as well as using mirrors, video feedback or classmate observations. Guided modelling with light hand contact is the primary tool for detecting and guiding the way past unnecessary effort. Suggestions for improvements are often student-specific.
Exercise as a teaching tool is deliberately omitted because of a common mistaken assumption that there exists a "correct" position. There are only two specific exercises practiced separately; the first is lying semi-supine; resting in this way uses "mechanical advantage" as a means of releasing cumulative muscular tension. This position is sometimes referred to as "constructive rest", or "the balanced resting state". It's also a specific time to practice Alexander's principle of conscious "directing" without "doing." The second exercise is the "Whispered Ah," which is used to co-ordinate and free breathing & vocal production.
Freedom, efficiency and patience are the prescribed values. Proscribed are unnecessary effort, self-limiting habits as well as mistaken perceptual assumptions. Students are led to change their largely automatic routines that are interpreted by the teacher to currently or cumulatively be physically limiting, inefficient or not in keeping with anatomical structure. The Alexander teacher provides verbal coaching while monitoring, guiding and preventing unnecessary habits at their source with a specialized hands-on assistance. This specialized hands-on requires Alexander teachers to demonstrate on themselves the improved physical co-ordination they are communicating to the student.
Alexander developed terminology to describe his methods, outlined in his four books that explain the sometimes paradoxical experience of learning and substituting new improvements.
Uses.
According to Alexander Technique instructor Michael J. Gelb, people tend to study the Alexander Technique either to rid themselves of pain, to increase their performance abilities, or for reasons of personal development and transformation.
As an example among performance-art applications, the Alexander technique is used and taught by classically trained vocal coaches and musicians. Its advocates claim that it allows for the free alignment of all aspects of the vocal tract by consciously increasing air-flow, allowing improved vocal technique and tone. Because the technique has allegedly been used to improve breathing and stamina in general, advocates also claim that athletes, people with asthma, tuberculosis, and panic attacks have also found improvements. The technique has been used by actors to reduce stage fright and to increase spontaneity. By improving stress-management, the technique can be an adjunct to psychotherapy for people with disabilities, Post-traumatic Stress Disorder, panic attacks, stuttering, and chronic pain.
Method.
The Alexander Technique is most commonly taught privately in a series of 10 to 40 private lessons which may last from 30 minutes to an hour. Students are often performers, such as actors, dancers, musicians, athletes and public speakers, or people who work on computers, or who are in frequent pain for other reasons. Instructors observe their students, then show them how to hold themselves and move with better poise and less strain. Sessions include chair work and table work, often in front of a mirror, during which the instructor and the student will stand, sit and lie down, moving efficiently while maintaining a correct relationship between the head, neck and spine.
To qualify as a teacher of Alexander Technique, instructors are required to complete at least 1,600 hours, spanning at least three years, of supervised teacher training. The result must be satisfactory to qualified peers to gain membership in professional societies.
Effectiveness.
The Alexander technique is cost-effective in the management of chronic pain.
There is moderate evidence that the technique helps reduce the disability associated with symptoms of Parkinson's disease.
Evidence suggests that Alexander technique lessons may help performance anxiety in musicians, but studies of the technique were inconclusive in improving music performance, respiratory function and the posture of musicians. 
No adequately designed clinical trials exist that allow evaluation of claims that the technique helps people with their asthma.
Influence.
The American philosopher and educator John Dewey became impressed with the Alexander technique after his headaches, neck pains, blurred vision, and stress symptoms largely improved during the time he used Alexander's advice to change his posture. In 1923, Dewey wrote the introduction to Alexander's "Constructive Conscious Control of the Individual".
Aldous Huxley had transformative lessons with Alexander, and continued doing so with other teachers after moving to the US. He rated Alexander's work highly enough to base the character of the doctor who saves the protagonist in 'Eyeless in Gaza' (an experimental form of autobiographical work) on F.M. Alexander, putting many of his phrases into the character's mouth. Huxley's work 'The Art of Seeing' also discusses his views on the technique.
Sir Stafford Cripps, George Bernard Shaw, Henry Irving and other stage grandees, Lord Lytton and other eminent people of the era also wrote positive appreciations of his work after taking lessons with Alexander.
Since Alexander's work in the field came at the start of the 20th century, his ideas influenced many originators in the field of mind-body improvement. Fritz Perls, who originated Gestalt therapy, credited Alexander as an inspiration for his psychological work. The Feldenkrais Method and the Mitzvah Technique were both influenced by the Alexander technique.

</doc>
<doc id="1960" url="http://en.wikipedia.org/wiki?curid=1960" title="Andrea Alciato">
Andrea Alciato

Andrea Alciato (8 May 1492 – 12 January 1550), commonly known as Alciati (Andreas Alciatus), was an Italian jurist and writer. He is regarded as the founder of the French school of legal humanists.
Biography.
Alciati was born in Alzate Brianza, near Milan, and settled in France in the early 16th century. He displayed great literary skill in his exposition of the laws, and was one of the first to interpret the civil law by the history, languages and literature of antiquity, and to substitute original research for the servile interpretations of the glossators. He published many legal works, and some annotations on Tacitus and accumulated a sylloge of Roman inscriptions from Milan and its territories, as part of his preparation for his history of Milan, written in 1504-05. 
Alciati is most famous for his "Emblemata," published in dozens of editions from 1531 onward. This collection of short Latin verse texts and accompanying woodcuts created an entire European genre, the emblem book, which attained enormous popularity in continental Europe and Great Britain.
Alciati died at Pavia in 1550.
Quotation.
Plenitudo potestatis nihil aliud est quam violentia.

</doc>
<doc id="1962" url="http://en.wikipedia.org/wiki?curid=1962" title="Apparent magnitude">
Apparent magnitude

The apparent magnitude (m) of a celestial body is a measure of its brightness as seen by an observer on Earth, adjusted to the value it would have in the absence of the atmosphere. The brighter the object appears, the lower the value of its magnitude. Generally the visible spectrum (vmag) is used as a basis for the apparent magnitude, but other regions of the spectrum, such as the near-infrared J-band, are also used. In the visible spectrum Sirius is the brightest star in the visible sky (excluding the Sun), whereas in the near-infrared J-band, Betelgeuse is the brightest.
History.
The scale used to indicate magnitude originates in the Hellenistic practice of dividing stars visible to the naked eye into six "magnitudes". The brightest stars in the night sky were said to be of first magnitude ("m" = 1), whereas the faintest were of sixth magnitude ("m" = 6), the limit of human visual perception (without the aid of a telescope). Each grade of magnitude was considered twice the brightness of the following grade (a logarithmic scale). although that ratio was subjective as no photodetectors existed. This rather crude scale for the brightness of stars was popularized by Ptolemy in his "Almagest", and is generally believed to originate with Hipparchus.
In 1856, Norman Robert Pogson formalized the system by defining a first magnitude star as a star that is 100 times as bright as a sixth-magnitude star, thereby establishing the logarithmic scale still in use today. This implies that a star of magnitude "m" is 2.512 times as bright as a star of magnitude "m+1". This figure, the fifth root of 100 became known as "Pogson's Ratio". The zero point of Pogson's scale was originally defined by assigning Polaris a magnitude of exactly 2. Astronomers later discovered that Polaris is slightly variable, so they switched to Vega as the standard reference star, assigning the brightness of Vega as the definition of zero magnitude at any specified wavelength.
Apart from small corrections, the brightness of Vega still serves as the definition of zero magnitude for visible and near infrared wavelengths, where its spectral energy distribution (SED) closely approximates that of a black body for a temperature of 11,000 K. However with the advent of infrared astronomy it was revealed that Vega's radiation includes an Infrared excess presumably due to a circumstellar disk consisting of dust at warm temperatures (but much cooler than the star's surface). At shorter (e.g. visible) wavelengths, there is negligible emission from dust at these temperatures. However in order to properly extend the magnitude scale further into the infrared, this peculiarity of Vega shouldn't affect the definition of the magnitude scale. Therefore the magnitude scale was extrapolated to "all" wavelengths on the basis of the black body radiation curve for an ideal stellar surface at 11,000 K uncontaminated by circumstellar radiation. On this basis the spectral irradiance (usually expressed in Janskys) for the zero magnitude point, as a function of wavelength can be computed (see ). Small deviations are specified between systems using measurement appartuses developed independently so that data obtained by different astronomers can be properly compared; of greater practical importance is the definition of magnitude not at a single wavelength but applying to the response of standard spectral filters used in photometry over various wavelength bands.
With the modern magnitude systems, brightness over a very wide range is specified according to the logarithmic definition detailed below, using this zero reference. In practice such apparent magnitudes do not exceed 30 (for detectable measurements). The brightness of Vega is exceeded by four stars in the night sky at visible wavelengths (and more at infrared wavelengths) as well as bright planets such as Venus, Mars, and Jupiter, and these must be described by "negative" magnitudes. For example, Sirius, the brightest star of the celestial sphere, has an apparent magnitude of −1.4 in the visible; negative magnitudes for other very bright astronomical objects can be found in the table below.
Calculations.
As the amount of light received actually depends on the thickness of the Earth's atmosphere in the line of sight to the object, the apparent magnitudes are adjusted to the value they would have in the absence of the atmosphere. The dimmer an object appears, the higher the numerical value given to its apparent magnitude. Note that brightness varies with distance; an extremely bright object may appear quite dim, if it is far away. Brightness varies inversely with the square of the distance. The absolute magnitude, "M", of a celestial body (outside the Solar System) is the apparent magnitude it would have if it were at 10 parsecs (~32.6 light years) and that of a planet (or other Solar System body) is the apparent magnitude it would have if it were 1 astronomical unit from both the Sun and Earth. The absolute magnitude of the Sun is 4.83 in the V band (yellow) and 5.48 in the B band (blue).
The apparent magnitude, "m", in the band, "x", can be defined as,
where formula_2 is the observed flux in the band x, and formula_3 and formula_4 are a reference magnitude, and reference flux in the same band x, such as that of Vega. An increase of 1 in the magnitude scale corresponds to a decrease in brightness by a factor of formula_5. Based on the properties of logarithms, a difference in magnitudes, formula_6, can be converted to a variation in brightness as formula_7.
Example: Sun and Moon.
"What is the ratio in brightness between the Sun and the full moon?"
The apparent magnitude of the Sun is −26.74 (brighter), and the mean apparent magnitude of the full moon is −12.74 (dimmer).
Difference in magnitude : 
Variation in Brightness : 
The Sun appears about 400,000 times brighter than the full moon.
Magnitude addition.
Sometimes, it might be useful to add magnitudes. For example, to determine the combined magnitude of a double star when the magnitudes of the individual components are known. This can be done by setting an equation using the brightness (in linear units) of each magnitude.
Solving for formula_11 yields
where formula_11 is the resulting magnitude after adding formula_14 and formula_15. Note that the negative of each magnitude is used because greater intensities equate to lower magnitudes.
Standard reference values.
It is important to note that the scale is logarithmic: the relative brightness of two objects is determined by the difference of their magnitudes. For example, a difference of 3.2 means that one object is about 19 times as bright as the other, because Pogson's Ratio raised to the power 3.2 is approximately 19.05.
A common misconception is that the logarithmic nature of the scale is because the human eye itself has a logarithmic response. In Pogson's time this was thought to be true (see Weber-Fechner law), but it is now believed that the response is a power law (see Stevens' power law).
Magnitude is complicated by the fact that light is not monochromatic. The sensitivity of a light detector varies according to the wavelength of the light, and the way it varies depends on the type of light detector. For this reason, it is necessary to specify how the magnitude is measured for the value to be meaningful. For this purpose the UBV system is widely used, in which the magnitude is measured in three different wavelength bands: U (centred at about 350 nm, in the near ultraviolet), B (about 435 nm, in the blue region) and V (about 555 nm, in the middle of the human visual range in daylight). The V band was chosen for spectral purposes and gives magnitudes closely corresponding to those seen by the light-adapted human eye, and when an apparent magnitude is given without any further qualification, it is usually the V magnitude that is meant, more or less the same as visual magnitude.
Because cooler stars, such as red giants and red dwarfs, emit little energy in the blue and UV regions of the spectrum their power is often under-represented by the UBV scale. Indeed, some L and T class stars have an estimated magnitude of well over 100, because they emit extremely little visible light, but are strongest in infrared.
Measures of magnitude need cautious treatment and it is extremely important to measure like with like. On early 20th century and older orthochromatic (blue-sensitive) photographic film, the relative brightnesses of the blue supergiant Rigel and the red supergiant Betelgeuse irregular variable star (at maximum) are reversed compared to what human eyes perceive, because this archaic film is more sensitive to blue light than it is to red light. Magnitudes obtained from this method are known as photographic magnitudes, and are now considered obsolete.
For objects within the Milky Way with a given absolute magnitude, 5 is added to the apparent magnitude for every tenfold increase in the distance to the object. This relationship does not apply for objects at very great distances (far beyond the Milky Way), because a correction for general relativity must then be taken into account due to the non-Euclidean nature of space. 
For planets and other Solar System bodies the apparent magnitude is derived from its phase curve and the distances to the Sun and observer.
Table of notable celestial objects.
Some of the above magnitudes are only approximate. Telescope sensitivity also depends on observing time, optical bandpass, and interfering light from scattering and airglow.

</doc>
<doc id="1963" url="http://en.wikipedia.org/wiki?curid=1963" title="Absolute magnitude">
Absolute magnitude

Absolute magnitude is the measure of a celestial object's intrinsic brightness. It is the hypothetical apparent magnitude of an object at a standard luminosity distance of exactly 10.0 parsecs or about 32.6 light years from the observer, assuming no astronomical extinction of starlight. This allows the true energy output of astronomical objects to be compared without regard to their variable distances. As with all astronomical magnitudes, the absolute magnitude can be specified for different wavelength intervals; for stars the most commonly quoted absolute magnitude is the absolute visual magnitude, which is the absolute magnitude in the visual (V) band of the UBV system. Also commonly used is the absolute bolometric magnitude, which is the total luminosity expressed in magnitude units; it takes into account energy radiated at all wavelengths, whether observed or not.
The absolute magnitude uses the same conventions as the visual magnitude: brighter objects have smaller magnitudes, and 5 magnitudes corresponds exactly to a factor of 100, so a factor of 100.4 (≈2.512) ratio of brightness corresponds to a difference of 1.0 in magnitude. The Milky Way, for example, has an absolute magnitude of about −20.5, so a quasar with an absolute magnitude of −25.5 is 100 times brighter than our galaxy. If this particular quasar and our galaxy could be seen side by side at the same distance, the quasar would be 5 magnitudes (or 100 times) brighter than our galaxy. Similarly, Canopus has an absolute visual magnitude of about -5.5, while Ross 248 has an absolute visual magnitude of +14.8, for a difference of slightly more than 20 magnitudes, so if the two stars were at the same distance, Canopus would be seen as about 20 magnitudes brighter; stated another way, Canopus gives off slightly more than 100 million (108) times more visual power than Ross 248.
Stars and galaxies ("M").
In stellar and galactic astronomy, the standard distance is 10 parsecs (about 32.616 light years, 308.57 petameters or 308.57 trillion kilometres).
A star at 10 parsecs has a parallax of 0.1" (100 milli arc seconds).
Galaxies (and other extended objects) are much larger than 10 parsecs, their light is radiated over an extended patch of sky, and their overall brightness cannot be directly observed from relatively short distances, but the same convention is used. A galaxy's magnitude is defined by measuring all the light radiated over the entire object, treating that integrated brightness as the brightness of a single point-like or star-like source, and computing the magnitude of that point-like source as it would appear if observed at the standard 10 parsecs distance. Consequently, the absolute magnitude of any object "equals" the apparent magnitude it "would have" if it was 10 parsecs away.
In using an absolute magnitude one must specify the type of electromagnetic radiation being measured. When referring to total energy output, the proper term is bolometric magnitude. The bolometric magnitude usually is computed from the visual magnitude plus a bolometric correction, formula_1. This correction is needed because very hot stars radiate mostly ultraviolet radiation, while very cool stars radiate mostly infrared radiation (see Planck's law).
Many stars visible to the naked eye have such a low absolute magnitude that they would appear bright enough to cast shadows if they were only 10 parsecs from the Earth: Rigel (−7.0), Deneb (−7.2), Naos (−6.0), and Betelgeuse (−5.6). For comparison, Sirius has an absolute magnitude of 1.4 which is brighter than the Sun, whose absolute visual magnitude is 4.83 (it actually serves as a reference point). The Sun's absolute bolometric magnitude is set arbitrarily, usually at 4.75.
Absolute magnitudes of stars generally range from −10 to +17. The absolute magnitudes of galaxies can be much lower (brighter). For example, the giant elliptical galaxy M87 has an absolute magnitude of −22 (i.e. as bright as about 60,000 stars of magnitude −10).
Computation.
For a negligible extinction, one can compute the absolute magnitude formula_2 of an object given its apparent magnitude formula_3 and luminosity distance formula_4:
where formula_4 is the star's luminosity distance in parsecs, where 1 parsec is 206,265 astronomical units, approximately 3.2616 light-years. For very large distances, the cosmological redshift complicates the relation between absolute and apparent magnitude, because the radiation observed at one wavelength was radiated at a significantly different one. For comparing the magnitudes of very distant objects with those of local objects, a k correction might have to be applied to the magnitudes of the distant objects.
For nearby astronomical objects (such as stars in our galaxy) luminosity distance "DL" is almost identical to the real distance to the object, because spacetime within our galaxy is almost Euclidean. For much more distant objects the Euclidean approximation is not valid, and General Relativity must be taken into account when calculating the luminosity distance of an object.
In the Euclidean approximation for nearby objects, the absolute magnitude formula_2 of a star can be calculated from its apparent magnitude and parallax:
where p is the star's parallax in arcseconds.
You can also compute the absolute magnitude formula_2 of an object given its apparent magnitude formula_3 and distance modulus formula_11:
Examples.
Rigel has a visual magnitude of formula_13 and distance about 860 light-years
Vega has a parallax of 0.129", and an apparent magnitude of +0.03
Alpha Centauri A has a parallax of 0.742" and an apparent magnitude of −0.01
The Black Eye Galaxy has a visual magnitude of mV=+9.36 and a distance modulus of 31.06.
Apparent magnitude.
Given the absolute magnitude formula_2, for objects within our galaxy you can also calculate the apparent magnitude formula_3 from any distance formula_20 (in parsecs):
For objects at very great distances (outside our galaxy) the luminosity distance "DL" must be used instead of "d" (in parsecs).
Given the absolute magnitude formula_2, you can also compute apparent magnitude formula_3 from its parallax formula_24:
Also calculating absolute magnitude formula_2 from distance modulus formula_11:
Bolometric magnitude.
Bolometric magnitude corresponds to luminosity, expressed in magnitude units; that is, after taking into account all electromagnetic wavelengths, including those unobserved due to instrumental pass-band, the Earth's atmospheric absorption, or extinction by interstellar dust. For stars, in the absence of extensive observations at many wavelengths, it usually must be computed assuming an effective temperature.
The difference in bolometric magnitude is related to the luminosity ratio according to:
which makes by inversion:
where
Solar System bodies ("H").
For planets and asteroids a different definition of absolute magnitude is used which is more meaningful for nonstellar objects.
In this case, the absolute magnitude (H) is defined as the apparent magnitude that the object would have if it were one astronomical unit (AU) from both the Sun and the observer. Because the object is illuminated by the Sun, absolute magnitude is a function of phase angle and this relationship is referred to as the phase curve.
To convert a stellar or galactic absolute magnitude into a planetary one, subtract 31.57. A comet's nuclear magnitude (M2) is a different scale and can not be used for a size comparison with an asteroid's (H) magnitude.
Apparent magnitude.
The absolute magnitude can be used to help calculate the apparent magnitude of a body under different conditions.
where formula_36 is 1 AU, formula_37 is the phase angle, the angle between the Sun–body and body–observer lines. By the law of cosines, we have:
formula_39 is the phase integral (integration of reflected light; a number in the 0 to 1 range).
Example: Ideal diffuse reflecting sphere. A reasonable first approximation for planetary bodies
A full-phase diffuse sphere reflects  2⁄3 as much light as a diffuse disc of the same diameter.
Distances:
Note: because Solar System bodies are never perfect diffuse reflectors, astronomers use empirically derived relationships to predict apparent magnitudes when accuracy is required.
Example.
Moon:
How bright is the Moon from Earth?
Meteors.
For a meteor, the standard distance for measurement of magnitudes is at an altitude of 100 km at the observer's zenith.

</doc>
<doc id="1965" url="http://en.wikipedia.org/wiki?curid=1965" title="Apollo 1">
Apollo 1

Apollo 1 (initially designated AS-204) was the first manned mission of the U.S. Apollo manned lunar landing program. The planned low Earth orbital test of the Apollo Command/Service Module never made its target launch date of February 21, 1967, because a cabin fire during a launch rehearsal test on January 27 at Cape Canaveral Air Force Station Launch Complex 34 killed all three crew members—Command Pilot Virgil I. "Gus" Grissom, Senior Pilot Edward H. White II, and Pilot Roger B. Chaffee—and destroyed the Command Module (CM). The name "Apollo 1", chosen by the crew, was officially retired by NASA in commemoration of them on April 24, 1967.
Immediately after the fire, NASA convened the "Apollo 204 Accident Review Board" to determine the cause of the fire, and both houses of the United States Congress launched their own committee inquiries to oversee NASA's investigation. During the investigation, a NASA internal document citing problems with prime Apollo contractor North American Aviation was publicly revealed by a Senator and became known as the "Phillips Report", embarrassing NASA Administrator James E. Webb, who was unaware of the document's existence, and attracting controversy to the Apollo program. Despite congressional displeasure at NASA's openness, both congressional committees ruled that the issues raised in the report had no bearing on the accident, and allowed NASA to continue with the program.
Although the ignition source could not be conclusively identified, the astronauts' deaths were attributed to a wide range of lethal design and construction flaws in the early Apollo Command Module. Manned Apollo flights were suspended for 20 months while these problems were corrected.
The Saturn IB launch vehicle, SA-204, scheduled for use on this mission, was later used for the first unmanned Lunar Module (LM) test flight, Apollo 5. The first successful manned Apollo mission was flown by Apollo 1's backup crew on Apollo 7 in October 1968.
Mission background.
AS-204 was to be the first manned test flight of the Apollo Command/Service Module (CSM) to Earth orbit, launched on a Saturn IB rocket. AS-204 was to test launch operations, ground tracking and control facilities and the performance of the Apollo-Saturn launch assembly and would have lasted up to two weeks, depending on how the spacecraft performed.
The CSM for this flight, number 012 built by North American Aviation (NAA), was a Block I version designed before the lunar orbit rendezvous landing strategy was chosen; therefore it lacked capability of docking with the Lunar Module. This was incorporated into the Block II CSM design, along with lessons learned in Block I. Block II would be test-flown with the LM when the latter was ready, and would be used on the Moon landing flights.
Deke Slayton, the Mercury astronaut who was grounded and became Director of Flight Crew Operations, selected the first Apollo crew in January,1966, with Grissom as Command Pilot, White as Senior Pilot, and rookie Donn F. Eisele as Pilot. But Eisele dislocated his shoulder twice aboard the KC135 weightlessness training aircraft, and had to undergo surgery on January 27. Slayton replaced him with Chaffee, and NASA announced the crew selection on March 21, 1966. James McDivitt, David Scott and Russell Schweickart were named as the backup crew. On September 29, Walter Schirra, Eisele, and Walter Cunningham were named as the prime crew for a second Block I CSM flight, AS-205. NASA planned to follow this with an unmanned test flight of the LM (AS-206), then the third manned mission would be a dual flight designated AS-278 (or AS-207/208), in which AS-207 would launch the first manned Block II CSM, which would then rendezvous and dock with the LM launched unmanned on AS-208.
In March, NASA was studying the possibility of flying the first Apollo mission as a joint space rendezvous with the final Project Gemini mission, Gemini 12 in November 1966. But by May, delays in making Apollo ready for flight just by itself, and the extra time needed to incorporate compatibility with the Gemini, made that impractical. This became moot when slippage in readiness of the AS-204 spacecraft caused the last-quarter 1966 target date to be missed, and the mission was rescheduled for February 21, 1967. Grissom was resolved to keep his craft in orbit for a full 14 days if there was any way to do so.
A newspaper article published on August 4, 1966, referred to the flight as "Apollo 1". CM-012 arrived at the Kennedy Space Center on August 26, labeled "Apollo One" by NAA on its packaging.
In October 1966, NASA announced the flight would carry a small television camera to broadcast live from the Command Module. The camera would also be used to allow flight controllers to monitor the spacecraft's instrument panel in flight. Television cameras were carried aboard all manned Apollo missions.
By December 1966, the second Block I flight AS-205 was canceled as unnecessary; and Schirra, Eisele and Cunningham were reassigned as the backup crew for Apollo 1. McDivitt's crew was now promoted to prime crew of the Block II / LM mission, re-designated AS-258 because the AS-205 launch vehicle would be used in place of AS-207. A third manned mission was planned to launch the CSM and LM together on a Saturn V (AS-503) to an elliptical medium Earth orbit (MEO), to be crewed by Frank Borman, Michael Collins and William Anders. McDivitt, Scott and Schweickart had started their training for AS-258 in CM-101 at the NAA plant in Downey, California, when the Apollo 1 accident occurred.
Mission insignia.
Grissom's crew received approval in June 1966 to design a mission patch with the name "Apollo 1". The design's center depicts a Command/Service Module flying over the southeastern United States with Florida (the launch point) prominent. The Moon is seen in the distance, symbolic of the eventual program goal. A yellow border carries the mission and astronaut names with another border set with stars and stripes, trimmed in gold. The insignia was designed by the crew, with the artwork done by North American Aviation employee Allen Stevens.
Spacecraft problems.
The Apollo Command/Service Module spacecraft was much bigger and far more complex than any previously implemented spacecraft design. In October 1963, Joseph F. Shea was named Apollo Spacecraft Program Office (ASPO) manager, responsible for managing the design and construction of both the CSM and the LM.
In a spacecraft review meeting held with Shea on August 19, 1966 (a week before delivery), the crew expressed concern about the amount of flammable material (mainly nylon netting and Velcro) in the cabin, which the technicians found convenient for holding tools and equipment in place. Though Shea gave the spacecraft a passing grade, after the meeting they gave him a crew portrait they had posed with heads bowed and hands clasped in prayer, with the inscription:
It isn't that we don't trust you, Joe, but this time we've decided to go over your head.
Shea gave his staff orders to tell North American to remove the flammables from the cabin, but did not supervise the issue personally.
North American shipped spacecraft CM-012 to Kennedy Space Center on August 26, 1966 under a conditional Certificate of Flight Worthiness: 113 significant incomplete planned engineering changes had to be completed at KSC. But that was not all; an additional 623 engineering change orders were made and completed after delivery. Grissom became so frustrated with the inability of the training simulator engineers to keep up with the spacecraft changes, that he took a lemon from a tree by his house and hung it on the simulator.
The Command and Service Modules were mated in the KSC altitude chamber in September, and combined system testing was performed. Altitude testing was performed first unmanned, then with both the prime and backup crews, from October 10 through December 30. During this testing, the Environmental Control Unit in the Command Module was found to have a design flaw, and was sent back to the manufacturer for design changes and rework. The returned ECU then leaked water/glycol coolant, and had to be returned a second time. Also during this time, a propellant tank in Service Module 017 had ruptured during testing at NAA, prompting the separation of the modules and removal from the chamber so the Service Module could be tested for signs of the tank problem. These tests were negative, and once all outstanding hardware problems were fixed, the reassembled spacecraft finally completed a successful altitude test with Schirra's backup crew.
According to the final report of the accident investigation board, "At the post-test debriefing the backup flight crew expressed their satisfaction with the condition and performance of the spacecraft." This would appear to contradict the account given in "Lost Moon: The Perilous Voyage of Apollo 13" by Jeffrey Kluger and astronaut James Lovell, that "When the trio climbed out of the ship, ... Schirra made it clear that he was not pleased with what he had seen," and that he later warned Grissom and Shea that "there's nothing wrong with this ship that I can point to, but it just makes me uncomfortable. Something about it just doesn't ring right," and that Grissom should get out at the first sign of trouble.
Following the successful altitude tests, the spacecraft was removed from the altitude chamber on January 3, 1967, and mated to its Saturn IB launch vehicle on pad 34 on January 6.
 You sort of have to put that out of your mind. There's always a possibility that you can have a catastrophic failure, of course; this can happen on any flight; it can happen on the last one as well as the first one. So, you just plan as best you can to take care of all these eventualities, and you get a well-trained crew and you go fly.
 — Gus Grissom, in a December 1966 interview
Accident.
Plugs-out test.
The launch simulation on January 27, 1967, was a "plugs-out" test to determine whether the spacecraft would operate nominally on (simulated) internal power while detached from all cables and umbilicals. Passing this test was essential to making the February 21 launch date. The test was considered non-hazardous because neither the launch vehicle nor the spacecraft was loaded with fuel or cryogenics, and all pyrotechnic systems were disabled.
At 1:00 pm EST (1800 GMT) on January 27, first Grissom, then Chaffee, and White entered the Command Module fully pressure-suited, and were strapped into their seats and hooked up to the spacecraft's oxygen and communication systems. There was an immediate problem: Grissom noticed a strange odor in the air circulating through his suit which he compared to "sour buttermilk", and the simulated countdown was held at 1:20 pm, while air samples were taken. No cause of the odor could be found, and the countdown was resumed at 2:42 pm. (The accident investigation found this odor not to be related in any way to the fire.)
Three minutes after the count was resumed, the hatch installation was started. The hatch consisted of three parts: a removable inner hatch, which stayed inside the cabin; a hinged outer hatch, which was part of the spacecraft's heat shield; and an outer hatch cover, which was part of the boost protective cover enveloping the entire Command Module to protect it from aerodynamic heating during launch and from launch escape rocket exhaust in the event of a launch abort. The boost hatch cover was partially but not fully latched in place, because the flexible boost protective cover was slightly distorted by some cabling run under it to provide the simulated internal power. (The spacecraft's fuel cell reactants were not loaded for this test.) After the hatches were sealed, the air in the cabin was replaced with pure oxygen at 16.7 psi, 2 psi higher than atmospheric pressure.
Further problems included episodes of high oxygen spacesuit flow, which tripped an alarm. The likely cause was determined to be the astronauts' movements, which were detected by the spacecraft's inertial guidance gyroscope and Grissom's stuck-open microphone. The open microphone was part of the third major problem, with the communications loop connecting the crew, the Operations and Checkout Building and the Complex 34 blockhouse control room. The problems led Grissom to remark: "How are we going to get to the Moon if we can't talk between three buildings?" The simulated countdown was held again at 5:40 pm while attempts were made to fix the problem. All countdown functions up to the simulated internal power transfer had been successfully completed by 6:20 pm, but at 6:30 the count remained on hold at T minus 10 minutes.
Fire.
The crew members were using the time to run through their checklist again, when a voltage transient was recorded at 6:30:54 (23:30:54 GMT). Ten seconds later (at 6:31:04), Chaffee exclaimed "Hey!", and scuffling sounds followed for two seconds. White then reported, "I've got a fire in the cockpit!". Some witnesses said that they saw White on the television monitors, reaching for the inner hatch release handle as flames in the cabin spread from left to right and licked the window. The final voice transmission is believed to have come from Chaffee. Six seconds after White's report of a "fire in the cockpit", a voice cried out, "There's a bad fire!". The sound of the spacecraft's hull rupturing was heard immediately afterwards, followed by "I'm burning up!" and a scream. The transmission then ended abruptly at 6:31:21, only 15 seconds after the first report of fire.
The intensity of the fire fed by pure oxygen caused the pressure to rise in that 15 seconds to 29 psi, which ruptured the Command Module's inner wall (initial phase of the fire). Flames and gases then rushed outside the Command Module through open access panels to two levels of the pad service structure. Intense heat, dense smoke, and ineffective gas masks designed for toxic fumes rather than heavy smoke hampered the ground crew's attempts to rescue the men. There were fears the Command Module had exploded, or soon would, and that the fire might ignite the solid fuel rocket in the launch escape tower above the Command Module, which would have likely killed nearby ground personnel, and possibly have destroyed the pad.
As the pressure was released by the cabin rupture, the convective rush of air caused the flames to spread across the cabin, beginning the second phase. The third phase began when most of the oxygen was consumed and was replaced with atmospheric air, essentially quenching the fire, but causing massive amounts of smoke, dust, carbon monoxide, and fumes to fill the cabin.
It took five minutes for the pad workers to open all three hatch layers, and they could not drop the inner hatch to the cabin floor as intended, so they pushed it out of the way to one side. Although the cabin lights remained lit, they were at first unable to find the astronauts through the dense smoke. As the smoke cleared, they found the bodies, but were not able to remove them. The fire had partly melted Grissom's and White's nylon space suits and the hoses connecting them to the life support system. Grissom had removed his restraints and was lying on the floor of the spacecraft. White's restraints were burned through, and he was found lying sideways just below the hatch. It was determined that he had tried to open the hatch per the emergency procedure, but was not able to do so against the internal pressure. Chaffee was found strapped into his right-hand seat, as procedure called for him to maintain communication until White opened the hatch. Because of the large strands of melted nylon fusing the astronauts to the cabin interior, removing the bodies took nearly 90 minutes.
Investigation.
As a result of the in-flight failure of the Gemini 8 mission on March 17, 1966, NASA Deputy Administrator Robert Seamans wrote and implemented "Management Instruction 8621.1" on April 14, 1966, defining "Mission Failure Investigation Policy And Procedures". This modified NASA's existing accident procedures, based on military aircraft accident investigation, by giving the Deputy Administrator the option of performing independent investigations of major failures, beyond those for which the various Program Office officials were normally responsible. It declared, "It is NASA policy to investigate and document the causes of all major mission failures which occur in the conduct of its space and aeronautical activities and to take appropriate corrective actions as a result of the findings and recommendations."
Immediately after the Apollo 1 fire, Seamans directed establishment of the "Apollo 204 Review Board" chaired by Langley Research Center director Floyd L. Thompson, which included astronaut Frank Borman, spacecraft designer Maxime Faget, and six others. To avoid the possible appearance of a conflict of interest, NASA Administrator James E. Webb got the approval of President Lyndon B. Johnson for an internal NASA investigation, and notified appropriate leaders of Congress. According to Webb's official NASA bio:
Seamans immediately ordered all Apollo 1 hardware and software impounded, to be released only under control of the Board. On February 3, two members, a Cornell University professor and North American's Chief engineer for Apollo, left the Board, and a U.S. Bureau of Mines professor joined. After thorough stereo photographic documentation of the CM-012 interior, the board ordered its disassembly using procedures tested by disassembling the identical CM-014, and conducted a thorough investigation of every part. The board also reviewed the astronauts' autopsy results and interviewed witnesses. Seamans sent Webb weekly status reports of the investigation's progress, and the Board issued its final report on April 5, 1967.
According to the Board, Grissom suffered severe third degree burns on over one-third of his body and his spacesuit was mostly destroyed. White suffered third degree burns on almost half of his body and a quarter of his spacesuit had melted away. Chaffee suffered third degree burns over almost a quarter of his body and a small portion of his spacesuit was damaged. The autopsy report confirmed that the primary cause of death for all three astronauts was cardiac arrest caused by high concentrations of carbon monoxide. Burns suffered by the crew were not believed to be major factors, and it was concluded that most of them had occurred postmortem. Asphyxiation happened after the fire melted the astronauts' suits and oxygen tubes, exposing them to the lethal atmosphere of the cabin.
The review board identified five major factors which combined to cause the fire and the astronauts' deaths:
Ignition source.
The review board determined that the electrical power momentarily failed at 23:30:55 GMT, and found evidence of several electric arcs in the interior equipment. However, they were unable to conclusively identify a single ignition source. They determined that the fire most likely started near the floor in the lower left section of the cabin, close to the Environmental Control Unit. It spread from the left wall of the cabin to the right, with the floor being affected only briefly.
The board noted that a silver-plated copper wire running through an environmental control unit near the center couch had become stripped of its Teflon insulation and abraded by repeated opening and closing of a small access door. This weak point in the wiring also ran near a junction in an ethylene glycol/water cooling line that had been prone to leaks. The electrolysis of ethylene glycol solution with the silver anode was a notable hazard capable of causing a violent exothermic reaction, igniting the ethylene glycol mixture in the CM's corrosive test atmosphere of pure, high-pressure oxygen.
Pure oxygen atmosphere.
The plugs-out test had been run to simulate the launch procedure, with the cabin pressurized with pure oxygen at the nominal launch level of 16.7 psi, 2 psi above standard sea level atmospheric pressure. This is more than five times the 3 psi partial pressure of oxygen in the atmosphere, and provides an environment in which materials not normally considered highly flammable will burst into flame.
The high-pressure oxygen atmosphere was consistent with that used in the Mercury and Gemini programs. The pressure before launch was deliberately greater than ambient in order to drive out the nitrogen-containing air and replace it with pure oxygen, and also to seal the plug door hatch cover. During launch, the pressure would have been gradually reduced to the in-flight level of 5 psi, providing sufficient oxygen for the astronauts to breathe while reducing the fire risk. The Apollo 1 crew had tested this procedure with their spacecraft in the Operations and Checkout Building altitude (vacuum) chamber on October 18 and 19, 1966, and the backup crew of Schirra, Eisele and Cunningham had repeated it on December 30. The investigation board noted that, during these tests, the Command Module had been fully pressurized with pure oxygen four times, for a total of six hours and fifteen minutes, two and a half hours longer than it had been during the plugs-out test.
When designing the Mercury spacecraft, NASA had considered using a nitrogen/oxygen mixture to reduce the fire risk near launch, but rejected it based on two considerations. First, nitrogen used with the in-flight pressure reduction carried the clear risk of decompression sickness (known as "the bends"). But the decision to eliminate the use of any gas but oxygen was crystalized when a serious accident occurred on April 21, 1960, in which McDonnell Aircraft test pilot G.B. North passed out and was seriously injured when testing a Mercury cabin / spacesuit atmosphere system in a vacuum chamber. The problem was found to be nitrogen-rich (oxygen-poor) air leaking from the cabin into his spacesuit feed. North American Aviation had suggested using an oxygen/nitrogen mixture for Apollo, but NASA overruled this. The pure oxygen design also carried the benefit of saving weight, by eliminating the need for nitrogen tanks.
In his monograph "Project Apollo: The Tough Decisions", Deputy Administrator Seamans wrote that NASA's single worst mistake in engineering judgment was not to run a fire test on the Command Module prior to the plugs-out test. In the first episode of the 2009 BBC documentary series "NASA: Triumph and Tragedy", Jim McDivitt said that NASA had no idea how a 100% oxygen atmosphere would influence burning. Similar remarks by other astronauts were expressed in the 2007 documentary film "In the Shadow of the Moon".
Other oxygen fires.
Several fires in high-oxygen environments had occurred prior to the Apollo fire. For example, in 1962, USAF Colonel B. Dean Smith was conducting a test of the Gemini space suit with a colleague in a pure oxygen chamber at Brooks Air Force Base in San Antonio, Texas when a fire broke out, destroying the chamber. Smith and his partner narrowly escaped.
Other oxygen fire occurrences are documented in certain U.S. reports archived in the National Air and Space Museum, such as:
On January 28, 1986, the Soviet Union disclosed that cosmonaut Valentin Bondarenko died after a fire in a high-oxygen isolation chamber on March 23, 1961, less than three weeks before the first Vostok manned space flight.
Flammable materials in the cabin.
The review board cited "many types and classes of combustible material" close to ignition sources. The NASA crew systems department had installed 34 sqft of Velcro throughout the spacecraft, almost like carpeting. This Velcro was found to be flammable in a high-pressure 100% oxygen environment. Up to 70 pounds of other non-metallic flammable materials had also crept into the design.
Buzz Aldrin states in his book "Men From Earth" that the flammable material had been removed (per the crew's August 19 complaints and Joseph Shea's order), but was replaced prior to the August 26 delivery to Cape Kennedy.
Hatch design.
The inner hatch cover used a plug door design, sealed by higher pressure inside the cabin than outside. The normal pressure level used for launch (2 psi above ambient) created sufficient force to prevent removing the cover until the excess pressure was vented. Emergency procedure called for Grissom to open the cabin vent valve first, allowing White to remove the cover, but Grissom was prevented from doing this because the valve was located to the left, behind the initial wall of flames. Also, while the system could easily vent the normal pressure, its flow capacity was utterly incapable of handling the rapid increase to 29 psi absolute caused by the intense heat of the fire.
North American had originally suggested the hatch open outward and use explosive bolts to blow the hatch in case of emergency, as had been done in Project Mercury. NASA did not agree, arguing the hatch could accidentally open, as it had on Grissom's "Liberty Bell 7" flight, so the inward-opening hatch was selected early in the Block I design.
Before the fire, the Apollo astronauts had recommended changing the design to an outward-opening hatch, and this was already slated for inclusion in the Block II Command Module design. According to Donald K. Slayton's testimony before the House investigation of the accident, this was based on ease of exit for spacewalks and at the end of flight, rather than for emergency exit.
Emergency preparedness.
The board noted that: the test planners had failed to identify the test as hazardous; the emergency equipment (such as gas masks) were inadequate to handle this type of fire; that fire, rescue, and medical teams were not in attendance; and that the spacecraft work and access areas contained many hindrances to emergency response such as steps, sliding doors, and sharp turns.
Political fallout.
Committees in both houses of the United States Congress with oversight of the space program soon launched investigations, including the Senate Committee on Aeronautical and Space Sciences, chaired by Senator Clinton P. Anderson. Seamans, Webb, Manned Space Flight Administrator Dr. George E. Mueller, and Apollo Program Director Maj Gen Samuel C. Phillips were called to testify before Sen. Anderson's committee.
In the February 27 hearing, Senator Walter F. Mondale asked Webb if he knew of a "report" of extraordinary problems with the performance of North American Aviation on the Apollo contract. Webb replied he did not, and deferred to his subordinates on the witness panel. Mueller and Phillips responded they too were unaware of any such "report".
However, in late 1965, just over a year before the accident, Phillips had headed a "tiger team" investigating the causes of inadequate quality, schedule delays, and cost overruns in both the Apollo CSM and the Saturn V second stage (for which North American was also prime contractor.) He gave an oral presentation (with transparencies) of his team's findings to Mueller and Seamans, and also presented them in a memo to North American president John L. Atwood, to which Mueller appended his own strongly worded memo to Atwood.
During Mondale's 1967 questioning about what was to become known as the "Phillips Report", Seamans was afraid Mondale might actually have seen a hard copy of Phillips' presentation, and responded that contractors have occasionally been subjected to on-site progress reviews; perhaps this was what Mondale's information referred to. Mondale continued to refer to "the Report" despite Phillips' refusal to characterize it as such, and angered by what he perceived as Webb's deception and concealment of important program problems from Congress, he questioned NASA's selection of North American as prime contractor. Seamans later wrote that Webb roundly chastised him in the cab ride leaving the hearing, for volunteering information which led to the disclosure of Phillips' memo.
On May 11, Webb issued a statement defending NASA's November 1961 selection of North American as the prime contractor for Apollo. This was followed on June 9 by Seamans filing a seven-page memorandum documenting the selection process. Webb eventually provided a controlled copy of Phillips' memo to Congress. The Senate committee noted in its final report NASA's testimony that "the findings of the [Phillips] task force had no effect on the accident, did not lead to the accident, and were not related to the accident", but stated in its recommendations:
"Notwithstanding that in NASA's judgment the contractor later made significant progress in overcoming the problems, the committee believes it should have been informed of the situation. The committee does not object to the position of the Administrator of NASA, that all details of Government/contractor relationships should not be put in the public domain. However, that position in no way can be used as an argument for not bringing this or other serious situations to the attention of the committee."
Freshman Senators Edward W. Brooke III and Charles H. Percy jointly wrote an "Additional Views" section appended to the committee report, chastising NASA more strongly than Anderson for not having disclosed the Phillips review to Congress. Mondale wrote his own, even more strongly worded Additional View, accusing NASA of "evasiveness, ... lack of candor, ... patronizing attitude toward Congress, ... refusal to respond fully and forthrightly to legitimate Congressional inquiries, and ... solicitous concern for corporate sensitivities at a time of national tragedy."
The potential political threat to Apollo blew over, due in large part to the support of President Lyndon B. Johnson, who at the time still wielded a measure of influence with the Congress from his own Senatorial experience. He was a staunch supporter of NASA since its inception, had even recommended the Moon program to President John F. Kennedy in 1961, and was skilled at portraying it as part of Kennedy's legacy.
Internal acrimony developed between NASA and North American over assignment of blame. North American argued unsuccessfully it was not responsible for the fatal error in spacecraft atmosphere design. Finally, Webb contacted Atwood, and demanded either he or Chief Engineer Harrison A. Storms resign. Atwood elected to fire Storms.
On the NASA side, Joseph Shea became unfit for duty in the aftermath and was removed from his position, although not fired.
Program recovery.
From this day forward, Flight Control will be known by two words: "Tough" and "Competent". "Tough" means we are forever accountable for what we do or what we fail to do. We will never again compromise our responsibilities... "Competent" means we will never take anything for granted... Mission Control will be perfect. When you leave this meeting today you will go to your office and the first thing you will do there is to write "Tough" and "Competent" on your blackboards. It will never be erased. Each day when you enter the room, these words will remind you of the price paid by Grissom, White, and Chaffee. These words are the price of admission to the ranks of Mission Control.
Gene Kranz, speech given to Mission Control after the accident.
Gene Kranz called a meeting of his staff in Mission Control three days after the accident, delivering a speech which has subsequently become one of NASA's principles. Speaking of the errors and overall attitude surrounding the Apollo program before the accident, he stated: "We were too 'gung-ho' about the schedule and we blocked out all of the problems we saw each day in our work. Every element of the program was in trouble and so were we." He reminded the team of the perils and mercilessness of their endeavor, and stated the new requirement that every member of every team in mission control be "tough and competent", requiring nothing less than perfection throughout NASA's programs. 36 years later, following the Space Shuttle "Columbia" disaster, then-NASA administrator Sean O'Keefe quoted Kranz's speech, adopting it in principle to honor the lives of Apollo 1's and Columbia's astronauts.
Command Module redesign.
After the fire, the Apollo program was grounded for review and redesign. The Command Module was found to be extremely hazardous and in some instances, carelessly assembled (for example, a misplaced socket wrench was found in the cabin).
It was decided that remaining Block I spacecraft would be used only for unmanned Saturn V test flights. All manned missions would use the Block II spacecraft, to which many Command Module design changes were made:
Thorough protocols were implemented for documenting spacecraft construction and maintenance.
New mission naming scheme.
The astronauts' widows asked that "Apollo 1" be reserved for the flight their husbands never made, and on April 24, 1967, Associate Administrator for Manned Space Flight, Dr. George E. Mueller, announced this change officially: AS-204 would be recorded as Apollo 1, "first manned Apollo Saturn flight – failed on ground test". Since three unmanned Apollo missions (AS-201, AS-202, and AS-203) had previously occurred, the next mission, the first unmanned Saturn V test flight (AS-501) would be designated Apollo 4, with all subsequent flights numbered sequentially in the order flown. The first three flights would not be renumbered, and the names "Apollo 2" and "Apollo 3" would go unused.
The manned flight hiatus allowed work to catch up on the Saturn V and Lunar Module, which were encountering their own delays. Apollo 4 flew in November 1967. Apollo 1's (AS-204) Saturn IB rocket was taken down from Launch Complex 34, later reassembled at Launch complex 37B and used to launch Apollo 5, an unmanned Earth orbital test flight of the first Lunar Module LM-1, in January 1968. A second unmanned Saturn V AS-502 flew as Apollo 6 in April 1968, and Grissom's backup crew of Wally Schirra, Don Eisele, and Walter Cunningham, finally flew the orbital test mission as Apollo 7 (AS-205), in a Block II CSM in October 1968.
Memorials.
Gus Grissom and Roger Chaffee were buried at Arlington National Cemetery. Ed White was buried at West Point Cemetery on the grounds of the United States Military Academy in West Point, New York.
Their names are among those of several astronauts and cosmonauts who have died in the line of duty, listed on the Space Mirror Memorial at the Kennedy Space Center Visitor Complex in Merritt Island, Florida.
An Apollo 1 mission patch was left on the Moon's surface after the first manned lunar landing by Apollo 11 crew members Neil Armstrong and Buzz Aldrin.
The Apollo 15 mission left on the surface of the Moon a tiny memorial statue, "Fallen Astronaut", along with a plaque containing the names of the Apollo 1 astronauts, among others including Soviet cosmonauts, who perished in the pursuit of human space flight.
Launch Complex 34.
After the Apollo 1 fire, Launch Complex 34 was subsequently used only for the launch of Apollo 7 and later dismantled down to the concrete launch pedestal, which remains at the site () along with a few other concrete and steel-reinforced structures. The pedestal bears two plaques commemorating the crew. Each year the families of the Apollo 1 crew are invited to the site for a memorial, and the Kennedy Space Center Visitor Center includes the site in its tour of the historic Cape Canaveral launch sites.
In January 2005, three granite benches, built by a college classmate of one of the astronauts, were installed at the site on the southern edge of the launch pad. Each bears the name of one of the astronauts and his military service insignia.
Remains of CM-012.
The Apollo 1 Command Module has never been on public display. After the accident, the spacecraft was removed and taken to Kennedy Space Center to facilitate the review board's disassembly in order to investigate the cause of the fire. When the investigation was complete, it was moved to the NASA Langley Research Center in Hampton, Virginia, and placed in a secured storage warehouse.
On February 17, 2007, the parts of CM-012 were moved approximately 90 ft to a newer, environmentally controlled warehouse. Only a few weeks earlier, Gus Grissom's brother Lowell publicly suggested CM-012 be permanently entombed in the concrete remains of Launch Complex 34.
References.
 This article incorporates  from websites or documents of the .
</dl>

</doc>
<doc id="1966" url="http://en.wikipedia.org/wiki?curid=1966" title="Apollo 10">
Apollo 10

Apollo 10 was the fourth manned mission in the United States Apollo space program. Launched on May 18, 1969, it was the F mission: a "dress rehearsal" for the first Moon landing, testing all of the components and procedures, just short of actually landing. The Lunar Module (LM) came to within 8.4 nmi of the lunar surface, the point where the powered descent to the lunar surface would begin. Its success enabled the first landing to be attempted on Apollo 11 in July, 1969.
According to the 2002 "Guinness World Records", Apollo 10 set the record for the highest speed attained by a manned vehicle at 39,897 km/h (11.08 km/s or 24,791 mph) during the return from the Moon on May 26, 1969.
Due to the use of their names as call signs, the "Peanuts" characters Charlie Brown and Snoopy became semi-official mascots for the mission. "Peanuts" creator Charles Schulz also drew some special mission-related artwork for NASA.
Crew.
Crew notes.
Apollo 10 was the first of only two Apollo missions with an entirely flight-experienced crew. Thomas P. Stafford had flown on Gemini 6 and Gemini 9; John W. Young had flown on Gemini 3 and Gemini 10, and Eugene A. Cernan had flown with Stafford on Gemini 9.
In addition, Apollo 10 marked the only Saturn V flight from Launch Complex 39B, as preparations for Apollo 11 at LC-39A had begun in March almost immediately after Apollo 9's launch.
They were also the only Apollo crew all of whose members went on to fly subsequent missions aboard Apollo spacecraft: Young later commanded Apollo 16, Cernan commanded Apollo 17 and Stafford commanded the US vehicle on the Apollo–Soyuz Test Project.
The Apollo 10 crew holds the distinction of being the humans who have traveled to the farthest point away from home, some 408950 km from their homes and families in Houston. While most Apollo missions orbited the Moon at the same 111 km from the lunar surface, timing makes this distinction possible as the distance between the Earth and Moon varies by approximately 43000 km (between perigee and apogee) throughout the year, and the Earth's rotation make the distance to Houston vary by another 12000 km each day. The Apollo 10 crew reached the farthest point in their orbit around the far side of the Moon at approximately the same time Earth had rotated around putting Houston nearly a full Earth diameter away. The Apollo 13 crew holds the distinction of being the farthest any human has traveled from the Earth's surface.
By the normal rotation in place during Apollo, the backup crew would have been scheduled to fly on Apollo 13. However, Alan Shepard was given the Apollo 13 command slot instead. L. Gordon Cooper, Jr., Commander of the Apollo 10 backup crew, was enraged and resigned from NASA. Later, Shepard's crew was forced to switch places with Jim Lovell's tentative Apollo 14 crew.
Deke Slayton wrote in his memoirs that Cooper and Donn F. Eisele were never intended to rotate to another mission as both were out of favor with NASA management for various reasons (Cooper for his lax attitude towards training and Eisele for incidents aboard Apollo 7 and an extramarital affair) and were assigned to the backup crew simply because of a lack of qualified manpower in the Astronaut Office at the time the assignment needed to be made. Cooper, Slayton noted, had a very small chance of receiving the Apollo 13 command if he did an outstanding job with the assignment, which he did not. Eisele, despite his issues with management, was always intended for future assignment to the Apollo Applications Program (which was eventually cut down to only the Skylab component) and not a lunar mission.
Mission parameters.
LM closest approach to lunar surface.
On May 22, 1969 at 20:35:02 UTC, a 27.4 second LM descent propulsion system burn inserted the LM into a descent orbit of 60.9 by so that the resulting lowest point in the orbit occurred about 15° from lunar landing site 2 (the Apollo 11 landing site). The lowest measured point in the trajectory was 47400 ft above the lunar surface at 21:29:43 UTC.
Mission highlights.
This dress rehearsal for a Moon landing brought the Apollo Lunar Module to 8.4 nmi from the lunar surface, at the point where powered descent would begin on the actual landing. Practicing this approach orbit would refine knowledge of the lunar gravitational field needed to calibrate the powered descent guidance system to within 1 nmi (LR altitude update lock) needed for a landing. Earth-based observations, unmanned spacecraft, and Apollo 8 had respectively allowed calibration to within 200 nmi, 20 nmi, and 5 nmi. Except for this final stretch, the mission went exactly as a landing would have gone, both in space and on the ground, putting NASA's flight controllers and extensive tracking and control network through a rehearsal.
Shortly after trans-lunar injection, the Command/Service Module (CSM) separated from the S-IVB stage, turned around, and docked its nose to the top of the Lunar Module (LM) still nestled in the S-IVB. The CSM/LM stack then separated from the S-IVB for the trip to the Moon.
Apollo 10 was the first mission to carry a color television camera inside the spacecraft, and made the first live color TV transmissions from space.
Upon reaching lunar orbit, Young remained alone in the Command Module (CM) "Charlie Brown" while Stafford and Cernan flew separately in the LM "Snoopy". The LM crew demonstrated their craft's radar and engines, rode out a momentary gyration in the lunar lander's motion (due to a faulty switch setting), and surveyed the Apollo 11 landing site in the Sea of Tranquility. The ascent stage was loaded with the amount of fuel it would have had remaining if it had lifted off from the surface and reached the altitude at which the Apollo 10 ascent stage fired. The fueled LM weighed 30735 lb, compared to 33278 lb for the Apollo 11 LM which made the first landing. Historian Craig Nelson wrote that NASA took special precaution to ensure Stafford and Cernan would not attempt to make the first landing. Nelson quoted Cernan as saying "A lot of people thought about the kind of people we were: 'Don't give those guys an opportunity to land, 'cause they might!' So the ascent module, the part we lifted off the lunar surface with, was short-fueled. The fuel tanks weren't full. So had we literally tried to land on the Moon, we couldn't have gotten off." In his own memoir, Cernan wrote "Our lander, LM-4...was still too heavy to guarantee safe margins for a moon landing."
Upon separation of the descent stage and ascent engine ignition, the Lunar Module began to roll violently due to the crew accidentally duplicating commands into the flight computer which took the LM out of abort mode, the correct configuration for this maneuver. The live network broadcasts caught Cernan and Stafford uttering several expletives before regaining control of the LM. Cernan has said he observed the horizon spinning eight times over, indicating eight rolls of the spacecraft under ascent engine power. While the incident was downplayed by NASA, the roll was just several revolutions from being unrecoverable, which would have resulted in the LM crashing into the lunar surface.
Splashdown occurred in the Pacific Ocean on May 26, 1969 at 16:52:23 UTC, approximately 400 nmi east of American Samoa. The astronauts were recovered by the USS "Princeton", and subsequently flown to Pago Pago International Airport in Tafuna for a greeting reception, before being flown on a C-141 cargo plane to Honolulu.
Hardware disposition.
The LM "Snoopy"'s descent stage was left in orbit, but eventually crashed onto the lunar surface because of the Moon's non-uniform gravitational field; its location was not tracked.
After being jettisoned, "Snoopy's" ascent stage engine was fired to fuel depletion, sending it on a trajectory past the Moon into a heliocentric orbit. All other ascent stages were either left in lunar orbit to eventually crash, intentionally steered into the Moon to obtain readings from seismometers placed on the lunar surface, or else burned up in Earth's atmosphere.
"Snoopy"'s ascent stage orbit was not tracked after 1969, and its current location is unknown. In 2011, a group of amateur astronomers in the UK started a project to search for it.
The Command Module "Charlie Brown" is currently on loan to the Science Museum in London, where it is on display. "Charlie Brown"'s Service Module (SM) was jettisoned just before re-entry and burned up in the Earth's atmosphere.
After Apollo 10, NASA required astronauts to choose more "dignified" names for their command and lunar module. The requirement was unenforceable: Apollo 16 astronauts Young, Mattingly and Duke chose "Casper", as in Casper the Friendly Ghost, for their Command Module name. The idea was to give children a way to identify with the mission by using humor.
After the insertion into trans-Lunar orbit, the Saturn IVB third stage became a object where it would continue to orbit the Sun for many years. , it remains in orbit.
Mission insignia.
The shield-shaped emblem for the flight shows a large, three-dimensional Roman numeral X sitting on the Moon's surface, in Stafford's words, "to show that we had left our mark." Although it did not land on the Moon, the prominence of the number represents the significant contributions the mission made to the Apollo program. A CSM circles the Moon as an LM ascent stage flies up from its low pass over the lunar surface with its engine firing. The Earth is visible in the background. On the mission patch, a wide, light blue border carries the word APOLLO at the top and the crew names around the bottom. The patch is trimmed in gold. The insignia was designed by Allen Stevens of Rockwell International.
References.
 This article incorporates  from websites or documents of the .
Bibliography.
</dl>
External links.
NASA reports
Multimedia

</doc>
<doc id="1967" url="http://en.wikipedia.org/wiki?curid=1967" title="Apollo 12">
Apollo 12

Apollo 12 was the sixth manned flight in the United States Apollo program and the second to land on the Moon (an H type mission). It was launched on November 14, 1969 from the Kennedy Space Center, Florida, four months after Apollo 11. Mission commander Charles "Pete" Conrad and Lunar Module Pilot Alan L. Bean performed just over one day and seven hours of lunar surface activity while Command Module Pilot Richard F. Gordon remained in lunar orbit. The landing site for the mission was located in the southeastern portion of the Ocean of Storms.
Unlike the first landing on Apollo 11, Conrad and Bean achieved a precise landing at their expected location, the site of the "Surveyor 3" unmanned probe, which had landed on April 20, 1967. They carried the first color television camera to the lunar surface on an Apollo flight, but transmission was lost after Bean accidentally destroyed the camera by pointing it at the Sun. On one of two moonwalks, they visited the "Surveyor" and removed some parts for return to Earth. The mission ended on November 24 with a successful splashdown.
Mission highlights.
Launch and transfer.
Apollo 12 launched on schedule from Kennedy Space Center, during a rainstorm. It was the first rocket launch attended by an incumbent US president, Richard Nixon. Thirty-six-and-a-half seconds after lift-off, the vehicle triggered a lightning discharge through itself and down to the Earth through the Saturn's ionized plume. Protective circuits on the fuel cells in the Service Module (SM) falsely detected overloads and took all three fuel cells offline, along with much of the Command/Service Module (CSM) instrumentation. A second strike at 52 seconds after launch knocked out the "8-ball" attitude indicator. The telemetry stream at Mission Control was garbled. However, the vehicle continued to fly correctly; the strikes had not affected the Saturn V Instrument Unit.
The loss of all three fuel cells put the CSM entirely on batteries, which were unable to maintain normal 75-ampere launch loads on the 28-volt DC bus. One of the AC inverters dropped offline. These power supply problems lit nearly every warning light on the control panel and caused much of the instrumentation to malfunction.
Electrical, Environmental and Consumables Manager (EECOM) John Aaron remembered the telemetry failure pattern from an earlier test when a power supply malfunctioned in the CSM Signal Conditioning Equipment (SCE), which converted raw signals from instrumentation to standard voltages for the spacecraft instrument displays and telemetry encoders.
Aaron made a call, "Try SCE to aux," which switched the SCE to a backup power supply. The switch was fairly obscure, and neither Flight Director Gerald Griffin, CAPCOM Gerald Carr, nor Mission Commander Pete Conrad immediately recognized it. Lunar Module Pilot Alan Bean, flying in the right seat as the spacecraft systems engineer, remembered the SCE switch from a training incident a year earlier when the same failure had been simulated. Aaron's quick thinking and Bean's memory saved what could have been an aborted mission, and earned Aaron the reputation of a "steely-eyed missile man". Bean put the fuel cells back on line, and with telemetry restored, the launch continued successfully. Once in Earth parking orbit, the crew carefully checked out their spacecraft before re-igniting the S-IVB third stage for trans-lunar injection. The lightning strikes had caused no serious permanent damage.
Initially, it was feared that the lightning strike could have caused the Command Module's (CM) parachute mechanism to prematurely fire, disabling the explosive bolts that open the parachute compartment to deploy them. If they were indeed disabled, the Command Module would have crashed uncontrollably into the Pacific Ocean and killed the crew instantly. Since there was no way to figure out whether or not this was the case, ground controllers decided not to tell the astronauts about the possibility. The parachutes deployed and functioned normally at the end of the mission.
After Lunar Module (LM) separation, the S-IVB was intended to fly into solar orbit. The S-IVB auxiliary propulsion system was fired, and the remaining propellants vented to slow it down to fly past the Moon's trailing edge (the Apollo spacecraft always approached the Moon's leading edge). The Moon's gravity would then slingshot the stage into solar orbit. However, a small error in the state vector in the Saturn's guidance system caused the S-IVB to fly past the Moon at too high an altitude to achieve Earth escape velocity. It remained in a semi-stable Earth orbit after passing the Moon on November 18, 1969. It finally escaped Earth orbit in 1971 but was briefly recaptured in Earth orbit 31 years later. It was discovered by amateur astronomer Bill Yeung who gave it the temporary designation J002E3 before it was determined to be an artificial object.
Moon Landing.
The Apollo 12 mission landed on November 19, 1969, on an area of the Ocean of Storms that had been visited earlier by several unmanned missions ("Luna 5", "Surveyor 3", and "Ranger 7"). The International Astronomical Union, recognizing this, christened this region "Mare Cognitum (Known Sea)". The Lunar coordinates of the landing site were 3.01239° S latitude, 23.42157° W longitude. The landing site would thereafter be listed as "Statio Cognitum" on lunar maps. Conrad and Bean did not formally name their landing site, though Conrad nicknamed the intended touchdown area "Pete's Parking Lot".
The second lunar landing was an exercise in precision targeting, which would be needed for future Apollo missions. Most of the descent was automatic, with manual control assumed by Conrad during the final few hundred feet of descent. Unlike Apollo 11, where Neil Armstrong had to use the manual control to direct his lander downrange of the computer's target which was strewn with boulders, Apollo 12 succeeded in landing at its intended target – within walking distance of the "Surveyor 3" probe, which had landed on the Moon in April 1967. This was the first – and, to date, only – occasion in which humans have "caught up" to a probe sent to land on another world.
Conrad actually landed "Intrepid" 580 ft short of "Pete's Parking Lot", because it looked rougher during final approach than anticipated, and was a little under 1180 ft from "Surveyor 3", a distance that was chosen to eliminate the possibility of lunar dust (being kicked up by "Intrepid's" descent engine during landing) from covering "Surveyor 3". But the actual touchdown point–approximately 600 ft from "Surveyor 3"–did cause high velocity sandblasting of the probe. It was later determined that the sandblasting removed more dust than it delivered onto the "Surveyor", because the probe was covered by a thin layer that gave it a tan hue as observed by the astronauts, and every portion of the surface exposed to the direct sandblasting was lightened back toward the original white color through the removal of lunar dust.
EVAs.
When Conrad, who was somewhat shorter than Neil Armstrong, stepped onto the lunar surface, his first words were "Whoopie! Man, that may have been a small one for Neil, but that's a long one for me." This was not an off-the-cuff remark: Conrad had made a US$ bet with reporter Oriana Fallaci he would say these words, after she had queried whether NASA had instructed Neil Armstrong what to say as he stepped onto the Moon. Conrad later said he was never able to collect the money.
To improve the quality of television pictures from the Moon, a color camera was carried on Apollo 12 (unlike the monochrome camera that was used on Apollo 11). Unfortunately, when Bean carried the camera to the place near the Lunar Module where it was to be set up, he inadvertently pointed it directly into the Sun, destroying the SEC tube. Television coverage of this mission was thus terminated almost immediately. See also: Apollo TV camera.
Apollo 12 successfully landed within walking distance of the "Surveyor 3" probe. Conrad and Bean removed pieces of the probe to be taken back to Earth for analysis. It is claimed that the common bacterium "Streptococcus mitis" was found to have accidentally contaminated the spacecraft's camera prior to launch and survived dormant in this harsh environment for two and a half years. However, this finding has since been disputed: see Reports of "Streptococcus mitis" on the Moon.
Astronauts Conrad and Bean also collected rocks and set up equipment that took measurements of the Moon's seismicity, solar wind flux and magnetic field, and relayed the measurements to Earth. The instruments were part of the first complete nuclear-powered ALSEP station set up by astronauts on the Moon to relay long-term data from the lunar surface. The instruments on Apollo 11 were not as extensive or designed to operate long term. The astronauts also took photographs, although by accident Bean left several rolls of exposed film on the lunar surface. Meanwhile Gordon, on board the "Yankee Clipper" in lunar orbit, took multi-spectral photographs of the surface.
The lunar plaque attached to the descent stage of "Intrepid" is unique in that unlike the other plaques, it (a) did not have a depiction of the Earth, and (b) it was textured differently (the other plaques had black lettering on polished stainless steel while the Apollo 12 plaque had the lettering in polished stainless steel while the background was brushed flat).
Return.
"Intrepid's" ascent stage was dropped (per normal procedures) after Conrad and Bean rejoined Gordon in orbit. It impacted the Moon on November 20, 1969, at . The seismometers the astronauts had left on the lunar surface registered the vibrations for more than an hour.
The crew stayed an extra day in lunar orbit taking photographs, for a total lunar surface stay of 31 and a half hours and a total time in lunar orbit of eighty-nine hours.
On the return flight to Earth after leaving lunar orbit, the crew of Apollo 12 witnessed (and photographed) a solar eclipse, though this one was of the Earth eclipsing the Sun.
Splashdown.
"Yankee Clipper" returned to Earth on November 24, 1969 at 20:58 UTC (3:58pm EST, 10:58am HST), in the Pacific Ocean, approximately 500 nautical miles (800 km) east of American Samoa. During splashdown, a 16 mm film camera dislodged from storage and struck Bean in the forehead, rendering him briefly unconscious. He suffered a mild concussion and needed six stitches. After recovery by the USS "Hornet", they were flown to Pago Pago International Airport in Tafuna for a reception, before being flown on a C-141 cargo plane to Honolulu.
Mission insignia.
The Apollo 12 mission patch shows the crew's navy background; all three astronauts at the time of the mission were U.S. Navy commanders. It features a clipper ship arriving at the Moon, representing the Command Module "Yankee Clipper". The ship trails fire, and flies the flag of the United States. The mission name APOLLO XII and the crew names are on a wide gold border, with a small blue trim. Blue and gold are traditional U.S. Navy colors. The patch has four stars on it — one each for the three astronauts who flew the mission and one for Clifton Williams, a U.S. naval aviator and astronaut who was killed on October 5, 1967, after a mechanical failure caused the controls of his T-38 trainer to stop responding and crash. He trained with Conrad and Gordon as part of the backup crew for what would be the Apollo 9 mission, and would have been assigned as Lunar Module Pilot for Apollo 12.
Spacecraft location.
The Apollo 12 Command Module "Yankee Clipper" is on display at the Virginia Air and Space Center in Hampton, Virginia.
In 2002, astronomers thought they might have discovered another moon orbiting Earth, which they designated J002E3, that turned out to be the third stage of the Apollo 12 Saturn V rocket.
The Lunar Module "Intrepid" impacted the Moon November 20, 1969 at 22:17:17.7 UT (5:17 PM EST) . In 2009, the Lunar Reconnaissance Orbiter (LRO) photographed the Apollo 12 landing site. The "Intrepid" Lunar Module descent stage, experiment package (ALSEP), "Surveyor 3" spacecraft, and astronaut footpaths are all visible. In 2011, the LRO returned to the landing site at a lower altitude to take higher resolution photographs.
Depiction in media.
Portions of the Apollo 12 mission are dramatized in the miniseries "From the Earth to the Moon" episode entitled "That's All There Is". Conrad, Gordon, and Bean were portrayed by Paul McCrane, Tom Verica, and Dave Foley, respectively. Conrad had been portrayed by a different actor, Peter Scolari, in the first episode.
References.
 This article incorporates  from websites or documents of the .
Bibliography.
</dl>
External links.
NASA reports
Multimedia

</doc>
<doc id="1968" url="http://en.wikipedia.org/wiki?curid=1968" title="Apollo 14">
Apollo 14

Apollo 14 was the eighth manned mission in the United States Apollo program, and the third to land on the Moon. It was the last of the "H missions," targeted landings with two-day stays on the Moon with two lunar EVAs, or moonwalks.
Commander Alan Shepard, Command Module Pilot Stuart Roosa, and Lunar Module Pilot Edgar Mitchell launched on their nine-day mission on January 31, 1971 at 4:04:02 pm local time after a 40-minute, 2 second delay due to launch site weather restrictions, the first such delay in the Apollo program. Shepard and Mitchell made their lunar landing on February 5 in the Fra Mauro formation; this had originally been the target of the aborted Apollo 13 mission. During the two lunar EVAs, 42 kg of Moon rocks were collected and several surface experiments, including seismic studies, were performed. Shepard famously hit two golf balls on the lunar surface with a makeshift club he had brought from Earth. Shepard and Mitchell spent about 33 hours on the Moon, with about 9½ hours on EVA.
In the aftermath of Apollo 13, several modifications were made to the service module to prevent a repeat of that accident, including improved wiring insulation and a third oxygen tank.
While Shepard and Mitchell were on the surface, Roosa remained in lunar orbit aboard the Command/Service Module "Kitty Hawk", performing scientific experiments and photographing the Moon, including the landing site of the future Apollo 16 mission. He took several hundred seeds on the mission, many of which were germinated on return, resulting in the so-called Moon trees. Shepard, Roosa, and Mitchell landed in the Pacific Ocean on February 9.
Crew.
Shepard was the oldest U.S. astronaut when he made his trip aboard Apollo 14. He is the only astronaut from Project Mercury (the original Mercury Seven astronauts) to reach the Moon. Another of the original seven, Gordon Cooper, had (as Apollo 10's backup commander) tentatively been scheduled to command the mission, but according to author Andrew Chaikin, his casual attitude toward training, along with problems with NASA hierarchy (reaching all the way back to the Mercury-Atlas 9 flight), resulted in his removal.
The mission was a personal triumph for Shepard, who had battled back from Ménière's disease which grounded him from 1964 to 1968. He and his crew were originally scheduled to fly on Apollo 13, but in 1969 NASA officials switched the scheduled crews for Apollos 13 and 14. This was done to allow Shepard more time to train for his flight, as he had been grounded for four years.
As of 2015, Mitchell is the only surviving member of the crew; Roosa died in 1994 from pancreatitis and Shepard in 1998 from leukemia.
Mission highlights.
Launch and flight to lunar orbit.
Apollo 14 launched during heavy cloud cover and the Saturn V booster quickly disappeared from view. However, NASA's long-range cameras, based 60 miles south in Vero Beach, had a clear shot of the remainder of the launch. Following the launch, the Launch Control Center at Kennedy Space Center was visited by U.S. Vice President Spiro T. Agnew, Prince Juan Carlos of Spain, and his wife, Princess Sofía.
At the beginning of the mission, the CSM "Kitty Hawk" had difficulty achieving capture and docking with the LM "Antares". Repeated attempts to dock went on for 1 hour and 42 minutes, until it was suggested that Roosa hold "Kitty Hawk" against "Antares" using its thrusters, then the docking probe would be retracted out of the way, hopefully triggering the docking latches. This attempt was successful, and no further docking problems were encountered during the mission.
Lunar descent.
After separating from the Command Module in lunar orbit, the LM "Antares" also had two serious problems. First, the LM computer began getting an ABORT signal from a faulty switch. NASA believed that the computer might be getting erroneous readings like this if a tiny ball of solder had shaken loose and was floating between the switch and the contact, closing the circuit. The immediate solution—tapping on the panel next to the switch—did work briefly, but the circuit soon closed again. If the problem recurred after the descent engine fired, the computer would think the signal was real and would initiate an auto-abort, causing the ascent stage to separate from the descent stage and climb back into orbit. NASA and the software teams at the Massachusetts Institute of Technology scrambled to find a solution, and determined the fix would involve reprogramming the flight software to ignore the false signal. The software modifications were transmitted to the crew via voice communication, and Mitchell manually entered the changes (amounting to over 80 keystrokes on the LM computer pad) just in time.
A second problem occurred during the powered descent, when the LM landing radar failed to lock automatically onto the Moon's surface, depriving the navigation computer of vital information on the vehicle's altitude and vertical descent speed (this was not a result of the modifications to the ABORT command; rather, the post-mission report indicated it was an unrelated bug in the radar's operation). After the astronauts cycled the landing radar breaker, the unit successfully acquired a signal near 18000 ft, again just in the nick of time. Shepard then manually landed the LM closer to its intended target than any of the other six Moon landing missions. Mitchell believes that Shepard would have continued with the landing attempt without the radar, using the LM inertial guidance system and visual cues. But a post-flight review of the descent data showed the inertial system alone would have been inadequate, and the astronauts probably would have been forced to abort the landing as they approached the surface.
Lunar surface operations.
Shepard and Mitchell named their landing site "Fra Mauro Base", and this designation is recognized by the International Astronomical Union (depicted in Latin on lunar maps as "Statio Fra Mauro").
Shepard's first words, after stepping onto the lunar surface were, "And it's been a long way, but we're here." Unlike Neil Armstrong on Apollo 11 and Pete Conrad on Apollo 12, Shepard had already stepped off the LM footpad and was a few yards (meters) away before he spoke.
Shepard's moonwalking suit was the first to utilize red stripes on the arms and legs and on the top of the lunar EVA sunshade "hood," so as to allow easy identification between the commander and LM pilot on the surface; on the Apollo 12 pictures, it had been almost impossible to distinguish between the two crewmen, causing a great deal of confusion. This feature was included on Jim Lovell's Apollo 13 suit; however, because no landing was made on that mission, Apollo 14 was the first to make use of it. This feature was used for the remaining Apollo missions, and for the EVAs of Space Shuttle flights afterwards, and it is still in use today on both the U.S. and Russian space suits on the International Space Station.
After landing in the Fra Mauro formation—the destination for Apollo 13—Shepard and Mitchell took two moonwalks, adding new seismic studies to the by now familiar Apollo experiment package (ALSEP), and using the Modular Equipment Transporter (MET), a pull cart for carrying equipment and samples, referred to as a "lunar rickshaw." Roosa, meanwhile, took pictures from on board Command Module "Kitty Hawk" in lunar orbit.
The second moonwalk, or EVA, was intended to reach the rim of the 1000 ft wide Cone Crater. However, the two astronauts were not able to find the rim amid the rolling terrain of the crater's slopes. They became physically exhausted from the attempt and with their suits' oxygen supplies starting to run low, the effort was called off. Later analysis, using the pictures that they took, determined that they had come within an estimated 65 ft of the crater's rim. Images from the Lunar Reconnaissance Orbiter (LRO) show the tracks of the astronauts and the MET come to within 30 m of the rim.
Shepard and Mitchell deployed and activated various scientific instruments and experiments and collected almost 100 lb of lunar samples for return to Earth. Other Apollo 14 achievements included: the only use of MET; longest distance traversed by foot on the lunar surface; first use of shortened lunar orbit rendezvous techniques; and the first extensive orbital science period conducted during CSM solo operations.
The astronauts also engaged in less serious activities on the Moon. Shepard brought along a six iron golf club head which he could attach to the handle of a lunar excavation tool, and two golf balls, and took several one-handed swings (due to the limited flexibility of the EVA suit). He exuberantly exclaimed that the second ball went "miles and miles and miles" in the low lunar gravity, but later estimated the distance as 200 to. Mitchell then threw a lunar scoop handle as if it were a javelin.
Return, splashdown and quarantine.
On the way back to Earth, the crew conducted the first U.S. materials processing experiments in space.
The Command Module "Kitty Hawk" splashed down in the South Pacific Ocean on February 9, 1971 at 21:05 [UTC], approximately 760 nmi south of American Samoa. After recovery by the ship USS "New Orleans", the crew was flown to Pago Pago International Airport in Tafuna for a reception before being flown on a C-141 cargo plane to Honolulu. The Apollo 14 astronauts were the last lunar explorers to be quarantined on their return from the Moon.
Roosa, who worked in forestry in his youth, took several hundred tree seeds on the flight. These were germinated after the return to Earth, and widely distributed around the world as commemorative Moon trees.
Mission insignia.
The oval insignia shows a gold NASA Astronaut Pin, given to U.S. astronauts upon completing their first space flight, traveling from the Earth to the Moon. A gold band around the edge includes the mission and astronaut names. The designer was Jean Beaulieu.
The backup crew spoofed the patch with its own version, with revised artwork showing a Wile E. Coyote cartoon character depicted as gray-bearded (for Shepard, who was 47 at the time of the mission and the oldest man on the Moon), pot-bellied (for Mitchell, who had a pudgy appearance) and red furred (for Roosa's red hair), still on the way to the Moon, while Road Runner (for the backup crew) is already on the Moon, holding a U.S. flag and a flag labeled "1st Team." The flight name is replaced by "BEEP BEEP" and the backup crew's names are given. Several of these patches were hidden by the backup crew and found during the flight by the crew in notebooks and storage lockers in both the CSM "Kitty Hawk" and the LM "Antares" spacecraft, and one patch was even stored on the MET lunar hand cart.
Spacecraft location.
The Apollo 14 Command Module "Kitty Hawk" is on display at the Apollo/Saturn V Center building at the Kennedy Space Center after being on display at the Astronaut Hall of Fame near Titusville, Florida, for several years.
The ascent stage of Lunar Module "Antares" impacted the Moon on February 7, 1971 at 00:45:25.7 UT (February 6, 7:45 PM EST) . "Antares"' descent stage and the mission's other equipment remain at Fra Mauro at .
Photographs taken in 2009 by the Lunar Reconnaissance Orbiter were released on July 17, and the Fra Mauro equipment was the most visible Apollo hardware at that time, owing to particularly good lighting conditions. In 2011, the LRO returned to the landing site at a lower altitude to take higher resolution photographs.
References.
 This article incorporates  from websites or documents of the .
Bibliography.
</dl>
External links.
NASA reports
Multimedia

</doc>
<doc id="1969" url="http://en.wikipedia.org/wiki?curid=1969" title="Apollo 15">
Apollo 15

Apollo 15 was the ninth manned mission in the United States' Apollo program, the fourth to land on the Moon, and the eighth successful manned mission. It was the first of what were termed "J missions," long stays on the Moon, with a greater focus on science than had been possible on previous missions. It was also the first mission on which the Lunar Roving Vehicle was used.
The mission began on July 26, 1971, and ended on August 7. At the time, NASA called it the most successful manned flight ever achieved.
Commander David Scott and Lunar Module Pilot James Irwin spent three days on the Moon, including 18½ hours outside the spacecraft on lunar extra-vehicular activity (EVA). The mission landed near Hadley rille, in an area of the Mare Imbrium called "Palus Putredinus" (Marsh of Decay). The crew explored the area using the first lunar rover, which allowed them to travel much farther from the Lunar Module (LM) than had been possible on missions without the rover. They collected 77 kg of lunar surface material. At the same time, Command Module Pilot Alfred Worden orbited the Moon, using a Scientific Instrument Module (SIM) in the Service Module (SM) to study the lunar surface and environment in great detail with a panoramic camera, a gamma-ray spectrometer, a mapping camera, a laser altimeter, a mass spectrometer, and a lunar sub-satellite deployed at the end of Apollo 15's stay in lunar orbit (an Apollo program first).
Although the mission accomplished its objectives, this success was somewhat overshadowed by negative publicity that accompanied public awareness of postage stamps carried without authorization by the astronauts, who had made plans to sell them upon their return. Ironically, this mission was one of very few that had been honored with the issue of a commemorative stamp, with this first use of a lunar rover happening one decade after the first Mercury astronaut launch.
Crew.
All three astronauts on the all-United States Air Force crew received an honorary degree or Master's degree from the University of Michigan, including Scott's honorary degree, awarded in the spring of 1971, months before the launch. Scott had attended the University of Michigan, but left before graduating to accept an appointment to the United States Military Academy. The crewmen did their undergraduate work at either the United States Military Academy or the United States Naval Academy.
Backup crew.
Schmitt was the first member of Group 4 to be selected as a prime or backup crew member for an Apollo flight; from Group 4 he was the only astronaut to make it to the Moon, with the last Apollo mission at the end of 1972.
Planning and training.
The crew for Apollo 15 had previously served as the backup crew for Apollo 12. There had been a friendly rivalry between that prime and backup crew on that mission, with the prime being all United States Navy, and the backup all United States Air Force.
Originally Apollo 15 would have been an H mission, like Apollos 12, 13 and 14. But on September 2, 1970, NASA announced it was canceling what were to be the current incarnations of the Apollo 15 and Apollo 19 missions. To maximize the return from the remaining missions, Apollo 15 would now fly as a J mission and have the honor of carrying the first lunar rover.
One of the major changes in the training for Apollo 15 was the geology training. Although on previous flights the crews had been trained in field geology, for the first time Apollo 15 would make it a high priority. Scott and Irwin would train with Leon Silver, a Caltech geologist who on Earth was interested in the Precambrian. Silver had been suggested by Harrison Schmitt as an alternative to the classroom lecturers that NASA had previously used. Among other things, Silver had made important refinements to the methods for dating rocks using the decay of uranium into lead in the late 1950s.
At first Silver would take the prime and backup crews to various geological sites in Arizona and New Mexico as if for a normal field geology lesson, but as launch time approached, these trips became more realistic. Crews began to wear mock-ups of the backpacks they would carry, and communicate using walkie-talkies to a CAPCOM in a tent. (During a mission the Capsule Communicators (CAPCOMs), always fellow astronauts, were the only people who normally would speak to the crew.) The CAPCOM was accompanied by a group of geologists unfamiliar with the area who would rely on the astronauts' descriptions to interpret the findings.
The decision to land at Hadley came in September 1970. The Site Selection Committees had narrowed the field down to two sites — Hadley Rille or the crater Marius, near which were a group of low, possibly volcanic, domes. Although not ultimately his decision, the commander of a mission always held great sway. To David Scott the choice was clear, with Hadley, being "exploration at its finest."
Command Module Pilot Alfred Worden undertook a different kind of geology training. Working with an Egyptian-born geologist, Farouk El-Baz, he flew over areas in an airplane simulating the speed at which terrain would pass below him while in the Apollo Command/Service Module (CSM) in orbit. He became quite adept at making geologic observations as objects passed below.
Mission highlights.
Launch and outbound trip.
Apollo 15 was launched on July 26, 1971, at 9:34 AM EDT from the Kennedy Space Center, at Cape Canaveral, Florida. During the launch, the S-IC did not completely shut off following staging for four seconds, creating the possibility of the spent stage banging into the S-II engines, damaging them and forcing an abort (the S-II exhaust also struck a telemetry package on the S-IC and caused it to fail). Despite this, the third stage and spacecraft reached its planned Earth parking orbit. A couple of hours into the mission, the third stage reignited to propel the spacecraft out of Earth orbit and on to the Moon.
A few days after launching from Florida, the spacecraft passed behind the far side of the Moon, where the Service Propulsion System (SPS) engine on the CSM ignited for a six-minute burn, to slow the craft down into an initial lunar orbit. Once the lowest point of altitude in the orbit was reached, the SPS engine was fired again, to place the spacecraft into the proper descent orbit for the Lunar Module landing at Hadley.
Moon Landing.
Most of the first part of the day after arriving in lunar orbit on July 30 was spent in preparing the Lunar Module for descent to the lunar surface later on that day. When preparations were complete, un-docking from the CSM was attempted; it did not occur, because of a faulty seal in the hatch mechanism. The Command Module Pilot, Alfred Worden, re-sealed the hatch; the LM then separated from the CSM. David Scott and James Irwin continued preparations for the descent while Worden remained in the CSM, returning to a higher orbit to perform lunar observations and await his crewmates' return a few days later.
Soon, Scott and Irwin began the descent to the Hadley landing site. Several minutes after descent was initiated, at pitch-over and the beginning of the approach phase of the landing, the LM was six kilometers east of the pre-selected landing target. On learning this, Scott altered the flight path of the LM. They touched down at 22:16:29 UTC on July 30 at Hadley, within a few hundred meters of the planned landing site. While previous crews had exited the Lunar Module shortly after landing, the crew of Apollo 15 elected to spend the rest of the day inside the LM, waiting until the next day to perform the first of three EVAs, or moonwalks, in order to preserve their sleep rhythm on a mission on which they were to spend a significantly longer time on the surface than previous crews had spent. Before they slept, Scott performed a stand-up EVA, during which the LM was depressurized and he photographed their surroundings from the top docking hatch.
Lunar surface.
Throughout the sleep period, Mission Control, in Houston, monitored a slow but steady oxygen leak. The data output of the onboard telemetry computers was limited during the night to conserve energy, so controllers could not determine the exact cause of the leak without awaking the crew. Scott and Irwin eventually were awakened an hour early, and the source of the leak was found to be an open valve on the urine transfer device. After the problem was solved, the crew began preparation for the first Moon walk.
Four hours later, Scott and Irwin became the seventh and eighth humans, respectively, to walk on the Moon. After unloading the Lunar Roving Vehicle (LRV), the two drove to the first moonwalk's primary destination, Elbow Crater, along the edge of Hadley Rille. On returning to the LM "Falcon", Scott and Irwin deployed the Apollo Lunar Surface Experiments Package (ALSEP). The first EVA lasted about 6½ hours.
The target of the second EVA, the next day, was the edge of Mount Hadley Delta, where the pair sampled boulders and craters along the Apennine Front. During this moonwalk, the astronauts recovered what came to be one of the more famous lunar samples collected on the Moon during Apollo, sample #15415, more commonly known as the "Genesis Rock." Once back at the landing site, Scott continued to try to drill holes for an experiment at the ALSEP site, with which he had struggled the day before. After conducting soil-mechanics experiments and erecting a U.S. flag, Scott and Irwin returned to the LM. EVA 2 lasted 7 hours and 12 minutes.
During EVA 3, the third and final moonwalk of the mission, the crew again ventured to the edge of Hadley Rille, this time to the northwest of the immediate landing site. After returning to the LM's location, Scott performed an experiment in view of the television camera, using a feather and hammer to demonstrate Galileo's theory that all objects in a given gravity field fall at the same rate, regardless of mass (in the absence of aerodynamic drag). He dropped the hammer and feather at the same time; because of the negligible lunar atmosphere, there was no drag on the feather, which hit the ground at the same time as the hammer.
Scott then drove the rover to a position away from the LM, where the television camera could be used to observe the lunar liftoff. Before the mission, the crew had contacted Belgian sculptor Paul Van Hoeydonck to create a small aluminum statuette called "Fallen Astronaut" to commemorate those astronauts and cosmonauts who lost their lives in the pursuit of space exploration. Scott left the sculpture by the rover, along with a plaque bearing the names of 14 American astronauts and Soviet cosmonauts who were known up to that time. The memorial was left while the television camera was turned off; only Irwin knew what Scott was doing at the time. Scott told mission control he was doing some cleanup activities around the rover. 
The EVA lasted 4 hours and 50 minutes. In total, the two astronauts spent 18½ hours outside the LM and collected approximately 77 kg of lunar samples.
Return to Earth.
After lifting off from the lunar surface 2 days and 18 hours after landing, the LM ascent stage rendezvoused and re-docked with the CSM with Worden aboard in orbit. After transferring samples and other items from the LM to the CSM, the LM was sealed off, jettisoned, and intentionally crashed into the lunar surface. After completing more observations of the Moon from orbit and releasing the sub-satellite, the three-person crew departed lunar orbit with another burn of the SPS engine.
The next day, on the return trip to Earth, Worden performed a spacewalk in deep space, the first of its kind, to retrieve exposed film from the SIM bay. Later on in the day, the crew set a record for the longest Apollo flight to that point.
On approach to Earth the next day, August 7, the Service Module was jettisoned, and the Command Module (CM) reentered the Earth's atmosphere. Although one of the three parachutes on the CM failed to deploy properly, only two were required for a safe landing (one extra for redundancy). Upon landing in the North Pacific Ocean, the crew were recovered and taken aboard the recovery ship, the USS "Okinawa" after a mission lasting 12 days, 7 hours, 11 minutes, and 53 seconds.
Hardware.
Spacecraft.
Apollo 15 used Command/Service Module CSM-112, which was given the call sign "Endeavour", named after the HMS "Endeavour" and Lunar Module LM-10, call sign "Falcon", named after the United States Air Force Academy mascot. If Apollo 15 had flown as an H mission, it would have been with CSM-111 and LM-9. That CSM was used by the Apollo–Soyuz Test Project in 1975, but the Lunar Module went unused and is now on display at the Kennedy Space Center Visitor Complex.
After re-entry, one of "Endeavour"'s three main parachutes collapsed after opening. Only two of the three parachutes were required for safe splashdown; the third was a contingency. "Endeavour" ultimately splashed down safely to end the mission.
Technicians at the Kennedy Space Center had many problems with the SIM bay in the Service Module. It was the first time it had flown and experienced problems from the start. Problems came from the fact the instruments were designed to operate in zero gravity, but had to be tested in the 1 g on the surface of the Earth. As such, things like the 7.5 m booms for the mass and gamma ray spectrometers could only be tested using railings that tried to mimic the space environment, and so they never worked particularly well. When the technicians tried to integrate the entire bay into the rest of the spacecraft, data streams would not synchronize, and lead investigators of the instruments would want to make last minute checks and changes. When it came time to test the operation of the gamma-ray spectrometer, it was necessary to stop every engine within 10 mi of the test site.
On the Lunar Module, the fuel and oxidizer tanks were enlarged on both the descent and ascent stages and the engine bell on the descent stage was extended. Batteries and solar cells were added for increased electrical power. In all this increased the weight of the Lunar Module to 36000 lb, 4000 lb heavier than previous models.
"Endeavour" is currently on display at the National Museum of the United States Air Force at Wright-Patterson Air Force Base in Dayton, Ohio.
Lunar Rover.
The Lunar Roving Vehicle had been in development since May 1969, with the contract awarded to Boeing. It could be folded into a space 5 ft by 20 in (1.5 m by 0.5 m). Unloaded it weighed 460 lb (209 kg) and when carrying two astronauts and their equipment, 1500 lb (700 kg). Each wheel was independently driven by a ¼ horsepower (200 W) electric motor. Although it could be driven by either astronaut, the Commander always drove. Travelling at speeds up to 6 to 8 mph (10 to 12 km/h), it meant that for the first time the astronauts could travel far afield from their lander and still have enough time to do some scientific experiments.
Lunar subsatellite.
The Apollo 15 subsatellite (PFS-1) was a small satellite released into lunar orbit from the SIM bay. Its main objectives were to study the plasma, particle, and magnetic field environment of the Moon and map the lunar gravity field. Specifically, it measured plasma and energetic particle intensities and vector magnetic fields, and facilitated tracking of the satellite velocity to high precision. A basic requirement was that the satellite acquire fields and particle data everywhere on the orbit around the Moon. The Moon's roughly circular orbit about the Earth at ~380,000 km (60 Earth radii) carried the subsatellite into both interplanetary space and various regions of the Earth's magnetosphere. The satellite orbited the Moon and returned data from August 4, 1971 until January 1973.
In later years, through a study of many lunar orbiting satellites, scientists came to discover that most low lunar orbits (LLO) are unstable. Fortunately, PFS-1 had been placed, unknown to mission planners at the time, very near to one of only four lunar "frozen orbits", where a lunar satellite may remain indefinitely.
Releasing the subsatellite was the crew's final activity in lunar orbit, occurring an hour before the burn to take them back to Earth. A virtually identical subsatellite was deployed by Apollo 16.
Launch vehicle.
The Saturn V that launched Apollo 15 was designated SA-510, the tenth flight-ready model of the rocket. As the payload of the rocket was greater, changes were made to its launch trajectory and Saturn V itself. The rocket was launched in a more southerly direction (80–100 degrees azimuth) and the Earth parking orbit lowered to 166 km above the Earth's surface. These two changes meant 1100 lb more could be launched. The propellant reserves were reduced and the number of retrorockets on the S-IC first stage (used to separate the spent first stage from the S-II second stage) reduced from eight to four. The four outboard engines of the S-IC would be burned longer and the center engine would also burn longer before being shut down (see Saturn V for more information on the launch sequence). Changes were also made to the S-II to stop pogo oscillations.
Once all the various components had been installed on the Saturn V, it was moved to the launch site, Launch Complex 39A. During late June and early July 1971, the rocket and Launch Umbilical Tower (LUT) were struck by lightning at least four times. All was well however, with only minor damage suffered.
Space suits.
The astronauts themselves wore new space suits. On all previous Apollo flights, including the non-lunar flights, the commander and lunar module pilot had worn suits with the life support, liquid cooling, and communications connections in two parallel rows of three. On Apollo 15, the new suits, dubbed the "A7LB," had the connectors situated in triangular pairs. This new arrangement, along with the relocation of the entry zipper (which went in an up-down motion on the old suits), from the right shoulder to the left hip, allowed the inclusion of a new waist joint, allowing the astronauts to bend completely over and to sit on the rover. Upgraded backpacks allowed for longer-duration moonwalks, and the Command Module Pilot, who wore a suit with three connectors, would wear a five-connector version of the old Moon suit — the liquid cooling water connector being removed, as the Command Module Pilot would make a "deep-space EVA" to retrieve film cartridges on the flight home.
Scandals.
After a successful mission, the reputations of the crew and NASA were tarnished by a deal the crew had made with a German stamp dealer. H. Walter Eiermann, who had many professional and social contacts with NASA employees and the astronaut corps, arranged for Scott to carry unauthorized commemorative postal covers in his space suit, in addition to the postal covers NASA had contracted to carry for the United States Postal Service. Eiermann had promised each astronaut $7,000 in the form of savings accounts in return for 100 covers signed after having been on the Moon. He told the astronauts that he would not advertise or sell the covers until the end of the Apollo program. Irwin wrote in his book "To Rule the Night" that the astronauts had agreed to the deal as a way to help finance their children's college tuition.
Another controversy arose after the flight, caused by the "Fallen Astronaut" statuette that Scott had left on the Moon. The crew claim they had agreed with the sculptor, Paul Van Hoeydonck, that no replicas were to be made, in order to satisfy NASA's aversion to commercial exploitation of the space program. After the sculpture's existence was publicly disclosed during their post-flight press conference, the National Air and Space Museum contacted the crew asking for a replica made for the museum. Van Hoeydonck, whose account of the agreement contradicts Scott's, subsequently advertised replicas for sale to the public. Under pressure from NASA, Van Hoeydonck withdrew the sale offer.
Mission insignia.
The three astronauts of Apollo 15 were all United States Air Force active duty officers, and their patch carries Air Force motifs (just as the Apollo 12 all-Navy crew's patch had featured a sailing ship). The circular patch features stylized red, white and blue birds flying over the Hadley Rille section of the Moon. Immediately behind the birds, a line of craters form the Roman numeral XV. The artwork is circled in red, with a white band giving the mission and crew names and a blue border. Scott contacted fashion designer Emilio Pucci to design the patch, who came up with the basic idea of the three-bird motif on a square patch. The crew changed the shape to round and the colors from blues and greens to a patriotic red, white and blue. Worden stated that each bird also represented an astronaut, white being his own color (and as Command Module Pilot, uppermost), with Scott the blue bird and Irwin the red. The Roman numeral design was created when NASA insisted that the mission number be displayed in Arabic numerals.
Visibility from space.
The halo area of the Apollo 15 landing site, generated by the LM's exhaust plume, was observed by a camera aboard the Japanese lunar orbiter SELENE and confirmed by comparative analysis of photographs in May 2008. This corresponds well to photographs taken from the Apollo 15 Command Module showing a change in surface reflectivity due to the plume, and was the first visible trace of manned landings on the Moon seen from space since the close of the Apollo program.
References.
 This article incorporates  from websites or documents of the .
Bibliography.
</dl>
External links.
NASA reports
Multimedia

</doc>
<doc id="1970" url="http://en.wikipedia.org/wiki?curid=1970" title="Apollo 16">
Apollo 16

Apollo 16 was the tenth manned mission in the United States Apollo space program, the fifth and penultimate to land on the Moon and the first to land in the lunar highlands. The second of the so-called "J missions," it was crewed by Commander John Young, Lunar Module Pilot Charles Duke and Command Module Pilot Ken Mattingly. Launched from the Kennedy Space Center in Florida at 12:54 PM EST on April 16, 1972, the mission lasted 11 days, 1 hour, and 51 minutes, and concluded at 2:45 PM EST on April 27.
John Young and Charles Duke spent 71 hours—just under three days—on the lunar surface, during which they conducted three extra-vehicular activities or moonwalks, totaling 20 hours and 14 minutes. The pair drove the Lunar Roving Vehicle (LRV), the second produced and used on the Moon, 26.7 km. On the surface, Young and Duke collected 95.8 kg of lunar samples for return to Earth, while Command Module Pilot Ken Mattingly orbited in the Command/Service Module (CSM) above to perform observations. Mattingly spent 126 hours and 64 revolutions in lunar orbit. After Young and Duke rejoined Mattingly in lunar orbit, the crew released a subsatellite from the Service Module (SM). During the return trip to Earth, Mattingly performed a one-hour spacewalk to retrieve several film cassettes from the exterior of the Service Module.
Apollo 16's landing spot in the highlands was chosen to allow the astronauts to gather geologically older lunar material than the samples obtained in the first four landings, which were in or near lunar maria. Samples from the Descartes Formation and the Cayley Formation disproved a hypothesis that the formations were volcanic in origin.
Crew.
Mattingly had originally been assigned to the prime crew of Apollo 13, but was exposed to the measles through Duke, at that time on the back-up crew for Apollo 13, who had caught it from one of his children. He never contracted the illness, but was nevertheless removed from the crew and replaced by his backup, Jack Swigert, three days before the launch. Young, a captain in the United States Navy, had flown on three spaceflights prior to Apollo 16: Gemini 3, Gemini 10 and Apollo 10, which orbited the Moon. One of 19 astronauts selected by NASA in April 1966, Duke had never flown in space before Apollo 16. He served on the support crew of Apollo 10 and was a Capsule Communicator (CAPCOM) for Apollo 11.
Backup crew.
Although not officially announced, the original backup crew consisted of Fred W. Haise (CDR), William R. Pogue (CMP) and Gerald P. Carr (LMP), who were targeted for the prime crew assignment on Apollo 19. However, after the cancellations of Apollos 18 and 19 were finalized in September 1970 this crew would not rotate to a lunar mission as planned. Subsequently, Roosa and Mitchell were recycled to serve as members of the backup crew after returning from Apollo 14, while Pogue and Carr were reassigned to the Skylab program where they flew on Skylab 4.
Mission insignia.
The insignia of Apollo 16 is dominated by a rendering of an American eagle and a red, white and blue shield, representing the people of the United States, over a gray background representing the lunar surface. Overlaying the shield is a gold NASA vector, orbiting the Moon. On its gold-outlined blue border, there are 16 stars, representing the mission number, and the names of the crew members: Young, Mattingly, Duke. The insignia was designed from ideas originally submitted by the crew of the mission.
Planning and training.
Landing site selection.
Apollo 16 was the second of the Apollo type J missions, featuring the use of the Lunar Roving Vehicle, increased scientific capability, and lunar surface stays of three days. As Apollo 16 was the penultimate mission in the Apollo program and there was no new hardware or procedures to test on the lunar surface, the last two missions (the other being Apollo 17) presented opportunities for astronauts to clear up some uncertainties in understanding the Moon's properties. Although previous Apollo expeditions, including Apollo 14 and Apollo 15, obtained samples of pre-mare lunar material, before lava began to upwell from the Moon's interior and flood the low areas and basins, none had actually visited the lunar highlands.
Apollo 14 had visited and sampled a ridge of material that had been ejected by the impact that created the Mare Imbrium impact basin. Likewise, Apollo 15 had also sampled material in the region of Imbrium, visiting the basin's edge. There remained the possibility, because the Apollo 14 and Apollo 15 landing sites were closely associated with the Imbrium basin, that different geologic processes were prevalent in areas of the lunar highlands far from Mare Imbrium. Several members of the scientific community remarked that the central lunar highlands resembled regions on Earth that were created by volcanic processes and hypothesized the same might be true on the Moon. They had hoped that scientific output from the Apollo 16 mission would provide an answer.
Two locations on the Moon were given primary consideration for exploration by the Apollo 16 expedition: the Descartes Highlands region west of Mare Nectaris and the crater Alphonsus. At Descartes, the Cayley and Descartes formations were the primary areas of interest in that scientists suspected, based on telescopic and orbital imagery, that the terrain found there was formed by magma more viscous than that which formed the lunar maria. The Cayley Formation's age was approximated to be about the same as Mare Imbrium based on the local frequency of impact craters. The considerable distance between the Descartes site and previous Apollo landing sites would be beneficial for the network of geophysical instruments, portions of which were deployed on each Apollo expedition beginning with Apollo 12.
At the Alphonsus, three scientific objectives were determined to be of primary interest and paramount importance: the possibility of old, pre-Imbrium impact material from within the crater's wall, the composition of the crater's interior and the possibility of past volcanic activity on the floor of the crater at several smaller "dark halo" craters. Geologists feared, however, that samples obtained from the crater might have been contaminated by the Imbrium impact, thus preventing Apollo 16 from obtaining samples of pre-Imbrium material. There also remained the distinct possibility that this objective had already been satisfied by the Apollo 14 and Apollo 15 missions, as the Apollo 14 samples had not yet been completely analyzed and samples from Apollo 15 had not yet been obtained.
It was decided to target the Apollo 16 mission for the Descartes site. Following the decision, the Alphonsus site was considered the most likely candidate for Apollo 17, but was eventually rejected. With the assistance of orbital photography obtained on the Apollo 14 mission, the Descartes site was determined to be safe enough for a manned landing. The specific landing site was between two young impact craters, North Ray and South Ray craters – 1000 and in diameter, respectively – which provided "natural drill holes" which penetrated through the lunar regolith at the site, thus leaving exposed bedrock that could be sampled by the crew.
After selecting the landing site for Apollo 16, sampling the Descartes and Cayley formations, two geologic units of the lunar highlands, was determined by mission planners to be the primary sampling interest of the mission. It was these formations that the scientific community widely suspected were formed by lunar volcanism, but this hypothesis was proven incorrect by the composition of lunar samples from the mission.
Training.
In preparation for their mission, the Apollo 16 astronauts participated in an extensive training program that included several field geology trips to introduce the astronauts to concepts and techniques they would use on the lunar surface. During these trips, the astronauts visited and provided scientific descriptions of geologic features they were likely to encounter. In July 1971, the Apollo 16 astronauts visited Sudbury, Ontario, Canada for geology training exercises, the first time U.S. astronauts did so. Geologists chose the area because of a 60 mi wide crater created about 1.8 billion years ago by a large meteorite. The Sudbury Basin shows evidence of shatter cone geology familiarizing the Apollo crew with geologic evidence of a meteor impact. During the training exercises the astronauts did not wear space suits, but carried radio equipment to converse with each other and a scientist-astronaut, practicing procedures they would use on the lunar surface.
In addition to field geology training, the astronauts also trained to use the space suits, adapt to the reduced lunar gravity, collect samples, maneuver in the Lunar Roving Vehicle, and land and recover after the mission. They also received survival training and preparation for other technical aspects of the mission.
Mission highlights.
Launch and outbound trip.
The launch of Apollo 16 was delayed one month from March 17 to April 16. This was the first launch delay in the Apollo program due to a technical problem. During the delay, the space suits, a spacecraft separation mechanism and batteries in the Lunar Module (LM) were modified and tested. There were concerns that the explosive mechanism designed to separate the docking ring from the Command Module (CM) would not create enough pressure to completely sever the ring. This, along with a dexterity issue in Young's space suit and fluctuations in the capacity of the Lunar Module batteries, required investigation and trouble-shooting. In January 1972, three months before the planned April launch date, a fuel tank in the Command Module was accidentally damaged during a routine test. The rocket was returned to the Vertical Assembly Building (VAB) and the fuel tank replaced, and the rocket returned to the launch pad in February in time for the scheduled launch.
The official mission countdown began on Monday, April 10, 1972, at 8:30 AM, six days before the launch. At this point the Saturn V rocket's three stages were powered up and drinking water was pumped into the spacecraft. As the countdown began, the crew of Apollo 16 was participating in final training exercises in anticipation of a launch on April 16. The astronauts underwent their final preflight physical examination on April 11. On April 15, liquid hydrogen and liquid oxygen propellants were pumped into the spacecraft, while the astronauts rested in anticipation of their launch the next day.
The Apollo 16 mission launched from the Kennedy Space Center in Florida at 12:54 PM EST on April 16, 1972. The launch was nominal; the crew experienced vibration similar to that of previous crews. The first and second stages of the Saturn V rocket performed nominally; the spacecraft entered orbit around Earth just under 12 minutes after lift-off. After reaching orbit, the crew spent time adapting to the zero-gravity environment and preparing the spacecraft for Trans Lunar Injection (TLI), the burn of the third-stage rocket that would propel them to the Moon. In Earth orbit, the crew faced minor technical issues, including a potential problem with the environmental control system and the S-IVB third stage's attitude control system, but eventually resolved or compensated for them as they prepared to depart towards the Moon. After two orbits, the rocket's third stage reignited for just over five minutes, propelling the craft towards the Moon at about 22000 mph. Six minutes after the burn of the S-IVB, the Command/Service Module, containing the crew, separated from the rocket and traveled for 15 m before turning around and retrieving the Lunar Module from inside the expended rocket stage. The maneuver, known as transposition, went smoothly and the LM was extracted from the S-IVB. Following transposition and docking, the crew noticed the exterior surface of the Lunar Module was giving off particles from a spot where the LM's skin appeared torn or shredded; at one point, Duke estimated they were seeing about five to ten particles per second. The crew entered the Lunar Module through the docking tunnel connecting it with the Command Module to inspect its systems, at which time they did not spot any major issues. Once on course towards the Moon, the crew put the spacecraft into a rotisserie "barbecue" mode in which the craft rotated along its long axis three times per hour to ensure even heat distribution about the spacecraft from the Sun. After further preparing the craft for the voyage, the crew began the first sleep period of the mission just under 15 hours after launch.
By the time Mission Control issued the wake-up call to the crew for flight day two, the spacecraft was about 98000 nmi away from the Earth, traveling at about 5322 ft/s. As it was not due to arrive in lunar orbit until flight day four, flight days two and three were largely preparatory days, consisting of spacecraft maintenance and scientific research. On day two, the crew performed an electrophoresis experiment, also performed on Apollo 14, in which they attempted to prove the higher purity of particle migrations in the zero-gravity environment. The remainder of day two included a two-second mid-course correction burn performed by the Command/Service Module's Service Propulsion System engine to tweak the spacecraft's trajectory. Later in the day, the astronauts entered the Lunar Module for the second time in the mission to further inspect the landing craft's systems. The crew reported they had observed additional paint peeling from a portion of the LM's outer aluminum skin. Despite this, the crew discovered that the spacecraft's systems were performing nominally. Following the LM inspection, the crew reviewed checklists and procedures for the following days in anticipation of their arrival and the Lunar Orbit Insertion burn. Command Module Pilot Mattingly reported a "gimbal lock" warning light, indicating the craft was not reporting an attitude. Mattingly alleviated this by realigning the guidance system using the Sun and Moon. At the end of day two, Apollo 16 was about 140000 nmi away from Earth.
At the beginning of day three, the spacecraft was about 157000 nmi away from the Earth. The velocity of the craft steadily decreased, as it had not yet reached the lunar sphere of gravitational influence. The early part of day three was largely housekeeping, spacecraft maintenance and exchanging status reports with Mission Control in Houston. The crew performed the Apollo light flash experiment, or ALFMED, to investigate "light flashes" that were seen by the astronauts when the spacecraft was dark, regardless of whether or not their eyes were open, on Apollo lunar flights. This was thought to be caused by the penetration of the eye by cosmic ray particles. During the second half of the day, Young and Duke again entered the Lunar Module to power it up and check its systems, and perform housekeeping tasks in preparation for lunar landing. The systems were found to be functioning as expected. Following this, the crew donned their space suits and rehearsed procedures that would be used on landing day. Just before the end of flight day three at 59 hours, 19 minutes, 45 seconds after liftoff, while 178673 nmi from the Earth and 33821 nmi from the Moon, the spacecraft's velocity began increasing as it accelerated towards the Moon after entering the lunar sphere of influence.
After waking up on flight day four, the crew began preparations for the maneuver that would brake the spacecraft into orbit around the Moon, or lunar orbit insertion. At a distance of 11142 nmi from the Moon, the Scientific Instrument Module (SIM) bay cover was jettisoned. At just over 74 hours into the mission, the spacecraft passed behind the Moon, losing direct contact with Mission Control. While over the far side of the Moon, the Command/Service Module's Service Propulsion System engine burned for 6 minutes and 15 seconds, braking the spacecraft into an orbit around the Moon with a low point (pericynthion) of 58.3 and a high point (apocynthion) of 170.4 nautical miles (108.0 and 315.6 km, respectively). After entering lunar orbit, the crew began preparations for the Descent Orbit Insertion (DOI) maneuver to further modify the spacecraft's orbital trajectory. The maneuver was successful, decreasing the craft's pericynthion to 10.7 nmi. The remainder of flight day four was spent making observations and preparing for activation of the Lunar Module, undocking, and landing the next day.
Lunar surface.
The crew continued preparing for Lunar Module activation and undocking shortly after waking up to begin flight day five. The boom that extended the mass spectrometer out from the Command/Service Module's Scientific Instruments Bay was stuck in a semi-deployed position. It was decided that Young and Duke would visually inspect the boom after undocking from the CSM in the LM. They entered the LM for activation and checkout of the spacecraft's systems. Despite entering the LM 40 minutes ahead of schedule, they completed preparations only 10 minutes early due to numerous delays in the process. With the preparations finished, they undocked in the LM "Orion" from Mattingly in the Command/Service Module "Casper" 96 hours, 13 minutes, 13 seconds into the mission. For the rest of the two crafts' passes over the near side of the Moon, Mattingly prepared to shift "Casper" to a circular orbit while Young and Duke prepared "Orion" for the descent to the lunar surface. At this point, during tests of the CSM's steerable rocket engine in preparation for the burn to modify the craft's orbit, a malfunction occurred in the engine's backup system. According to mission rules, "Orion" would have then re-docked with "Casper", in case Mission Control decided to abort the landing and use the Lunar Module's engines for the return trip to Earth. After several hours of analysis, however, mission controllers determined that the malfunction could be worked around and Young and Duke could proceed with the landing. As a result of this, powered descent to the lunar surface began about six hours behind schedule. Because of the delay, Young and Duke began their descent to the surface at an altitude higher than that of any previous mission, at 20.1 km. At an altitude of about 4000 m, Young was able to view the landing site in its entirety. Throttle-down of the LM's landing engine occurred on time and the spacecraft tilted forward to its landing orientation at an altitude of 2200 m. The LM landed 270 m north and 60 m west of the planned landing site at 104 hours, 29 minutes, and 35 seconds into the mission, at 2:23:35 UTC on April 21.
After landing, Young and Duke began powering down some of the LM's systems to conserve battery power. Upon completing their initial adjustments, the pair configured "Orion" for their three-day stay on the lunar surface, removed their space suits and took initial geological observations of the immediate landing site. They then settled down for their first meal on the surface. After eating, they configured the cabin for their first sleep period on the Moon. The landing delay caused by the malfunction in the Command/Service Module's main engine necessitated significant modifications to the mission schedule. Apollo 16 would spend one less day in lunar orbit after surface exploration had been completed to afford the crew contingency time to compensate for any further problems and to conserve expendables. In order to improve Young's and Duke's sleep schedule, the third and final moonwalk of the mission was trimmed from seven hours to five.
The next morning, flight day five, Young and Duke ate breakfast and began preparations for the first extra-vehicular activity (EVA), or moonwalk. After the pair donned and pressurized their space suits and depressurized the Lunar Module cabin, Young climbed out onto the "porch" of the LM, a small platform above the ladder. Duke handed Young a jettison bag full of trash to dispose of on the surface. Young then lowered the equipment transfer bag (ETB), containing equipment for use during the EVA, to the surface. Young descended the ladder and, upon setting foot on the lunar surface, became the ninth human to walk on the Moon. Upon stepping onto the surface, Young expressed his sentiments about being there: "There you are: Mysterious and Unknown Descartes. Highland plains. Apollo 16 is gonna change your image. I'm sure glad they got ol' Brer Rabbit, here, back in the briar patch where he belongs." Duke soon descended the ladder and joined Young on the surface, becoming the tenth and youngest human to walk on the Moon, at age 36. After setting foot on the lunar surface, Duke expressed his excitement, commenting: "Fantastic! Oh, that first foot on the lunar surface is super, Tony!" The pair's first task of the moonwalk was to unload the Lunar Roving Vehicle, the Far Ultraviolet Camera/Spectrograph (UVC), and other equipment, from the Lunar Module. This was done without problems. On first driving the lunar rover, Young discovered that the rear steering was not working. He alerted Mission Control to the problem before setting up the television camera and planting the flag of the United States with Duke. The day's next task was to deploy the Apollo Lunar Surface Experiments Package (ALSEP); while they were parking the lunar rover, on which the TV camera was mounted, to observe the deployment, the rear steering began functioning without explanation. While deploying a heat-flow experiment that had burned up with the Lunar Module "Aquarius" on Apollo 13 and had been attempted without success on Apollo 15, a cable was inadvertently snapped after getting caught around Young's foot. After ALSEP deployment, they collected samples in the vicinity. About four hours after the beginning of EVA-1, they mounted the lunar rover and drove to the first geologic stop, Plum Crater, a 36 m crater on the rim of Flag Crater, a crater 290 m across. There, at a distance of 1.4 km from the LM, they sampled material from the vicinity of Flag Crater, which scientists believed penetrated through the upper regolith layer to the underlying Cayley Formation. It was there that Young retrieved, at the request of Mission Control, the largest rock returned by an Apollo mission, a breccia nicknamed Big Muley after mission geology principal investigator William R. Muehlberger. The next stop of the day was Buster Crater, about 1.6 km from the LM. There, Duke took pictures of Stone Mountain and South Ray Crater while Young deployed a magnetic field experiment. At that point, scientists began to reconsider their pre-mission hypothesis that Descartes had been the setting of ancient volcanic activity, as the two astronauts had yet to find any volcanic material. Following their stop at Buster, Young did a demonstration drive of the lunar rover while Duke filmed with a 16 mm movie camera. After completing more tasks at the ALSEP, they returned to the LM to close out the moonwalk. They reentered the LM 7 hours, 6 minutes, and 56 seconds after the start of the EVA. Once inside, they pressurized the LM cabin, went through a half-hour briefing with scientists in Mission Control, and configured the cabin for the sleep period.
Shortly after waking up on the morning of flight day six three and a half minutes early, they discussed with Mission Control in Houston the day's timeline of events. The second lunar excursion's primary objective was to visit Stone Mountain to climb up the slope of about 20 degrees to reach a cluster of five craters known as "Cinco Craters." After preparations for the day's moonwalk were completed, the astronauts climbed out of the Lunar Module. After departing the immediate landing site in the lunar rover, they arrived at the day's first destination, the Cinco Craters, 3.8 km from the LM. At 152 m above the valley floor, the pair were at the highest elevation above the LM of any Apollo mission. After marveling at the view (including South Ray) from the side of Stone Mountain, which Duke described as "spectacular," the astronauts gathered samples in the vicinity. After spending 54 minutes on the slope, they climbed aboard the lunar rover en route to the day's second stop, station five, a crater 20 m across. There, they hoped to find Descartes material that had not been contaminated by ejecta from South Ray Crater, a large crater south of the landing site. The samples they collected there, although their origin is still not certain, are, according to geologist Don Wilhelms, "a reasonable bet to be Descartes." The next stop, station six, was a 10 m blocky crater, where the astronauts believed they could sample the Cayley Formation as evidenced by the firmer soil found there. Bypassing station seven to save time, they arrived at station eight on the lower flank of Stone Mountain, where they sampled material on a ray from South Ray Crater for about an hour. There, they collected black and white breccias and smaller, crystalline rocks rich in plagioclase. At station nine, an area known as the "Vacant Lot," which was believed to be free of ejecta from South Ray, they spent about 40 minutes gathering samples. Twenty-five minutes after departing station nine, they arrived at the final stop of the day, halfway between the ALSEP site and the LM. There, they dug a double core and conducted several penetrometer tests along a line stretching 50 m east of the ALSEP. At the request of Young and Duke, the moonwalk was extended by ten minutes. After returning to the LM to wrap up the second lunar excursion, they climbed back inside the landing craft's cabin, sealing and pressurizing the interior after 7 hours, 23 minutes, and 26 seconds of EVA time, breaking a record that had been set on Apollo 15. After eating a meal and proceeding with a debriefing on the day's activities with Mission Control, they reconfigured the LM cabin and prepared for the sleep period.
Flight day seven was their third and final day on the lunar surface, returning to orbit to rejoin Mattingly in the Command/Service Module following the day's moonwalk. During the third and final lunar excursion, they were to explore North Ray Crater, the largest of any of the craters any Apollo expedition had visited. After exiting "Orion", the pair drove the lunar rover 0.8 km away from the LM before adjusting their heading to travel 1.4 km to North Ray Crater. The drive was smoother than that of the previous day, as the craters were shallower and boulders were less abundant north of the immediate landing site. Boulders gradually became larger and more abundant as they approached North Ray in the lunar rover. Upon arriving at the rim of North Ray crater, they were 4.4 km away from the LM. After their arrival, the duo took photographs of the 1 km wide and 230 m deep crater. They visited a large boulder, taller than a four-story building, which became known as 'House Rock'. Samples obtained from this boulder delivered the final blow to the pre-mission volcanic hypothesis, proving it incorrect. House Rock had numerous bullet hole-like marks where micrometeoroids from space had impacted the rock. About 1 hour and 22 minutes after arriving, they departed for station 13, a large boulder field about 0.5 km from North Ray. On the way, they set a lunar speed record, traveling at an estimated 17.1 km/h downhill. They arrived at a 3 m high boulder, which they called 'Shadow Rock'. Here, they sampled permanently shadowed soil. During this time, Mattingly was preparing the Command/Service Module in anticipation their return approximately six hours later. After three hours and six minutes, they returned to the LM, where they completed several experiments and offloaded the rover. A short distance from the LM, Duke placed a photograph of his family and a United States Air Force commemorative medallion on the surface. Young drove the rover to a point about 90 m east of the LM, known as the 'VIP site,' so its television camera, controlled remotely by Mission Control, could observe Apollo 16's liftoff from the Moon. They then reentered the LM after a 5 hour and 40 minute final excursion. After pressurizing the LM cabin, the crew began preparing to return to lunar orbit.
Return to Earth.
Eight minutes before departing the lunar surface, CAPCOM James Irwin notified Young and Duke from Mission Control that they were go for liftoff. Two minutes before launch, they activated the "Master Arm" switch and then the "Abort Stage" button, after which they awaited ignition of "Orion"’s ascent stage engine. When the ascent stage ignited, small explosive charges severed the ascent stage from the descent stage and cables connecting the two were severed by a guillotine-like mechanism. Six minutes after liftoff, at a speed of about 5000 km/h, Young and Duke reached lunar orbit. Young and Duke successfully rendezvoused and re-docked with Mattingly in the Command/Service Module. To minimize the transfer of lunar dust from the LM cabin into the CSM, Young and Duke cleaned the cabin before opening the hatch separating the two spacecraft. After opening the hatch and reuniting with Mattingly, the crew transferred the samples Young and Duke had collected on the surface into the CSM for transfer to Earth. After transfers were completed, the crew would sleep before jettisoning the empty Lunar Module ascent stage the next day, when it was to be crashed intentionally into the lunar surface.
The next day, after final checks were completed, the expended LM ascent stage was jettisoned. Because of a failure by the crew to activate a certain switch in the LM before sealing it off, it initially tumbled after separation and did not execute the rocket burn necessary for the craft's intentional de-orbit. The ascent stage eventually crashed into the lunar surface nearly a year after the mission. The crew's next task, after jettisoning the Lunar Module ascent stage, was to release a subsatellite into lunar orbit from the CSM's Scientific Instrument Bay. The burn to alter the CSM's orbit to that desired for the subsatellite had been cancelled; as a result, the subsatellite lasted half of its anticipated lifetime. Just under five hours later, on the CSM's 65th orbit around the Moon, its Service Propulsion System main engine was reignited to propel the craft on a trajectory that would return it to Earth. The SPS engine performed the burn flawlessly despite the malfunction that had delayed the lunar landing several days before.
At a distance of about 170000 nmi from Earth, Mattingly performed a "deep-space" extra-vehicular activity, or spacewalk, during which he retrieved several film cassettes from the CSM's SIM bay. While outside the spacecraft, Mattingly set up a biological experiment, the Microbial Ecology Evaluation Device (MEED). The MEED experiment was only performed on Apollo 16. The crew carried out various housekeeping and maintenance tasks aboard the spacecraft and ate a meal before concluding the day.
The penultimate day of the flight was largely spent performing experiments, aside from a twenty-minute press conference during the second half of the day. During the press conference, the astronauts answered questions pertaining to several technical and non-technical aspects of the mission prepared and listed by priority at the Manned Spacecraft Center in Houston by journalists covering the flight. In addition to numerous housekeeping tasks, the astronauts prepared the spacecraft for its atmospheric reentry the next day. At the end of the crew's final full day in space, the spacecraft was approximately 77000 nmi from Earth and closing at a rate of about 7000 ft/s.
When the wake-up call was issued to the crew for their final day in space by CAPCOM Tony England, it was about 45000 nmi out from Earth, traveling just over 9000 ft/s. Just over three hours before splashdown in the Pacific Ocean, the crew performed a final course correction burn, changing their velocity by 1.4 ft/s. Approximately ten minutes before reentry into Earth's atmosphere, the cone-shaped Command Module containing the three crewmembers separated from the Service Module, which would burn up during reentry. At 265 hours and 37 minutes into the mission, at a velocity of about 36000 ft/s, Apollo 16 began atmospheric reentry. At its maximum, the temperature of the heat shield was between 4000 and. After successful parachute deployment and less than 14 minutes after reentry began, the Command Module splashed down in the Pacific Ocean 350 km southeast of the island of Kiritimati (or "Christmas Island"), 290 hours, 37 minutes, 6 seconds after liftoff. The spacecraft and its crew was retrieved by the USS "Ticonderoga". They were safely aboard the "Ticonderoga" 37 minutes after splashdown.
Lunar subsatellite PFS-2.
The Apollo 16 subsatellite (PFS-2) was a small satellite released into lunar orbit from the Service Module. Its principal objective was to measure charged particles and magnetic fields all around the Moon as the Moon orbited Earth, similar to its sister spacecraft, PFS-1, released eight months earlier by Apollo 15. "The low orbits of both subsatellites were to be similar ellipses, ranging from 55 to above the lunar surface."
"Instead, something bizarre happened. The orbit of PFS-2 rapidly changed shape and distance from the Moon. In 2-1/2 weeks the satellite was swooping to within a hair-raising 6 mi of the lunar surface at closest approach. As the orbit kept changing, PFS-2 backed off again, until it seemed to be a safe 30 miles away. But not for long: inexorably, the subsatellite's orbit carried it back toward the Moon. And on May 29, 1972—only 35 days and 425 orbits after its release"—PFS-2 crashed into the Lunar surface.
In later years, through a study of many lunar orbiting satellites, scientists came to discover that most low lunar orbits (LLO) are unstable. PFS-2 had been placed, unknown to mission planners at the time, squarely into one of the most unstable of orbits, at 11 degrees orbital inclination, far from the four "frozen lunar orbits" discovered only later at 27º, 50º, 76º, and 86º inclination.
Spacecraft locations.
The aircraft carrier USS "Ticonderoga" delivered the Apollo 16 Command Module to the North Island Naval Air Station, near San Diego, California, on Friday, May 5, 1972. On Monday, May 8, 1972, ground service equipment being used to empty the residual toxic reaction control system fuel in the Command Module tanks exploded in a Naval Air Station hangar. Forty-six people were sent to the hospital for 24 to 48 hours observation, most suffering from inhalation of toxic fumes. Most seriously injured was a technician who suffered a fractured kneecap when the GSE cart overturned on him. A hole was blown in the hangar roof 250 feet above; about 40 windows in the hangar were shattered. The Command Module suffered a three-inch gash in one panel.
The Apollo 16 Command Module "Casper" is on display at the U.S. Space & Rocket Center in Huntsville, Alabama. The Lunar Module ascent stage separated 24 April 1972 but a loss of attitude control rendered it out of control. It orbited the Moon for about a year. Its impact site on the Moon is unknown.
Duke donated some flown items, including a lunar map, to Kennesaw State University in Kennesaw, Georgia. He left two items on the Moon, both of which he photographed. The most famous is a plastic-encased photo portrait of his family (NASA Photo AS16-117-18841). The reverse of the photo is signed by Duke's family and bears this message: "This is the family of Astronaut Duke from Planet Earth. Landed on the Moon, April 1972." The other item was a commemorative medal issued by the United States Air Force, which was celebrating its 25th anniversary in 1972. He took two medals, leaving one on the Moon and donating the other to the Wright-Patterson Air Force Base museum.
In 2006, shortly after Hurricane Ernesto affected Bath, North Carolina, eleven-year-old Kevin Schanze discovered a piece of metal debris on the ground near his beach home. Schanze and a friend discovered a "stamp" on the 36 in flat metal sheet, which upon further inspection turned out to be a faded copy of the Apollo 16 mission insignia. NASA later confirmed the object to be a piece of the first stage of the Saturn V rocket that launched Apollo 16 into space. In July 2011, after returning the piece of debris at NASA's request, 16-year-old Schanze was given an all-access tour of the Kennedy Space Center and VIP seating for the launch of STS-135, the final mission of the Space Shuttle program.
References.
 This article incorporates  from websites or documents of the .
Bibliography.
</dl>

</doc>
<doc id="1971" url="http://en.wikipedia.org/wiki?curid=1971" title="Apollo 17">
Apollo 17

Apollo 17 was the final mission of the United States' Apollo program, the enterprise that landed the first humans on the Moon. Launched at 12:33 am Eastern Standard Time (EST) on December 7, 1972, with a three-member crew consisting of Commander Eugene Cernan, Command Module Pilot Ronald Evans, and Lunar Module Pilot Harrison Schmitt, It was the last use of Apollo hardware for its original mission. After Apollo 17, extra Apollo spacecraft were used in the Skylab and Apollo–Soyuz Test Project programs.
Apollo 17 was the first night launch of a U.S. human spaceflight and the final crewed launch of a Saturn V rocket. It was a "J-type mission," which included a three-day lunar surface stay, extended scientific capability, and the third Lunar Roving Vehicle (LRV). While Evans remained in lunar orbit above in the Command/Service Module (CSM), Cernan and Schmitt spent just over three days on the lunar surface in the Taurus–Littrow valley, conducting three periods of extra-vehicular activity, or moonwalks, during which they collected lunar samples and deployed scientific instruments. Cernan, Evans, and Schmitt returned to Earth on December 19 after an approximately 12-day mission.
The decision to land in the Taurus-Littrow valley was made with the primary objectives for Apollo 17 in mind: to sample lunar highland material older than the impact that formed Mare Imbrium and investigating the possibility of relatively young volcanic activity in the same vicinity. Taurus-Littrow was selected with the prospects of finding highland material in the valley's north and south walls and the possibility that several craters in the valley surrounded by dark material could be linked to volcanic activity.
Apollo 17 also broke several records set by previous flights, including the longest manned lunar landing flight; the longest total lunar surface extravehicular activities; the largest lunar sample return, and the longest time in lunar orbit. Apollo 17 remains the most recent manned Moon landing and also the last time humans have travelled beyond low Earth orbit.
Crew.
Eugene Cernan, Ronald Evans, and former X-15 pilot Joe Engle were assigned to the backup crew of Apollo 14. Engle flew sixteen X-15 flights, three of which exceeded the 50 mi border of space. Following the rotation pattern that a backup crew would fly as the prime crew three missions later, Cernan, Evans, and Engle would have flown Apollo 17. Harrison Schmitt served on the backup crew of Apollo 15 and, following the crew rotation cycle, was slated to fly as Lunar Module Pilot on Apollo 18. However, Apollo 18 was cancelled in September 1970. Following this decision, the scientific community pressured NASA to assign a geologist to an Apollo landing, as opposed to a pilot trained in geology. In light of this pressure, Harrison Schmitt, a professional geologist, was assigned the Lunar Module Pilot position on Apollo 17.
Subsequent to the decision to assign Schmitt to Apollo 17, there remained the question of which crew (the full backup crew of Apollo 15, Dick Gordon, Vance Brand, and Schmitt, or the backup crew of Apollo 14) would become prime crew of the mission. NASA Director of Flight Crew Operations Deke Slayton ultimately assigned the backup crew of Apollo 14 (Cernan and Evans), along with Schmitt, to the prime crew of Apollo 17.
Backup crew.
Replacement.
The Apollo 15 prime crew received the backup assignment since this was to be the last lunar mission and the backup crew would not rotate to another mission. However, when the Apollo 15 postage stamp incident became public in early 1972 the crew was reprimanded by NASA and the United States Air Force (they were active duty officers). Director of Flight Crew Operations Deke Slayton removed them from flight status and replaced them with Young and Duke from the Apollo 16 prime crew and Roosa from the Apollo 14 prime and Apollo 16 backup crews.
Mission insignia.
The insignia's most prominent feature is an image of the Greek sun god Apollo backdropped by a rendering of an American eagle, the red bars on the eagle mirroring those on the flag of the United States. Three white stars above the red bars represent the three crewmen of the mission. The background includes the Moon, the planet Saturn and a galaxy or nebula. The wing of the eagle partially overlays the Moon, suggesting man's established presence there. The gaze of Apollo and the direction of the eagle's motion embody man's intention to explore further destinations in space.
The patch includes, along with the colors of the U.S. flag (red, white, and blue), the color gold, representative of a "golden age" of spaceflight that was to begin with Apollo 17. The image of Apollo in the mission insignia is a rendering of the "Apollo Belvedere" sculpture. The insignia was designed by Robert McCall, with input from the crew.
Planning and training.
Like Apollo 15 and Apollo 16, Apollo 17 was slated to be a "J-mission," an Apollo mission type that featured lunar surface stays of three days, higher scientific capability, and the usage of the Lunar Roving Vehicle. Since Apollo 17 was to be the final lunar landing of the Apollo program, high-priority landing sites that had not been visited previously were given consideration for potential exploration. A landing in the crater Copernicus was considered, but was ultimately rejected because Apollo 12 had already obtained samples from that impact, and three other Apollo expeditions had already visited the vicinity of Mare Imbrium. A landing in the lunar highlands near the crater Tycho was also considered, but was rejected because of the rough terrain found there and a landing on the lunar far side in the crater Tsiolkovskiy was rejected due to technical considerations and the operational costs of maintaining communication during surface operations. A landing in a region southwest of Mare Crisium was also considered, but rejected on the grounds that a Soviet spacecraft could easily access the site; Luna 20 eventually did so shortly after the Apollo 17 site selection was made.
After the elimination of several sites, three sites made the final consideration for Apollo 17: Alphonsus crater, Gassendi crater, and the Taurus-Littrow valley. In making the final landing site decision, mission planners took into consideration the primary objectives for Apollo 17: obtaining old highlands material from a substantial distance from Mare Imbrium, sampling material from young volcanic activity (i.e., less than three billion years), and having minimal ground overlap with the orbital ground tracks of Apollo 15 and Apollo 16 to maximize the amount of new data obtained.
The Taurus-Littrow site was selected with the prediction that the crew would be able to obtain samples of old highland material from the remnants of a landslide event that occurred on the south wall of the valley and the possibility of relatively young, explosive volcanic activity in the area. Although the valley is similar to the landing site of Apollo 15 in that it is on the border of a lunar mare, the advantages of Taurus-Littrow were believed to outweigh the drawbacks, thus leading to its selection as the Apollo 17 landing site.
Apollo 17 was the only lunar landing mission to carry the Traverse Gravimeter Experiment (TGE), an experiment built by Draper Laboratory at the Massachusetts Institute of Technology designed to provide relative gravity measurements throughout the landing site at various locations during the mission's moonwalks. Scientists would then use this data to gather information about the geological substructure of the landing site and the surrounding vicinity.
As with previous lunar landings, the Apollo 17 astronauts underwent an extensive training program that included training to collect samples on the surface, usage of the spacesuits, navigation in the Lunar Roving Vehicle, field geology training, survival training, splashdown and recovery training, and equipment training.
Mission hardware and experiments.
Traverse Gravimeter.
Apollo 17 was the only Apollo lunar landing mission to carry the Traverse Gravimeter Experiment. As gravimeters have proven to be useful in the geologic investigation of the Earth, the objective of this experiment was to determine the feasibility of using the same techniques on the Moon to learn about its internal structure. The gravimeter was used to obtain readings at the landing site in the immediate vicinity of the Lunar Module (LM), as well as various locations on the mission's traverse routes. The TGE was carried on the Lunar Roving Vehicle; measurements were taken by the astronauts while the LRV was not in motion or after the gravimeter was placed on the surface.
A total of twenty-six measurements were taken with the TGE during the mission's three moonwalks, with productive results. As part of the Apollo Lunar Surface Experiments Package (ALSEP), the astronauts also deployed the Lunar Surface Gravimeter, a similar experiment, which ultimately failed to function properly.
Scientific Instrument Module.
Sector one of the Apollo 17 Service Module (SM) contained the Scientific Instrument Module (SIM) bay. The SIM bay housed three experiments for use in lunar orbit: a lunar sounder, an infrared scanning radiometer, and a far-ultraviolet spectrometer. A mapping camera, panoramic camera, and a laser altimeter were also included in the SIM bay.
The lunar sounder beamed electromagnetic impulses toward the lunar surface, which were designed with the objective of obtaining data to assist in developing a geological model of the interior of the Moon to an approximate depth of 1.3 km.
The Infrared Scanning Radiometer was designed with the objective of generating a temperature map of the lunar surface to aid in locating surface features such as rock fields, structural differences in the lunar crust, and volcanic activity.
The Far-Ultraviolet Spectrometer was to be used to obtain data pertaining to the composition, density, and constituency of the lunar atmosphere. The spectrometer was also designed to detect far-UV radiation emitted by the Sun that has been reflected off the lunar surface.
The Laser Altimeter was designed with the intention of measuring the altitude of the spacecraft above the lunar surface within approximately two meters (6.5 feet), and providing altitude information to the panoramic and mapping cameras.
Light flash phenomenon.
Throughout the Apollo lunar missions, the crew members observed light flashes that penetrated closed eyelids. These flashes, described as "streaks" or "specks" of light, were usually observed by astronauts while the spacecraft was darkened during a sleep period. These flashes, while not observed on the lunar surface, would average about two per minute and were observed by the crew members during the trip out to the Moon, back to Earth, and in lunar orbit.
The Apollo 17 crew conducted an experiment, also conducted on Apollo 16, with the objective of linking these light flashes with cosmic rays. As part of an experiment conducted by NASA and the University of Houston, one astronaut wore a device that recorded the time, strength, and path of high-energy atomic particles that penetrated the device. Analysis of the results concluded that the evidence supported the hypothesis that the flashes occurred when charged particles travelled through the retina in the eye.
Surface Electrical Properties Experiment.
Apollo 17 was the only lunar surface expedition to include the Surface Electrical Properties (SEP) experiment. The experiment included two major components: a transmitting antenna deployed near the Lunar Module and a receiving antenna located on the Lunar Roving Vehicle. At different stops during the mission's traverses, electrical signals traveled from the transmitting device, through the ground, and received at the LRV. The electrical properties of the lunar soil could be determined by comparison of the transmitted and received electrical signals. The results of this experiment, which are consistent with lunar rock composition, show that the top 2 km of the Moon are extremely dry.
Lunar Roving Vehicle.
Apollo 17 was the third mission (the others being Apollo 15 and Apollo 16) to make use of a Lunar Roving Vehicle. The LRV, in addition to being used by the astronauts for transport from station to station on the mission's three moonwalks, was used to transport the astronauts' tools, communications equipment, and samples. The Apollo 17 LRV was also used to carry experiments unique to the mission, such as the Traverse Gravimeter and Surface Electrical Properties experiment. The Apollo 17 LRV traveled a cumulative distance of approximately 35.9 km in a total drive time of about four hours and twenty-six minutes; the greatest distance Eugene Cernan and Harrison Schmitt traveled from the Lunar Module was about 7.6 km.
Biological cosmic ray experiment.
Apollo 17 included a biological cosmic ray experiment (BIOCORE), carrying mice that had been implanted with radiation monitors to see whether they suffered damage from cosmic rays.
Five pocket mice ("Perognathus longimembris") were implanted with radiation monitors under their scalps and flown on the mission. The species was chosen because it was well-documented, small, easy to maintain in an isolated state (not requiring drinking water for the duration of the mission and with highly concentrated waste), and for its ability to withstand environmental stress. Four of the five mice survived the flight; the cause of death of the fifth mouse was not determined.
The study found lesions in the scalp itself and liver. The scalp lesions and liver lesions appeared to be unrelated to one another, and were not thought to be the result of cosmic rays. No damage was found in the mice's retinas or viscera. At the time of the publication of the Apollo 17 Preliminary Science Report, the mouse brains had not yet been examined. However, subsequent studies showed no significant effect on the brains.
Mission highlights.
Launch and outbound trip.
Apollo 17 was launched at 12:33 am EST on December 7, 1972, from launch pad 39-A at the Kennedy Space Center. It was the last manned Saturn V launch and the only night launch. The launch was delayed by two hours and forty minutes due to an automatic cutoff in the launch sequencer at the T-30 second mark in the countdown. The issue was quickly determined to be a minor technical error. The clock was reset and held at the T-22 minute mark while technicians worked around the malfunction in order to continue with the launch. This pause was the only launch delay in the Apollo program caused by this type of hardware failure. The count resumed and the rocket lifted off achieving a normal low Earth orbit.
Approximately 500,000 people were estimated to have observed the launch in the immediate vicinity of Kennedy Space Center, despite the early morning hour. The launch was visible as far away as 800 km; observers in Miami, Florida, saw a "red streak" crossing the northern sky.
At 3:46 am EST, the S-IVB third stage was re-ignited to propel the spacecraft towards the Moon.
At approximately 2:47 pm EST on December 10, the Service Propulsion System engine on the Command/Service Module ignited to slow down the CSM/Lunar Module stack into lunar orbit. Following orbit insertion and orbital stabilization, the crew began preparations for landing in the Taurus-Littrow valley.
Moon Landing.
After separating from the Command/Service Module, the Lunar Module "Challenger" and its crew of two, Eugene Cernan and Harrison Schmitt, adjusted their orbit and began preparations for the descent to Taurus-Littrow. While Cernan and Schmitt prepared for landing, Command Module Pilot Ron Evans remained in orbit to take observations, perform experiments and await the return of his crew-mates a few days later.
Soon after completing their preparations for landing, Cernan and Schmitt began their descent to the Taurus-Littrow valley on the lunar surface. Several minutes after the descent phase was initiated, the Lunar Module pitched over, giving the crew their first look at the landing site during the descent phase and allowing Cernan to guide the spacecraft to a desirable landing target while Schmitt provided data from the flight computer essential for landing. The LM touched down on the lunar surface at 2:55 pm EST on December 11. Shortly thereafter, the two astronauts began re-configuring the LM for their stay on the surface and began preparations for the first moonwalk of the mission, or EVA-1.
Lunar surface.
The first moonwalk (EVA) of the mission began approximately four hours after landing, at about 6:55 pm on December 11. The first task of the first lunar excursion was to offload the Lunar Roving Vehicle and other equipment from the Lunar Module. While working near the rover, a fender was accidentally broken off when Gene Cernan brushed up against it, his hammer getting caught under the right-rear fender, breaking off the rear extension. The same incident had also occurred on Apollo 16 as Commander John Young maneuvered around the rover. Although this was not a mission-critical issue, the loss of the fender caused Cernan and Schmitt to be covered with dust thrown up when the rover was in motion. The crew used duct tape to fix the problem, but the dust picked up on the surface prevented the tape from sticking for the length of the exploration. The crew then deployed the Apollo Lunar Surface Experiments Package (ALSEP) west of the immediate landing site. After completing this, Cernan and Schmitt departed on the first geologic traverse of the mission towards Steno crater to the south of the landing site, during which they gathered 14 kg of samples; took seven gravimeter measurements; and deployed two explosive packages, which were later detonated remotely to test geophones that had been placed by the astronauts and seismometers that had been placed on previous Apollo missions. The EVA ended after seven hours and twelve minutes.
On December 12, at 6:28 pm EST, Cernan and Schmitt began their second lunar excursion. One of the first tasks of the EVA was repairing the right-rear fender on the LRV, the rearward extension of which had been broken off the previous day. The pair did this by taping together four cronopaque maps with duct tape and clamping the replacement fender extension to the fender, thus providing a means of preventing dust from raining down upon them while in motion. During this EVA, the pair sampled several different types of geologic deposits found in the valley, including the avalanche at the base of the South Massif, orange-colored soil at Shorty crater, and ejecta of Camelot crater. The crew completed this moonwalk after seven hours and thirty-seven minutes. They collected 34 kg of samples, deployed three more explosive packages and took seven gravimeter measurements.
The third moonwalk, the last of the Apollo program, began at 5:26 pm EST on December 13. During this excursion, the crew collected 66 kg of lunar samples and took nine gravimeter measurements. They drove the rover to the north and east of the landing site and explored the base of the North Massif, the Sculptured Hills, and the unusual crater Van Serg. Before ending the moonwalk, the crew collected a rock, a breccia, and dedicated it to several different nations which were represented in Mission Control Center in Houston, Texas, at the time. A plaque located on the Lunar Module, commemorating the achievements made during the Apollo program, was then unveiled. Before reentering the LM for the final time, Gene Cernan expressed his thoughts:
...I'm on the surface; and, as I take man's last step from the surface, back home for some time to come - but we believe not too long into the future - I'd like to just [say] what I believe history will record. That America's challenge of today has forged man's destiny of tomorrow. And, as we leave the Moon at Taurus-Littrow, we leave as we came and, God willing, as we shall return, with peace and hope for all mankind. "Godspeed the crew of Apollo 17."
Cernan then followed Schmitt into the Lunar Module after spending approximately seven hours and 15 minutes outside during the mission's final lunar excursion.
Return to Earth.
Eugene Cernan and Harrison Schmitt successfully lifted off from the lunar surface in the ascent stage of the Lunar Module on December 14, at 5:55 pm EST. After a successful rendezvous and docking with Ron Evans in the Command/Service Module in orbit, the crew transferred equipment and lunar samples between the LM and the CSM for return to Earth. Following this, the LM ascent stage was sealed off and jettisoned at 1:31 am on December 15. The ascent stage was then deliberately crashed into the Moon in a collision recorded by seismometers deployed on Apollo 17 and previous Apollo expeditions.
On December 17, during the trip back to Earth, at 3:27 pm EST, Ron Evans successfully conducted a one hour and seven minute spacewalk to retrieve exposed film from the instrument bay on the exterior of the CSM.
On December 19, the crew jettisoned the no-longer-needed Service Module, leaving only the Command Module for return to Earth. The Apollo 17 spacecraft reentered Earth's atmosphere and landed safely in the Pacific Ocean at 2:25 pm, 6.4 km from the recovery ship, the USS "Ticonderoga". Cernan, Evans and Schmitt were then retrieved by a recovery helicopter and were safely aboard the recovery ship 52 minutes after landing.
Spacecraft locations.
The Command Module "America" is currently on display at Space Center Houston at the Lyndon B. Johnson Space Center in Houston, Texas.
The ascent stage of lunar module "Challenger" impacted the Moon December 15, 1972 at 06:50:20.8 UT (1:50 am EST), at . The descent stage remains on the Moon at the landing site, .
In 2009 and again in 2011, the Lunar Reconnaissance Orbiter photographed the landing site from increasingly low orbits.
Depiction of mission in fiction.
Portions of the Apollo 17 mission are dramatized in the 1998 HBO miniseries "From the Earth to the Moon" episode entitled "Le Voyage dans la Lune."
The prologue to the 1999 novel "Back to the Moon", by Homer Hickam, begins with a dramatized depiction of the end of the second Apollo 17 EVA. The orange soil then becomes the major driver of the plot of the rest of the story.
The 2005 novel "Tyrannosaur Canyon" by Douglas Preston opens with a depiction of the Apollo 17 moonwalks using quotes taken from the official mission transcript.
Additionally, there have been fictional astronauts in film, literature and television who have been described as "the last man to walk on the Moon," implying they were crew members on Apollo 17. One such character was Steve Austin in the television series "The Six Million Dollar Man". In the 1972 novel "Cyborg", upon which the series was based, Austin remembers watching the Earth "fall away during Apollo XVII." In the 1998 film "Deep Impact" fictional astronaut Spurgeon "Fish" Tanner, portrayed by Robert Duvall, was described at a Presidential press conference as the "last man to walk on the moon" by the President of the United States, portrayed by Morgan Freeman.
In the Anime Aldnoah.Zero, the Apollo 17 mission locates an ancient transporter gate leading to Mars left by an unknown, extinct alien race. This discovery is the divergence point for the story's alternate history.
References.
 This article incorporates  from websites or documents of the .
Bibliography.
</dl>

</doc>
<doc id="1973" url="http://en.wikipedia.org/wiki?curid=1973" title="American Revolution">
American Revolution

The American Revolution was a political upheaval that took place between 1765 and 1783 during which rebel colonists in the Thirteen American Colonies rejected the British monarchy and aristocracy, overthrew the authority of Great Britain, and founded the United States of America.
Starting in 1765, members of American colonial society rejected the authority of the British Parliament to tax them and resisted renewed British attempts to collect duties on goods such as sugar and molasses that for many years had gone uncollected through widespread smuggling by colonists. During the following decade, protests by rebellious colonists—known as patriots—continued to escalate, as in the Boston Tea Party in 1773 during which patriots destroyed a consignment of taxed English tea whose price had been reduced to combat smuggling. The British responded by imposing punitive laws—the Coercive Acts—on Massachusetts in 1774 until the tea had been paid for, following which Patriots in the other colonies rallied behind Massachusetts. In late 1774 the Patriots set up their own alternative government to better coordinate their resistance efforts against Britain, while other colonists, known as loyalists, preferred to remain subjects of the British Crown. For some who identified themselves with the Patriot cause, particularly colonial merchants in Virginia, a break with Britain offered a chance to repudiate long-standing debts to British creditors. 
Tensions escalated to the outbreak of fighting between Patriot militia and British regulars at Lexington and Concord in April 1775, after which the Patriot Suffolk Resolves effectively replaced the Royal government of Massachusetts and confined the British to control of the city of Boston. The conflict then evolved into a civil war, during which the Patriots (and later their French, Spanish and Dutch allies) fought the British and Loyalists in what became known as the American Revolutionary War (1775–1783). Patriots in each of the thirteen colonies formed a Provincial Congress that usurped power from the old colonial governments and suppressed Loyalism. Claiming King George III's rule to be tyrannical and infringing the colonists' "rights as Englishmen", the Continental Congress declared the colonies free and independent states in July 1776. The Patriot leadership professed the political philosophies of liberalism and republicanism to reject monarchy and aristocracy, and proclaimed that all men are created equal. Congress rejected British proposals for compromise that would keep them under the king.
The British were forced out of Boston in 1776, but then captured and held New York City for the duration of the war, nearly capturing General Washington and his army. The British blockaded the ports and captured other cities for brief periods, but failed to defeat Washington's forces. In early 1778, following a failed patriot invasion of Canada, a British army was captured by a patriot army at the Battle of Saratoga, following which the French entered the war as allies of the United States. The war later turned to the American South, where the British captured an army at South Carolina, but failed to enlist enough volunteers from Loyalist civilians to take effective control. A combined American–French force captured a second British army at Yorktown in 1781, effectively ending the war in the United States. A peace treaty in 1783 confirmed the new nation's complete separation from the British Empire. The United States took possession of nearly all the territory east of the Mississippi River and south of the Great Lakes, with the British retaining control of Canada and Spain taking Florida.
In the period after the peace treaty in 1783, Loyalists were subjected to extreme suppression and acts of arbitrary violence, including murder by lynching, despite a promise by patriot leaders to British negotiators that Loyalist rights would be respected. A large proportion were driven off their land and forced to flee as refugees to Canada.
Among the significant results of the revolution was the creation of a democratically-elected representative government theoretically responsible to the will of the people, but which as a result of the 'Three-Fifths Compromise' allowed the southern slaveholders to consolidate power and maintain slavery in America for another eighty years. The new Constitution established a relatively strong federal national government that included a strong elected president, national courts, a bicameral Congress that represented both states in the Senate and population in the House of Representatives. Congress had powers of taxation that were lacking under the old Articles. The United States Bill of Rights of 1791 comprised the first ten amendments to the Constitution, guaranteeing many "natural rights" that were influential in justifying the revolution, and attempted to balance a strong national government with strong state governments and broad personal liberties. The American shift to liberal republicanism, and the gradually increasing democracy, caused an upheaval of traditional social hierarchy and gave birth to the ethic that has formed a core of political values in the United States.
Origins.
Background to 1763.
The British began colonizing North America in the 17th century. The colonies established along the Atlantic coast were governed by charters granted by the King, each permitting a substantial amount of self-governance. Crown colonies (Massachusetts, New Hampshire, New York, New Jersey, Virginia, North Carolina, South Carolina and Georgia) imitated the "mixed monarchy" constitutional structure of Great Britain. Each had an elected assembly which constituted the lower house of the legislature, a council appointed (except in Massachusetts) by the crown constituting the upper house, and an appointed governor with executive powers representing the King. All laws had to be submitted to the home government for approval, but otherwise there was little interference. Proprietary colonies (Pennsylvania, Delaware, and Maryland) also had elected assemblies but the proprietors, not the crown, appointed the governors. Charter colonies (Connecticut and Rhode island) elected both houses of the legislature and the governor and did not have to submit their laws for approval.
Parliament legislated regarding matters of an imperial concern. As early as 1621 London introduced legislation to levy duties on shipments of Virginian tobacco that passed into English ports, though in return the planters enjoyed protection and a guaranteed market. English growers were prohibited by law to raise tobacco crops, and producers of rice and indigo (a blue plant dye) in South Carolina received similar privileges. In common with all European nations which had colonies, the English Navigation Acts of the late 17th century restricted colonial trade for the benefit of the mother country in accordance with mercantilist theory. To ensure adequate auxiliary vessels were available in wartime, the Acts also encouraged the colonists to invest in shipping, but particularly in New England, an unintentional outcome was a flourishing and very hard to control, smuggling industry. The sheer scale of the problem of patrolling 3000 miles of American coastline with a tiny number of English customs and revenue cutters meant that colonial shippers could evade duties with comparative ease. Because the Acts did not apply to inter-colonial trade, colonial shippers were afforded plenty of opportunities for bypassing the meagre British customs controls, using a mixture of convoluted routes, bribery and false paperwork which misrepresented or under-declared their cargoes so that very little duty was paid at all. The close proximity of the European island plantations in the Caribbean provided easy transit points for colonial shippers, who made regular round trips south to with livestock, timber, grain and tobacco which they bartered for slaves, fine cloth, linens, soap, sugar and molasses, used in the production of rum.
The French and Indian War ended in 1763 with the conquest of French Canada and the expulsion of France from mainland North America by British and provincial forces. The war left Britain in considerable debt, and it therefore made plans to ensure a more productive collection of existing duties from the colonists. In addition, following the Pontiac Rebellion, which led to considerable loss of life and territory by Native Americans, the British Crown issued the Royal Proclamation of 1763, which set a boundary running along the foot of the Appalachian Mountains from Florida and Georgia to the Gulf of the St. Lawrence beyond which colonists were not to settle. The purpose was to save money from having to administer any new lands and prevent additional war with the inhabitants.
1764–1766: Taxes imposed and withdrawn.
In 1764 Parliament passed the Currency Act to restrain the use of paper money that British merchants saw as a means to evade debt payments. Parliament also passed the Sugar Act imposing customs duties on a number of articles. That same year Prime Minister George Grenville proposed to impose direct taxes on the colonies to raise revenue, but delayed action to see if the colonies would propose some way to raise the revenue themselves. None did, and in March 1765 Parliament passed the Stamp Act which imposed direct taxes on the colonies for the first time. All official documents, newspapers, almanacs and pamphlets—even decks of playing cards—were required to have the stamps.
The colonists objected chiefly on the grounds not that the taxes were high (they were low), but because they had no representation in the Parliament. Benjamin Franklin testified in Parliament in 1766 that Americans already contributed heavily to the defense of the Empire. He said local governments had raised, outfitted and paid 25,000 soldiers to fight France—as many as Britain itself sent—and spent many millions from American treasuries doing so in the French and Indian War alone. Stationing a standing army in Great Britain during peacetime was politically unacceptable. London had to deal with 1,500 politically well-connected British officers who became redundant; it would have to discharge them or station them in North America.
In 1765 the Sons of Liberty formed. They used public demonstrations, boycott, violence and threats of violence to ensure that the British tax laws were unenforceable. While openly hostile to what they considered an oppressive Parliament acting illegally, colonists persisted in sending numerous petitions and pleas for intervention from a monarch to whom they still claimed loyalty. In Boston, the Sons of Liberty burned the records of the vice-admiralty court and looted the home of the chief justice, Thomas Hutchinson. Several legislatures called for united action, and nine colonies sent delegates to the Stamp Act Congress in New York City in October 1765. Moderates led by John Dickinson drew up a "Declaration of Rights and Grievances" stating that taxes passed without representation violated their rights as Englishmen. At the same time, however, they rejected the idea of being provided with representation in Parliament, declaring it impossible due to the distance involved. Colonists emphasized their determination by boycotting imports of British merchandise.
The Parliament at Westminster saw itself as the supreme lawmaking authority throughout all British possessions and thus entitled to levy any tax without colonial approval. They argued that the colonies were legally British corporations that were completely subordinate to the British parliament and pointed to numerous instances where Parliament had made laws binding on the colonies in the past. They did not see anything in the unwritten British constitution that made taxes special and noted that Parliament had taxed American trade for decades. Parliament insisted that the colonies effectively enjoyed a "virtual representation" like most British people did, as only a small minority of the British population elected representatives to Parliament. Americans such as James Otis maintained the Americans were not in fact virtually represented.
In London, the Rockingham government came to power (July 1765) and Parliament debated whether to repeal the stamp tax or to send an army to enforce it. Benjamin Franklin made the case for repeal, explaining the colonies had spent heavily in manpower, money, and blood in defense of the empire in a series of wars against the French and Indians, and that further taxes to pay for those wars were unjust and might bring about a rebellion. Parliament agreed and repealed the tax (February 21, 1766), but in the Declaratory Act of March 1766 insisted that parliament retained full power to make laws for the colonies "in all cases whatsoever". The repeal nonetheless caused widespread celebrations in the colonies.
1767–1773: Townshend Acts and the Tea Act.
In 1767 the Parliament passed the Townshend Acts, which placed duties on a number of essential goods including paper, glass, and tea and established a Board of Customs in Boston to more rigorously execute trade regulations. The new taxes were enacted on the belief that Americans only objected to internal taxes and not external taxes like custom duties. The Americans, however, argued against the constitutionality of the act because its purpose was to raise revenue and not regulate trade. Colonists responded by organizing new boycotts of British goods. These boycotts were less effective, however, as the Townshend goods were widely used.
In February 1768 the Assembly of Massachusetts Bay issued a circular letter to the other colonies urging them to coordinate resistance. The governor dissolved the assembly when it refused to rescind the letter. Meanwhile, in June 1768 a riot broke out in Boston over the seizure of the sloop "Liberty", owned by John Hancock, for alleged smuggling. Custom officials were forced to flee, prompting the British to deploy troops to Boston. A Boston town meeting declared no obedience was due to parliamentary laws and called for the convening of a convention. A convention assembled but only issued a mild protest before dissolving itself. In January 1769 Parliament responded to the unrest by reactivating the Treason Act 1543 which permitted subjects outside the realm to face trials for treason in England. The governor of Massachusetts was instructed to collect evidence of said treason, and although the threat was not carried out it caused widespread outrage.
On March 5, 1770 a large mob gathered around a group of British soldiers. The mob grew more and more threatening, throwing snowballs, rocks and debris at the soldiers. One soldier was clubbed and fell. There was no order to fire but the soldiers fired into the crowd anyway. They hit 11 people; three civilians died at the scene of the shooting, and two died after the incident. The event quickly came to be called the Boston Massacre. Although the soldiers were tried and acquitted (defended by John Adams), the widespread descriptions soon became propaganda to turn colonial sentiment against the British. This in turn began a downward spiral in the relationship between Britain and the Province of Massachusetts.
A new ministry under Lord North came to power in 1770 and Parliament withdrew all taxes except the tax on tea, giving up its efforts to raise revenue while maintaining the right to tax. This temporarily resolved the crisis and the boycott of British goods largely ceased, with only the more radical patriots such as Samuel Adams continuing to agitate.
In June 1772, in what became known as the "Gaspee" Affair, American patriots including John Brown burned a British warship that had been vigorously enforcing unpopular trade regulations. The affair was investigated for possible treason, but no action was taken.
In 1772 it became known that the Crown intended to pay fixed salaries to the governors and judges in Massachusetts. Samuel Adams in Boston set about creating new Committees of Correspondence, which linked Patriots in all 13 colonies and eventually provided the framework for a rebel government. In early 1773 Virginia, the largest colony, set up its Committee of Correspondence, on which Patrick Henry and Thomas Jefferson served.
A total of about 7000 to 8000 Patriots served on "Committees of Correspondence" at the colonial and local levels, comprising most of the leadership in their communities — Loyalists were excluded. The committees became the leaders of the American resistance to British actions, and largely determined the war effort at the state and local level. When the First Continental Congress decided to boycott British products, the colonial and local Committees took charge, examining merchant records and publishing the names of merchants who attempted to defy the boycott by importing British goods.
In 1773 private letters were published where Massachusetts Governor Thomas Hutchinson claimed the colonists could not enjoy all English liberties, and Lieutenant Governor Andrew Oliver called for the direct payment of colonial officials. The letters, whose contents were used as evidence of a systematic plot against American rights, discredited Hutchinson in the eyes of the people the Assembly petitioned for his recall. Benjamin Franklin, post-master general for the colonies, acknowledged that he leaked the letters which led to him being berated by British officials and fired from his job.
Meanwhile, Parliament passed the Tea Act to lower the price of taxed tea exported to the colonies in order to help the East India Company undersell smuggled Dutch tea. Special consignees were appointed to sell the tea in order to bypass colonial merchants. The act was opposed not only by those who resisted the taxes but also by smugglers who stood to lose business. In most instances the consignees were forced to resign and the tea was turned back, but Massachusetts governor Hutchinson refused to allow Boston merchants to give into pressure. A town meeting in Boston determined that the tea would not be landed, and ignored a demand from the governor to disperse. On December 16, 1773 a group of men, led by Samuel Adams and dressed to evoke American Indians, boarded the ships of the British East India Company and dumped £10,000 worth of tea from their holds (approximately £636,000 in 2008) into Boston Harbor. Decades later this event became known as the Boston Tea Party and remains a significant part of American patriotic lore.
1774–1775: Intolerable Acts and the Quebec Act.
The British government responded by passing several Acts which came to be known as the Intolerable Acts, which further darkened colonial opinion towards the British. They consisted of four laws enacted by the British parliament. The first, the Massachusetts Government Act, altered the Massachusetts charter and restricted town meetings. The second Act, the Administration of Justice Act, ordered that all British soldiers to be tried were to be arraigned in Britain, not in the colonies. The third Act was the Boston Port Act, which closed the port of Boston until the British had been compensated for the tea lost in the Boston Tea Party. The fourth Act was the Quartering Act of 1774, which allowed royal governors to house British troops in the homes of citizens without requiring permission of the owner.
In response, Massachusetts patriots issued the Suffolk Resolves and formed an alternative shadow government known as the "Provincial Congress" which began training militia outside British-occupied Boston. In September 1774, the First Continental Congress convened, consisting of representatives from each of the colonies, to serve as a vehicle for deliberation and collective action. During secret debates conservative Joseph Galloway proposed the creation of a colonial Parliament that would be able to approve or disapprove of acts of the British Parliament but his idea was not accepted. The Congress instead endorsed the proposal of John Adams that Americans would obey Parliament voluntarily but would resist all taxes in disguise. Congress called for a boycott beginning on 1 December 1774 of all British goods; it was enforced by new committees authorized by the Congress.
The Quebec Act of 1774 extended Quebec's boundaries to the Ohio River, shutting out the claims of the 13 colonies. By then, however, the Americans had little regard for new laws from London; they were drilling militia and organizing for war.
The British retaliated by confining all trade of the New England colonies to Britain and excluding them from the Newfoundland fisheries. Lord North advanced a compromise proposal in which Parliament would not tax so long as the colonies made fixed contributions for defense and to support civil government. This would also be rejected.
Creating new state constitutions.
Following the Battle of Bunker Hill in June 1775, the Patriots had control of Massachusetts outside the Boston city limits; the Loyalists suddenly found themselves on the defensive with no protection from the British army. In all 13 colonies, Patriots had overthrown their existing governments, closing courts and driving British officials away. They had elected conventions and "legislatures" that existed outside any legal framework; new constitutions were drawn up in each state to supersede royal charters. They declared that they were states now, not colonies.
On January 5, 1776, New Hampshire ratified the first state constitution. In May 1776, Congress voted to suppress all forms of crown authority, to be replaced by locally created authority. Virginia, South Carolina, and New Jersey created their constitutions before July 4. Rhode Island and Connecticut simply took their existing royal charters and deleted all references to the crown. The new states were all committed to republicanism, with no inherited offices. They decided not only what form of government to create, and also how to select those who would craft the constitutions and how the resulting document would be ratified. But there would be no universal suffrage and real power, including the right to elect the future President would still lay in the hands of a few selected elites for many years. On 26 May 1776 John Adams wrote James Sullivan from Philadelphia;
"Depend upon it, sir, it is dangerous to open so fruitful a source of controversy and altercation, as would be opened by attempting to alter the qualifications of voters. There will be no end of it. New claims will arise. Women will demand a vote. Lads from twelve to twenty one will think their rights not enough attended to, and every man, who has not a farthing, will demand an equal voice with any other in all acts of state. It tends to confound and destroy all distinctions, and prostrate all ranks, to one common level".
In states where the wealthy exerted firm control over the process, such as Maryland, Virginia, Delaware, New York and Massachusetts – the last-mentioned of these state's constitutions still being in force in the 21st century, continuously since its ratification on June 15, 1780 – the results were constitutions that featured:
In states where the less affluent had organized sufficiently to have significant power—especially Pennsylvania, New Jersey, and New Hampshire—the resulting constitutions embodied
The radical provisions of Pennsylvania's constitution lasted only 14 years. In 1790, conservatives gained power in the state legislature, called a new constitutional convention, and rewrote the constitution. The new constitution substantially reduced universal white-male suffrage, gave the governor veto power and patronage appointment authority, and added an upper house with substantial wealth qualifications to the unicameral legislature. Thomas Paine called it a constitution unworthy of America.
Military hostilities begin.
Massachusetts was declared in a state of rebellion in February 1775 and the British garrison received orders to disarm the rebels and arrest their leaders, leading to the Battles of Lexington and Concord on 19 April 1775. The Patriots set siege to Boston, expelled royal officials from all the colonies, and took control through the establishment of Provincial Congresses. The Battle of Bunker Hill followed on June 17, 1775. While a British victory, it was at a great cost; about 1,000 British casualties from a garrison of about 6,000, as compared to 500 American casualties from a much larger force. First ostensibly loyal to the king and desiring to govern themselves while remaining in the empire, the repeated pleas by the First Continental Congress for royal intervention on their behalf with Parliament resulted in the declaration by the King that the states were "in rebellion", and the members of Congress were traitors.
In the winter of 1775, the Americans invaded Canada. General Richard Montgomery captured Montreal but a joint attack on Quebec was a total failure; many Americans were captured or died of smallpox.
In March 1776, with George Washington as the commander of the new army, the Continental Army forced the British to evacuate Boston. The revolutionaries were now in full control of all 13 colonies and were ready to declare independence. While there still were many Loyalists, they were no longer in control anywhere by July 1776, and all of the Royal officials had fled.
Prisoners.
In August 1775, George III declared Americans in arms against royal authority to be traitors to the Crown. Following their surrender at the Battles of Saratoga in October 1777, there were thousands of British and Hessian soldiers in American hands. Although Lord Germain took a hard line, the British generals on the scene never held treason trials; they treated captured enemy soldiers as prisoners of war. The dilemma was that tens of thousands of Loyalists were under American control and American retaliation would have been easy. The British built much of their strategy around using these Loyalists. Therefore, no Americans were put on trial for treason. The British maltreated the prisoners they held, resulting in more deaths to American sailors and soldiers than from combat operations. At the end of the war, both sides released their surviving prisoners.
Independence and Union.
In April 1776 the North Carolina Provincial Congress issued the Halifax Resolves, explicitly authorizing its delegates to vote for independence. In May Congress called on all the states to write constitutions, and eliminate the last remnants of royal rule.
By June nine colonies were ready for independence; one by one the last four—Pennsylvania, Delaware, Maryland and New York—fell into line. Richard Henry Lee was instructed by the Virginia legislature to propose independence, and he did so on June 7, 1776. On the 11th a committee was created to draft a document explaining the justifications for separation from Britain. After securing enough votes for passage, independence was voted for on July 2. The Declaration of Independence, drafted largely by Thomas Jefferson and presented by the committee, was slightly revised and unanimously adopted by the entire Congress on July 4, marking the formation of a new sovereign nation, which called itself the United States of America.
The Second Continental Congress approved a new constitution, the "Articles of Confederation," for ratification by the states on November 15, 1777, and immediately began operating under their terms. The Articles were formally ratified on March 1, 1781. At that point, the Continental Congress was dissolved and on the following day a new government of the United States in Congress Assembled took its place, with Samuel Huntington as presiding officer.
Defending the Revolution.
British return: 1776–1777.
According to British historian Jeremy Black, the British had significant advantages including a highly trained army, the world's largest navy and a highly efficient system of public finance that could easily fund the war. However, the British were seriously handicapped by their misunderstanding of the depth of support for the Patriot position. Ignoring the advice of General Gage, they misinterpreted the situation as merely a large-scale riot. London decided that by sending a large military and naval force they could overawe the Americans and force them to be loyal again:
Convinced that the Revolution was the work of a full few miscreants who had rallied an armed rabble to their cause, they expected that the revolutionaries would be intimidated…. Then the vast majority of Americans, who were loyal but cowed by the terroristic tactics… would rise up, kick out the rebels, and restore loyal government in each colony.
After Washington forced the British out of Boston in the spring of 1776, neither the British nor the Loyalists controlled any significant areas. The British, however, were massing forces at their naval base at Halifax, Nova Scotia. They returned in force in July 1776, landing in New York and defeating Washington's Continental Army at the Battle of Brooklyn in August. After winning the Battle of Brooklyn, the British requested a meeting with representatives from Congress to negotiate an end to hostilities.
A delegation including John Adams and Benjamin Franklin met Howe on Staten Island in New York Harbor on September 11, in what became known as the Staten Island Peace Conference. Howe demanded a retraction of the Declaration of Independence, which was refused, and negotiations ended. The British then quickly seized New York City and nearly captured Washington's army. They made New York their main political and military base of operations in North America, holding it until November 1783. The city became the destination for Loyalist refugees, and a focal point of Washington's intelligence network.
The British also took New Jersey, pushing the Continental Army into Pennsylvania. In a surprise attack in late December 1776 Washington crossed the Delaware River back into New Jersey and defeated Hessian and British armies at Trenton and Princeton, thereby regaining control of most of New Jersey. The victories gave an important boost to Patriots at a time when morale was flagging, and have become iconic events of the war.
In 1777, as part of a grand strategy to end the war, the British sent an invasion force from Canada to seal off New England, which the British perceived as the primary source of agitators. In a major case of mis-coordination, the British army in New York City went to Philadelphia which it captured from Washington. The invasion army under Burgoyne waited in vain for reinforcements from New York, and became trapped in northern New York state. It surrendered after the Battle of Saratoga in October 1777. From early October 1777 until November 15 a pivotal siege at Fort Mifflin, Philadelphia, Pennsylvania distracted British troops and allowed Washington time to preserve the Continental Army by safely leading his troops to harsh winter quarters at Valley Forge.
American alliances after 1778.
The capture of a British army at Saratoga encouraged the French to formally enter the war in support of Congress, as Benjamin Franklin negotiated a permanent military alliance in early 1778, significantly becoming the first country to officially recognize the Declaration of Independence. On February 6, 1778, a Treaty of Amity and Commerce and a Treaty of Alliance were signed between the United States and France. William Pitt spoke out in parliament urging Britain to make peace in America, and unite with America against France, while other British politicians who had previously sympathised with colonial grievances now turned against the American rebels for allying with Britain's international rival and enemy.
Later Spain (in 1779) and the Dutch (1780) became allies of the French, leaving the British Empire to fight a global war alone without major allies, and requiring it to slip through a combined blockade of the Atlantic. The American theater thus became only one front in Britain's war. The British were forced to withdraw troops from continental America to reinforce the valuable sugar-producing Caribbean colonies, which were considered more important.
Because of the alliance with France and the deteriorating military situation, Sir Henry Clinton, the British commander, evacuated Philadelphia to reinforce New York City. General Washington attempted to intercept the retreating column, resulting in the Battle of Monmouth Court House, the last major battle fought in the north. After an inconclusive engagement, the British successfully retreated to New York City. The northern war subsequently became a stalemate, as the focus of attention shifted to the smaller southern theater.
The British move South, 1778–1783.
The British strategy in America now concentrated on a campaign in the southern states. With fewer regular troops at their disposal, the British commanders saw the "southern strategy" as a more viable plan, as the south was perceived as being more strongly Loyalist, with a large population of recent immigrants as well as large numbers of slaves who might be captured or run away to join the British.
Beginning in late December 1778, the British captured Savannah and controlled the Georgia coastline. In 1780 they launched a fresh invasion and took Charleston as well. A significant victory at the Battle of Camden meant that royal forces soon controlled most of Georgia and South Carolina. The British set up a network of forts inland, hoping the Loyalists would rally to the flag.
Not enough Loyalists turned out, however, and the British had to fight their way north into North Carolina and Virginia, with a severely weakened army. Behind them much of the territory they had already captured dissolved into a chaotic guerrilla war, fought predominantly between bands of Loyalist and American militia, which negated many of the gains the British had previously made.
Yorktown 1781.
The British army under Cornwallis marched to Yorktown, Virginia where they expected to be rescued by a British fleet. The fleet showed up but so did a larger French fleet, so the British fleet after the Battle of the Chesapeake returned to New York for reinforcements, leaving Cornwallis trapped. In October 1781 under a combined siege by the French and Continental armies under Washington, the British surrendered their second invading army of the war.
The end of the war.
Historians continue to debate whether the odds for American victory were long or short. John E. Ferling says the odds were so long that the American victory was "Almost A Miracle." On the other hand, Joseph Ellis says the odds favored the Americans, and asks whether there ever was any realistic chance for the British to win. He argues that this opportunity came only once, in the summer of 1776 and the British failed that test. Admiral Howe and his brother General Howe, "missed several opportunities to destroy the Continental Army...Chance, luck, and even the vagaries of the weather played crucial roles." Ellis's point is that the strategic and tactical decisions of the Howes were fatally flawed because they underestimated the challenges posed by the Patriots. Ellis concludes that once the Howe brothers failed, the opportunity for a British victory "would never come again."
Support for the conflict had never been strong in Britain, where many sympathized with the rebels, but now it reached a new low. Although King George III personally wanted to fight on, his supporters lost control of Parliament, and no further major land offensives were launched in the American Theater.
Washington could not know that after Yorktown the British would not reopen hostilities. They still had 26,000 troops occupying New York City, Charleston and Savannah, together with a powerful fleet. The French army and navy departed, so the Americans were on their own in 1782–83. The treasury was empty, and the unpaid soldiers were growing restive, almost to the point of mutiny or possible "coup d'état". The unrest among officers of the Newburgh Conspiracy was personally dispelled by Washington in 1783, and Congress subsequently created the promise of a five years bonus for all officers.
Peace treaty.
The peace treaty with Britain, known as the Treaty of Paris, gave the U.S. all land east of the Mississippi River and south of the Great Lakes, though not including Florida (On September 3, 1783, Britain entered into a separate agreement with Spain under which Britain ceded Florida back to Spain.) The British abandoned the Indian allies living in this region; they were not a party to this treaty and did not recognize it until they were defeated militarily by the United States. Issues regarding boundaries and debts were not resolved until the Jay Treaty of 1795. Since the blockade was lifted and the old imperial restrictions were gone, American merchants were free to trade with any nation anywhere in the world, and their businesses flourished.
Impact on Britain.
Losing the war and the 13 colonies was a shock to Britain. The war revealed the limitations of Britain's fiscal-military state when it discovered it suddenly faced powerful enemies, with no allies, and dependent on extended and vulnerable transatlantic lines of communication. The defeat heightened dissension and escalated political antagonism to the King's ministers. Inside parliament, the primary concern changed from fears of an over-mighty monarch to the issues of representation, parliamentary reform, and government retrenchment. Reformers sought to destroy what they saw as widespread institutional corruption.
The result was a powerful crisis, 1776–1783. The peace in 1783 left France financially prostrate, while the British economy boomed thanks to the return of American business. The crisis ended after 1784 thanks to the King's shrewdness in outwitting Charles James Fox (the leader of the Fox-North Coalition), and renewed confidence in the system engendered by the leadership of the new Prime Minister, William Pitt. Historians conclude that loss of the American colonies enabled Britain to deal with the French Revolution with more unity and better organization than would otherwise have been the case. Britain turned towards Asia, the Pacific and later Africa with subsequent exploration leading to the rise of the Second British Empire.
Finance.
Britain's war against the Americans, French and Spanish cost about £100 million. The Treasury borrowed 40% of the money it needed. Heavy spending brought France to the verge of bankruptcy and revolution, while the British had relatively little difficulty financing their war, keeping their suppliers and soldiers paid, and hiring tens of thousands of German soldiers.
Britain had a sophisticated financial system based on the wealth of thousands of landowners, who supported the government, together with banks and financiers in London. The efficient British tax system collected about 12 percent of the GDP in taxes during the 1770s.
In sharp contrast, Congress and the American states had no end of difficulty financing the war. In 1775 there was at most 12 million dollars in gold in the colonies, not nearly enough to cover current transactions, let alone finance a major war. The British made the situation much worse by imposing a tight blockade on every American port, which cut off almost all imports and exports. One partial solution was to rely on volunteer support from militiamen, and donations from patriotic citizens.
Another was to delay actual payments, pay soldiers and suppliers in depreciated currency, and promise it would be made good after the war. Indeed, in 1783 the soldiers and officers were given land grants to cover the wages they had earned but had not been paid during the war. Not until 1781, when Robert Morris was named Superintendent of Finance of the United States, did the national government have a strong leader in financial matters.
Morris used a French loan in 1782 to set up the private Bank of North America to finance the war. Seeking greater efficiency, Morris reduced the civil list, saved money by using competitive bidding for contracts, tightened accounting procedures, and demanded the national government's full share of money and supplies from the confederated states.
Congress used four main methods to cover the cost of the war, which cost about 66 million dollars in specie (gold and silver). Congress made two issues of paper money, in 1775–1780, and in 1780–81. The first issue amounted to 242 million dollars. This paper money would supposedly be redeemed for state taxes, but the holders were eventually paid off in 1791 at the rate of one cent on the dollar. By 1780, the paper money was "not worth a Continental", as people said.
The skyrocketing inflation was a hardship on the few people who had fixed incomes—but 90 percent of the people were farmers, and were not directly affected by that inflation. Debtors benefited by paying off their debts with depreciated paper.The greatest burden was borne by the soldiers of the Continental Army, whose wages—usually in arrears—declined in value every month, weakening their morale and adding to the hardships suffered by their families.
Beginning in 1777, Congress repeatedly asked the states to provide money. But the states had no system of taxation either, and were little help. By 1780 Congress was making requisitions for specific supplies of corn, beef, pork and other necessities—an inefficient system that kept the army barely alive.
Starting in 1776, the Congress sought to raise money by loans from wealthy individuals, promising to redeem the bonds after the war. The bonds were in fact redeemed in 1791 at face value, but the scheme raised little money because Americans had little specie, and many of the rich merchants were supporters of the Crown. Starting in 1776, the French secretly supplied the Americans with money, gunpowder, and munitions in order to weaken its arch enemy, Great Britain. When France officially entered the war in 1778, the subsidies continued, and the French government, as well as bankers in Paris and Amsterdam loaned large sums to the American war effort. These loans were repaid in full in the 1790s.
Concluding the Revolution.
Creating a "more perfect union" and guaranteeing rights.
After the war finally ended in 1783, there was a period of prosperity. The national government, still operating under the Articles of Confederation, was able to settle the issue of the western territories, which were ceded by the states to Congress. American settlers moved rapidly into those areas, with Vermont, Kentucky and Tennessee becoming states in the 1790s.
However, the national government had no money to pay either the war debts owed to European nations and the private banks, or to pay Americans who had been given millions of dollars of promissory notes for supplies during the war. Nationalists, led by Washington, Alexander Hamilton and other veterans, feared that the new nation was too fragile to withstand an international war, or even internal revolts such as the Shays' Rebellion of 1786 in Massachusetts.
Calling themselves "Federalists," the nationalists convinced Congress to call the Philadelphia Convention in 1787. It adopted a new Constitution that provided for a much stronger federal government, including an effective executive in a check-and-balance system with the judiciary and legislature. After a fierce debate in the states over the nature of the proposed new government, the Constitution was ratified in 1788. The new government under President George Washington took office in New York in March 1789. As assurances to those who were cautious about federal power, amendments to the Constitution guaranteeing many of the inalienable rights that formed a foundation for the revolution were spearheaded in Congress by James Madison, and later ratified by the states in 1791.
National debt.
The national debt after the American Revolution fell into three categories. The first was the $12 million owed to foreigners—mostly money borrowed from France. There was general agreement to pay the foreign debts at full value. The national government owed $40 million and state governments owed $25 million to Americans who had sold food, horses, and supplies to the revolutionary forces. There were also other debts that consisted of promissory notes issued during the Revolutionary War to soldiers, merchants, and farmers who accepted these payments on the premise that the new Constitution would create a government that would pay these debts eventually.
The war expenses of the individual states added up to $114 million compared to $37 million by the central government. In 1790, at the recommendation of first Secretary of the Treasury, Alexander Hamilton, Congress combined the remaining state debts with the foreign and domestic debts into one national debt totaling $80 million. Everyone received face value for wartime certificates, so that the national honor would be sustained and the national credit established.
Ideology and factions.
The population of the 13 Colonies was far from homogeneous, particularly in their political views and attitudes. Loyalties and allegiances varied widely not only within regions and communities, but also within families and sometimes shifted during the course of the Revolution.
Ideology behind the Revolution.
The ideological movement known as the American Enlightenment was a critical precursor to the American Revolution. Chief among the ideas of the American Enlightenment were the concepts of liberalism, republicanism and fear of corruption. Collectively, the acceptance of these concepts by a growing number of American colonists began to foster an intellectual environment which would lead to a new sense of political and social identity.
Natural rights and republicanism.
John Locke's (1632–1704) ideas on liberty greatly influenced the political thinking behind the revolution, especially through his indirect influence on English writers such as John Trenchard, Thomas Gordon, and Benjamin Hoadly, whose political ideas in turn had a strong influence on the American revolutionaries. Locke is often referred to as "the philosopher of the American Revolution", and is credited with leading Americans to the critical concepts of social contract, natural rights, and "born free and equal." Locke's Two Treatises of Government, published in 1689, were especially influential; Locke in turn was influenced by Protestant theology. He argued that, as all humans were created equally free, governments needed the "consent of the governed." Both Lockean concepts were central to the United States Declaration of Independence, which deduced human equality, "life, liberty, and the pursuit of happiness" from the biblical belief in creation: "All men are "created" equal, ... they are endowed by their "Creator" with certain unalienable Rights." In late eighteenth-century America, belief in "equality by creation" and "rights by creation" was still widespread. 
The Declaration also referred to the "Laws of Nature and of Nature's God" as justification for the Americans' separation from the British monarchy. Most eighteenth-century Americans believed that nature, the entire universe, was God's creation. Therefore he was "Nature's God." Everything, including man, was part of the "universal order of things", which began with God and was pervaded and directed by his providence. Accordingly, the signers of the Declaration professed their "firm reliance on the Protection of divine Providence." And they appealed to "the Supreme Judge [God] for the rectitude of [their] intentions." Like most of his countrymen, George Washington was firmly convinced that he was an instrument of providence, to the benefit not only of the American people but of all of humanity.
The theory of the "social contract" influenced the belief among many of the Founders that among the "natural rights" of man was the right of the people to overthrow their leaders, should those leaders betray the historic rights of Englishmen. In terms of writing state and national constitutions, the Americans heavily used Montesquieu's analysis of the wisdom of the "balanced" British Constitution (mixed government).
A motivating force behind the revolution was the American embrace of a political ideology called "republicanism", which was dominant in the colonies by 1775, but of minor importance back in Great Britain. The republicanism was inspired by the "country party" in Great Britain, whose critique of British government emphasized that corruption was a terrible reality in Great Britain. Americans feared the corruption was crossing the Atlantic; the commitment of most Americans to republican values and to their rights, energized the revolution, as Britain was increasingly seen as hopelessly corrupt and hostile to American interests. Britain seemed to threaten the established liberties that Americans enjoyed. The greatest threat to liberty was depicted as corruption—not just in London but at home as well. The colonists associated it with luxury and, especially, inherited aristocracy, which they condemned.
The Founding Fathers were strong advocates of republican values, particularly Samuel Adams, Patrick Henry, John Adams, Benjamin Franklin, Thomas Jefferson, Thomas Paine, George Washington, James Madison and Alexander Hamilton, which required men to put civic duty ahead of their personal desires. Men had a civic duty to be prepared and willing to fight for the rights and liberties of their countrymen and countrywomen. John Adams, writing to Mercy Otis Warren in 1776, agreed with some classical Greek and Roman thinkers in that "Public Virtue cannot exist without private, and public Virtue is the only Foundation of Republics." He continued:
There must be a positive Passion for the public good, the public Interest, Honour, Power, and Glory, established in the Minds of the People, or there can be no Republican Government, nor any real Liberty. And this public Passion must be Superior to all private Passions. Men must be ready, they must pride themselves, and be happy to sacrifice their private Pleasures, Passions, and Interests, nay their private Friendships and dearest connections, when they Stand in Competition with the Rights of society.
For women, "republican motherhood" became the ideal, exemplified by Abigail Adams and Mercy Otis Warren; the first duty of the republican woman was to instill republican values in her children and to avoid luxury and ostentation.
Fusing republicanism and liberalism.
While some republics had emerged throughout history, such as the Roman Republic of the ancient world, one based on liberal principles had never existed. Thomas Paine's best-seller pamphlet "Common Sense" appeared in January 1776, after the Revolution had started. It was widely distributed and loaned, and often read aloud in taverns, contributing significantly to spreading the ideas of republicanism and liberalism together, bolstering enthusiasm for separation from Great Britain, and encouraging recruitment for the Continental Army.
Paine provided a new and widely accepted argument for independence, by advocating a complete break with history. "Common Sense" is oriented to the future in a way that compels the reader to make an immediate choice. It offered a solution for Americans disgusted and alarmed at the threat of tyranny.
Impact of Great Awakening.
Dissenting (i.e. Protestant, non-Church of England) churches of the day were the "school of democracy." President John Witherspoon of the College of New Jersey (now Princeton University) wrote widely circulated sermons linking the American Revolution to the teachings of the Hebrew Bible. Throughout the colonies, dissenting Protestant ministers (Congregationalist, Baptist, and Presbyterian) preached Revolutionary themes in their sermons, while most Church of England clergymen preached loyalty to the King. Religious motivation for fighting tyranny reached across socioeconomic lines to encompass rich and poor, men and women, frontiersmen and townsmen, farmers and merchants.
Historian Bernard Bailyn argues that the evangelicalism of the era challenged traditional notions of natural hierarchy by preaching that the Bible taught all men are equal, so that the true value of a man lies in his moral behavior, not his class. Kidd argues that religious disestablishment, belief in a God as the source of human rights, and shared convictions about sin, virtue, and divine providence worked together to unite rationalists and evangelicals and thus encouraged American defiance of the Empire, whereas Bailyn denied that religion played such a critical role. Alan Heimert argued, however, that New Light antiauthoritarianism was essential to the further democratization of colonial American society, and set the stage for a confrontation with British monarchical and aristocratic rule.
Class and psychology of the factions.
Looking back, John Adams concluded in 1818:
The Revolution was effected before the war commenced. The Revolution was in the minds and hearts of the people ... This radical change in the principles, opinions, sentiments, and affections of the people was the real American Revolution.
In terms of class, Loyalists tended to have longstanding social and economic connections to British merchants and government; for instance, prominent merchants in major port cities such as New York, Boston and Charleston tended to be Loyalists, as did men involved with the fur trade along the northern frontier. In addition, officials of colonial government and their staffs, those who had established positions and status to maintain, favored maintaining relations with Great Britain. They often were linked to British families in England by marriage as well.
By contrast, Patriots by number tended to be yeomen farmers, especially in the frontier areas of New York and the backcountry of Pennsylvania, Virginia and down the Appalachian mountains. They were craftsmen and small merchants. Leaders of both the Patriots and the Loyalists were men of educated, propertied classes. The Patriots included many prominent men of the planter class from Virginia and South Carolina, for instance, who became leaders during the Revolution, and formed the new government at the national and state levels.
To understand the opposing groups, historians have assessed evidence of their hearts and minds. In the mid-20th century, historian Leonard Woods Labaree identified eight characteristics of the Loyalists that made them essentially conservative; traits to those characteristic of the Patriots. Older and better established men, Loyalists tended to resist innovation. They thought resistance to the Crown—which they insisted was the only legitimate government—was morally wrong, while the Patriots thought morality was on their side.
Loyalists were alienated when the Patriots resorted to violence, such as burning houses and tarring and feathering. Loyalists wanted to take a centrist position and resisted the Patriots' demand to declare their opposition to the Crown. Many Loyalists, especially merchants in the port cities, had maintained strong and long-standing relations with Britain (often with business and family links to other parts of the British Empire).
Many Loyalists realized that independence was bound to come eventually, but they were fearful that revolution might lead to anarchy, tyranny or mob rule. In contrast, the prevailing attitude among Patriots, who made systematic efforts to use mob violence in a controlled manner, was a desire to seize the initiative. Labaree also wrote that Loyalists were pessimists who lacked the confidence in the future displayed by the Patriots.
Historians in the early 20th century, such as J. Franklin Jameson, examined the class composition of the Patriot cause, looking for evidence of a class war inside the revolution. In the last 50 years, historians have largely abandoned that interpretation, emphasizing instead the high level of ideological unity. Just as there were rich and poor Loyalists, the Patriots were a 'mixed lot', with the richer and better educated more likely to become officers in the Army.
Ideological demands always came first: the Patriots viewed independence as a means to gain freedom from British oppression and taxation and, above all, to reassert what they considered to be their rights as English subjects. Most yeomen farmers, craftsmen, and small merchants joined the Patriot cause to demand more political equality. They were especially successful in Pennsylvania but less so in New England, where John Adams attacked Thomas Paine's "Common Sense" for the "absurd democratical notions" it proposed.
King George III.
The war became a personal issue for the king, fueled by his growing belief that British leniency would be taken as weakness by the Americans. The king also sincerely believed he was defending Britain's constitution against usurpers, rather than opposing patriots fighting for their natural rights.
Patriots.
At the time, revolutionaries were called "Patriots", "Whigs", "Congress-men", or "Americans". They included a full range of social and economic classes, but were unanimous regarding the need to defend the rights of Americans and uphold the principles of republicanism in terms of rejecting monarchy and aristocracy, while emphasizing civic virtue on the part of the citizens. Newspapers were strongholds of patriotism (although there were a few Loyalist papers), and printed many pamphlets, announcements, patriotic letters and pronouncements.
According to historian Robert Calhoon, the consensus of historians is that 40–45% of the white population in the Thirteen Colonies supported the Patriots' cause, 15–20% supported the Loyalists, and the remainder were neutral or kept a low profile. Mark Lender explores why ordinary folk became insurgents against the British even though they were unfamiliar with the ideological rationales being offered. They held very strongly a sense of "rights" that they felt the British were violating – rights that stressed local autonomy, fair dealing, and government by consent. They were highly sensitive to the issue of tyranny, which they saw manifested in the British response to the Boston Tea Party. The arrival in Boston of the British Army heightened their sense of violated rights, leading to rage and demands for revenge. They had faith that God was on their side.
Loyalists.
The consensus of scholars is that about 15–20% of the white population remained loyal to the British Crown. Those who actively supported the king were known at the time as "Loyalists", "Tories", or "King's men". The Loyalists never controlled territory unless the British Army occupied it. Loyalists were typically older, less willing to break with old loyalties, often connected to the Church of England, and included many established merchants with strong business connections across the Empire, as well as royal officials such as Thomas Hutchinson of Boston. There were 500 to 1000 black loyalists who were held as slaves by patriots, escaped to British lines and joined the British army. Most died of disease but Britain took the survivors to Canada as free men.
The revolution could divide families. The most dramatic example was when William Franklin, son of Benjamin Franklin and royal governor of the Province of New Jersey, remained loyal to the Crown throughout the war; they never spoke again. Recent immigrants who had not been fully Americanized were also inclined to support the King, such as recent Scottish settlers in the back country; among the more striking examples of this, see Flora MacDonald.
After the war, the great majority of the 450,000–500,000 Loyalists remained in America and resumed normal lives. Some, such as Samuel Seabury, became prominent American leaders. Estimates vary, but about 62,000 Loyalists relocated to Canada, and others to Britain (7,000) or to Florida or the West Indies (9,000). The exiles represented approximately 2% of the total population of the colonies. Nearly all black loyalists left for Nova Scotia, Florida, or England, where they could remain free. When Loyalists left the South in 1783, they took thousands of their slaves with them to be slaves in the British West Indies.
Neutrals.
A minority of uncertain size tried to stay neutral in the war. Most kept a low profile, but the Quakers, especially in Pennsylvania, were the most important group to speak out for neutrality. As Patriots declared independence, the Quakers, who continued to do business with the British, were attacked as supporters of British rule, "contrivers and authors of seditious publications" critical of the revolutionary cause.
Role of women.
Women contributed to the American Revolution in many ways, and were involved on both sides. While formal Revolutionary politics did not include women, ordinary domestic behaviors became charged with political significance as Patriot women confronted a war that permeated all aspects of political, civil, and domestic life. They participated by boycotting British goods, spying on the British, following armies as they marched, washing, cooking, and tending for soldiers, delivering secret messages, and in a few cases like Deborah Samson, fighting disguised as men. Also, Mercy Otis Warren held meetings in her house and cleverly attacked Loyalists with her creative plays and histories. Above all, they continued the agricultural work at home to feed their families and the armies. They maintained their families during their husbands' absences and sometimes after their deaths.
American women were integral to the success of the boycott of British goods, as the boycotted items were largely household items such as tea and cloth. Women had to return to knitting goods, and to spinning and weaving their own cloth — skills that had fallen into disuse. In 1769, the women of Boston produced 40,000 skeins of yarn, and 180 women in Middletown, Massachusetts wove 20522 yd of cloth.
A crisis of political loyalties could disrupt the fabric of colonial America women's social worlds: whether a man did or did not renounce his allegiance to the King could dissolve ties of class, family, and friendship, isolating women from former connections. A woman's loyalty to her husband, once a private commitment, could become a political act, especially for women in America committed to men who remained loyal to the King. Legal divorce, usually rare, was granted to Patriot women whose husbands supported the King.
Other participants.
France.
In early 1776, France set up a major program of aid to the Americans, and the Spanish secretly added funds. Each country spent one million "livres tournaises" to buy munitions. A dummy corporation run by Pierre Beaumarchais concealed their activities. American rebels obtained some munitions through the Dutch Republic as well as French and Spanish ports in the West Indies.
Spain.
Spain did not officially recognize the U.S. but became an informal ally when it declared war on Britain on June 21, 1779. Bernardo de Gálvez y Madrid, general of the Spanish forces in New Spain, also served as governor of Louisiana. He led an expedition of colonial troops to force the British out of Florida and keep open a vital conduit for supplies.
Native Americans.
Most Native Americans rejected pleas that they remain neutral and supported the British Crown, both because of trading relationships and Britain's effort to establish an Indian reserve and prohibit colonial settlement west of the Appalachian Mountains. The great majority of the 200,000 Native Americans east of the Mississippi distrusted the colonists and supported the British cause, hoping to forestall continued colonial encroachment on their territories. Those tribes that were more closely involved in colonial trade tended to side with the revolutionaries, although political factors were important as well.
Except for warriors and bands associated with four of the Iroquois nations in New York and Pennsylvania, which allied with the British, most Native Americans did not participate directly in the war. The British did have other allies especially in the upper Midwest. They provided Indians with funding and weapons to attack American outposts. Some Indians tried to remain neutral, seeing little value in joining a European conflict and fearing reprisals from whichever side they opposed. The Oneida and Tuscarora, among the Iroquois of central and western New York, supported the American cause.
The British provided arms to Indians, who were led by Loyalists in war parties to raid frontier settlements from the Carolinas to New York. They killed many settlers on the frontier, especially in Pennsylvania and New York's Mohawk Valley.
In 1776 Cherokee war parties attacked American colonists all along the southern frontier of the uplands through Tennessee and Kentucky. While the Cherokee launched raids numbering a couple hundred warriors, as seen in the Chickamauga Wars, they could not mobilize enough forces to fight a major invasion of colonial areas without the help of allies, most often the Creek.
Joseph Brant of the powerful Mohawk nation, part of the Iroquois Confederacy based in New York, was the most prominent Native American leader against the rebel forces. In 1778 and 1780, he led 300 Iroquois warriors and 100 white Loyalists in multiple attacks on small frontier settlements in New York and Pennsylvania, killing many settlers and destroying villages, crops and stores. The Seneca, Onondaga and Cayuga of the Iroquois Confederacy also allied with the British against the Americans.
In 1779 the Continentals retaliated with an American army under John Sullivan, which raided and destroyed 40 empty Iroquois villages in central and western New York. Sullivan's forces systematically burned the villages and destroyed about 160,000 bushels of corn that comprised the winter food supply. Facing starvation and homeless for the winter, the Iroquois fled to the Niagara Falls area and to Canada, mostly to what became Ontario. The British resettled them there after the war, providing land grants as compensation for some of their losses.
At the peace conference following the war, the British ceded lands which they did not really control, and did not consult their Indian allies. They "transferred" control to the United States of all the land east of the Mississippi and north of Florida. The historian Calloway concludes:
Burned villages and crops, murdered chiefs, divided councils and civil wars, migrations, towns and forts choked with refugees, economic disruption, breaking of ancient traditions, losses in battle and to disease and hunger, betrayal to their enemies, all made the American Revolution one of the darkest periods in American Indian history.
The British did not give up their forts in the West (what is now the Ohio to Wisconsin) until 1796; they kept alive the dream of forming a satellite Indian nation there, which they called a Neutral Indian Zone. That goal was one of the causes of the War of 1812.
African Americans.
Free blacks in the North and South fought on both sides of the Revolution, but most fought for the patriots. Gary Nash reports that recent research concludes there were about 9000 black Patriot soldiers, counting the Continental Army and Navy, and state militia units, as well as privateers, wagoneers in the Army, servants to officers, and spies. Ray Raphael notes that while thousands did join the Loyalist cause, "A far larger number, free as well as slave, tried to further their interests by siding with the patriots." Crispus Attucks, who was shot dead by British soldiers in the Boston Massacre in 1770, is an iconic martyr to Patriots. Both sides offered freedom and re-settlement to slaves who were willing to fight for them, recruiting slaves whose owners supported the opposing cause.
Many African-American slaves sided with the Loyalists. Tens of thousands in the South used the turmoil of war to escape, and the southern plantation economies of South Carolina and Georgia especially were disrupted. During the Revolution, the British tried to turn slavery against the Americans.
Historian David Brion Davis explains the difficulties with a policy of wholesale arming of the slaves:
But England greatly feared the effects of any such move on its own West Indies, where Americans had already aroused alarm over a possible threat to incite slave insurrections. The British elites also understood that an all-out attack on one form of property could easily lead to an assault on all boundaries of privilege and social order, as envisioned by radical religious sects in Britain's seventeenth-century civil wars.
Davis underscored the British dilemma: "Britain, when confronted by the rebellious American colonists, hoped to exploit their fear of slave revolts while also reassuring the large number of slave-holding Loyalists and wealthy Caribbean planters and merchants that their slave property would be secure". The colonists accused the British of encouraging slave revolts.
American advocates of independence were commonly lampooned in Britain for what was termed their hypocritical calls for freedom, at the same time that many of their leaders were planters who held hundreds of slaves. Samuel Johnson snapped, "how is it we hear the loudest yelps for liberty among the [slave] drivers of the Negroes?" Benjamin Franklin countered by criticizing the British self-congratulation about "the freeing of one Negro" (Somersett) while they continued to permit the Slave Trade.
Phyllis Wheatley, a black poet who popularized the image of Columbia to represent America, came to public attention when her "Poems on Various Subjects, Religious and Moral" appeared in 1773.
During the war, slaves escaped from across New England and the mid-Atlantic area to British-occupied cities, such as New York. The effects of the war were more dramatic in the South. In Virginia the royal governor Lord Dunmore recruited black men into the British forces with the promise of freedom, protection for their families, and land grants. Tens of thousands of slaves escaped to British lines throughout the South, causing dramatic losses to slaveholders and disrupting cultivation and harvesting of crops. For instance, South Carolina was estimated to lose about 25,000 slaves, or one third of its slave population, to flight, migration or death. From 1770 to 1790, the black proportion of the population (mostly slaves) in South Carolina dropped from 60.5 percent to 43.8 percent; and in Georgia from 45.2 percent to 36.1 percent.
When the British evacuated its forces from Savannah and Charleston, it also gave transportation to 10,000 slaves, carrying through on its commitment to them. They evacuated and resettled more than 3,000 "Black Loyalists" from New York to Nova Scotia, Upper and Lower Canada. Others sailed with the British to England or were resettled as freedmen in the West Indies of the Caribbean. But slaves who were carried to the Caribbean under control of Loyalist masters generally remained slaves until British abolition in its colonies in 1834. More than 1200 of the Black Loyalists of Nova Scotia later resettled in the British colony of Sierra Leone, where they became leaders of the Krio ethnic group of Freetown and the later national government. Many of their descendants still live in Sierra Leone, as well as other African countries.
Effects of the Revolution.
Loyalist expatriation.
About 60,000 to 70,000 Loyalists left the newly founded republic; some migrated to Britain. The remainder, known as United Empire Loyalists, received land and subsidies for resettlement in British colonies in North America, especially Quebec (concentrating in the Eastern Townships), Prince Edward Island, and Nova Scotia. The new colonies of Upper Canada (now Ontario) and New Brunswick were expressly created by Britain for their benefit, where the Crown awarded land to Loyalists as compensation for losses in the United States. Britain wanted to develop the frontier of Upper Canada on a British colonial model. But about 80% of the Loyalists stayed in the United States and became full, loyal citizens; some of the exiles later returned to the U.S.
Interpretations.
Interpretations about the effect of the Revolution vary. Contemporary participants referred to the events as "the revolution." Greene argues that the events were not "revolutionary," as the relationships and property rights of colonial society were not transformed: a distant government was simply replaced with a local one; the Revolution is still known outside the United States as the American War of Independence.
Historians such as Bernard Bailyn, Gordon Wood, and Edmund Morgan accept the contemporary view of the participants that the American Revolution was a unique and radical event that produced deep changes and had a profound effect on world affairs, based – building on debates in the seventeenth-century English Civil War and subsequently – on an increasing belief in the principles of the Enlightenment, as reflected in how liberalism was understood during the period, and republicanism. These were demonstrated by a leadership and government that espoused protection of natural rights, and a system of laws chosen by the people. However, what was then considered "the people" was still mostly restricted to free white males who were able to pass a property-qualification. Such a restriction made a significant gain of the revolution irrelevant to women, African Americans and slaves, poor white men, youth, and Native Americans. Only with the development of the American system over the following centuries would "a government by the people," promised by the revolution, be won for a greater proportion of the population.
Morgan has argued that in terms of long-term impact on American society and values:
Inspiring all colonies.
After the Revolution, genuinely democratic politics became possible in the former colonies. The rights of the people were incorporated into state constitutions. Concepts of liberty, individual rights, equality among men and hostility toward corruption became incorporated as core values of liberal republicanism. The greatest challenge to the old order in Europe was the challenge to inherited political power and the democratic idea that government rests on the consent of the governed. The example of the first successful revolution against a European empire, and the first successful establishment of a republican form of democratically elected government, provided a model for many other colonial peoples who realized that they too could break away and become self-governing nations with directly elected representative government.
The Dutch Republic, also at war with Britain, was the next country to sign a treaty with the United States, on October 8, 1782. On April 3, 1783, Ambassador Extraordinary Gustaf Philip Creutz, representing King Gustav III of Sweden, and Benjamin Franklin, signed a Treaty of Amity and Commerce with the U.S.
The American Revolution was the first wave of the Atlantic Revolutions: the French Revolution, the Haitian Revolution, and the Latin American wars of independence. Aftershocks reached Ireland in the Irish Rebellion of 1798, in the Polish-Lithuanian Commonwealth, and in the Netherlands.
The Revolution had a strong, immediate influence in Great Britain, Ireland, the Netherlands, and France. Many British and Irish Whigs spoke in favor of the American cause. In Ireland, there was a profound impact; the Protestants who controlled Ireland were demanding more and more self-rule. Under the leadership of Henry Grattan, the so-called "Patriots" forced the reversal of mercantilist prohibitions against trade with other British colonies. The King and his cabinet in London could not risk another rebellion on the American model, and made a series of concessions to the Patriot faction in Dublin. Armed Protestant volunteer units were set up to protect against an invasion from France. As in America, so too in Ireland the King no longer had a monopoly of lethal force.
The Revolution, along with the Dutch Revolt (end of the 16th century) and the 17th century English Civil War, was among the examples of overthrowing an old regime for many Europeans who later were active during the era of the French Revolution, such as Marquis de Lafayette. The American Declaration of Independence influenced the French Declaration of the Rights of Man and the Citizen of 1789.
The spirit of the Declaration of Independence led to laws ending slavery in all the Northern states and the Northwest Territory, with New Jersey the last in 1804—long before the British Parliament acted in 1833 to abolish slavery in its colonies. States such as New Jersey and New York adopted gradual emancipation, which kept some people as slaves for more than two decades longer.
Status of American women.
The democratic ideals of the Revolution inspired changes in the roles of women.
The concept of republican motherhood was inspired by this period and reflects the importance of Republicanism as the dominant American ideology. It assumed that a successful republic rested upon the virtue of its citizens. Women were considered to have the essential role of instilling their children with values conducive to a healthy republic. During this period, the wife's relationship with her husband also became more liberal, as love and affection instead of obedience and subservience began to characterize the ideal marital relationship. In addition, many women contributed to the war effort through fundraising and running family businesses in the absence of husbands.
The traditional constraints gave way to more liberal conditions for women. Patriarchy faded as an ideal; young people had more freedom to choose their spouses and more often used birth control to regulate the size of their families. Society emphasized the role of mothers in child rearing, especially the patriotic goal of raising republican children rather than those locked into aristocratic value systems. There was more permissiveness in child-rearing. Patriot women married to Loyalists who left the state could get a divorce and obtain control of the ex-husband's property.
Whatever gains they had made, however, women still found themselves subordinated, legally and socially, to their husbands, disfranchised and usually with only the role of mother open to them. But, some women earned livelihoods as midwives and in other roles in the community, which were not originally recognized as significant by men.
Abigail Adams expressed to her husband, the president, the desire of women to have a place in the new republic:
I desire you would remember the Ladies, and be more generous and favourable to them than your ancestors. Do not put such unlimited power into the hands of the Husbands.
Zagarri in 2007 argued that the American Revolution created a continuing debate on the rights of woman and an environment favorable to women's participation in U.S. politics. She asserts that for a brief decade, a "comprehensive transformation in women's rights, roles, and responsibilities seemed not only possible but perhaps inevitable." But, the changes also engendered a backlash that set back the cause of women's rights and led to a greater rigidity that marginalized women from political life.
Status of African Americans.
In the first two decades after the American Revolution, state legislatures and individuals took actions to free numerous slaves, in part based on revolutionary ideals. Northern states passed new constitutions that contained language about equal rights or specifically abolished slavery; some states, such as New York and New Jersey, where slavery was more widespread, passed laws by the end of the 18th century to abolish slavery by a gradual method; in New York, the last slaves were freed in 1827.
While no southern state abolished slavery, for a period individual owners could free their slaves by personal decision, often providing for manumission in wills but sometimes filing deeds or court papers to free individuals. Numerous slaveholders who freed their slaves cited revolutionary ideals in their documents; others freed slaves as a reward for service. Records also suggest that some slaveholders were freeing their own mixed-race children, born into slavery to slave mothers.
Memory.
The American Revolution has a central place in the American memory. As the founding story, it is covered in the schools, memorialized by a national holiday, and commemorated in innumerable monuments. Thus Independence Day (the "Fourth of July") is a major national holiday celebrated annually. Besides local sites such as Bunker Hill, one of the first national pilgrimages for memorial tourists was Mount Vernon, George Washington's estate (near Washington City), which attracted ten thousand visitors a year by the 1850s.
Crider points out that in the 1850s, editors and orators both North and South claimed their region was the true custodian of the legacy of 1776, as they used the Revolution symbolically in their rhetoric. Ryan, noting that the Bicentennial was celebrated a year after the United States' humiliating 1975 withdrawal from Vietnam, says the Ford administration stressed the themes of renewal and rebirth based on a restoration of traditional values, and presented a nostalgic approach to 1776 that made it seem eternally young and fresh.
Albanese argues that the Revolution became the main source of the non-denominational "American civil religion" that has shaped patriotism, and the memory and meaning of the nation's birth ever since. She says that specific battles are not central (as they are for the Civil War) but rather certain events and people have been celebrated as icons of certain virtues (or vices). Thus she points out the Revolution produced a Moses-like leader (George Washington), prophets (Thomas Jefferson, Tom Paine), disciples (Alexander Hamilton, James Madison) and martyrs (Boston Massacre, Nathan Hale), as well as devils (Benedict Arnold), sacred places (Valley Forge, Bunker Hill), rituals (Boston Tea Party), emblems (the new flag), sacred holidays (Independence Day), and a holy scripture whose every sentence is carefully studied and applied in current law cases (The Declaration of Independence, the Constitution and the Bill of Rights).
References.
</dl>

</doc>
<doc id="1974" url="http://en.wikipedia.org/wiki?curid=1974" title="April 17">
April 17

April 17 is the day of the year in the Gregorian calendar.

</doc>
<doc id="1975" url="http://en.wikipedia.org/wiki?curid=1975" title="Alan Ayckbourn">
Alan Ayckbourn

Sir Alan Ayckbourn, CBE (born 12 April 1939) is a prolific English playwright. He has written and produced more than seventy full-length plays in Scarborough and London and was, between 1972 and 2009, the artistic director of the Stephen Joseph Theatre in Scarborough, where all but four of his plays have received their first performance. More than 40 have subsequently been produced in the West End, at the Royal National Theatre or by the Royal Shakespeare Company since his first hit "Relatively Speaking" opened at the Duke of York's Theatre in 1967.
Major successes include "Absurd Person Singular" (1975), "The Norman Conquests" trilogy (1973), "Bedroom Farce" (1975), "Just Between Ourselves" (1976), "A Chorus of Disapproval" (1984), "Woman in Mind" (1985), "A Small Family Business" (1987), "Man Of The Moment" (1988), "House" & "Garden" (1999) and "Private Fears in Public Places" (2004). His plays have won numerous awards, including seven London "Evening Standard" Awards. They have been translated into over 35 languages and are performed on stage and television throughout the world. Ten of his plays have been staged on Broadway, attracting two Tony nominations, and one Tony award.
Life.
Childhood.
Ayckbourn was born in Hampstead, London. His mother Irene Worley ("Lolly") was a writer of short stories who published under the name "Mary James". His father, Horace Ayckbourn, was an orchestral violinist, at one time deputy leader of the London Symphony Orchestra. His parents, who separated shortly after World War II, never married, and Ayckbourn's mother divorced her "first" husband to marry again in 1948.
Ayckbourn wrote his first play at Wisborough Lodge (a preparatory school in the village of Wisborough Green) when he was about 10. Whilst at prep school as a boarder, his mother wrote to tell him she was marrying Cecil Pye, a bank manager. When he went home for the holidays his new family consisted of his mother, his stepfather and Christopher, his stepfather's son by an earlier marriage. This relationship too, reportedly ran into difficulties early on.
Ayckbourn attended Haileybury and Imperial Service College, in the village of Hertford Heath, and whilst there toured Europe and America with the school's Shakespeare company.
Adult life.
After leaving school at 17, Ayckbourn's career took several temporary jobs in various places before starting a temporary job at the Scarborough Library Theatre, where he was introduced to the artistic director, Stephen Joseph. It is said that Joseph became both a mentor and father figure for Ayckbourn until his untimely death in 1967, and he has consistently spoken highly of him.
Ayckbourn's career was briefly interrupted when he was called for National Service. He was swiftly discharged, officially on medical grounds, but it is suggested that a doctor who noticed his reluctance to join the Armed Forces deliberately failed the medical as a favour. Although Ayckbourn continued to move where his career took him, he settled in Scarborough, eventually buying Longwestgate House, the house formerly owned by Stephen Joseph.
In 1957, Ayckbourn married Christine Roland, another member of the Library Theatre company, and indeed Ayckbourn's first two plays were written jointly with her under the pseudonym of "Roland Allen". They had two sons, Steven and Philip. However, the marriage had difficulties which eventually led to their separation in 1971. Alan Ayckbourn said that his relationship with Christine became easy once they agreed their marriage was over. Around this time, he started to share a home with Heather Stoney, an actress he had first met ten years earlier. Like his mother, neither he nor Christine sought a divorce for the next thirty years and it was only in 1997 that they formally divorced; Ayckbourn married Heather Stoney. One side-effect of the timing is that, as Alan was awarded a knighthood a few months before the divorce, both his first and second wife are entitled to take the title of Lady Ayckbourn.
In February 2006, he suffered a stroke in Scarborough, and stated: "I hope to be back on my feet, or should I say my left leg, as soon as possible, but I know it is going to take some time. In the meantime I am in excellent hands and so is the Stephen Joseph Theatre." He left hospital after eight weeks and returned to directing after six months, but the following year he announced he would step down as artistic director of the Stephen Joseph Theatre. Ayckbourn, however, continues to write and direct his own work at the theatre.
Influence on plays.
Since Alan Ayckbourn's plays started becoming established in the West End, interviewers have raised the question of whether his work is autobiographical. There is no clear answer to this question. There has only been one biography, written by Paul Allen, and this primarily covers his career in the theatre. Ayckbourn has frequently said he sees aspects of himself in all his characters. For example, in "Bedroom Farce" (1975), he admitted to being, in some respects, all four of the men in the play. It has been suggested that, after Ayckbourn himself, the person who is used the most in his plays is his mother, particularly as Susan in "Woman in Mind" (1985).
What is less clear is how much influence events in Ayckbourn's life have had on his writing. It is true that the theme of marriages in various difficulties was heavily present throughout his plays in the early seventies, around the time his own marriage was coming to an end. However, by this time, he had also witnessed the failures of his parents' relationships as well as those of some of his friends. Which relationships, if any, he drew on for his plays, is unclear. In Paul Allen’s biography, Ayckbourn is briefly compared to Dafydd and Guy in "A Chorus of Disapproval" (1984). Both characters feel themselves in trouble, and there was speculation that Alan Ayckbourn himself may have felt himself to be in trouble. At the time, he had reportedly become seriously involved with another actress, which threatened his relationship with Heather Stoney. But again, it is unclear whether this had any effect on the writing, and Paul Allen's view is that it is not current experience that Ayckbourn uses for his plays.
It could be that Ayckbourn had written plays with himself and his own issues in mind, but as Ayckbourn is portrayed as a guarded and private man, it is hard to imagine him exposing his own life in his plays to any great degree. In the biography, Paul Allen wrote, regarding a suggestion in "Cosmopolitan" that his plays were becoming autobiographical: "If we take that to mean that his plays tell his own life story, he still hasn't started."
Career.
Early career and acting.
On leaving school his theatrical career started immediately, with an introduction to Sir Donald Wolfit by his French master. Ayckbourn joined Wolfit on tour to the Edinburgh Festival Fringe as an acting assistant stage manager (meaning a role that involved both acting and stage management) for three weeks, with his first role on the professional stage being various parts in "The Strong are Lonely" by Fritz Hochwälder. In the following year, Ayckbourn appeared in six other plays at the Connaught Theatre, Worthing, and the Thorndyke theatre, Leatherhead.
In 1957, Ayckbourn was employed by the director Stephen Joseph at the Library Theatre, Scarborough, the predecessor to the modern Stephen Joseph Theatre. His role, again, was initially an acting stage manager. This employment led to Ayckbourn's first professional script commission, in 1958. When he complained about the quality of a script he was performing, Joseph challenged him to write a better one. The result was "The Square Cat", written under the pseudonym Roland Allen and first performed in 1959. In this play, Ayckbourn himself played the character Jerry Watiss.
After thirty-four appearances in plays at the Library Theatre, including four of his own, in 1962 Ayckbourn moved to Stoke-on-Trent to help set up the Victoria Theatre, (now the New Vic), where he appeared in a further eighteen plays. His final appearance in one of his own plays was as the Crimson Gollywog in the disastrous children's play "Christmas v Mastermind". He left the Stoke company in 1964, officially to commit his time to the London production of "Mr. Whatnot", but reportedly because was having trouble working with the artistic director, Peter Cheeseman. By now, his career as a writer was coming to fruition, and his acting career was sidelined.
His final role on stage was as Jerry in "Two for the Seesaw" by William Gibson, at the Civic Theatre in Rotherham. He was left stranded on stage because Heather Stoney was unable to re-appear because the props had been left unpacked, and this led him to decide acting was more trouble than it was worth. The assistant stage manager on the production, Bill Kenwright, would become one of the UK's most successful producers.
Writing.
Alan Ayckbourn's earliest plays were written and produced at a time when the Scarborough Library theatre, like most regional theatres, regularly commissioned work from their own actors to keep costs down (the other notable actor whose work was being commissioned being David Campton). His first play, "The Square Cat", was sufficiently popular locally to secure further commissions, but neither this nor the following three plays had any major impact outside of Scarborough. But, after his transfer to Victoria Theatre in Stoke-on-Trent, there came "Christmas v Mastermind", which flopped and is now universally regarded as Ayckbourn's greatest disaster.
His fortunes began to revive in 1963 with "Mr. Whatnot", again premièring at the Victoria Theatre. This was the first play that Ayckbourn was sufficiently happy with to allow performances today, and the first play to receive a West End performance. However, the West End production flopped, in part down to misguided casting. After this, Ayckbourn experimented by collaborating with comedians, first writing a monologue for Tommy Cooper, and later with Ronnie Barker, who played Lord Slingsby-Craddock in the London production of "Mr Whatnot" in 1964, for the scripts of for LWT's "Hark at Barker". Ayckbourn used the pseudonym 'Peter Caulfield' because he was under exclusive contract to the BBC at the time.
Then, in 1965, back at the Scarborough Library Theatre, "Meet my Father" was produced, later retitled "Relatively Speaking". This time, the play was a massive success, both in Scarborough and the West End, making Alan Ayckbourn rich and earning him a congratulatory telegram from Noël Coward. This was not quite the end of Ayckbourn's hit-and-miss record, because his following play, "The Sparrow" only ran for three weeks at Scarborough. However, the following play, "How the Other Half Loves", secured his runaway success as a playwright.
The height of Ayckbourn's commercial success included "Absurd Person Singular" (1975), "The Norman Conquests" trilogy (1973), "Bedroom Farce" (1975) and "Just Between Ourselves" (1976), all plays that focused heavily on marriage in the British middle classes. The only failure during this period was a 1975 musical with Andrew Lloyd Webber, "Jeeves", and even this did little to dent Ayckbourn's popularity. Although his plays have received major West End productions almost from the beginning of his writing career, and hence have been reviewed in British newspapers, Ayckbourn's work was for years routinely dismissed as being too slight for serious study. Recently, scholars have begun to view Ayckbourn as an important commentator on the lifestyles of the British suburban middle class, and as a stylistic innovator who experiments with theatrical styles within the boundaries set by popular tastes.
From the 1980s, Ayckbourn began to move away from the recurring themes of marriage and explore other contemporary themes, one example being "Woman in Mind", a play performed entirely from the perspective of a Woman going through a nervous breakdown. He also experimented with several more unconventional ways of writing plays, such as "Intimate Exchanges", which has one beginning and sixteen possible endings, and "House & Garden", where two plays take place simultaneously of two different stages, as well as diversifying into children's theatre (such as "Mr A's Amazing Maze Plays" and musical plays, such as "By Jeeves" (a more successful rewrite of the original "Jeeves").
With a résumé of over seventy plays, of which more than forty have played at the National Theatre or in the West End, Alan Ayckbourn is one of England’s most successful living playwrights. Despite his success, honours and awards (which include a prestigious Laurence Olivier Award), Alan Ayckbourn remains a relatively anonymous figure dedicated to regional theatre. Throughout his writing career, all but four of his plays were premièred at the Stephen Joseph Theatre in Scarborough in its three different locations.
Alan Ayckbourn received the CBE in 1987 and was knighted in 1997. It is frequently claimed (but not proven) that Alan Ayckbourn is the most performed living English playwright, and the second most performed of all time after Shakespeare.
Although Alan Ayckbourn's plays no longer dominate the theatrical scene on the scale of his earlier works, he continues to write, his most recent major success being "Private Fears in Public Places" that had a hugely successful Off-Broadway run at 59E59 Theaters, and in 2006 was made into a film "Cœurs", directed by Alain Resnais. After suffering a stroke, there was uncertainly as to whether he could continue to write (the Ayckbourn play premièred immediately after the stroke, "If I Were You", was written before his illness), but his first play written afterwards, "Life and Beth", was premièred in the summer of 2008. Ayckbourn continues to write for the Stephen Joseph Theatre on invitation of his successor as artistic director, Chris Monks, with the first new play under this arrangement, "My Wonderful Day", performed in October 2009. His latest play, "Roundelay" is scheduled to open in September 2014; the order in which each of the five acts is played in each performance is to be left to chance (allowing 120 possible permutations), with members of the audience being invited to extract five coloured ping pong balls from a bag beforehand.
Many of Ayckbourn's plays have had their New York premiere at 59E59 Theaters as part of their annual Brits Off Broadway Festitval including "Private Fears in Public Places", "Intimate Exchanges", "My Wonderful Day" and "Neighbourhood Watch" among others.
Directing.
Although Alan Ayckbourn is best known as a writer, it is said that he only spends 10% of his time writing plays. Most of the rest of his time is spent directing.
Alan Ayckbourn began directing at the Scarborough Library Theatre in 1961, with a production of "Gas Light" by Patrick Hamilton. He directed five other plays that year and the following year in Scarborough, and after transferring to the Victoria Theatre, directed a further six plays in 1963. Between 1964 and 1967 (when much of his time was taken up by various productions of his early successes "Mr. Whatnot" and "Relatively Speaking") he only directed one play ("The Sparrow", written by himself, later withdrawn), but in 1968 he resumed regularly directing plays, mostly at Scarborough. At this time he also worked as a radio drama producer for the BBC, based in Leeds.
At first, his directing career was separate from his writing career. It was not until 1963 that Ayckbourn directed a play of his own (a revival of "Standing Room Only"), 1967 that Ayckbourn directed a première of his own ("The Sparrow"). The London premières remained in the hands of other directors for longer, with the first play of his both written and directed by him in London ("Bedroom Farce") waiting until 1977.
After the death of Stephen Joseph in 1967, the position of Director of Productions was appointed on an annual basis. Alan Ayckbourn was offered this position in 1969 and 1970, succeeding Rodney Wood, but he handed the position over to Caroline Smith in 1971 (having spent most of his time that year in the USA with "How the Other Half Loves"). He became Director of Productions again in 1972, and this time, on 12 November that same year, he was made the permanent artistic director of the theatre.
In mid-1986, Ayckbourn accepted an invitation to work as a visiting director for two years at the Royal National Theatre in London, form his own company, and perform a play in each of the three auditoria provided at least one was a new play of his own. Using a stock company that included established performers like Michael Gambon, Polly Adams and Simon Cadell. The three plays became four, and were: "Tons of Money" by Will Evans and Valentine, with adaptations by Ayckbourn (Lyttelton), Arthur Miller's "A View From the Bridge" (Cottesloe), his own "A Small Family Business" (Olivier) and John Ford's "'Tis Pity She's a Whore" (Olivier again). During this time, Alan Ayckbourn shared his role of artistic director of the Stephen Joseph Theatre with Robin Herford and returned in 1987 to direct the première of "Henceforward...".
He announced in 1999 that he would step back from directing the work of other playwrights, in order to concentrate on his own plays, the last one being Rob Shearman's "Knights in Plastic Armour" in 1999; the exception being in 2002 when he directed the world première of Tim Firth's "The Safari Party".
In 2002, following a dispute over the Duchess Theatre's handling of "Damsels in Distress", Ayckbourn sharply criticised both this and the West End's treatment of theatre in general, in particular their casting of celebrities. Although he did not explicitly say he would boycott the West End, he did not return to direct in the West End again until 2009 with a revival of "Woman in Mind" (although he did allow other West End producers to revive "Absurd Person Singular" in 2007 and "The Norman Conquests" in 2008).
After Ayckbourn suffered a stroke in February 2006, he returned to work in September and premièred his 70th play "If I Were You" at the Stephen Joseph Theatre the following month.
He announced in June 2007 that he would retire as artistic director of the Stephen Joseph Theatre after the 2008 season. His successor, Chris Monks, took over at the start of the 2009–2010 season, but Ayckbourn remained to direct premières and revivals of his work at the theatre, beginning with "How the Other Half Loves" in June 2009.
In March 2010 he directed an in-the-round revival of his play "Taking Steps" at the Orange Tree Theatre, winning universal press acclaim.
In July 2014, Ayckbourn directed a of "The Boy Who Fell Into A Book", with musical adaptation and lyrics by Paul James and music by Eric Angus and Cathy Shostak. The show ran in The Stephen Joseph Theatre and received critical acclaim.
Works.
One-act plays.
There are seven one-act plays written by Alan Ayckbourn. Five of them ("Mother Figure", "Drinking Companion", "Between Mouthfuls", "Gosforth’s Fete" and "Widows Might") were written for "Confusions", first performed in 1974.
The other two one-act plays were:
Film adaptations of Ayckbourn plays.
Plays adapted as films include:
References.
</dl>

</doc>
<doc id="1979" url="http://en.wikipedia.org/wiki?curid=1979" title="Alpha Centauri">
Alpha Centauri

Alpha Centauri (α Cen); also known as Rigil Kent () or Toliman, is the brightest star in the southern constellation of Centaurus and the third brightest star in the night sky. The Alpha Centauri system is located 1.34 parsecs or 4.37 light years from the Sun, making it the closest star system to our Solar System. Although it appears to the unaided eye as a single object, Alpha Centauri is actually a binary star system (designated Alpha Centauri AB or α Cen AB) whose combined visual magnitude of −0.27 makes it the third brightest star (other than the Sun) seen from Earth after the −1.46 magnitude Sirius and the −0.72 magnitude Canopus.
Its component stars are named Alpha Centauri A (α Cen A), with 110% of the mass and 151.9% the luminosity of the Sun, and Alpha Centauri B (α Cen B), at 90.7% of the Sun's mass and 44.5% of its luminosity. During the pair's 79.91-year orbit about a common center, the distance between them varies from about that between Pluto and the Sun to that between Saturn and the Sun.
A third star, known as Proxima Centauri, Proxima, or Alpha Centauri C (α Cen C), is probably gravitationally associated with Alpha Centauri AB. Proxima is at the slightly smaller distance of 1.29 parsecs or 4.24 light years from the Sun, making it the closest star to the Sun even though it is not visible to the naked eye. The separation of Proxima from Alpha Centauri AB is about 0.06 parsecs, 0.2 light years or 15,000 astronomical units (AU); equivalent to 500 times the size of Neptune's orbit.
The Alpha Centauri system has been reported to contain one planet, the Earth-sized Alpha Centauri Bb, which would be the closest known exoplanet to Earth, but its existence has been questioned.
Nature and components.
"Alpha Centauri" is the name given to what appears as a single star to the naked eye and the brightest star in the southern constellation of Centaurus. At −0.27v visual magnitude, it is fainter only than Sirius and Canopus. The next brightest star in the night sky is Arcturus. Actually a multiple star system, its two main stars are Alpha Centauri A (α Cen A) and Alpha Centauri B (α Cen B), usually defined to identify them as the different components of the binary α Cen AB. A third companion—Proxima Centauri (or Proxima or α Cen C)—has a distance much greater than the observed separation between stars A and B and is probably gravitationally associated with the AB system. As viewed from Earth, it is located at an angular separation of 2.2° from the two main stars. If it were bright enough to be seen without a telescope, Proxima Centauri would appear to the naked eye as a star separate from α Cen AB. Alpha Centauri AB and Proxima Centauri form a "visual" double star. Direct evidence that Proxima Centauri has an elliptical orbit typical of binary stars has yet to be found. Together all three components make a triple star system, referred to by double-star observers as the triple star (or multiple star), α Cen AB-C.
Alpha Centauri A is the principal member, or "primary", of the binary system, being slightly larger and more luminous than the Sun. It is a solar-like main-sequence star with a similar yellowish color, whose stellar classification is spectral type G2 V. From the determined mutual orbital parameters, Alpha Centauri A is about 10% more massive than the Sun, with a radius about 23% larger. The projected rotational velocity ( "v"·sin "i" ) of this star is 2.7 ± 0.7 km·s−1, resulting in an estimated rotational period of 22 days, which gives it a slightly faster rotational period than the Sun's 25 days. When considered among the individual brightest stars in the sky (excluding the Sun), Alpha Centauri A is the fourth brightest at −0.01 magnitude, being fractionally fainter than Arcturus at −0.04v magnitude.
Alpha Centauri B is the companion star, or "secondary", of the binary system, and is slightly smaller and less luminous than the Sun. It is a main-sequence star of spectral type K1 V, making it more an orange color than the primary star. Alpha Centauri B is about 90% the mass of the Sun and 14% smaller in radius. The projected rotational velocity ( "v"·sin "i" ) is 1.1 ± 0.8 km·s−1, resulting in an estimated rotational period of 41 days. (An earlier, 1995 estimate gave a similar rotation period of 36.8 days.) Although it has a lower luminosity than component A, star B emits more energy in the X-ray band. The light curve of B varies on a short time scale and there has been at least one observed flare. Alpha Centauri B at 1.33v magnitude would be twenty-first in brightness if it could be seen independently of Alpha Centauri A.
Alpha Centauri C, also known as Proxima Centauri, is of spectral class M5 Ve or M5 VIe, suggesting this is either a small main-sequence star (Type V) or subdwarf (VI) with emission lines. Its B−V color index is +1.90 and its mass is about 0.123 solar masses (M☉), or 129 Jupiter masses.
Together, the bright visible components of the binary star system are called Alpha Centauri AB (α Cen AB). This "AB" designation denotes the apparent gravitational centre of the main binary system relative to other companion star(s) in any multiple star system. "AB-C" refers to the orbit of Proxima around the central binary, being the distance between the centre of gravity and the outlying companion. Some older references use the confusing and now discontinued designation of A×B. Because the distance between the Sun and Alpha Centauri AB does not differ significantly from either star, gravitationally this binary system is considered as if it were one object.
Asteroseismic studies, chromospheric activity, and stellar rotation (gyrochronology), are all consistent with the α Cen system being similar in age to, or slightly older than, the Sun, with typical ages quoted between 4.5 and 7 billion years (Gyr). Asteroseismic analyses that incorporate the tight observational constraints on the stellar parameters for α Cen A and/or B have yielded age estimates of 4.85 ± 0.5 Gyr, 5.0 ± 0.5 Gyr, 5.2–7.1 Gyr, 6.4 Gyr, and 6.52 ± 0.3 Gyr. Age estimates for stars A and B based on chromospheric activity (Calcium H & K emission) yield 4.4–6.5 Gyr, whereas gyrochronology yields 5.0 ± 0.3 Gyr.
Observation.
The two Alpha Centauri AB binary stars are too close together to be resolved by the naked eye, because the angular separation varies between 2 and 22 arcsec, but through much of the orbit, both are easily resolved in binoculars or small 5 cm telescopes.
In the southern hemisphere, Alpha Centauri forms the outer star of "The Pointers" or "The Southern Pointers", so called because the line through Beta Centauri (Hadar/Agena),
some 4.5° west, points directly to the constellation Crux — the Southern Cross. The Pointers easily distinguish the true Southern Cross from the fainter asterism known as the False Cross.
South of about 29° S latitude, Alpha Centauri is circumpolar and never sets below the horizon. Both stars, including Crux, are too far south to be visible for mid-latitude northern observers. Below about 29° N latitude to the equator (roughly Hermosillo, Chihuahua in Mexico, Galveston, Texas, Ocala, Florida and Lanzarote, the Canary Islands of Spain) during the northern summer, Alpha Centauri lies close to the southern horizon. The star culminates each year at midnight on 24 April or 9 p.m. on 8 June.
As seen from Earth, Proxima Centauri lies 2.2° southwest from Alpha Centauri AB. This is about four times the angular diameter of the Full Moon, and almost exactly half the distance between Alpha Centauri AB and Beta Centauri. Proxima usually appears as a deep-red star of 13.1v visual magnitude in a poorly populated star field, requiring moderately sized telescopes to see. Listed as V645 Cen in the "General Catalogue of Variable Stars (G.C.V.S.) Version 4.2", this UV Ceti-type flare star can unexpectedly brighten rapidly to about 11.0v or 11.09V magnitude. Some amateur and professional astronomers regularly monitor for outbursts using either optical or radio telescopes.
Observational history.
English explorer Robert Hues brought Alpha Centauri to the attention of European observers in his 1592 work "Tractatus de Globis", along with Canopus and Achernar, noting "Now, therefore, there are but three Stars of the first magnitude that I could perceive in all those parts which are never seene here in England. The first of these is that bright Star in the sterne of Argo which they call Canobus. The second is in the end of Eridanus. The third [Alpha Centauri] is in the right foote of the Centaure."
The binary nature of Alpha Centauri AB was first recognized in December 1689 by astronomer and Jesuit priest Jean Richaud. The finding was made incidentally while observing a passing comet from his station in Puducherry. Alpha Centauri was only the second binary star system to be discovered, preceded only by Alpha Crucis.
By 1752, French astronomer Abbé Nicolas Louis de Lacaille made astrometric positional measurements using a meridian circle. Later, John Herschel made the first micrometrical observations in 1834. Since the early 20th century, measures have been made with photographic plates.
By 1926, South African astronomer William Stephen Finsen calculated the approximate orbit elements close to those now accepted for this system. All future positions are now sufficiently accurate for visual observers to determine the relative places of the stars from a binary star ephemeris. Others, like the Belgian astronomer D. Pourbaix (2002), have regularly refined the precision of any new published orbital elements.
Alpha Centauri is the closest star system to the Solar System. It lies about 4.37 light-years in distance, or about 41.5 trillion kilometres, 25.8 trillion miles or 277,600 AU. Scottish astronomer Thomas Henderson made the original discovery from many exacting observations of the trigonometric parallaxes of the AB system between April 1832 and May 1833. He withheld the results because he suspected they were too large to be true, but eventually published in 1839 after Friedrich Wilhelm Bessel released his own accurately determined parallax for 61 Cygni in 1838. For this reason, Alpha Centauri is considered as the second star to have its distance measured because it was not formally recognized first. Alpha Centauri is inside the G-cloud, and the nearest known system to it is Luhman 16 at 3.6 light years.
Scottish astronomer Robert Innes discovered Proxima Centauri in 1915 by blinking photographic plates taken at different times during a dedicated proper motion survey. This showed the large proper motion and parallax of the star was similar in both size and direction to those of Alpha Centauri AB, suggesting immediately it was part of the system and slightly closer to us than Alpha Centauri AB. Lying 4.24 light-years away, Proxima Centauri is the nearest star to the Sun. All current derived distances for the three stars are from the parallaxes obtained from the Hipparcos star catalog (HIP).
Distance.
Alpha Centauri AB distance estimates
Binary system.
With the orbital period of 79.91 years, the A and B components of this binary star can approach each other to 11.2 astronomical units, equivalent to 1.67 billion km or about the mean distance between the Sun and Saturn, or may recede as far as 35.6 AU (5.3 billion km—approximately the distance from the Sun to Pluto). This is a consequence of the binary's moderate orbital eccentricity "e" = 0.5179. From the orbital elements, the total mass of both stars is about 2.0 M☉—or twice that of the Sun. The average individual stellar masses are 1.09 M☉ and 0.90 M☉, respectively, though slightly higher masses have been quoted in recent years, such as 1.14 M☉ and 0.92 M☉, or totalling 2.06 M☉. Alpha Centauri A and B have absolute magnitudes of +4.38 and +5.71, respectively. Stellar evolution theory implies both stars are slightly older than the Sun at 5 to 6 billion years, as derived by both mass and their spectral characteristics.
Viewed from Earth, the "apparent orbit" of this binary star means that the separation and position angle (P.A.) are in continuous change throughout the projected orbit. Observed stellar positions in 2010 are separated by 6.74 arcsec through the P.A. of 245.7°, reducing to 6.04 arcsec through 251.8° in 2011. Next closest approach will be in February 2016, at 4.0 arcsec through 300°. Observed maximum separation of these stars is about 22 arcsec, while the minimum distance is 1.7 arcsec. Widest separation occurred during February 1976 and the next will be in January 2056.
In the "true orbit", closest approach or periastron was in August 1955, and next in May 2035. Furthest orbital separation at apastron last occurred in May 1995 and the next will be in 2075. The apparent distance between the two stars is rapidly decreasing, at least until 2019.
Companion: Proxima Centauri.
The much fainter red dwarf named Proxima Centauri, or simply Proxima, is about 15,000 AU away from Alpha Centauri AB. This is equivalent to 0.24 light years or 2.2 trillion kilometres—about 5% the distance between Alpha Centauri AB and the Sun. Proxima is likely gravitationally bound to Alpha Centauri AB, orbiting it with a period between 100,000 and 500,000 years. However, it is also possible that Proxima is not gravitationally bound and thus moving along a hyperbolic trajectory with respect to Alpha Centauri AB. The main evidence for a bound orbit is that Proxima's association with Alpha Centauri AB is unlikely to be coincidental, because they share approximately the same motion through space. Theoretically, Proxima could leave the system after several million years. It is not yet certain whether Proxima and Alpha Centauri are truly gravitationally bound.
Proxima is a red dwarf of spectral class M5.5V with an absolute magnitude of +15.53, which is only a small fraction of the Sun's luminosity. By mass, Proxima is calculated as 0.123 ± 0.06 M☉ (rounded to 0.12 M☉) or about one-eighth that of the Sun.
Kinematics.
All components of Alpha Centauri display significant proper motions against the background sky, similar to the first magnitude stars Sirius and Arcturus. Over the centuries, this causes the apparent stellar positions to slowly change. Such motions define the "high-proper-motion stars". These stellar motions were unknown to ancient astronomers. Most assumed that all stars were immortal and permanently fixed on the celestial sphere, as stated in the works of the philosopher Aristotle.
Edmond Halley in 1718 found that some stars had significantly moved from their ancient astrometric positions. For example, the bright star Arcturus (α Boo) in the constellation of Boötes showed an almost 0.5° difference in 1800 years, as did the brightest star, Sirius, in Canis Major (α CMa). Halley's positional comparison was Ptolemy's catalogue of stars contained in the Almagest whose original data included portions from an earlier catalog by Hipparchos during the 1st century BCE. Halley's proper motions were mostly for northern stars, so the southern star Alpha Centauri was not determined until the early 19th century.
Scottish-born observer Thomas James Henderson in the 1830s at the Royal Observatory at the Cape of Good Hope discovered the true distance to Alpha Centauri. He soon realised this system displayed an unusually high proper motion, and therefore its observed true velocity through space should be much larger. In this case, the apparent stellar motion was found using Abbé Nicolas Louis de Lacaille's astrometric observations of 1751–1752, by the observed differences between the two measured positions in different epochs. Using the Hipparcos Star Catalogue (HIP) data, the mean individual proper motions are −3678 mas/yr or −3.678 arcsec per year in right ascension and +481.84 mas/yr or 0.48184 arcsec per year in declination. As proper motions are cumulative, the motion of Alpha Centauri is about 6.1 arcmin each century, and 61.3 arcmin or 1.02° each millennium. These motions are about one-fifth and twice, respectively, the diameter of the full moon. Using spectroscopy the mean radial velocity has been determined to be 25.1 ± 0.3 km/s towards the Solar System.
As the stars of Alpha Centauri approach us, the measured proper motion and trigonometric parallax slowly increase. Changes are also observed in the size of the semi-major axis of the orbital ellipse, increasing by 0.03 arcsec per century. This change slightly shortens the observed orbital period of Alpha Centauri AB by some 0.006 years per century. This small effect is gradually decreasing until the star system is at its closest to us, and is then reversed as the distance increases again. Consequently, the observed position angles of the stars are subject to changes in the orbital elements over time, as first determined by W. H. van den Bos in 1926. Some slight differences of about 0.5% in the measured proper motions are caused by Alpha Centauri AB's orbital motion.
Based on these observed proper motions and radial velocities, Alpha Centauri will continue to gradually brighten, passing just north of the Southern Cross or Crux, before moving northwest and up towards the celestial equator and away from the galactic plane. By about 29,700 AD, in the present-day constellation of Hydra, Alpha Centauri will be 1.00 pc or 3.26 ly away. Then it will reach the stationary radial velocity (RVel) of 0.0 km/s and the maximum apparent magnitude of −0.86V (which is comparable to present-day magnitude of Canopus). However, even during the time of this nearest approach, the apparent magnitude of Alpha Centauri will still not surpass that of Sirius (which will brighten incrementally over the next 60,000 years, and will continue to be the brightest star as seen from Earth for the next 210,000 years).
The Alpha Centauri system will then begin to move away from the Solar System, showing a positive radial velocity. Due to visual perspective, about 100,000 years from now, these stars will reach a final vanishing point and slowly disappear among the countless stars of the Milky Way. Here this once bright yellow star will fall below naked-eye visibility somewhere in the faint present day southern constellation of Telescopium (this unusual location results from the fact that Alpha Centauri's orbit around the galactic centre is highly tilted with respect to the plane of the Milky Way).
In about 4000 years, the proper motion of Alpha Centauri will mean that from the point of view of Earth it will appear close enough to Beta Centauri to form an optical double star. Beta Centauri is in reality far more distant than Alpha Centauri.
Planets.
Until the 1990s, technologies did not exist that could detect planets outside the Solar System. Since then, exoplanet-detection capabilities have steadily improved to the point where Earth-mass planets can be detected.
Alpha Centauri Bb.
On 16 October 2012, researchers, mainly from the Observatory of Geneva and from the Centre for Astrophysics of the University of Porto, announced that an Earth-mass (M⊕) planet had been likely detected in orbit around Alpha Centauri B using the radial velocity technique. Over three years of observations had been needed for the difficult analysis, finding the planet orbiting very close to the host star at just 0.04 AU and completing one orbit every 3.236 days. It is not in Alpha Centauri B's habitable zone, with a surface temperature estimated to be 1200 °C (about 1500 K), far too hot for liquid water and also above the melting temperatures of many silicate magmas. For comparison, the surface temperature of Venus, the hottest planet in the Solar System, is 462 °C (735 K). Alpha Centauri Bb has a minimum mass of 1.13 M⊕.
Other possibly detected planets.
On 25 March 2015, a scientific paper by Demory et al. published transit results for Alpha Centauri B using the Hubble Space Telescope for a total of 40 hours. Although the team could rule out, with 96.6% confidence, transit events for Alpha Centauri Bb (which does not preclude its existence, merely the fact that it is in the same plane as the Sun and Alpha Centauri B), they evidenced a transit event possibly corresponding to a planetary body with different orbital parameters. This planet would most likely orbit Alpha Centauri B with an orbital period of 20.4 days or less, with only a 5% chance of it having a longer orbit. The median average of the likely orbits being 12.4 days with an impact parameter of around 0–0.3. Its orbit would likely have an eccentricity of 0.24 or less. If confirmed, this planet would be called Alpha Centauri Bc. This planet would also still be far too close to its parent star to harbour life.
Possibility of additional planets.
The discovery of planets orbiting other star systems, including similar binary systems (Gamma Cephei), raises the possibility that additional planets may exist in the Alpha Centauri system. Such planets could orbit Alpha Centauri A or Alpha Centauri B individually, or be on large orbits around the binary Alpha Centauri AB. Because both the principal stars are fairly similar to the Sun (for example, in age and metallicity), astronomers have been especially interested in making detailed searches for planets in the Alpha Centauri system. Several established planet-hunting teams have used various radial velocity or star transit methods in their searches around these two bright stars. All the observational studies have so far failed to find any evidence for brown dwarfs or gas giants.
In 2009, computer simulations (then unaware of the close-in planet Bb) showed that a planet might have been able to form near the inner edge of Alpha Centauri B's habitable zone, which extends from 0.5 to 0.9 AU from the star. Certain special assumptions, such as considering that Alpha Centauri A and B may have initially formed with a wider separation and later moved closer to each other (as might be possible if they formed in a dense star cluster) would permit an accretion-friendly environment farther from the star. Bodies around A would be able to orbit at slightly farther distances due to A's stronger gravity. In addition, the lack of any brown dwarfs or gas giants in close orbits around A or B make the likelihood of terrestrial planets greater than otherwise. Theoretical studies on the detectability via radial velocity analysis have shown that a dedicated campaign of high-cadence observations with a 1–m class telescope can reliably detect a hypothetical planet of 1.8 M⊕ in the habitable zone of B within three years.
Radial velocity measurements of Alpha Centauri B with HARPS spectrograph ruled out planets of more than 4 M⊕ to the distance of the habitable zone of the star (orbital period P = 200 days).
Alpha Centauri is envisioned as the first target for unmanned interstellar exploration. Crossing the huge distance between the Sun and Alpha Centauri using current spacecraft technologies would take several millennia, though the possibility of solar sail or nuclear pulse propulsion technology could cut this down to a matter of decades.
Theoretical planets.
Early computer-generated models of planetary formation predicted the existence of terrestrial planets around both Alpha Centauri A and B, but most recent numerical investigations have shown that the gravitational pull of the companion star renders the accretion of planets very difficult. Despite these difficulties, given the similarities to the Sun in spectral types, star type, age and probable stability of the orbits, it has been suggested that this stellar system could hold one of the best possibilities for harbouring extraterrestrial life on a potential planet.
In the Solar System both Jupiter and Saturn were probably crucial in perturbing comets into the inner Solar System. Here, the comets provided the inner planets with their own source of water and various other ices. In the Alpha Centauri system Proxima Centauri may have influenced the planetary disk as the Alpha Centauri system was forming, enriching the area around Alpha Centauri A and B with volatile materials. This would be discounted if, for example, Alpha Centauri B happened to have gas giants orbiting Alpha Centauri A (or conversely, Alpha Centauri A for Alpha Centauri B), or if the stars B and A themselves were able to perturb comets into each other's inner system as Jupiter and Saturn presumably have done in the Solar System. Such icy bodies probably also reside in Oort clouds of other planetary systems, when they are influenced gravitationally by either the gas giants or disruptions by passing nearby stars many of these icy bodies then travel starwards. There is no direct evidence yet of the existence of such an Oort cloud around Alpha Centauri AB, and theoretically this may have been totally destroyed during the system's formation.
To be in the star's habitable zone, any suspected planet around Alpha Centauri A would have to be placed about 1.25 AU away – about halfway between the distances of Earth's orbit and Mars's orbit in the Solar System – so as to have similar planetary temperatures and conditions for liquid water to exist. For the slightly less luminous and cooler Alpha Centauri B, the habitable zone would lie closer at about 0.7 AU (100 million km), approximately the distance that Venus is from the Sun.
With the goal of finding evidence of such planets, both Proxima Centauri and Alpha Centauri AB were among the listed "Tier 1" target stars for NASA's Space Interferometry Mission (SIM). Detecting planets as small as three Earth-masses or smaller within two astronomical units of a "Tier 1" target would have been possible with this new instrument. The SIM mission, however, was cancelled due to financial issues in 2010.
View from this system.
Viewed from near the Alpha Centauri system, the sky would appear very much as it does for an observer on Earth, except that Centaurus would be missing its brightest star. The Sun would be a yellow +0.5 visual magnitude star in eastern Cassiopeia at the antipodal point of Alpha Centauri's current RA and Dec. at 02h 39m 35s ° 50′ (2000). This place is close to the 3.4 magnitude star ε Cassiopeiae. An interstellar or alien observer would find the \/\/ of Cassiopeia had become a /\/\/ shape nearly in front of the Heart Nebula in Cassiopeia. Sirius lies less than a degree from Betelgeuse in the otherwise unmodified Orion and is with −1.2 a little fainter than from Earth but still the brightest star in the Alpha Centauri sky. Procyon is also displaced into the middle of Gemini, outshining Pollux, whereas both Vega and Altair are shifted northwestward relative to Deneb (which barely moves, due to its great distance)- giving the Summer Triangle a more equilateral appearance.
From Proxima itself, Alpha Centauri AB would appear like two close bright stars with the combined magnitude of −6.8. Depending on the binary's orbital position, the bright stars would appear noticeably divisible to the naked eye, or occasionally, but briefly, as single unresolved star. Based on the calculated absolute magnitudes, the visual magnitudes of Alpha Centauri A and B would be −6.5 and −5.2, respectively.
View from a hypothetical planet.
An observer on a hypothetical planet orbiting around either Alpha Centauri A or Alpha Centauri B would see the other star of the binary system as an intensely bright object in the night sky, showing a small but discernible disk.
For example, some theoretical planet orbiting about 1.25 AU from Alpha Centauri A (so that the star appears roughly as bright as the Sun viewed from the Earth) would see Alpha Centauri B orbit the entire sky once roughly every one year and three months (or 1.3(4) a), the planet's own orbital period. Added to this would be the changing apparent position of Alpha Centauri B during its long eighty-year elliptical orbit with respect to Alpha Centauri A (comparable in speed to Uranus here). Depending on the position on its orbit, Alpha Centauri B would vary in apparent magnitude between −18.2 (dimmest) and −21.0 (brightest). These visual magnitudes are much dimmer than the observed −26.7 magnitude for the Sun as viewed from the Earth. The difference of 5.7 to 8.6 magnitudes means Alpha Centauri B would appear, on a linear scale, 2500 to 190 times dimmer than Alpha Centauri A (or the Sun viewed from the Earth), but also 190 to 2500 times brighter than the −12.5 magnitude full Moon as seen from the Earth.
Also, if another similar planet orbited at 0.71 AU from Alpha Centauri B (so that in turn Alpha Centauri B appeared as bright as the Sun seen from the Earth), this hypothetical planet would receive slightly more light from the more luminous Alpha Centauri A, which would shine 4.7 to 7.3 magnitudes dimmer than Alpha Centauri B (or the Sun seen from the Earth), ranging in apparent magnitude between −19.4 (dimmest) and −22.1 (brightest). Thus Alpha Centauri A would appear between 830 and 70 times dimmer than the Sun but some 580 to 6900 times brighter than the full Moon. During such planet's orbital period of 0.6(3) a, an observer on the planet would see this intensely bright companion star circle the sky just as we see with the Solar System's planets. Furthermore, Alpha Centauri A sidereal period of approximately eighty years means that this star would move through the local ecliptic as slowly as Uranus with its eighty-four year period, but as the orbit of Alpha Centauri A is more elliptical, its apparent magnitude will be far more variable. Although intensely bright to the eye, the overall illumination would not significantly affect climate nor influence normal plant photosynthesis.
An observer on the hypothetical planet would notice a change in orientation to VLBI reference points commensurate with the binary orbit periodicity plus or minus any local effects such as precession or nutation.
Assuming this hypothetical planet had a low orbital inclination with respect to the mutual orbit of Alpha Centauri A and B, then the secondary star would start beside the primary at 'stellar' conjunction. Half the period later, at 'stellar' opposition, both stars would be opposite each other in the sky. Then, for about half the planetary year the appearance of the night sky would be a darker blue – similar to the sky during totality at any total solar eclipse. Humans could easily walk around and clearly see the surrounding terrain, and reading a book would be quite possible without any artificial light. After another half period in the stellar orbit, the stars would complete their orbital cycle and return to the next stellar conjunction, and the familiar day and night cycle would return.
Traditional names.
The colloquial name of Alpha Centauri is "Rigel Kent" or "Rigil Kent", short for "Rigil/Rigel Kentaurus", the romanization of the Arabic name رجل القنطورس "Rijl Qanṭūris", from the phrase "Rijl al-Qanṭūris" "the foot of the Centaur". This is sometimes further abbreviated to "Rigel", though that is ambiguous with Beta Orionis, which is also called Rigel. Although the short form "Rigel Kent" is common in English, the stars are most often referred to by their Bayer designation "Alpha Centauri."
A medieval name is "Toliman", whose etymology may be Arabic الظلمان "al-Ẓulmān" "the ostriches". During the 19th century, the northern amateur popularist Elijah H. Burritt used the now-obscure name Bungula, possibly coined from "β" and the Latin "ungula" ("hoof"). Together, Alpha and Beta Centauri form the "Southern Pointers" or "The Pointers", as they point towards the Southern Cross, the asterism of the constellation of Crux.
In Mandarin, 南門 "Nán Mén", meaning "Southern Gate", refers to an asterism consisting of α Centauri and ε Centauri. Consequently, α Centauri itself is known as 南門二 "Nán Mén Èr", the Second Star of the Southern Gate.
To the Australian aboriginal Boorong people of northwestern Victoria, Alpha and Beta Centauri are "Bermbermgle", two brothers noted for their courage and destructiveness, who speared and killed "Tchingal" "The Emu" (the Coalsack Nebula). The form in Wotjobaluk is "Bram-bram-bult".
External links.
Hypothetical planets or exploration.
Coordinates: 

</doc>
<doc id="1980" url="http://en.wikipedia.org/wiki?curid=1980" title="Amiga">
Amiga

The Amiga is a family of personal computers sold by Commodore in the 1980s and 1990s. The first model, the A1000, was launched in 1985 and became popular for its graphical, audio and multi-tasking abilities. The Amiga provided a significant upgrade from 8-bit computers, such as the Commodore 64, and the platform quickly grew in popularity among computer enthusiasts. The name "Amiga" was chosen because it is the Spanish word for "(female) friend", and alphabetically it appears before Apple in lists of computer makers. It originated as a project code-named "Lorraine", therefore the female was used instead of the male and general version "Amigo".
The best selling model, the Amiga 500, was introduced in 1987 and became one of the leading home computers of the late 1980s and early 1990s with approximately six million sold. The A3000, introduced in 1990, started the second generation of Amiga Systems, followed by the A500+ and the A600. Finally, as the third generation, the A1200 and the A4000 were released in 1992. However, poor marketing and failure to repeat the technological advances of the first systems meant that the Amiga quickly lost its market share to competing platforms, such as the fourth generation game consoles, Apple Macintosh, and IBM PC compatibles.
Based on the Motorola 68000 family of microprocessors, the machine has a custom chipset with graphics and sound capabilities that were unprecedented for the price, and a pre-emptive multitasking operating system called AmigaOS. The original operating system, partly based on TRIPOS and written in BCPL, is called AmigaDOS and the GUI is called Workbench. When it was eventually renamed AmigaOS, the BCPL parts were rewritten in the C language.
Although early Commodore advertisements attempt to cast the computer as an all-purpose business machine, especially when outfitted with the Amiga Sidecar PC compatibility addon, the Amiga was most commercially successful as a home computer, with a wide range of games and creative software. It was also a less expensive alternative to the Apple Macintosh and IBM PC as a general-purpose business or home computer. Initially, the Amiga was developed alongside various PC Compatible Systems by Commodore but later Commodore left the PC market. The platform became particularly popular for gaming and programming demos. It also found a prominent role in the desktop video, video production, and show control business, leading to affordable video editing systems such as the Video Toaster. The Amiga's native ability to simultaneously play back multiple digital sound samples made it a popular platform for early "tracker" music software. The relatively powerful processor and ability to access several megabytes of memory led to the development of several 3D rendering packages, including LightWave 3D and Aladdin 4D.
Since the demise of Commodore, various groups have marketed successors to the original Amiga line, including Genesi, Eyetech, ACube Systems and A-EON Technology. Likewise, AmigaOS has influenced replacements, clones and compatible systems such as MorphOS, AmigaOS 4 and AROS. The demise of Commodore has been commonly attributed to numerous factors such as poor marketing, a lack of sufficient third party developers, and a failure to compete with cheaper PC clones with "multimedia" features and low-cost color-capable Macintosh models such as the Macintosh LC.
History.
"The Amiga was so far ahead of its time that almost nobody—including Commodore's marketing department—could fully articulate what it was all about. Today, it's obvious the Amiga was the first multimedia computer, but in those days it was derided as a game machine because few people grasped the importance of advanced graphics, sound, and video. Nine years later, vendors are still struggling to make systems that work like 1985 Amigas.— "Byte Magazine", August 1994
Concept and early development.
Jay Miner joined Atari in the 1970s to develop custom integrated circuits, and led development of the Atari 2600's TIA. Almost as soon as its development was complete, they began developing a much more sophisticated set of chips, CTIA, ANTIC and POKEY, that formed the basis of the Atari 8-bit family. With the 8-bit's launch in 1979, Miner again started looking at a next generation chipset. Nolan Bushnell had sold the company to Warner Communications in 1978, and the new management was much more interested in squeezing profits from the existing lines than development of new products that might cut into sales of existing lines. Miner wanted to start work with the new Motorola 68000, but management was only interested in another MOS 6502 based system. Miner left the company.
Shortly thereafter, in 1982, Larry Kaplan was approached by a number of investors who wanted to develop a new game platform. Kaplan hired Miner to run the hardware side of the newly formed "Hi-Toro". When Kaplan left the company late in 1982 to rejoin Atari, Miner was promoted to head engineer and the company relaunched as Amiga Corporation. The system was code-named "Lorraine" in keeping with Miner's policy of giving systems female names, in this case the company president's wife. A prototype was largely completed by late 1983, and shown at the January 1984 Consumer Electronics Show (CES) with the famous Boing Ball demo. A further developed version was demonstrated at the June 1984 CES and shown to many companies in hopes of garnering further funding, but found little interest in a market that was in the final stages of the North American video game crash of 1983.
Atari was in turmoil as a result of the crash, and the computer division was sold to Jack Tramiel in June 1984. Amiga and Atari had been in talks since March, but these were going nowhere. Tramiel provided a $500,000 loan to keep them running, with the proviso that failure to pay it back in one month would leave Atari owning the technology.
Commodore launch.
Tramiel's move to Atari resulted in a considerable number of Commodore employees moving as well, including a number of the senior technical staff. This left Commodore with no workable path to a new generation computer. The company approached Amiga offering to fund development as a home computer system. They quickly arranged to repay the Atari loan, ending that threat. The two companies were initially arranging a $4 million license agreement before Commodore offered $24 million to purchase Amiga outright.
By late 1984 the prototype breadboard chipset had successfully been turned into IC's, and the system hardware was being readied for production. At this time the operating system (OS) was not as ready, and led to a deal to port an OS known as TRIPOS to the platform. TRIPOS was a multitasking system that had been written in BCPL during the 1970s for minicomputer systems like the PDP-11, but later experimentally ported to the 68000. A late change was the introduction of vertical supports on either side of the case to provide a "garage" under the main section of the system where the keyboard could be stored.
The first model was announced in 1985 as simply "The Amiga from Commodore", later to be retroactively dubbed the Amiga 1000. They were first offered for sale in August, but by October only 50 had been built, all of which were used by Commodore. Machines only began to arrive in quantity in mid-November, meaning they missed the Christmas buying rush. By the end of the year, they had sold 35,000 machines, and severe cashflow problems made the company pull out of the January 1986 CES. Bad or entirely missing marketing, a forced move to the east coast, notorious stability problems and other blunders limited sales in early 1986 to 10 to 15,000 units a month.
Commercial success.
In late 1985 Thomas Rattigan was promoted to COO of Commodore, and then to CEO in February 1986. He immediately implemented a ambitious plan that covered almost all of the company's operations. Among these were the long overdue cancelation of the now outdated PET and VIC-20 lines, a variety of poorly selling Commodore 64 offshoots, and the Commodore 900 workstation effort.
Another of the changes was to split the Amiga into two products, a new high-end version of the Amiga aimed at the creative market, and a cost-reduced version that would take over for the Commodore 64 in the low-end market. These new designs were released in 1987 as the Amiga 2000 and Amiga 500, the later of which went on to widespread success and became their best selling model.
Similar high-end/low-end models would make up the Amiga line for the rest of its history; follow-on designs included the Amiga 3000/Amiga 500+, and the Amiga 4000/Amiga 1200. These models incorporated a series of technical upgrades known as the ECS and AGA, which added higher resolution displays among many other improvements and simplifications.
Ultimately the Amiga line would sell an estimated 4,850,000 machines over its lifetime. The machines were most popular in the UK and Germany, with about 1.5 million sold in each country, and sales in the high hundreds of thousands in other European nations. The machine was less popular in North America, where an estimated 700,000 were sold.
Bankruptcy.
In spite of his successes in making the company profitable and bringing the Amiga line success, Rattigan was soon forced out of the company in a power struggle with majority shareholder, Irving Gould. This is widely regarded as the turning point in the line's success, as further improvements in the systems were eroded by rapid improvements in other platforms.
In 1994, Commodore filed for bankruptcy and its assets were purchased by Escom, a German PC manufacturer, who created the subsidiary company Amiga Technologies. They re-released the A1200 and A4000T, and introduced a new 68060 version of the A4000T. However, Escom in turn went bankrupt in 1997.
The Amiga brand was then sold to another PC manufacturer, Gateway 2000, which had announced grand plans for it. However, in 2000, Gateway sold the Amiga brand without having released any products. The current owner of the trademark, Amiga, Inc, licensed the rights to sell hardware using the Amiga or AmigaOne brand to computer vendors Commodore USA, Eyetech Group, Ltd. and A-Eon Technology CVBA. Unofficial AmigaOne clones were developed by Italian hardware company, Acube.
Hardware.
At its core, the Amiga has a custom chipset consisting of several coprocessors, which handle audio, video and direct memory access independently of the Central Processing Unit (CPU). This architecture freed up the Amiga's processor for other tasks and gave the Amiga a performance edge over its competitors, particularly in terms of video-intensive applications and games.
The general Amiga architecture uses two distinct bus subsystems, namely, the chipset bus and the CPU bus. The chipset bus allows the custom coprocessors and CPU to address "Chip RAM". The CPU bus provides addressing to other subsystems, such as conventional RAM, ROM and the Zorro II or Zorro III expansion subsystems. This architecture enables independent operation of the subsystems; the CPU "Fast" bus can be much faster than the chipset bus. CPU expansion boards may provide additional custom buses. Additionally, "busboards" or "bridgeboards" may provide ISA or PCI buses.
Central processing unit.
The Motorola 68000 series of microprocessors was used in all Amiga models from Commodore. While the 68000 family has a 32-bit design, the 68000 used in several early models is generally referred to as 16-bit. The 68000 has a 16-bit external data bus so must transfer 32 bits of data in two consecutive steps, a technique called multiplexing: all this is transparent to the software, which was 32-bit from the beginning. The 68000 could address 16 MB of physical memory. Later Amiga models featured full 32-bit CPUs with a larger address space and instruction pipeline facilities. Commodore's design choice to remain with the 68000 architecture ensured that code was backward-compatible across the Amiga line.
CPU upgrades were offered by both Commodore and third-party manufacturers. Most Amiga models can be upgraded either by direct CPU replacement or through expansion boards. Such boards often featured faster and higher capacity memory interfaces and hard disk controllers.
Towards the end of Commodore's time in charge of Amiga development there were suggestions that Commodore intended to move away from the 68000 series to higher performance RISC processors, such as the PA-RISC. However, these ideas were never developed before Commodore filed for bankruptcy. Despite this, third-party manufacturers designed upgrades featuring a combination of 68000 series and PowerPC processors along with a PowerPC native microkernel and software. Later Amiga clones featured PowerPC processors only.
Custom chipset.
The custom chipset at the core of the Amiga design appeared in three distinct generations, with a large degree of backward-compatibility. The Original Chip Set (OCS) appeared with the launch of the A1000 in 1985. OCS was eventually followed by the modestly improved Enhanced Chip Set (ECS) in 1990 and finally by the partly 32-bit Advanced Graphics Architecture (AGA) in 1992. Each chipset consists of several coprocessors which handle graphics acceleration, digital audio, direct memory access and communication between various peripherals (e.g., CPU, memory and floppy disks). In addition, some models featured auxiliary custom chips which performed tasks such as SCSI control and display de-interlacing.
Graphics.
All Amiga systems can display full-screen animated graphics with 2, 4, 8, 16, 32, 64 (EHB Mode) or 4096 colors (HAM Mode). Models with the AGA chipset (A1200 and A4000) also have non-EHB 64, 128, 256 and 262 144 (HAM Mode) color modes and a palette expanded from 4096 to 16.8 million colors.
The Amiga chipset can "genlock", which is the ability to adjust its own screen refresh timing to match an NTSC or PAL video signal. When combined with setting transparency, this allows an Amiga to overlay an external video source with graphics. This ability made the Amiga popular for many applications, and provides the ability to do character generation and CGI effects far more cheaply than earlier systems. This ability has been frequently utilized by wedding videographers, TV stations and their weather forecasting divisions (for weather graphics and radar), advertising channels, music video production, and desktop videographers. The NewTek Video Toaster was made possible by the genlock ability of the Amiga.
In 1988, the release of the Amiga A2024 fixed-frequency monochrome monitor with built-in framebuffer and flicker fixer hardware provided the Amiga with a choice of high-resolution graphic modes (1024×800 for NTSC and 1024×1024 for PAL).
ReTargetable Graphics.
ReTargetable Graphics is an API for device drivers mainly used by 3rd party graphics hardware to interface with AmigaOS via a set of libraries. The software libraries may include software tools to adjust resolution, screen colors, pointers and screenmodes. It uses available hardware and does not extend the capabilities in any way. Amiga intuition.library is limited to display depths of 8-bits but RTG libraries makes is possible to handle higher depths like 24-bits.
Sound.
The sound chip, named Paula, supports four PCM-sample-based sound channels (two for the left speaker and two for the right) with 8-bit resolution for each channel and a 6-bit volume control per channel. The analog output is connected to a low-pass filter, which filters out high-frequency aliases when the Amiga is using a lower sampling rate (see Nyquist limit). The brightness of the Amiga's power LED is used to indicate the status of the Amiga's low-pass filter. The filter is active when the LED is at normal brightness, and deactivated when dimmed (or off on older A500 Amigas). On Amiga 1000 (and first Amiga 500 and Amiga 2000 model), the power LED had no relation to the filter's status, and a wire needed to be manually soldered between pins on the sound chip to disable the filter. Paula can read directly from the system's RAM, using direct memory access (DMA), making sound playback without CPU intervention possible.
Although the hardware is limited to four separate sound channels, software such as "OctaMED" uses software mixing to allow eight or more virtual channels, and it was possible for software to mix two hardware channels to achieve a single 14-bit resolution channel by playing with the volumes of the channels in such a way that one of the source channels contributes the most significant bits and the other the least.
The quality of the Amiga's sound output, and the fact that the hardware is ubiquitous and easily addressed by software, were standout features of Amiga hardware unavailable on PC platforms for years. Third-party sound cards exist that provide DSP functions, multi-track direct-to-disk recording, multiple hardware sound channels and 16-bit and beyond resolutions. A retargetable sound API called AHI was developed allowing these cards to be used transparently by the OS and software.
Kickstart firmware.
Kickstart is the firmware upon which AmigaOS is bootstrapped. Its purpose is to initialize the Amiga hardware and core components of AmigaOS and then attempt to boot from a bootable volume, such as a floppy disk or hard disk drive. Most models (excluding the Amiga 1000) come equipped with Kickstart on an embedded ROM-chip.
Keyboard and mouse.
The keyboard on Amiga computers is similar to that found on a mid 80s IBM PC: Ten function keys, a numeric keypad, and four separate directional arrow keys. Caps Lock and Control share space to the left of A. Missing are the Home, End, Page Up, and Page Down keys: These are accomplished on Amigas by pressing shift and the appropriate arrow key. The Amiga keyboard adds a Help key, which a function key usually acts as such on PCs (usually F1). In addition to the Control and Alt modifier keys, the Amiga has 2 'Amiga' keys, rendered as 'Open Amiga' and 'Closed Amiga' similar to the Open/Closed Apple logo keys on Apple II keyboards. The left is used to manipulate the operating system (moving screens and the like) and the right delivered commands to the application. The absence of Num lock frees space for more math symbols around the number pad. Contemporary Macintosh computers, for comparison, lack function keys completely.
The mouse has two buttons like Windows, but unlike Windows pressing and holding the right button replaces the system status line at the top of the screen with a Maclike menu bar. As with Apple's Mac OS prior to Mac OS 8, menu options are selected by releasing the button over that option, not by left clicking.
The mouse plugs into one of two controller ports also used for joysticks, game paddles, and graphics tablets. Although compatible with analog joysticks, Atari 2600-style digital joysticks became standard.
Other peripherals and expansions.
The Amiga was one of the first computers for which inexpensive sound sampling and video digitization accessories were available. As a result of this and the Amiga's audio and video capabilities, the Amiga became a popular system for editing and producing both music and video.
Many expansion boards were produced for Amiga computers to improve the performance and capability of the hardware, such as memory expansions, SCSI controllers, CPU boards, and graphics boards. Other upgrades include genlocks, network cards for Ethernet, modems, sound cards and samplers, video digitizers, extra serial ports, and IDE controllers. Additions after the demise of Commodore company are USB cards. The most popular upgrades were memory, SCSI controllers and CPU accelerator cards. These were sometimes combined into the one device.
Early CPU accelerator cards feature full 32-bit CPUs of the 68000 family such as the Motorola 68020 and Motorola 68030, almost always with 32-bit memory and usually with FPUs and MMUs or the facility to add them. Later designs feature the Motorola 68040 or Motorola 68060. Both CPUs feature integrated FPUs and MMUs. Many CPU accelerator cards also had integrated SCSI controllers.
Phase5 designed the PowerUP boards (Blizzard PPC and CyberStorm PPC) featuring both a 68k (a 68040 or 68060) and a PowerPC (603 or 604) CPU, which are able to run the two CPUs at the same time and share the system memory. The PowerPC CPU on PowerUP boards is usually used as a coprocessor for heavy computations; a powerful CPU is needed to run MAME for example, but even decoding JPEG pictures and MP3 audio was considered heavy computation at the time. It is also possible to ignore the 68k CPU and run Linux on the PPC via project Linux APUS, but a PowerPC-native AmigaOS promised by Amiga Technologies GmbH was not available when the PowerUP boards first appeared.
24-bit graphics cards and video cards were also available. Graphics cards are designed primarily for 2D artwork production, workstation use, and later, gaming. Video cards are designed for inputting and outputting video signals, and processing and manipulating video.
In the North American market, the "NewTek Video Toaster" was a video effects board which turned the Amiga into an affordable video processing computer which found its way into many professional video environments. One particularly well-known use was to create the special effects in early series of "Babylon 5". Due to its NTSC-only design, it did not find a market in countries that used the PAL standard, such as in Europe. In those countries, the "OpalVision" card was popular, although less featured and supported than the Video Toaster. Low-cost timebase correctors (TBC) specifically designed to work with the Toaster quickly came to market, most of which were designed as standard Amiga bus cards.
Various manufacturers started producing PCI busboards for the A1200 and A4000, allowing standard Amiga computers to use PCI cards such as Voodoo graphic cards, Sound Blaster sound cards, 10/100 Ethernet cards, and television tuner cards. Other manufacturers produced hybrid boards which contained an Intel x86 series chip, allowing the Amiga to emulate a PC.
PowerPC upgrades with Wide SCSI controllers, PCI busboards with Ethernet, sound and 3D graphics cards, and tower cases allowed the A1200 and A4000 to survive well into the late nineties.
Expansion boards were made by Richmond Sound Design that allow their show control and sound design software to communicate with their custom hardware frames either by ribbon cable or fiber optic cable for long distances, allowing the Amiga to control up to eight million digitally controlled external audio, lighting, automation, relay and voltage control channels spread around a large theme park, for example. See Amiga software for more information on these applications.
Other popular devices included the following:
Serial ports.
The Commodore A2232 provides seven RS-232C serial ports. Each port can be driven independently at speeds of 50 to 19,200 bits/s. There is however a driver available on Aminet that allows two of the serial ports to be driven at 115 200 bits/s. The serial card used the 65CE02 CPU clocked at 3.58 MHz. This CPU was also part of the CSG 4510 CPU core that was used in the Commodore 65 computer.
Networking.
Amiga has three networking interface APIs:
Different network media were used:
Models and variants.
The original Amiga models were produced from 1985 to 1996. They are, in order of appearance: 1000, 2000, 500, 1500, 2500, 3000, 3000UX, 3000T, CDTV, 500+, 600, 4000, 1200, CD32, and 4000T. The PowerPC based AmigaOne was later produced from 2002 to 2005, and again in 2012. Several companies and private persons have also released Amiga clones and still do so today.
Commodore Amiga.
The first Amiga model, the Amiga 1000, was launched in 1985 and became popular for its impressive graphics, video and audio capabilities. In 2006, PC World rated the Amiga 1000 as the seventh greatest PC of all time, stating "Years ahead of its time, the Amiga was the world's first multimedia, multitasking personal computer".
Following the A1000, Commodore updated the desktop line of Amiga computers with the Amiga 2000 in 1987, the Amiga 3000 in 1990, and the Amiga 4000 in 1992, each offering improved capabilities and expansion options. However, the best selling models were the budget models, particularly the highly successful Amiga 500 (1987) and the Amiga 1200 (1992). The Amiga 500+ (1991) was the shortest lived model, replacing the Amiga 500 and lasting only six months until it was phased out and replaced with the Amiga 600 (1992), which in turn was also quickly replaced by the Amiga 1200.
The CDTV, launched in 1991, was a CD-ROM based all-in-one multimedia system. It was an early attempt at a multi-purpose multimedia appliance in an era before multimedia consoles and CD-ROM drives were common. Unfortunately for Commodore, the system never achieved any real commercial success. Like the Commodore 64GS that was a video game console based on a computer, the CDTV was designed as a video game console and multimedia platform. It had existed before the Sony Playstation and Sega Saturn, but had influenced them. It competed with the Turbo-Grafx CD and Sega CD system add ons when it was being sold.
Commodore's last Amiga offering before filing for bankruptcy was an attempt to capture a portion of the highly competitive 1990s console market with the Amiga CD32 (1993), a 32-bit CD-ROM games console. Though discontinued after Commodore's demise it met with moderate commercial success in Europe. The CD32 was a next generation CDTV, and it was designed to save Commodore by entering the growing video game console market.
Following purchase of Commodore's assets by Escom in 1995, the A1200 and A4000T continued to be sold in small quantities until 1996, though the ground lost since the initial launch and the prohibitive expense of these units meant that the Amiga line never regained any real popularity.
Several Amiga models contained references to songs by the rock band The B-52s. Early A500 units, at least, had the words "B52/ROCK LOBSTER" silk-screen printed onto their printed circuit board, a reference to the popular song "Rock Lobster" The Amiga 600 referenced "JUNE BUG" (after the song "Junebug") and the Amiga 1200 had "CHANNEL Z" (after "Channel Z").
Most original casing was made from ABS plastics which may become brown with time. This can be reversed by using the public domain chemical mix "Retr0bright".
AmigaOS 4 systems.
AmigaOS 4 is designed for PowerPC Amiga systems. It is mainly based on AmigaOS 3.1 source code, with some parts of version 3.9. Currently runs on both Amigas equipped with CyberstormPPC or BlizzardPPC accelerator boards, on the Teron series based AmigaOne computers built by Eyetech under license by Amiga Inc, on the Pegasos II from Genesi/bPlan GmbH, on the Acube Systems Sam440ep / Sam460ex / AmigaOne 500 systems and on the A-EON AmigaOne X1000.
AmigaOS 4.0 had been available only in developer pre-releases for numerous years until it was officially released in December 2006. Due to the nature of some provisions of the contract between Amiga Inc. and Hyperion Entertainment (the Belgian company which is developing the OS), the commercial AmigaOS 4 had been available only to licensed buyers of AmigaOne motherboards.
AmigaOS 4.0 for Amigas equipped with PowerUP accelerator boards was released in November 2007. Version 4.1 was released in August 2008 for AmigaOne systems, and in May 2011 for Amigas equipped with PowerUP accelerator boards. The most recent release of AmigaOS for all supported platforms is 4.1 update 5. Starting with release 4.1 update 4 there is an Emulation drawer containing official AmigaOS 3.x ROMs (all classic Amiga models including CD32) and relative Workbench files.
Acube Systems entered an agreement with Hyperion under which it has ported AmigaOS 4 to its Sam440ep and Sam460ex line of PowerPC-based motherboards. In 2009 a version for Pegasos II was released in co-operation with Acube Systems. In 2012, A-EON Technology Ltd manufactured and released the AmigaOne X1000 to consumers through their distributor, AmigaKit.com.
Amiga hardware clones.
Long-time Amiga developer MacroSystem entered the Amiga-clone market with their DraCo non-linear video editing system. It appears in two versions, initially a tower model and later a cube. DraCo expanded upon and combined a number of earlier expansion cards developed for Amiga (VLabMotion, Toccata, WarpEngine, RetinaIII) into a true Amiga-clone powered by the Motorola 68060 processor. The DraCo can run AmigaOS 3.1 up through AmigaOS 3.9. It is the only Amiga-based system to support FireWire for video I/O. DraCo also offers an Amiga-compatible Zorro-II expansion bus and introduced a faster custom DraCoBus, capable of 30 MB/sec transfer rates (faster than Commodore's Zorro-III). The technology was later used in the Casablanca system, a set-top-box also designed for non-linear video editing.
In 1998, Index Information released the Access, an Amiga-clone similar to the Amiga 1200, but on a motherboard which could fit into a standard 5¼" drive bay. It features either a 68020 or 68030 CPU, with a redesigned AGA chipset, and runs AmigaOS 3.1.
In 1998, former Amiga employees (John Smith, Peter Kittel, Dave Haynie and Andy Finkel to mention few) formed a new company called PIOS. Their hardware platform, PIOS One, was aimed at Amiga, Atari and Macintosh users. The company was renamed to Met@box in 1999 until it folded.
The NatAmi (short for "Native Amiga") hardware project began in 2005 with the aim of designing and building an Amiga clone motherboard that is enhanced with modern features. The NatAmi motherboard is a standard Mini-ITX-compatible form factor computer motherboard, powered by a Motorola/Freescale 68060 and its chipset. It is compatible with the original Amiga chipset, which has been inscribed on a programmable FPGA Altera chip on the board. The NatAmi is the second Amiga clone project after the Minimig motherboard, and its history is very similar to that of the C-One mainboard developed by Jeri Ellsworth and Jens Schönfeld. From a commercial point of view, Natami's circuitry and design are currently closed source. One goal of the NatAmi project is to design an Amiga-compatible motherboard that includes up-to-date features but that does not rely on emulation (as in WinUAE), modern PC Intel components, or a modern PowerPC mainboard. As such, NatAmi is not intended to become another evolutionary heir to classic Amigas, such as with AmigaOne or Pegasos computers. This "purist" philosophy essentially limits the resulting processor speed but puts the focus on bandwidth and low latencies. The developers also recreated the entire Amiga chipset, freeing it from legacy Amiga limitations such as two megabytes of audio and video graphics RAM as in the AGA chipset, and rebuilt this new chipset by programming a modern FPGA Altera Cyclone IV chip. Later, the developers decided to create from scratch a new software-form processor chip, codenamed "N68050" that resides in the physical Altera FPGA programmable chip.
In 2006, two new Amiga clones were announced, both using FPGA based hardware synthesis to replace the Amiga OCS custom chipset. The first, the Minimig, is a personal project of Dutch engineer Dennis van Weeren. Referred to as "new Amiga hardware", the original model was built on a Xilinx Spartan-3 development board, but soon a dedicated board was developed. The minimig uses the FPGA to reproduce the custom Denise, Agnus, Paula and Gary chips as well as both 8520 CIAs and implements a simple version of Amber. The rest of the chips are an actual 68000 CPU, ram chips, and a PIC microcontroller for BIOS control. The design for Minimig was released as open source on July 25, 2007. In February 2008, an Italian company Acube Systems began selling Minimig boards. A third party upgrade replaces the PIC microcontroller with a more powerful ARM processor, providing more functionality such as write access and support for hard disk images. The Minimig core has been ported to the FPGArcade "Replay" board. The Replay uses an FPGA with about 3 times more capacity and which does support the AGA chipset and a 68020 soft core with 68030 capabilities. The Replay board is designed to implement many older computers and classic arcade machines.
The second is the Clone-A system announced by Individual Computers. As of mid 2007 it has been shown in its development form, with FPGA-based boards replacing the Amiga chipset and mounted on an Amiga 500 motherboard.
In 2011, by ArcadeRetroGaming, called the Multiple Classic Computer, which emulates the Commodore 64. Support for Amiga software is planned.
Emulation.
Like many popular but discontinued platforms, the Amiga has been emulated so that software developed for the Amiga can be run on other computer platforms without the original hardware. Such emulators attempt to replicate the functionality of the Amiga architecture in software. As mentioned above, attempts have also been made to replicate the Amiga chipset in FPGA chips.
One of the most challenging aspects of emulation is the design of the Amiga chipset, which relies on cycle-critical timings. As a result, early emulators did not always achieve the intended results though later emulator versions can now accurately reproduce the behavior of Amiga systems.
Operating systems.
AmigaOS.
 "[AmigaOS] remains one of the great operating systems of the past 20 years, incorporating a small kernel and tremendous multitasking capabilities the likes of which have only recently been developed in OS/2 and Windows NT. The biggest difference is that the AmigaOS could operate fully and multitask in as little as 250 K of address space.
— "John C. Dvorak, PC Magazine", October 1996.
AmigaOS is a single-user multitasking operating system. It was developed first by Commodore International, and initially introduced in 1985 with the Amiga 1000. Original versions run on the Motorola 68000 series of microprocessors, while AmigaOS 4 runs only on PowerPC microprocessors. At the time of release AmigaOS put an operating system that was well ahead of its time into the hands of the average consumer. It was one of the first commercially available consumer operating systems for personal computers to implement preemptive multitasking.
Another notable feature was the combined use of both a command-line interface and graphical user interface. AmigaDOS was the disk operating system and command line portion of the OS and Workbench the native graphical windowing, icons, menu and pointer environment for file management and launching applications. Notably, AmigaDOS allowed long filenames (up to 107 characters) with whitespace and did not require file extensions. The windowing system and user interface engine which handles all input events is called Intuition.
The multi-tasking kernel is called Exec. It acts as a scheduler for tasks running on the system, providing pre-emptive multitasking with prioritised round-robin scheduling. It enabled true pre-emptive multitasking in as little as 256 KB of free memory.
AmigaOS is one of the few microkernel-based operating systems not to implement memory protection, though this lack is common amongst many of its contemporary operating systems. The lack of memory protection is because the 68000 CPU does not include a memory management unit and therefore there is no way to enforce protection of memory. Although this speeds and eases inter-process communication because programs can communicate by simply passing a pointer back and forth, the lack of memory protection made the AmigaOS more vulnerable to crashes from badly behaving programs than other multitasking systems that did implement memory protection, and Amiga OS is fundamentally incapable of enforcing any form of security model since any program had full access to the system. A co-operational memory protection feature was implemented in AmigaOS 4 and could be retrofitted to old AmigaOS systems using Enforcer or CyberGuard tools.
The problem was somewhat exacerbated by Commodore's initial decision to release documentation relating not only to the OS's underlying software routines, but also to the hardware itself, enabling intrepid programmers who had developed their skills on the Commodore 64 to POKE the hardware directly, as was done on the older platform. While the decision to release the documentation was a popular one and allowed the creation of fast, sophisticated sound and graphics routines in games and demos, it also contributed to system instability as some programmers lacked the expertise to program at this level. For this reason, when the new AGA chipset was released, Commodore declined to release low-level documentation in an attempt to force developers into using the approved software routines.
Influence on other operating systems.
AmigaOS directly or indirectly inspired the development of various operating systems. MorphOS and AROS clearly inherit heavily from the structure of AmigaOS as explained directly in articles regarding these two operating systems. AmigaOS also influenced BeOS, which featured a centralized system of Datatypes, similar to that present in AmigaOS. Likewise, DragonFlyBSD was also inspired by AmigaOS as stated by Dragonfly developer Matthew Dillon who is a former Amiga developer. WindowLab and amiwm are among several window managers for the X Window System seek to mimic the Workbench interface. IBM licensed the Amiga GUI from Commodore in exchange for the REXX language license. This allowed OS/2 to have the WPS (Work Place Shell) GUI shell for OS/2 2.0 a 32-bit operating system.
Unix and Unix-like systems.
Commodore-Amiga produced Amiga Unix, informally known as Amix, based on AT&T SVR4. It supports the Amiga 2500 and Amiga 3000 and is included with the Amiga 3000UX. Among other unusual features of Amix is a hardware-accelerated windowing system which can scroll windows without copying data. Amix is not supported on the later Amiga systems based on 68040 or 68060 processors.
Other, still maintained, operating systems are available for the classic Amiga platform, including Linux and NetBSD. Both require a CPU with MMU such as the 68020 with 68851 or full versions of the 68030, 68040 or 68060. There is also a version of Linux for Amigas with PowerPC accelerator cards. Debian and Yellow Dog Linux can run on the AmigaOne.
There is an official, older version of OpenBSD. The last Amiga release is 3.2. Minix 1.5.10 also runs on Amiga.
Emulating other systems.
The Amiga is able to emulate other computer platforms ranging from many 8-bit systems such as the Sinclair ZX Spectrum, Commodore 64, Nintendo Game Boy, Nintendo Entertainment System, Apple II and the TRS-80. The Commodore PC-Transformer software emulated an IBM 5150 at 1 MHz in Monochrome mode. Later PC-Bridgecards were a full hardware PC on a card with 8086/80286/80386 Intel chips running MS-DOS and Windows in an Amiga window. A-Max emulated an Apple Macintosh using a serial port dongle that had a Macintosh ROM on it. The Amiga had the same 68000 CPU as the Macintosh and, using a Macintosh emulator, could run Mac 68K operating systems and programs. However, the Amiga could not directly read Macintosh 3.5" floppies due to their proprietary form. Further, it required a compatible Macintosh for a copy of its ROM. The Atari ST was also emulated. MAME (the arcade machine emulator) is also available for Amiga systems with PPC accelerator card upgrades.
Amiga software.
In the late 1980s and early 1990s the platform became particularly popular for gaming, demoscene activities and creative software uses. During this time commercial developers marketed a wide range of games and creative software, often developing titles simultaneously for the Atari ST due to the similar hardware architecture. Popular creative software included, 3D rendering (ray-tracing) packages, bitmap graphics editors, desktop video software, software development packages and "tracker" music editors.
Until the late 1990s the Amiga remained a popular platform for non-commercial software, often developed by enthusiasts, and much of which was freely redistributable. An on-line archive, Aminet, was created in 1992 and until around 1996 was the largest public archive of software, art and documents for any platform.
Marketing.
The name "Amiga" was chosen by the developers from the Spanish word for a female friend, because they knew Spanish, and because it occurred before Apple and Atari alphabetically. It also conveyed the message that the Amiga computer line was "user friendly" as a pun or play on words.
The first official Amiga logo was a rainbow-colored double checkmark. In later marketing material Commodore largely dropped the checkmark and used logos styled with various typefaces. Though it was never adopted as a trademark by Commodore, the "Boing Ball" has been synonymous with Amiga since its launch. It became an unofficial and enduring theme after a visually impressive animated demonstration at the 1984 Winter Consumer Electronics Show in January 1984 showing a checkered ball bouncing and rotating. Following Escom's purchase of Commodore in 1996, the Boing Ball theme was incorporated into a new logo.
Early Commodore advertisements attempted to cast the computer as an all-purpose business machine, though the Amiga was most commercially successful as a home computer. Throughout the 1980s and early 1990s Commodore primarily placed advertising in computer magazines and occasionally in national newspapers and on television.
Legacy.
Since the demise of Commodore, various groups have marketed successors to the original Amiga line:
AmigaOS and MorphOS are commercial proprietary operative systems. AmigaOS 4, based on AmigaOS 3.1 source code with some parts of version 3.9, is developed by Hyperion Entertainment and runs on PowerPC based hardware. MorphOS, based on some parts of AROS source code, is developed by MorphOS Team and is continued on Apple and other PowerPC based hardware.
There is also AROS, a free and open source operative system (re-implementation of the AmigaOS 3.1 APIs), for Amiga 68k, x86 and ARM hardware (one version runs Linux-hosted on the Raspberry Pi). In particular, AROS for Amiga 68k hardware aims to create an open source Kickstart ROM replacement for emulation purpose and/or for use on real "classic" hardware.
Amiga community.
After Commodore went bankrupt in 1994, there remained a very active Amiga community, which continued to support the platform long after mainstream commercial vendors abandoned it. The most popular Amiga magazine, "Amiga Format", continued to publish editions until 2000, some six years after Commodore filed for bankruptcy. Another magazine, "Amiga Active", was launched in 1999 and was published until 2001. Several notable magazines are in publication today: "Amiga Future", which is available in both English and German; "Bitplane.it", a bi-monthly magazine in Italian; and "AmigaPower", a long-running French magazine.
In spite of declining interest in the platform there was a bi-weekly specialist column in the UK weekly magazine Micro Mart. There is also a fan website, that has served as a community discussion and support resource since the 1994 bankruptcy. Other popular English-language "forums" also exist, particularly amiga.org since 1994 and amigaworld.net and English Amiga Board.
Notable historic uses.
The Amiga series of computers found a place in early computer graphic design and television presentation. Below are some examples of notable uses and users:
In addition, many other celebrities and notable individuals have made use of the Amiga:
Other uses.
The Amiga was also used in a number of special purpose applications:

</doc>
<doc id="1985" url="http://en.wikipedia.org/wiki?curid=1985" title="Absorption">
Absorption

Absorption may refer to:

</doc>
<doc id="1986" url="http://en.wikipedia.org/wiki?curid=1986" title="Actinophryid">
Actinophryid

The actinophryids are small, familiar group of heliozoan protists. They are the most common heliozoa in fresh water, and are especially frequent in lakes and rivers, but a few are found in marine and soil habitats as well. Each actinophryid are unicellular and roughly spherical in shape, without any shell or test, and with many pseudopodia supported by axopods radiating outward from the cell body, which adhere to passing prey and allows it to roll or float about. The outer portion of the cell, or ectoplasm, is distinct and is filled with many tiny vacuoles, which assist in flotation. This is very similar to the process of osmosis. The movement of water from inside the cell to the outside is not because of water concentration in this case. It is the cell pushing the excess water out. A few contractile vacuoles around the periphery of the cell expel excess water, and are visible as clear bulges when full.
There are two genera included here. "Actinophrys" have a single, central nucleus. Most are around 40-50 μm in diameter, with axopods up to 100 μm in length, though this varies. "Actinosphaerium" are several times larger, from 200-1000 μm in diameter, with many nuclei, and are found exclusively in fresh water. Two other genera, "Echinosphaerium" and "Camptonema", have been described but appear to be synonyms.
Reproduction takes place by fission, with open mitosis. Under unfavourable conditions, the organism will form a cyst, which is multi-walled and covered in spikes. While encysted it may undergo a peculiar process of autogamy or self-fertilization, where it goes through meiosis and divides to form two gametes, which then fuse together again. This is the only form of sexual reproduction that occurs within the group, though it is really more genetic reorganization than reproduction.
The axopods are supported by microtubules arranged in a unique and characteristic double-coil pattern. In "Actinophrys", these arise from the nuclear membrane, while in "Actinosphaerium" some do and others don't. Other heliozoa where the microtubules arise from the nucleus have been considered possible relatives, and it now appears that the actinophryids developed from axodines such as "Pedinella". These are specialized heterokont algae, related to golden algae, diatoms, brown algae, and the like, which have microtubule-supported tentacles.
As far as the diet of the Actinophyrys goes, the protist feeds on small flagellates, diminutive cilates, microscopic algae, etc.

</doc>
<doc id="1988" url="http://en.wikipedia.org/wiki?curid=1988" title="Abel Tasman">
Abel Tasman

Abel Janszoon Tasman (]; 1603 – 10 October 1659) was a Dutch seafarer, explorer, and merchant, best known for his voyages of 1642 and 1644 in the service of the Dutch East India Company (VOC). He was the first known European explorer to reach the islands of Van Diemen's Land (now Tasmania) and New Zealand, and to sight the Fiji islands. His navigator François Visscher and his merchant Isaack Gilsemans mapped substantial portions of Australia, New Zealand, and some Pacific Islands.
First Pacific voyage.
Abel Jans Tasman was born in 1603 in Lutjegast in what is now the province of Groningen, the Netherlands. The oldest available source mentioning him dates 27 December 1631 when, as a widower living in Amsterdam, he became engaged to marry 21-year-old Jannetje Tjaers from the Jordaan district of the city. In 1633 he sailed from Texel to Batavia in the service of the Dutch East India Company (VOC), taking the southern Brouwer Route. Tasman helped to punish the people from Seram Island who had sold spices to others than the Dutch. In August 1637 he was back in Amsterdam, and the following year he signed on for another ten years and took his wife with him to Batavia. On 25 March 1638 he tried to sell his property in the Jordaan, but the purchase was cancelled. In 1639 he was second-in-command of an exploration expedition in the north Pacific under Matthijs Quast. The fleet included the ships "Engel" and "Gracht" and reached Fort Zeelandia (Dutch Formosa) and Deshima.
In August 1642, the Council of the Indies, consisting of Antonie van Diemen, Cornelis van der Lijn, Joan Maetsuycker, Justus Schouten, Salomon Sweers, Cornelis Witsen, and Pieter Boreel in Batavia despatched Tasman and Franchoijs Visscher on a voyage of which one of the objects was to obtain knowledge of "all the totally unknown provinces of Beach". This expedition used two small ships, the "Heemskerck" and the "Zeehaen".
Beach and Terra Australis.
"Beach" appeared on maps of the time, notably that of Abraham Ortelius of 1570 and that of Jan Huygen van Linschoten of 1596, as the northernmost part of the southern continent, the "Terra Australis", along with "Locach". According to Marco Polo, "Locach" was a kingdom where gold was “so plentiful that no one who did not see it could believe it”. "Beach" was in fact a mistranscription of "Locach". Locach was Marco Polo’s name for the southern Thai kingdom of Lavo, or Lop Buri, the “city of Lavo”, (ลพบร, after Lavo, the son of Rama in Hindu mythology). In Chinese (Cantonese), Lavo was pronounced “Lo-huk” (羅斛), from which Marco Polo took his rendition of the name. In the German cursive script, “Locach” and “Boeach” look similar, and in the 1532 edition of Marco Polo’s "Travels" his Locach was changed to "Boëach", later shortened to "Beach".
They seem to have drawn on the map of the world published in Florence in 1489 by Henricus Martellus, in which "provincia boëach" appears as the southern neighbour of "provincia ciamba". Book III of Marco Polo’s "Il Milione" described his journey by sea from China to India by way of Champa (= Southern Vietnam), Java (which he called "Java Major"), Locach and Sumatra (called "Java Minor"). After a chapter describing the kingdom of Champa there follows a chapter describing Java (which he did not visit himself). The narrative then resumes, describing the route southward from Champa toward Sumatra, but by a slip of the pen the name “Java” was substituted for “Champa” as the point of departure, locating Sumatra 1,300 miles to the south of Java instead of Champa. Locach, located between Champa and Sumatra, was likewise misplaced far to the south of Java, by some geographers on or near an extension of the "Terra Australis".
As explained by Sir Henry Yule, the editor of an English edition of Marco Polo’s "Travels": “Some geographers of the 16th century, following the old editions which carried the travellers south-east of Java to the land of “Boeach” (or Locac), introduced in their maps a continent in that situation”. Gerard Mercator did just that on his 1541 globe, placing "Beach provincia aurifera" (“Beach the gold-bearing province”) in the northernmost part of the "Terra Australis" in accordance with the faulty text of Marco Polo’s "Travels".
It remained in this location on his world map of 1569, with the amplified description, quoting Marco Polo, "Beach provincia aurifera quam pauci ex alienis regionibus adeunt propter gentis inhumanitatem" (“Beach the gold-bearing province, wither few go from other countries because of the inhumanity of its people”) with "Lucach regnum" shown somewhat to its south west. Following Mercator, Abraham Ortelius also showed "BEACH" and "LVCACH" in these locations on his world map of 1571. Likewise, Linschoten’s very popular 1596 map of the East Indies showed "BEACH" projecting from the map’s southern edge, leading (or misleading) Visscher and Tasman in their voyage of 1642 to seek Beach with its plentiful gold in a location to the south of the Solomon Islands somewhere between Staten Land near Cape Horn and the Cape of Good Hope.
Confirmation that land existed where the maps showed "Beach" to be had come from Dirk Hartog’s landing in October 1616 on its west coast, which he called Eendrachtsland after the name of his ship.
Mauritius.
In accordance with Visscher's directions, Tasman sailed from Batavia on 14 August 1642 and arrived at Mauritius on 5 September 1642. The reason for this was the crew could be fed well on the island; there was plenty of fresh water and timber to repair the ships. Tasman got the assistance of the governor Adriaan van der Stel. Because of the prevailing winds Mauritius was chosen as a turning point. After a four-week stay on the island both ships left on 8 October using the Roaring Forties to sail east as fast as possible. (No one had gone as far as Pieter Nuyts in 1626/27.) On 7 November snow and hail influenced the ship's council to alter course to a more north-eastern direction, expecting to arrive one day at the Solomon Islands.
Tasmania.
On 24 November 1642 Abel Tasman sighted the west coast of Tasmania, north of Macquarie Harbour. He named his discovery Van Diemen's Land after Antonio van Diemen, Governor-General of the Dutch East Indies. Proceeding south he skirted the southern end of Tasmania and turned north-east, Tasman then tried to work his two ships into Adventure Bay on the east coast of South Bruny Island where he was blown out to sea by a storm, this area he named Storm Bay. Two days later Tasman anchored to the North of Cape Frederick Hendrick just North of the Forestier Peninsula. Tasman then landed in Blackman Bay – in the larger Marion Bay. The next day, an attempt was made to land in North Bay; however, because the sea was too rough the carpenter swam through the surf and planted the Dutch flag in North Bay. Tasman then claimed formal possession of the land on 3 December 1642.
New Zealand.
After some exploration, Tasman had intended to proceed in a northerly direction but as the wind was unfavourable he steered east. Tasman endured a very rough journey from Tasmania to New Zealand. In one of his diary entries Tasman credits his compass, claiming it was the only thing that kept him alive. On 13 December they sighted land on the north-west coast of the South Island, New Zealand, becoming the first Europeans to do so. Tasman named it "Staten Landt" on the assumption that it was connected to an island (Staten Island, Argentina) at the south of the tip of South America. He sailed north, then east and 5 days later anchored about 7 km from the coast. He sent ship's boats to gather water, but one of his boats was attacked by Māori in a double hulled waka (canoes) and four of his men were attacked and killed by mere. As Tasman sailed out of the bay he was again attacked, this time by 11 waka . The waka approached the "Zeehaen" which fired and hit one Maori who fell down. Canister shot hit the side of a waka. Archeological research has shown the Dutch had tried to land at a major agricultural area, which the Māori may have been trying to protect. Tasman named the bay "Murderers' Bay" (now known as Golden Bay) and sailed north, but mistook Cook Strait for a bight (naming it "Zeehaen's Bight"). Two names he gave to New Zealand landmarks still endure, Cape Maria van Diemen and Three Kings Islands, but "Kaap Pieter Boreels" was renamed by Cook 125 years later to Cape Egmont.
The return voyage.
On route back to Batavia, Tasman came across the Tongan archipelago on 20 January 1643. While passing the Fiji Islands Tasman's ships came close to being wrecked on the dangerous reefs of the north-eastern part of the Fiji group. He charted the eastern tip of Vanua Levu and Cikobia before making his way back into the open sea. He eventually turned north-west to New Guinea, and arrived at Batavia on 15 June 1643.
Second Pacific voyage.
With three ships on his second voyage ("Limmen", "Zeemeeuw" and the tender "Braek") in 1644, he followed the south coast of New Guinea eastwards. He missed the Torres Strait between New Guinea and Australia, and continued his voyage westwards along the north Australian coast. He mapped the north coast of Australia making observations on the land, called New Holland, and its people.
From the point of view of the Dutch East India Company, Tasman's explorations were a disappointment: he had neither found a promising area for trade nor a useful new shipping route. Although received modestly, the company was upset to a degree that Tasman didn't fully explore the lands he found, and decided that a more "persistent explorer" should be chosen for any future expeditions. For over a century, until the era of James Cook, Tasmania and New Zealand were not visited by Europeans – mainland Australia was visited, but usually only by accident.
Later life.
On 2 November 1644 Abel Tasman was appointed a member of the Council of Justice at Batavia. He went to Sumatra in 1646, and in August 1647 to Siam (now Thailand) with letters from the company to the King. In May 1648 he was in charge of an expedition sent to Manila to try to intercept and loot the Spanish silver ships coming from America, but he had no success and returned to Batavia in January 1649. In November 1649 he was charged and found guilty of having in the previous year hanged one of his men without trial, was suspended from his office of commander, fined, and made to pay compensation to the relatives of the sailor. On 5 January 1651 he was formally reinstated in his rank and spent his remaining years at Batavia. He was in good circumstances, being one of the larger landowners in the town. He died at Batavia on 10 October 1659 and was survived by his second wife and a daughter by his first wife. His property was divided between his wife and his daughter by his first marriage. In his testimony (dating from 1657) he left only 25 guilders to the poor of his village.
Legacy.
Multiple places have been named after Tasman, including:
Also named after Tasman are:
His portrait has been on 4 New Zealand postage stamp issues, on a 1992 5 NZD coin, and on one 1985 Australian postage stamp
Tasman Map.
The original Tasman Map is held in the collection of the State Library of New South Wales. The map shows a general outline of parts of the coastline of Australia. In the foyer of the Mitchell wing at the State Library, the map is reproduced in a marble floor.

</doc>
<doc id="1990" url="http://en.wikipedia.org/wiki?curid=1990" title="August 5">
August 5

August 5 is the day of the year in the Gregorian calendar.

</doc>
<doc id="1991" url="http://en.wikipedia.org/wiki?curid=1991" title="Angula">
Angula

The word Angula may refer to one of the following:

</doc>
<doc id="1994" url="http://en.wikipedia.org/wiki?curid=1994" title="ASP">
ASP

ASP may refer to:

</doc>
<doc id="1997" url="http://en.wikipedia.org/wiki?curid=1997" title="Algebraic geometry">
Algebraic geometry

Algebraic geometry is a branch of mathematics, classically studying zeros of multivariate polynomial equations. Modern algebraic geometry is based on more abstract techniques of abstract algebra, especially commutative algebra, with the language and the problems of geometry.
The fundamental objects of study in algebraic geometry are algebraic varieties, which are geometric manifestations of solutions of systems of polynomial equations. Examples of the most studied classes of algebraic varieties are: plane algebraic curves, which include lines, circles, parabolas, ellipses, hyperbolas, cubic curves like elliptic curves and quartic curves like lemniscates, and Cassini ovals. A point of the plane belongs to an algebraic curve if its coordinates satisfy a given polynomial equation. Basic questions involve the study of the points of special interest like the singular points, the inflection points and the points at infinity. More advanced questions involve the topology of the curve and relations between the curves given by different equations.
Algebraic geometry occupies a central place in modern mathematics and has multiple conceptual connections with such diverse fields as complex analysis, topology and number theory. Initially a study of systems of polynomial equations in several variables, the subject of algebraic geometry starts where equation solving leaves off, and it becomes even more important to understand the intrinsic properties of the totality of solutions of a system of equations, than to find a specific solution; this leads into some of the deepest areas in all of mathematics, both conceptually and in terms of technique.
In the 20th century, algebraic geometry has split into several subareas.
Much of the development of the main stream of algebraic geometry in the 20th century occurred within an abstract algebraic framework, with increasing emphasis being placed on "intrinsic" properties of algebraic varieties not dependent on any particular way of embedding the variety in an ambient coordinate space; this parallels developments in topology, differential and complex geometry. One key achievement of this abstract algebraic geometry is Grothendieck's scheme theory which allows one to use sheaf theory to study algebraic varieties in a way which is very similar to its use in the study of differential and analytic manifolds. This is obtained by extending the notion of point: In classical algebraic geometry, a point of an affine variety may be identified, through Hilbert's Nullstellensatz, with a maximal ideal of the coordinate ring, while the points of the corresponding affine scheme are all prime ideals of this ring. This means that a point of such a scheme may be either a usual point or a subvariety. This approach also enables a unification of the language and the tools of classical algebraic geometry, mainly concerned with complex points, and of algebraic number theory. Wiles's proof of the longstanding conjecture called Fermat's last theorem is an example of the power of this approach.
Basic notions.
Zeros of simultaneous polynomials.
In classical algebraic geometry, the main objects of interest are the vanishing sets of collections of polynomials, meaning the set of all points that simultaneously satisfy one or more polynomial equations. For instance, the two-dimensional sphere in three-dimensional Euclidean space R3 could be defined as the set of all points ("x","y","z") with
A "slanted" circle in R3 can be defined as the set of all points ("x","y","z") which satisfy the two polynomial equations
Affine varieties.
First we start with a field "k". In classical algebraic geometry, this field was always the complex numbers C, but many of the same results are true if we assume only that "k" is algebraically closed. We consider the affine space of dimension "n" over "k", denoted An("k") (or more simply A"n", when "k" is clear from the context). When one fixes a coordinates system, one may identify An("k") with "k""n". The purpose of not working with "k""n" is to emphasize that one "forgets" the vector space structure that "k"n carries.
A function "f" : A"n" → A1 is said to be "polynomial" (or "regular") if it can be written as a polynomial, that is, if there is a polynomial "p" in "k"["x"1...,"x""n"] such that "f"("M") = "p"("t"1...,"t""n") for every point "M" with coordinates ("t"1...,"t""n") in A"n". The property of a function to be polynomial (or regular) does not depend on the choice of a coordinate system in A"n".
When a coordinate system is chosen, the regular functions on the affine "n"-space may be identified with the ring of polynomial functions in "n" variables over "k". Therefore the set of the regular functions on A"n" is a ring, which is denoted "k"[A"n"].
We say that a polynomial "vanishes" at a point if evaluating it at that point gives zero. Let "S" be a set of polynomials in "k"[An]. The "vanishing set of S" (or "vanishing locus" or "zero set") is the set "V"("S") of all points in A"n" where every polynomial in "S" vanishes. In other words,
A subset of A"n" which is "V"("S"), for some "S", is called an "algebraic set". The "V" stands for "variety" (a specific type of algebraic set to be defined below).
Given a subset "U" of A"n", can one recover the set of polynomials which generate it? If "U" is "any" subset of A"n", define "I"("U") to be the set of all polynomials whose vanishing set contains "U". The "I" stands for ideal: if two polynomials "f" and "g" both vanish on "U", then "f"+"g" vanishes on "U", and if "h" is any polynomial, then "hf" vanishes on "U", so "I"("U") is always an ideal of the polynomial ring "k"[A"n"].
Two natural questions to ask are:
The answer to the first question is provided by introducing the Zariski topology, a topology on A"n" whose closed sets are the algebraic sets, and which directly reflects the algebraic structure of "k"[A"n"]. Then "U" = "V"("I"("U")) if and only if "U" is an algebraic set or equivalently a Zariski-closed set. The answer to the second question is given by Hilbert's Nullstellensatz. In one of its forms, it says that "I"("V"("S")) is the radical of the ideal generated by "S". In more abstract language, there is a Galois connection, giving rise to two closure operators; they can be identified, and naturally play a basic role in the theory; the example is elaborated at Galois connection.
For various reasons we may not always want to work with the entire ideal corresponding to an algebraic set "U". Hilbert's basis theorem implies that ideals in "k"[A"n"] are always finitely generated.
An algebraic set is called "irreducible" if it cannot be written as the union of two smaller algebraic sets. Any algebraic set is a finite union of irreducible algebraic sets and this decomposition is unique. Thus its elements are called the "irreducible components" of the algebraic set. An irreducible algebraic set is also called a "variety". It turns out that an algebraic set is a variety if and only if it may be defined as the vanishing set of a prime ideal of the polynomial ring.
Some authors do not make a clear distinction between algebraic sets and varieties and use "irreducible variety" to make the distinction when needed.
Regular functions.
Just as continuous functions are the natural maps on topological spaces and smooth functions are the natural maps on differentiable manifolds, there is a natural class of functions on an algebraic set, called "regular functions" or "polynomial functions". A regular function on an algebraic set "V" contained in An is the restriction to "V" of a regular function on An. For an algebraic set defined on the field of the complex numbers, the regular functions are smooth and even analytic.
It may seem unnaturally restrictive to require that a regular function always extend to the ambient space, but it is very similar to the situation in a normal topological space, where the Tietze extension theorem guarantees that a continuous function on a closed subset always extends to the ambient topological space.
Just as with the regular functions on affine space, the regular functions on "V" form a ring, which we denote by "k"["V"]. This ring is called the "coordinate ring of V".
Since regular functions on V come from regular functions on An, there is a relationship between the coordinate rings. Specifically, if a regular function on "V" is the restriction of two functions "f" and "g" in "k"[An], then "f" − "g" is a polynomial function which is null on "V" and thus belongs to "I"("V"). Thus "k"["V"] may be identified with "k"[An]/"I"("V").
Morphism of affine varieties.
Using regular functions from an affine variety to A1, we can define regular maps from one affine variety to another. First we will define a regular map from a variety into affine space: Let "V" be a variety contained in An. Choose "m" regular functions on "V", and call them "f"1, ..., "f""m". We define a "regular map" "f" from "V" to Am by letting "f" = ("f"1, ..., "f""m"). In other words, each "f""i" determines one coordinate of the range of "f".
If "V"' is a variety contained in Am, we say that "f" is a "regular map" from "V" to "V"' if the range of "f" is contained in "V"'.
The definition of the regular maps apply also to algebraic sets.
The regular maps are also called "morphisms", as they make the collection of all affine algebraic sets into a category, where the objects are the affine algebraic sets and the morphisms are the regular maps. The affine varieties is a subcategory of the category of the algebraic sets.
Given a regular map "g " from "V" to "V"' and a regular function "f" of "k"["V"'], then "f"∘"g"∈"k"["V"]. The map "f"→"f"∘"g" is a ring homomorphism from "k"["V"'] to "k"["V"]. Conversely, every ring homomorphism from "k"["V"'] to "k"["V"] defines a regular map from "V" to "V"'. This defines an equivalence of categories between the category of algebraic sets and the opposite category of the finitely generated reduced "k"-algebras. This equivalence is one of the starting points of scheme theory.
Rational function and birational equivalence.
Contrarily to the preceding ones, this section concerns only varieties and not algebraic sets. On the other hand the definitions extend naturally to projective varieties (next section), as an affine variety and its projective completion have the same field of functions.
If "V" is an affine variety, its coordinate ring is an integral domain and has thus a field of fractions which is denoted "k"("V") and called the "field of the rational functions" on "V" or, shortly, the "function field" of "V". Its elements are the restrictions to "V" of the rational functions over the affine space containing "V". The domain of a rational function "f" is not "V" but the complement of the subvariety (a hypersurface) where the denominator of "f" vanishes.
Like for regular maps, one may define a "rational map" from a variety "V" to a variety "V"'. Like for the regular maps, the rational maps from "V" to "V"' may be identified to the field homomorphisms from "k"("V"') to "k"("V").
Two affine varieties are "birationally equivalent" if there are two rational functions between them which are inverse one to the other in the regions where both are defined. Equivalently, they are birationally equivalent if their function fields are isomorphic.
An affine variety is a "rational variety" if it is birationally equivalent to an affine space. This means that the variety admits a rational parameterization. For example, the circle of equation formula_5 is a rational curve, as it has the parameterization
which may also be viewed as a rational map from the line to the circle.
The problem of resolution of singularities is to know if every algebraic variety is birationally equivalent to a variety whose projective completion is nonsingular (see also smooth completion). It has been positively solved in characteristic 0 by Heisuke Hironaka in 1964 and is yet unsolved in finite characteristic.
Projective variety.
Just as the formulas for the roots of 2nd, 3rd and 4th degree polynomials suggest extending real numbers to the more algebraically complete setting of the complex numbers, many properties of algebraic varieties suggest extending affine space to a more geometrically complete projective space. Whereas the complex numbers are obtained by adding the number i, a root of the polynomial x^2 + 1, projective space is obtained by adding in appropriate points "at infinity", points where parallel lines may meet.
To see how this might come about, consider the variety "V"("y" − "x"2). If we draw it, we get a parabola. As "x" goes to positive infinity, the slope of the line from the origin to the point ("x", "x"2) also goes to positive infinity. As "x" goes to negative infinity, the slope of the same line goes to negative infinity.
Compare this to the variety "V"("y" − "x"3). This is a cubic curve. As "x" goes to positive infinity, the slope of the line from the origin to the point ("x", "x"3) goes to positive infinity just as before. But unlike before, as "x" goes to negative infinity, the slope of the same line goes to positive infinity as well; the exact opposite of the parabola. So the behavior "at infinity" of "V"("y" − "x"3) is different from the behavior "at infinity" of "V"("y" − "x"2).
The consideration of the "projective completion" of the two curves, which is their prolongation "at infinity" in the projective plane, allows to quantify this difference: the point at infinity of the parabola is a regular point, whose tangent is the line at infinity, while the point at infinity of the cubic curve is a cusp. Also, both curves are rational, as they are parameterized by "x", and Riemann-Roch theorem implies that the cubic curve must have a singularity, which must be at infinity, as all its points in the affine space are regular.
Thus many of the properties of algebraic varieties, including birational equivalence and all the topological properties, depends on the behavior "at infinity" and so it is natural to study the varieties in projective space. Furthermore, the introduction of projective techniques made many theorems in algebraic geometry simpler and sharper: For example, Bézout's theorem on the number of intersection points between two varieties can be stated in its sharpest form only in projective space. For these reasons, projective space plays a fundamental role in algebraic geometry.
Nowadays, the "projective space" P"n" of dimension "n" is usually defined as the set of the lines passing through a point, considered as the origin, in the affine space of dimension "n"+1, or equivalently to the set of the vector lines in a vector space of dimension "n"+1. When a coordinate system has been chosen in the space of dimension "n"+1, all the points of a line have the same set of coordinates, up to the multiplication by an element of "k". This defines the homogeneous coordinates of a point of P"n" as a sequence of "n"+1 elements of the base field "k", defined up to the multiplication by a nonzero element of "k" (the same for the whole sequence).
Given a polynomial in "n"+1 variables, it vanishes at all the point of a line passing through the origin if and only if it is homogeneous. In this case, one says that the polynomial "vanishes" at the corresponding point of P"n". This allows to define a "projective algebraic set" in P"n" as the set "V"("f"1, ..., "f""k") where a finite set of homogeneous polynomials {"f"1, ..., "f""k"} vanishes. Like for affine algebraic sets, there is a bijection between the projective algebraic sets and the reduced homogeneous ideals which define them. The "projective varieties" are the projective algebraic sets whose defining ideal is prime. In other words, a projective variety is a projective algebraic set, whose homogeneous coordinate ring is an integral domain, the "projective coordinates ring" being defined as the quotient of the graded ring or the polynomials in "n"+1 variables by the homogeneous (reduced) ideal defining the variety. Every projective algebraic set may be uniquely decomposed into a finite union of projective varieties.
The only regular functions which may be defined properly on a projective variety are the constant functions. Thus this notion is not used in projective situations. On the other hand the "field of the rational functions" or "function field " is a useful notion, which, similarly as in the affine case, is defined as the set of the quotients of two homogeneous elements of the same degree in the homogeneous coordinate ring.
Real algebraic geometry.
The real algebraic geometry is the study of the real points of the algebraic geometry.
The fact that the field of the reals number is an ordered field may not be occulted in such a study. For example, the curve of equation formula_8 is a circle if formula_9, but does not have any real point if formula_10. It follows that real algebraic geometry is not only the study of the real algebraic varieties, but has been generalized to the study of the "semi-algebraic sets", which are the solutions of systems of polynomial equations and polynomial inequalities. For example, a branch of the hyperbola of equation formula_11 is not an algebraic variety, but is a semi-algebraic set defined by formula_12 and formula_13 or by formula_12 and formula_15.
One of the challenging problems of real algebraic geometry is the unsolved Hilbert's sixteenth problem: Decide which respective positions are possible for the ovals of a nonsingular plane curve of degree 8.
Computational algebraic geometry.
One may date the origin of computational algebraic geometry to meeting EUROSAM'79 (International Symposium on Symbolic and Algebraic Manipulation) held at Marseille, France in June 1979. At this meeting,
Since then, most results in this area are related to one or several of these items either by using or improving one of these algorithms, or by finding algorithms whose complexity is simply exponential in the number of the variables.
Gröbner basis.
A Gröbner basis is a system of generators of a polynomial ideal whose computation allows the deduction of many properties of the affine algebraic variety defined by the ideal.
Given an ideal "I" defining an algebraic set "V":
Gröbner basis computations do not allow to compute directly the primary decomposition of "I" nor the prime ideals defining the irreducible components of "V", but most algorithms for this involve Gröbner basis computation. The algorithms which are not based on Gröbner bases use regular chains but may need Gröbner bases in some exceptional situations.
Gröbner base are deemed to be difficult to compute. In fact they may contain, in the worst case, polynomials whose degree is doubly exponential in the number of variables and a number of polynomials which is also doubly exponential. However, this is only a worst case complexity, and the complexity bound of Lazard's algorithm of 1979 may frequently apply. Faugère's F4 and F5 algorithms realize this complexity, as F5 algorithm may be viewed as an improvement of Lazard's 1979 algorithm. It follows that the best implementations allow to compute almost routinely with algebraic sets of degree more than 100. This means that, presently, the difficulty of computing a Gröbner basis is strongly related to the intrinsic difficulty of the problem.
Cylindrical Algebraic Decomposition (CAD).
CAD is an algorithm which had been introduced in 1973 by G. Collins to implement with an acceptable complexity the Tarski–Seidenberg theorem on quantifier elimination over the real numbers.
This theorem concerns the formulas of the first-order logic whose atomic formulas are polynomial equalities or inequalities between polynomials with real coefficients. These formulas are thus the formulas which may be constructed from the atomic formulas by the logical operators "and" (∧), "or" (∨), "not" (¬), "for all" (∀) and "exists" (∃). Tarski's theorem asserts that, from such a formula, one may compute an equivalent formula without quantifier (∀, ∃).
The complexity of CAD is doubly exponential in the number of variables. This means that CAD allow, in theory, to solve every problem of real algebraic geometry which may be expressed by such a formula, that is almost every problem concerning explicitly given varieties and semi-algebraic sets.
While Gröbner basis computation has doubly exponential complexity only in rare cases, CAD has almost always this high complexity. This implies that, unless if most polynomials appearing in the input are linear, it may not solve problems with more than four variables.
Since 1973, most of the research on this subject is devoted either to improve CAD or to find alternate algorithms in special cases of general interest.
As an example of the state of art, there are efficient algorithms to find at least a point in every connected component of a semi-algebraic set, and thus to test if a semi-algebraic set is empty. On the other hand CAD is yet, in practice, the best algorithm to count the number of connected components.
Asymptotic complexity vs. practical efficiency.
The basic general algorithms of computational geometry have a double exponential worst case complexity. More precisely, if "d" is the maximal degree of the input polynomials and "n" the number of variables, their complexity is at most formula_16 for some constant "c", and, for some inputs, the complexity is at least formula_17 for another constant "c"′.
During the last 20 years of 20th century, various algorithms have been introduced to solve specific subproblems with a better complexity. Most of these algorithms have a complexity formula_18.
Among these algorithms which solve a sub problem of the problems solved by Gröbner bases, one may cite "testing if an affine variety is empty" and "solving nonhomogeneous polynomial systems which have a finite number of solutions." Such algorithms are rarely implemented because, on most entries Faugère's F4 and F5 algorithms have a better practical efficiency and probably a similar or better complexity ("probably" because the evaluation of the complexity of Gröbner basis algorithms on a particular class of entries is a difficult task which has be done only in few special cases).
The main algorithms of real algebraic geometry which solve a problem solved by CAD are related to the topology of semi-algebraic sets. One may cite "counting the number of connected components", "testing if two points are in the same components" or "computing a Whitney stratification of a real algebraic set". They have a complexity of
formula_18, but the constant involved by "O" notation is so high that using them to solve any nontrivial problem effectively solved by CAD, is impossible even if one could use all the existing computing power in the world. Therefore these algorithms have never been implemented and this is an active research area to search for algorithms with have together a good asymptotic complexity and a good practical efficiency.
Abstract modern viewpoint.
The modern approaches to algebraic geometry redefine and effectively extend the range of basic objects in various levels of generality to schemes, formal schemes, ind-schemes, algebraic spaces, algebraic stacks and so on. The need for this arises already from the useful ideas within theory of varieties, e.g. the formal functions of Zariski can be accommodated by introducing nilpotent elements in structure rings; considering spaces of loops and arcs, constructing quotients by group actions and developing formal grounds for natural intersection theory and deformation theory lead to some of the further extensions.
Most remarkably, in late 1950s, algebraic varieties were subsumed into Alexander Grothendieck's concept of a scheme. Their local objects are affine schemes or prime spectra which are locally ringed spaces which form a category which is antiequivalent to the category of commutative unital rings, extending the duality between the category of affine algebraic varieties over a field "k", and the category of finitely generated reduced "k"-algebras. The gluing is along Zariski topology; one can glue within the category of locally ringed spaces, but also, using the Yoneda embedding, within the more abstract category of presheaves of sets over the category of affine schemes. The Zariski topology in the set theoretic sense is then replaced by a Grothendieck topology. Grothendieck introduced Grothendieck topologies having in mind more exotic but geometrically finer and more sensitive examples than the crude Zariski topology, namely the étale topology, and the two flat Grothendieck topologies: fppf and fpqc; nowadays some other examples became prominent including Nisnevich topology. Sheaves can be furthermore generalized to stacks in the sense of Grothendieck, usually with some additional representability conditions leading to Artin stacks and, even finer, Deligne-Mumford stacks, both often called algebraic stacks.
Sometimes other algebraic sites replace the category of affine schemes. For example, Nikolai Durov has introduced commutative algebraic monads as a generalization of local objects in a generalized algebraic geometry. Versions of a tropical geometry, of an absolute geometry over a field of one element and an algebraic analogue of Arakelov's geometry were realized in this setup.
Another formal generalization is possible to Universal algebraic geometry in which every variety of algebras has its own algebraic geometry. The term "variety of algebras" should not be confused with "algebraic variety".
The language of schemes, stacks and generalizations has proved to be a valuable way of dealing with geometric concepts and became cornerstones of modern algebraic geometry.
Algebraic stacks can be further generalized and for many practical questions like deformation theory and intersection theory, this is often the most natural approach. One can extend the Grothendieck site of affine schemes to a higher categorical site of derived affine schemes, by replacing the commutative rings with an infinity category of differential graded commutative algebras, or of simplicial commutative rings or a similar category with an appropriate variant of a Grothendieck topology. One can also replace presheaves of sets by presheaves of simplicial sets (or of infinity groupoids). Then, in presence of an appropriate homotopic machinery one can develop a notion of derived stack as such a presheaf on the infinity category of derived affine schemes, which is satisfying certain infinite categorical version of a sheaf axiom (and to be algebraic, inductively a sequence of representability conditions). Quillen model categories, Segal categories and quasicategories are some of the most often used tools to formalize this yielding the "derived algebraic geometry", introduced by the school of Carlos Simpson, including Andre Hirschowitz, Bertrand Toën, Gabrielle Vezzosi, Michel Vaquié and others; and developed further by Jacob Lurie, Bertrand Toën, and Gabrielle Vezzosi. Another (noncommutative) version of derived algebraic geometry, using A-infinity categories has been developed from early 1990-s by Maxim Kontsevich and followers.
History.
Prehistory: before the 16th century.
Some of the roots of algebraic geometry date back to the work of the Hellenistic Greeks from the 5th century BC. The Delian problem, for instance, was to construct a length "x" so that the cube of side "x" contained the same volume as the rectangular box "a"2"b" for given sides "a" and "b". Menaechmus (circa 350 BC) considered the problem geometrically by intersecting the pair of plane conics "ay" = "x"2 and "xy" = "ab". The later work, in the 3rd century BC, of Archimedes and Apollonius studied more systematically problems on conic sections, and also involved the use of coordinates. The Arab mathematicians were able to solve by purely algebraic means certain cubic equations, and then to interpret the results geometrically. This was done, for instance, by Ibn al-Haytham in the 10th century AD. Subsequently, Persian mathematician Omar Khayyám (born 1048 A.D.) discovered the general method of solving cubic equations by intersecting a parabola with a circle. Each of these early developments in algebraic geometry dealt with questions of finding and describing the intersections of algebraic curves.
Renaissance.
Such techniques of applying geometrical constructions to algebraic problems were also adopted by a number of Renaissance mathematicians such as Gerolamo Cardano and Niccolò Fontana "Tartaglia" on their studies of the cubic equation. The geometrical approach to construction problems, rather than the algebraic one, was favored by most 16th and 17th century mathematicians, notably Blaise Pascal who argued against the use of algebraic and analytical methods in geometry. The French mathematicians Franciscus Vieta and later René Descartes and Pierre de Fermat revolutionized the conventional way of thinking about construction problems through the introduction of coordinate geometry. They were interested primarily in the properties of "algebraic curves", such as those defined by Diophantine equations (in the case of Fermat), and the algebraic reformulation of the classical Greek works on conics and cubics (in the case of Descartes).
During the same period, Blaise Pascal and Gérard Desargues approached geometry from a different perspective, developing the synthetic notions of projective geometry. Pascal and Desargues also studied curves, but from the purely geometrical point of view: the analog of the Greek "ruler and compass construction". Ultimately, the analytic geometry of Descartes and Fermat won out, for it supplied the 18th century mathematicians with concrete quantitative tools needed to study physical problems using the new calculus of Newton and Leibniz. However, by the end of the 18th century, most of the algebraic character of coordinate geometry was subsumed by the "calculus of infinitesimals" of Lagrange and Euler.
19th and early 20th century.
It took the simultaneous 19th century developments of non-Euclidean geometry and Abelian integrals in order to bring the old algebraic ideas back into the geometrical fold. The first of these new developments was seized up by Edmond Laguerre and Arthur Cayley, who attempted to ascertain the generalized metric properties of projective space. Cayley introduced the idea of "homogeneous polynomial forms", and more specifically quadratic forms, on projective space. Subsequently, Felix Klein studied projective geometry (along with other types of geometry) from the viewpoint that the geometry on a space is encoded in a certain class of transformations on the space. By the end of the 19th century, projective geometers were studying more general kinds of transformations on figures in projective space. Rather than the projective linear transformations which were normally regarded as giving the fundamental Kleinian geometry on projective space, they concerned themselves also with the higher degree birational transformations. This weaker notion of congruence would later lead members of the 20th century Italian school of algebraic geometry to classify algebraic surfaces up to birational isomorphism.
The second early 19th century development, that of Abelian integrals, would lead Bernhard Riemann to the development of Riemann surfaces.
In the same period began the algebraization of the algebraic geometry through commutative algebra. The prominent results in this direction are Hilbert's basis theorem and Hilbert's Nullstellensatz, which are the basis of the connexion between algebraic geometry and commutative algebra, and Macaulay's multivariate resultant, which is the basis of elimination theory. Probably because of the size of the computation which is implied by multivariate resultants, elimination theory was forgotten during the middle of the 20th century until it was renewed by singularity theory and computational algebraic geometry.
20th century.
B. L. van der Waerden, Oscar Zariski and André Weil developed a foundation for algebraic geometry based on contemporary commutative algebra, including valuation theory and the theory of ideals. One of the goals was to give a rigorous framework for proving the results of Italian school of algebraic geometry. In particular, this school used systematically the notion of generic point without any precise definition, which was first given by these authors during the 1930s.
In the 1950s and 1960s Jean-Pierre Serre and Alexander Grothendieck recast the foundations making use of sheaf theory. Later, from about 1960, and largely lead by Grothendieck, the idea of schemes was worked out, in conjunction with a very refined apparatus of homological techniques. After a decade of rapid development the field stabilized in the 1970s, and new applications were made, both to number theory and to more classical geometric questions on algebraic varieties, singularities and moduli.
An important class of varieties, not easily understood directly from their defining equations, are the abelian varieties, which are the projective varieties whose points form an abelian group. The prototypical examples are the elliptic curves, which have a rich theory. They were instrumental in the proof of Fermat's last theorem and are also used in elliptic curve cryptography.
In parallel with the abstract trend of the algebraic geometry, which is concerned with general statements about varieties, methods for effective computation with concretely-given varieties have also been developed, which lead to the new area of computational algebraic geometry. One of the founding methods of this area is the theory of Gröbner bases, introduced by Bruno Buchberger in 1965. Another founding method, more specially devoted to real algebraic geometry, is the cylindrical algebraic decomposition, introduced by George E. Collins in 1973.
Analytic geometry.
An analytic variety is defined locally as the set of common solutions of several equations involving analytic functions. It is analogous to the included concept of real or complex algebraic variety. Any complex manifold is an analytic variety. Since analytic varieties may have singular points, not all analytic varieties are manifolds.
Modern analytic geometry is essentially equivalent to real and complex algebraic geometry, as has been shown by Jean-Pierre Serre in his paper "GAGA", the name of which is French for "Algebraic geometry and analytic geometry". Nevertheless, the two fields remain distinct, as the methods of proof are quite different and algebraic geometry includes also geometry in finite characteristic.
Applications.
Algebraic geometry now finds applications in statistics, control theory, robotics, error-correcting codes, phylogenetics and geometric modelling. There are also connections to string theory, game theory, graph matchings, solitons and integer programming.

</doc>
<doc id="1998" url="http://en.wikipedia.org/wiki?curid=1998" title="Austin, Texas">
Austin, Texas

Austin (  ) ( or ) is the capital of the US state of Texas and the seat of Travis County. Located in Central Texas, Austin is the 11th-most populous city in the United States and the fourth-most populous city in Texas. It was the third-fastest-growing large city in the nation from 2000 to 2006. Austin is also the second largest state capital in the United States, after Phoenix, Arizona. Austin had a July 1, 2014 population of 912,791 (U.S. Census Bureau estimate). The city is the cultural and economic center of the Austin–Round Rock metropolitan area, which had an estimated population of 1,883,051 as of July 1, 2013.
In the 1830s, pioneers began to settle the area in central Austin along the Colorado River. After Republic of Texas Vice President Mirabeau B. Lamar visited the area during a buffalo-hunting expedition between 1837 and 1838, he proposed that the republic's capital then located in Houston, be relocated to the area situated on the north bank of the Colorado River near the present-day Congress Avenue Bridge. In 1839, the site was officially chosen as the republic's new capital (the republic's seventh and final location) and was incorporated under the name Waterloo. Shortly thereafter, the name was changed to Austin in honor of Stephen F. Austin, the "Father of Texas" and the republic's first secretary of state.
The city grew throughout the 19th century and became a center for government and education with the construction of the Texas State Capitol and the University of Texas at Austin. After a lull in growth from the Great Depression, Austin resumed its development into a major city and, by the 1980s, it emerged as a center for technology and business. A number of Fortune 500 companies have headquarters or regional offices in Austin including Advanced Micro Devices, Apple Inc., Cisco, eBay, Google, IBM, Intel, Texas Instruments, 3M, Oracle Corporation and Whole Foods Market. Dell's worldwide headquarters is located in nearby Round Rock, a suburb of Austin.
Residents of Austin are known as Austinites. They include a diverse mix of government employees (e.g., university faculty and staff, law enforcement, political staffers); foreign and domestic college students; musicians; high-tech workers; blue-collar workers and businesspeople. The city is home to development centers for many technology corporations; it adopted the "Silicon Hills" nickname in the 1990s. However, the current official slogan promotes Austin as "The Live Music Capital of the World", a reference to the many musicians and live music venues within the area, and the long-running PBS TV concert series "Austin City Limits". In recent years, some Austinites have also adopted the unofficial slogan "Keep Austin Weird". This interpretation of the classic, "Texas-style" sense of independence refers to: a desire to protect small, unique, local businesses from being overrun by large corporations. However, many residents have rejected this slogan, inspiring Austinite and award-winning actor and director Ken Webster to offer "Keep Austin Cool" as a compromise. In the late 1800s, Austin also became known as the City of the "Violet Crown" for the wintertime violet glow of color across the hills just after sunset. Even today, many Austin businesses use the term "violet crown" in their name. Austin is known as a "clean-air city" for the city's stringent no-smoking ordinances that apply to all public places and buildings, including restaurants and bars. The FBI ranked Austin as the second safest major city in the U.S. for the year 2012.
History.
Austin, Travis County and Williamson County have been the site of human habitation since at least 9200 BC. The earliest known inhabitants of the area lived during the late Pleistocene (Ice Age) and are linked to the Clovis culture around 9200 BC (11,200 years ago), based on evidence found throughout the area and documented at the much-studied Gault Site, midway between Georgetown and Fort Hood.
When settlers first arrived from Europe, the area was inhabited by the Tonkawa tribe, and the Comanches and Lipan Apaches were known to travel through the area as well. Spanish colonists, including the Espinosa-Olivares-Aguirre expedition, traveled through the area for centuries, though few permanent settlements were created for some time. In 1730, three missions from East Texas were combined and reestablished as one mission on the south side of the Colorado River, in what is now Zilker Park, in Austin. The mission was in this area for only about seven months, and then was moved to San Antonio de Béxar and split into three missions. In the mid-18th century, the San Xavier missions were located along the Colorado River, in what is now western Milam County, to facilitate exploration.
Early in the 19th century, Spanish forts were established in what are now Bastrop and San Marcos. Following the independence of Mexico, new settlements were established in Central Texas, but growth in the region was stagnant because of conflicts with the regional Native Americans.
In 1835–1836, Texans fought and won independence from Mexico. Texas thus became its own independent country with its own president, congress, and monetary system. In 1839, the Texas Congress formed a commission to seek a site for a new capital to be named for Stephen F. Austin. Mirabeau B. Lamar, second president of the newly formed Republic of Texas, advised the commissioners to investigate the area named Waterloo, noting the area's hills, waterways, and pleasant surroundings. Waterloo was selected and the name "Austin" was chosen as the town's new name. The location was seen as a convenient crossroads for trade routes between Santa Fe and Galveston Bay, as well as routes between northern Mexico and the Red River. Austin is also the site where the southern leg of the Chisholm Trail leads to the Colorado River.
Edwin Waller was picked by Lamar to survey the village and draft a plan laying out the new capital. The original site was narrowed to 640 acres that fronted the Colorado River between two creeks, Shoal Creek and Waller Creek, which was later named in his honor. The 14-block grid plan was bisected by a broad north-south thoroughfare, Congress Avenue, running up from the river to Capital Square, where the new Texas State Capitol was to be constructed. A temporary one-story capitol was erected on the corner of Colorado and 8th Streets. On August 1, 1839, the first auction of 217 out of 306 lots total was held. The grid plan Waller designed and surveyed now forms the basis of downtown Austin.
In 1840, a series of conflicts between the Texas Rangers and the Comanches, known as the Council House Fight and the Battle of Plum Creek, finally pushed the Comanches westward, mostly ending conflicts in Central Texas. Settlement in the area began to expand quickly. Travis County was established in 1840, and the surrounding counties were mostly established within the next two decades.
Initially, the new capital thrived. But Lamar's political enemy, Sam Houston, used two Mexican army incursions to San Antonio as an excuse to move the government. Sam Houston fought bitterly against Lamar's decision to establish the capital in such a remote wilderness. The men and women who traveled mainly from Houston to conduct government business were intensely disappointed as well. By 1840, the population had risen to 856, of whom nearly half fled from Austin when Congress recessed. The resident Black population listed in January of this same year was 176. The fear of Austin's proximity to the Indians and Mexico, which still considered Texas a part of their land, created an immense motive for Sam Houston, the first and third President of the Republic of Texas, to relocate the capital once again in 1841. Upon threats of Mexican troops in Texas, Houston raided the Land Office to transfer all official documents to Houston for safe keeping in what was later known as the Archive War, but the people of Austin would not allow this unaccompanied decision to be executed. The documents stayed, but the capital would temporarily move from Austin to Houston to Washington-on-the-Brazos. Without the governmental body, Austin's population declined to a low of only a few hundred people throughout the early 1840s. The voting by the fourth President of the Republic, Anson Jones, and Congress, who reconvened in Austin in 1845, settled the issue to keep Austin the seat of government as well as annex the Republic of Texas into the United States.
In 1860, 38% of Travis County residents were slaves. In 1861, with the outbreak of the American Civil War, voters in Austin and other Central Texas communities voted against secession. However, as the war progressed and fears of attack by Union forces increased, Austin contributed hundreds of men to the Confederate forces. The African American population of Austin swelled dramatically after the enforcement of the Emancipation Proclamation in Texas by Union General Gordon Granger at Galveston in an event commemorated as Juneteenth. Black communities such as Wheatville, Pleasant Hill, and Clarksville were established with Clarksville being the oldest surviving freedomtown ‒ the original post-Civil War settlements founded by former African-American slaves ‒ west of the Mississippi River. In 1870, blacks made up 36.5% of Austin's population. The postwar period saw dramatic population and economic growth. The opening of the Houston and Texas Central Railway (H&TC) in 1871 turned Austin into the major trading center for the region with the ability to transport both cotton and cattle. The Missouri, Kansas, and Texas (MKT) line followed close behind. Austin was also the terminus of the southernmost leg of the Chisholm Trail and "drovers" pushed cattle north to the railroad. Cotton was one of the few crops produced locally for export and a cotton gin engine was located downtown near the trains for "ginning" cotton of its seeds and turning the product into bales for shipment. However, as other new railroads were built through the region in the 1870s, Austin began to lose its primacy in trade to the surrounding communities. In addition, the areas east of Austin took over cattle and cotton production from Austin, especially in towns like Hutto and Taylor that sit over the blackland prairie, with its deep, rich soils for producing cotton and hay.
In September 1881, Austin public schools held their first classes. The same year, Tillotson Collegiate and Normal Institute (now part of Huston-Tillotson University) opened its doors. The University of Texas at Austin held its first classes in 1883, although classes had been held in the original wooden state Capitol for four years before.
During the 1880s, Austin gained new prominence as the state capitol building was completed in 1888 and claimed as the seventh largest building in the world. In the late 19th century, Austin expanded its city limits to more than three times its former area, and the first granite dam was built on the Colorado River to power a new street car line and the new "moon towers". Unfortunately, the first dam washed away in a flood on April 7, 1900.
In the 1920s and 1930s, Austin launched a series of civic development and beautification projects that created much of the city's infrastructure and many of its parks. In addition, the state legislature established the Lower Colorado River Authority (LCRA) that, along with the city of Austin, created the system of dams along the Colorado River to form the Highland Lakes. These projects were enabled in large part because the Public Works Administration provided Austin with greater funding for municipal construction projects than other Texas cities.
During the early twentieth century, a three-way system of social segregation emerged in Austin, with Anglos, African Americans and Mexicans being separated by custom or law in most aspects of life, including housing, health care, and education. Many of the municipal improvement programs initiated during this period—such as the construction of new roads, schools, and hospitals—were deliberately designed to institutionalize this system of segregation. Racial segregation actually increased in Austin during the first half of the twentieth century, with African Americans and Mexicans experiencing high levels of discrimination and social marginalization.
In 1940, the destroyed granite dam on the Colorado River was finally replaced by a hollow concrete dam that formed Lake McDonald (now called Lake Austin) and which has withstood all floods since. In addition, the much larger Mansfield Dam was built by the LCRA upstream of Austin to form the flood-control lake, Lake Travis. In the early 20th century, the Texas Oil Boom took hold, creating tremendous economic opportunities in Southeast Texas and North Texas. The growth generated by this boom largely passed by Austin at first, with the city slipping from fourth largest to 10th largest in Texas between 1880 and 1920.
After the mid-20th century, Austin became established as one of Texas' major metropolitan centers. In 1970, the United States Census Bureau reported Austin's population as 14.5% Hispanic, 11.9% black, and 73.4% non-Hispanic white. In the late 20th century, Austin emerged as an important high tech center for semiconductors and software. The University of Texas at Austin emerged as a major university.
The 1970s saw Austin's emergence in the national music scene, with local artists such as Willie Nelson, Asleep at the Wheel, and Stevie Ray Vaughan and iconic music venues such as the Armadillo World Headquarters. Over time, the long-running television program "Austin City Limits", its namesake Austin City Limits Festival, and the South by Southwest music festival solidified the city's place in the music industry.
Geography.
The most southerly of the capitals of the contiguous forty-eight states, Austin is located in Central Texas, along the Balcones Escarpment and Interstate 35, 150 miles northwest of Houston. It is also 160 miles south of Dallas and 75 miles north of San Antonio. Its elevation varies from 425 ft to approximately 1000 ft above sea level. In 2010, the city occupied a total area of 271.8 sqmi. Approximately 6.9 sqmi of this area is water.
Austin is situated on the Colorado River, with three man-made (artificial) lakes within the city limits: Lady Bird Lake (formerly known as Town Lake), Lake Austin (both created by dams along the Colorado River), and Lake Walter E. Long that is partly used for cooling water for the Decker Power Plant. Mansfield Dam and the foot of Lake Travis are located within the city's limits. Lady Bird Lake, Lake Austin, and Lake Travis are each on the Colorado River. As a result of its straddling the Balcones Fault, much of the eastern part of the city is flat, with heavy clay and loam soils, whereas, the western part and western suburbs consist of rolling hills on the edge of the Texas Hill Country. Because the hills to the west are primarily limestone rock with a thin covering of topsoil, portions of the city are frequently subjected to flash floods from the runoff caused by thunderstorms. To help control this runoff and to generate hydroelectric power, the Lower Colorado River Authority operates a series of dams that form the Texas Highland Lakes. The lakes also provide venues for boating, swimming, and other forms of recreation within several parks on the lake shores.
Austin is located at the intersection of four major ecological regions, and is consequently a temperate-to-hot green oasis with a highly variable climate having some characteristics of the desert, the tropics, and a wetter climate. The area is very diverse ecologically and biologically, and is home to a variety of animals and plants. Notably, the area is home to many types of wildflowers that blossom throughout the year but especially in the spring, including the popular bluebonnets, some planted in an effort by "Lady Bird" Johnson, wife of former President Lyndon Johnson.
A popular point of prominence in Austin is Mount Bonnell. At about 780 ft above sea level, it is a natural limestone formation overlooking Lake Austin on the Colorado River, with an observation deck about 200 ft below its summit.
The soils of Austin range from shallow, gravelly clay loams over limestone in the western outskirts to deep, fine sandy loams, silty clay loams, silty clays or clays in the city's eastern part. Some of the clays have pronounced shrink-swell properties and are difficult to work under most moisture conditions. Many of Austin's soils, especially the clay-rich types, are slightly to moderately alkaline and have free calcium carbonate.
Cityscape.
Buildings that make up most of Austin's skyline are modest in height and somewhat spread out. The latter characteristic is partly due to a restriction that preserves the view of the Texas State Capitol building from various locations around Austin (known as the Capitol View Corridor). However, many new high-rise towers have been constructed and the downtown area is looking more modern and dense. The city's tallest building, The Austonian, was topped out on September 17, 2009. Austin is currently undergoing a skyscraper boom, which includes recent construction on the now complete 360 Condominiums at 563 ft, Spring (condominiums), the Austonian at 683 ft, and several others that are mainly for residential use.
At night, parts of Austin are lit by "artificial moonlight" from Moonlight Towers built to illuminate the central part of the city. The 165 ft moonlight towers were built in the late 19th century and are now recognized as historic landmarks. Only 15 of the 31 original innovative towers remain standing in Austin, and none remain in any of the other cities where they were installed. The towers are featured in the 1993 film "Dazed and Confused".
Downtown.
The central business district of Austin is home to some of the tallest condo towers in the state, with The Austonian topping out at 56 floors (the tallest residential building in the U.S. west of the Mississippi River) and 360 at 44 floors. Former Mayor Will Wynn set out a goal for having up to 25,000 people living Downtown by 2015, and the city provided incentives for building residential units in the urban core. Because of this, the city has been driven to increase density in Austin's urban core. The skyline has drastically changed in recent years, and the residential real estate market has remained relatively strong.
Downtown growth has been aided by the presence of a popular live music and nightlife scene, museums, restaurants, and Lady Bird Lake, considered one of the city's best recreational spots. The 2nd Street District consists of several new residential projects, restaurants, upscale boutiques and other entertainment venues, as well as Austin's City Hall. Across 2nd Street from Austin's City Hall is the new ACL Live @ the Moody Theatre where the long-running PBS program "Austin City Limits", is filmed. It is located at the base of the new 478 ft W Hotel. The annual South by Southwest (SXSW) Music, Film and Interactive Festival is located in downtown Austin and includes one of the world's largest music festivals; with more than 3,000 acts from every continent except Antarctica, playing in more than 100 venues, over five days, in March.
Climate.
Under the Koppen climate classification, Austin has a humid subtropical climate. The city is characterized by hot summers and mild winter days and usually cool to cold winter nights. Austin is usually at least partially sunny, receiving nearly 2650 hours, or 60.3% of the possible total, of bright sunshine per year. Summer dewpoint averages at around 68 °F.
Austin summers are usually hot, with average July and August highs in the high-90s °F (34–36 °C). Highs reach 90 F on 116 days per year, and 100 °F on 18. The highest recorded temperature was 112 °F occurring on September 5, 2000 and August 28, 2011.
Winters in Austin are mild and relatively dry. For the entire year, Austin averages 88 days below 45 °F and 19 days when the minimum temperature falls at or below freezing. The lowest recorded temperature was -2 °F on January 31, 1949. About every two years or so, Austin experiences an ice storm that freezes roads over and affects much of the city for 24 to 48 hours. Snowfall is rare in Austin; a 3 inch snowstorm brought the city to a near standstill in 1985.
Monthly averages for Austin's weather data are shown in a graphical format to the right, and in a more detailed tabular format below.
2011 drought.
From October 2010 through September 2011, both major reporting stations in Austin, Camp Mabry and Bergstrom Int'l, had the least rainfall of a water year on record, receiving less than a third of normal precipitation. This was a result of La Niña conditions in the eastern Pacific Ocean where water was significantly cooler than normal. David Brown, a regional official with the National Oceanic and Atmospheric Administration, has explained that "these kinds of droughts will have effects that are even more extreme in the future, given a warming and drying regional climate."
Demographics.
According to the 2010 United States Census, the racial composition of Austin is:
At the 2000 United States Census, there were people, households, and families residing in the city (roughly comparable in size to San Francisco, Leeds, UK; and Ottawa, Canada). The population density was 2610.4 PD/sqmi. There were housing units at an average density of 1100.7 /sqmi. There were households out of which 26.8% had children under the age of 18 living with them, 38.1% were married couples living together, 10.8% had a female householder with no husband present, and 46.7% were non-families. 32.8% of all households were made up of individuals and 4.6% had someone living alone who was 65 years of age or older. The average household size was 2.40 and the average family size was 3.14.
In the city the population was spread out with 22.5% under the age of 18, 16.6% from 18 to 24, 37.1% from 25 to 44, 17.1% from 45 to 64, and 6.7% who were 65 years of age or older. The median age was 30 years. For every 100 females there were 105.8 males.
The median income for a household in the city was , and the median income for a family was $. Males had a median income of $ vs. $ for females. The per capita income for the city was $. About 9.1% of families and 14.4% of the population were below the poverty line, including 16.5% of those under age 18 and 8.7% of those age 65 or over. The median house price was $ in 2009, and it has increased every year since 2004.
A 2014 University of Texas study stated that Austin was the only U.S. city with a fast growth rate that was losing African-Americans.
Economy.
The Greater Austin metropolitan statistical area had a Gross Domestic Product of $86 billion in 2010. Austin is considered to be a major center for high tech. Thousands of graduates each year from the engineering and computer science programs at the University of Texas at Austin provide a steady source of employees that help to fuel Austin's technology and defense industry sectors. The region's rapid growth has led "Forbes" to rank the Austin metropolitan area number one among all big cities for jobs for 2012 in their annual survey and WSJ Marketwatch to rank the area number one for growing businesses. By 2013, Austin ranked No. 14 on "Forbes"' list of the Best Places for Business and Careers (directly below Dallas, No. 13 on the list). As a result of the high concentration of high-tech companies in the region, Austin was strongly affected by the dot-com boom in the late 1990s and subsequent bust. Austin's largest employers include the Austin Independent School District, the City of Austin, Dell, the U.S. Federal Government, Freescale Semiconductor (spun off from Motorola in 2004), IBM, St. David's Healthcare Partnership, Seton Family of Hospitals, the State of Texas, the Texas State University, and the University of Texas at Austin.
Other high-tech companies with operations in Austin include 3M, Apple, Hewlett-Packard, Google, Qualcomm, Inc., AMD, Applied Materials, Cirrus Logic, ARM Holdings, Cisco Systems, Electronic Arts, Flextronics, Facebook, eBay/PayPal, Bioware, Blizzard Entertainment, Hoover's, Intel Corporation, National Instruments, Nvidia, Rackspace, RetailMeNot, Rooster Teeth, Spansion, Troux Technologies, Buffalo Technology, Silicon Laboratories, Xerox, Oracle, Hostgator, Samsung Group, HomeAway, and United Devices. In 2010, Facebook accepted a grant to build a downtown office that could bring as many as 200 jobs to the city. The proliferation of technology companies has led to the region's nickname, "the Silicon Hills", and spurred development that greatly expanded the city.
Austin is also emerging as a hub for pharmaceutical and biotechnology companies; the city is home to about 85 of them. The city was ranked by the Milken Institute as the No.12 biotech and life science center in the United States. Companies such as Hospira, Pharmaceutical Product Development, and ArthroCare Corporation are located there.
Whole Foods Market (often called just "Whole Foods") is an upscale, international grocery store chain specializing in fresh and packaged food products—many having an organic-/local-/"natural"-theme. It was founded and is headquartered in Austin.
Other companies based in Austin include Freescale Semiconductor, Temple-Inland, Sweet Leaf Tea Company, Keller Williams Realty, National Western Life, GSD&M, Dimensional Fund Advisors, Golfsmith, Forestar Group, and EZCorp.
In addition to national and global corporations, Austin features a strong network of independent, unique, locally owned firms and organizations.
Arts and culture.
"Keep Austin Weird" has been a local motto for years, featured on bumper stickers and T-shirts. This motto has not only been used in promoting Austin's eccentricity and diversity, but is also meant to bolster support of local independent businesses. According to the 2010 book, "Weird City", the phrase was begun by a local Austin Community College librarian, Red Wassenich, and his wife, Karen Pavelka, who were concerned about Austin's "rapid descent into commercialism and overdevelopment." The slogan has been interpreted many ways since its inception, but remains an important symbol for many Austinites who wish to voice concerns over rapid growth and irresponsible development. Austin has a long history of vocal citizen resistance to development projects perceived to degrade the environment, or to threaten the natural and cultural landscapes.
According to the Nielsen Company, adults in Austin read and contribute to blogs more than those in any other U.S. metropolitan area. Austin residents have the highest internet usage in all of Texas. Austin was selected as the No. 2 Best Big City in "Best Places to Live" by "Money" magazine in 2006, and No. 3 in 2009, and also the "Greenest City in America" by MSN. According to "Travel & Leisure" magazine, Austin ranks No. 1 on the list of cities with the best people, referring to the personalities and attributes of the citizens. In 2012, the city was listed among the 10 best places to retire in the U.S. by CBS Money Watch.
SoCo is a shopping district stretching down South Congress Avenue from Downtown. This area is home to coffee shops, eccentric stores, restaurants, food trucks, trailers and festivals. It prides itself on "Keeping Austin Weird", especially with development in the surrounding area(s).
Old Austin.
"Old Austin" is an adage often used by the native citizens in Austin, Texas when being nostalgic to refer to the olden days of the capital city of Texas. Although Austin is also known internationally as the live music capital of the world and its catch phrase/slogan Keep Austin Weird can be heard echoed in places as far as Buffalo, NY and Santa Monica, CA - the term "Old Austin" refers to a time when the city was smaller and better known for its lack of traffic, hipsters, and urban sprawl. It is often used by the folks who have lived in the city for a long time as they express displeasure to the rapid changing culture
The growth and popularity of Austin can be seen by the expansive development taking place in its downtown landscape. Forbes ranked Austin as the second most fastest-growing city in 2015. The Austin American Statesman (local Austin newspaper) published a blog column title ", showing that a conversation takes places regarding whether the growth of austin impacts the Old Austin cultural significance of the city.
A great overview of the Old Austin lifestyle is captured in the song and music video " and also by the interested in capturing the nostalgia of Old Austin.
Annual cultural events.
The O. Henry House Museum hosts the annual O. Henry Pun-Off, a pun contest where the successful contestants exhibit wit akin to that of the author William Sydney Porter.
Other annual events include Eeyore's Birthday Party, Spamarama, the Austin Reggae Festival, Kite Festival, Art City Austin in April, East Austin Studio Tour in November, and Carnaval Brasileiro in February. Sixth Street features annual festivals such as the Pecan Street Festival and Halloween night. The three-day Austin City Limits Music Festival has been held in Zilker Park every year since 2002. Every year around the end of March and the beginning of April, Austin is home to "Texas Relay Weekend."
Austin's Zilker Park Tree is a Christmas display made of lights strung from the top of a Moonlight tower in Zilker Park. The Zilker Tree is lit in December along with the "Trail of Lights," an Austin Christmas tradition. In 2010 and 2011, the Trail of Lights was canceled due to budget shortfalls, but the trail was turned back on for the 2012 holiday season.
Music.
As Austin's official slogan is "The Live Music Capital of the World", the city has a vibrant live music scene with more music venues per capita than any other U.S. city. Austin's music revolves around the many nightclubs on 6th Street and an annual film/music/interactive festival known as South by Southwest (SXSW). The concentration of restaurants, bars, and music venues in the city's downtown core is a major contributor to Austin's live music scene, as the zip code encompassing the downtown entertainment district hosts the most bar or alcohol-serving establishments in the U.S.
The longest-running concert music program on American television, "Austin City Limits", is recorded at ACL Live at The Moody Theater. "Austin City Limits" and C3 Presents produce the Austin City Limits Music Festival, an annual music and art festival held at Zilker Park in Austin. Other music events include the Urban Music Festival, Fun Fun Fun Fest, Chaos In Tejas and Old Settler's Music Festival. Austin Lyric Opera performs multiple operas each year (including the 2007 opening of Philip Glass's "Waiting for the Barbarians", written by University of Texas at Austin alumnus J. M. Coetzee). The Austin Symphony Orchestra performs a range of classical, pop and family performances and is led by Music Director and Conductor Peter Bay.
Film.
Austin hosts several film festivals including SXSW Film and Austin Film Festival, which draws films of many different types from all over the world. In 2004 the city was first in "MovieMaker Magazine's" annual top ten cities to live and make movies.
Austin has been the location for a number of motion pictures, partly due to the influence of The University of Texas at Austin Department of Radio-Television-Film. Films produced in Austin include "The Texas Chain Saw Massacre" (1974), "Songwriter" (1984), "Man of the House", "Secondhand Lions", "Waking Life", "Spy Kids","The Faculty", "Dazed and Confused", "Wild Texas Wind", "Office Space", "The Life of David Gale", "Miss Congeniality", "Doubting Thomas", "Slacker", "Idiocracy", "The New Guy", "Hope Floats", "The Alamo", "Blank Check", "The Wendall Baker Story", "School of Rock", "A Slipping-Down Life", "A Scanner Darkly", "Saturday Morning Massacre", and most recently, the Coen brothers' "True Grit", "Grindhouse", "Machete", "How to Eat Fried Worms" and "Bandslam". In order to draw future film projects to the area, the Austin Film Society has converted several airplane hangars from the former Mueller Airport into filmmaking center Austin Studios. Projects that have used facilities at Austin Studios include music videos by The Flaming Lips and feature films such as "25th Hour" and "Sin City". Austin also hosted the MTV series, "" in 2005. The film review websites Spill.com and Ain't It Cool News are based in Austin. Rooster Teeth Productions, creator of popular web series such as "Red vs. Blue", and "RWBY" is also located in Austin.
Theater.
Austin has a strong theater culture, with dozens of itinerant and resident companies producing a variety of work. The city also has live performance theater venues such as the Zachary Scott Theatre Center, Vortex Repertory Company, Salvage Vanguard Theater, Rude Mechanicals' the Off Center, Austin Playhouse, Scottish Rite Children's Theater, Hyde Park Theatre, the Blue Theater, The Hideout Theatre, and Esther's Follies. The Victory Grill was a renowned venue on the Chitlin' circuit. Public art and performances in the parks and on bridges are popular. Austin hosts the Fuse Box Festival each April featuring international, leading-edge theater artists.
The Paramount Theatre, opened in downtown Austin in 1915, contributes to Austin's theater and film culture, showing classic films throughout the summer and hosting regional premieres for films such as "Miss Congeniality". The Zilker Park Summer Musical is a long-running outdoor musical.
The Long Center for the Performing Arts is a 2,300-seat theater built partly with materials reused from the old Lester E. Palmer Auditorium.
Ballet Austin is the fourth largest ballet academy in the country. Each year Ballet Austin's 20-member professional company performs ballets from a wide variety of choreographers, including their international award winning artistic director, Stephen Mills. The city is also home to the Ballet East Dance Company, a modern dance ensemble, and the Tapestry Dance Company which performs a variety of dance genres.
The Austin improvisational theatre scene has several theaters: ColdTowne Theater, The Hideout Theater, The New Movement Theater, and The Institution Theater. Austin also hosts the Out of Bounds Improv Festival, which draws comedic artists in all disciplines to Austin.
Museums and other points of interest.
Museums in Austin include the Texas Memorial Museum, the Blanton Museum of Art (reopened in 2006), the Bob Bullock Texas State History Museum across the street (which opened in 2000), the Austin Museum of Art (AMOA), the Elisabet Ney Museum and the galleries at the Harry Ransom Center. The Texas State Capitol itself is also a major tourist attraction. The Driskill Hotel built in 1886, once owned by George W. Littlefield, and located at 6th and Brazos streets, was finished just before the construction of the Capitol building. Sixth Street is a musical hub for the city. The Enchanted Forest, a multi-acre outdoor music, art, and performance art space in South Austin hosts events such as fire-dancing and circus-like-acts. Austin is also home to the Lyndon Baines Johnson Library and Museum, which houses documents and artifacts related to the Johnson administration, including LBJ's limousine and a re-creation of the Oval Office.
Locally produced art is featured at the South Austin Museum of Popular Culture. The Mexic-Arte Museum is a Latin American art museum founded in 1983. Austin is also home to the O. Henry House Museum, which served as the residence of O. Henry from 1893 to 1895. Farmers' markets are popular attractions, providing a variety of locally grown and often organic foods.
Austin also has many odd statues and landmarks, such as the Stevie Ray Vaughan statue, the Willie Nelson statue, the Mangia dinosaur, the Loca Maria lady at Taco Xpress, the Hyde Park Gym's giant flexed arm, and Daniel Johnston's "Hi, How are You?" Jeremiah the Innocent frog mural.
The Ann W. Richards Congress Avenue Bridge houses the world's largest urban population of Mexican Free-tailed Bats. Starting in March, up to 1.5 million bats take up residence inside the bridge's expansion and contraction zones as well as in long horizontal grooves running the length of the bridge's underside, an environment ideally suited for raising their young. Every evening around sunset, the bats emerge in search of insects, an exit visible on weather radar. Watching the bat emergence is an event that is popular with locals and tourists, with more than 100,000 viewers per year. The bats migrate to Mexico each winter.
The Austin Zoo, located in unincorporated western Travis County, is a rescue zoo that provides sanctuary to displaced animals from a variety of situations, including those involving neglect.
Sports.
Many Austinites support the athletic programs of the University of Texas at Austin known as the Texas Longhorns. During the 2005–06 academic term, Longhorns football team was named the NCAA Division I FBS National Football Champion, and Longhorns baseball team won the College World Series. The Texas Longhorns play home games in the state's second-largest sports stadium, Darrell K Royal-Texas Memorial Stadium, seating over 101,000 fans. Baseball games are played at UFCU Disch–Falk Field.
Austin is the most populous city in the United States without a club in a major-league professional sports team. Minor-league professional sports came to Austin in 1996, when the Austin Ice Bats began playing at the Travis County Expo Center; they were later replaced by the AHL Texas Stars. Austin now hosts a number of other professional teams, including the Austin Spurs of the NBA Development League, the Austin Aztex of the United Soccer League, the Austin Outlaws in WFA football, and the Austin Aces in WTT tennis.
Natural features like the bicycle-friendly Texas Hill Country and generally mild climate make Austin the home of several endurance and multi-sport races and communities. The Capitol 10,000 is the largest 10 K race in Texas, and approximately fifth largest in the United States. The Austin Marathon has been run in the city every year since 1992.
The Austin-founded American Swimming Association hosts several swim races around town. Austin is also the hometown of several cycling groups and the former seven-time Tour de France champion cyclist Lance Armstrong. Combining these three disciplines is a growing crop of triathlons, including the Capital of Texas Triathlon held every Memorial Day on and around Lady Bird Lake, Auditorium Shores, and Downtown Austin.
In June 2010 it was announced that the Austin area would host the Formula One, United States Grand Prix, from 2012 until 2021. The State pledged $25 million in public funds annually for 10 years to pay the sanctioning fees for the race. A Formula One circuit was built at an estimated cost of $250 to $300 million, and is located just east of the Austin Bergstrom International Airport. Circuit of the Americas also plays host to MotoGP World Championships from 2013.
The summer of 2014 marked the inaugural season for World TeamTennis team Austin Aces, formerly Orange County Breakers of the southern California region. Austin Aces play their matches at the Cedar Park Center northwest of Austin, and feature former professionals Andy Roddick and Marion Bartoli, as well as current WTA tour player Vera Zvonareva.
Parks and recreation.
The Austin Parks and Recreation Department received the Excellence in Aquatics award in 1999 and the Gold Medal Awards in 2004 from the National Recreation and Park Association. Home to more than 50 public swimming pools, Austin has parks and pools throughout the city. There are several well-known swimming locations. These include Deep Eddy Pool, Texas' oldest man-made swimming pool, and Barton Springs Pool, the nation's largest natural swimming pool in an urban area. Barton Springs Pool is spring-fed while Deep Eddy is well-fed. Both range in temperature from about 68.0 °F during the winter to about 71.6 °F during the summer. Hippie Hollow Park, a county park situated along Lake Travis, is the only officially sanctioned clothing-optional public park in Texas. Activities include rockclimbing, kayaking, swimming, mountain biking, exploring, and hiking along the greenbelt, a long-spanning area that runs through the city. Zilker Park, a large green area close to downtown, forms part of the greenbelt along the Colorado River. Hamilton Pool is a pool and wildlife park located about 30 minutes from the city.
Government and law.
City government.
Austin is administered by an 11-member city council (10 council members elected by geographic district plus a mayor elected at large). The council is accompanied by a hired city manager under the manager-council system of municipal governance. Council and mayoral elections are non-partisan, with a runoff in case there is no majority winner. A referendum approved by voters on November 6, 2012 changed the council composition from six council members plus a mayor elected at large to the current "10+1" district system. November 2014 marked the first election under the new system.
Austin formerly operated its city hall at 128 West 8th Street. Antoine Predock and Cotera Kolar Negrete & Reed Architects designed a new city hall building, which was intended to reflect what "The Dallas Morning News" referred to as a "crazy-quilt vitality, that embraces everything from country music to environmental protests and high-tech swagger." The new city hall, built from recycled materials, has solar panels in its garage. The city hall, at 301 West Second Street, opened in November 2004. The current[ [update]] mayor of Austin is Steve Adler.
Law enforcement in Austin is provided by the Austin Police Department, except for state government buildings, which are patrolled by the Texas Department of Public Safety. The University of Texas Police operate from the University of Texas.
Fire protection within the city limits is provided by the Austin Fire Department, while the surrounding county is divided into twelve geographical areas known as Emergency Services Districts, which are covered by separate regional fire departments. Emergency Medical Services are provided for the whole county by "Austin-Travis County Emergency Medical Services".
State and federal representation.
The Texas Department of Transportation operates the Austin District Office in Austin.
The Texas Department of Criminal Justice (TDCJ) operates the Austin I and Austin II district parole offices in Austin.
The United States Postal Service operates several post offices in Austin.
Politics.
Austin is known as an enclave of liberal politics in a generally conservative state—so much so, that the city is sometimes sarcastically called the "People's Republic of Austin" by residents of other parts of Texas, and conservatives in the Texas Legislature.
Since redistricting following the 2010 United States Census, Austin has been divided between six congressional districts at the federal level: Texas's 35th, Texas's 25th, Texas's 10th, Texas's 21st, Texas's 17th, and Texas's 31st. Texas's 35th congressional district is represented by Democrat Lloyd Doggett. The other five districts are represented by Republicans, of whom only one, Michael McCaul of the 10th district, lives in Austin.
As a result of the major party realignment that began in the 1970s, central Austin became a stronghold of the Democratic Party, while the suburbs tend to vote Republican. A controversial turning point in the political history of the Austin area was the 2003 Texas redistricting. Opponents characterized the resulting district layout as excessively partisan gerrymandering, and the plan was challenged in court by Democratic and minority activists; of note, the Supreme Court of the United States has never struck down a redistricting plan for being excessively partisan. The plan was subsequently upheld by a three-judge federal panel in late 2003, and on June 28, 2006, the matter was largely settled when the Supreme Court, in a 7–2 decision, upheld the entire congressional redistricting plan with the exception of a Hispanic-majority district in southwest Texas. This affected Austin's districting, as U.S. Rep. Lloyd Doggett's district (U.S. Congressional District 25) was found to be insufficiently compact to compensate for the reduced minority influence in the southwest district; it was redrawn so that it took in most of southeastern Travis County and several counties to its south and east.
Overall, the city is a blend of downtown liberalism and suburban conservatism but leans to the political left as a whole. The city last went to a Republican candidate in 2000 when Texan resident George W. Bush successfully ran for President. This was helped in part by Ralph Nader of the Green Party splitting the center-left vote by winning a sizeable 10.4%, which was largely at the expense of the Democrats. In 2004, the Democrats rebounded strongly as John Kerry enjoyed a 14.0% margin over Bush, who once again won Texas.
In 2003, the city adopted a resolution against the USA PATRIOT Act that reaffirmed constitutionally guaranteed rights. Of Austin's six state legislative districts, three are strongly Democratic and three are swing districts, two of which are held by Democrats and one of which is held by a Republican. However, two of its three congressional districts (the 10th and the 21st) are presently held by Republicans, with only the 25th held by a Democrat. This is largely due to the 2003 redistricting, which left downtown Austin without an exclusive congressional seat of its own. Travis County was also the only county in Texas to reject Texas Constitutional Amendment Proposition 2 that effectively outlawed gay marriage and status equal or similar to it and did so by a wide margin (40% for, 60% against).
Two of the candidates for president in the 2004 race called Austin home. Michael Badnarik, the Libertarian Party candidate, and David Cobb of the Green Party both had lived in Austin. During the run up to the election in November, a presidential debate was held at the University of Texas at Austin student union involving the two minor party candidates. While the Commission on Presidential Debates only invites Democrats and Republicans to participate in televised debates, the debate at UT was open to all presidential candidates. Austin also hosted one of the last presidential debates between Barack Obama and Hillary Clinton during their heated race for the Democratic nomination in 2008.
In the 2012 Presidential election, Travis County, which contains the majority of Austin, voted to re-elect President Barack H. Obama (D) by a 24-point margin (60.1% to 36.2%).
Environmental movement.
The distinguishing political movement of Austin politics has been that of the environmental movement, which spawned the parallel neighborhood movement, then the more recent conservationist movement (as typified by the Hill Country Conservancy), and eventually the current ongoing debate about "sense of place" and preserving the Austin quality of life. Much of the so-called environmental movement has matured into a debate on issues related to saving and creating an Austin "sense of place." In 2012, Austin became just one of a few cities in Texas to ban the sale and use of plastic bags.
Education.
Researchers at Central Connecticut State University ranked Austin the 16th most literate city in the United States for 2008. The Austin Public Library operates the John Henry Faulk Library and various library branches. In addition, the University of Texas at Austin operates the seventh-largest academic library in the nation.
Austin was voted "America's No.1 College Town" by the Travel Channel. Over 43 percent of Austin residents age 25 and over hold a bachelor's degree, while 16 percent hold a graduate degree. In 2009, greater Austin ranked eighth among metropolitan areas in the United States for bachelor's degree attainment with nearly 39 percent of area residents over 25 holding a bachelor's degree.
Higher education.
Austin is home to the University of Texas at Austin, the flagship institution of the University of Texas System with over 38,000 undergraduate students and 12,000 graduate students. In 2015 rankings, the university was ranked 53rd among "National Universities" (17th among public universities) by "U.S. News & World Report." UT has annual research expenditures of over $595 million and has the highest-ranked business, engineering, and law programs of any university in the state of Texas.
Other institutions of higher learning in Austin include St. Edward's University, Austin Community College, Concordia University, Huston-Tillotson University, the Seminary of the Southwest, the Acton School of Business, Texas Health and Science University, Austin Graduate School of Theology, Austin Presbyterian Theological Seminary, Virginia College's Austin Campus, The Art Institute of Austin, Southern Careers Institute of Austin, Austin Conservatory and a branch of Park University.
Public primary and secondary education.
The Austin area has 29 public school districts, 17 charter schools and 69 private schools. Most of the city is served by the Austin Independent School District. This district includes notable schools such as the magnet Liberal Arts and Science Academy High School of Austin, Texas (LASA), which, by test scores, has consistently been within the top thirty high schools in the nation, as well as The Ann Richards School for Young Women Leaders. Some parts of Austin are served by other districts, including Round Rock, Pflugerville, Leander, Manor, Del Valle, Lake Travis, Dripping Springs, Hays, and Eanes ISDs. Four of the metro's major public school systems, representing 54% of area enrollment, are included in "Expansion Management" magazine's latest annual education quality ratings of nearly 2,800 school districts nationwide. Two districts—Eanes and Round Rock—are rated "gold medal", the highest of the magazine's cost-performance categories.
Private and alternative education.
Private and alternative education institutions for children in preschool-12th grade include ACE Academy, Regents School of Austin, Redeemer Lutheran School, Garza (public), Austin Discovery School (public charter), Austin Jewish Academy, Austin Peace Academy, The Austin School for the Performing and Visual Arts, The Austin Waldorf School, The Griffin School, The Khabele School, Concordia Academy, Kirby Hall School, St. Ignatius Martyr Catholic School,Holy Family Catholic School, San Juan Diego Catholic High School, Brentwood Christian School, Renaissance Academy, St. Austin Catholic School, St. Stephen's Episcopal School, St. Mary's, St. Theresa's, St. Michael's Catholic Academy, St. Gabriel's Catholic School, St. Andrew's Episcopal School, St. Francis Episcopal School, St. Paul Lutheran School, Trinity Episcopal School, Huntington-Surrey, Cleaview Sudbury School, Inside Outside School, Paragon Preparatory Middle School, Austin International School, Progress School, Bronze Doors Academy, and a number of Montessori schools.
Along with homeschooling & "unschooling" communities, Austin is home to a number of part-time learning environments designed to offer basic academics and inspired mentoring. Such current resources include the Whole Life Learning Center and AHB Community School.
Austin is also home to child developmental institutions including the Center for Autism and Related Disorders, the Central Texas Autism Center, Johnson Center for Child Health and Development and many more.
Media.
Austin's main daily newspaper is the "Austin American-Statesman". "The Austin Chronicle" is Austin's alternative weekly, while "The Daily Texan" is the student newspaper of the University of Texas at Austin. Austin's business newspaper is the weekly "Austin Business Journal". Austin also has numerous smaller special interest or sub-regional newspapers such as the "Oak Hill Gazette", "Westlake Picayune", "Hill Country News", "Round Rock Leader", "NOKOA", and "The Villager" among others. "Texas Monthly", a major regional magazine, is also headquartered in Austin. The "Texas Observer", a muckraking biweekly political magazine, has been based in Austin for over five decades. The weekly "Community Impact Newspaper" newspaper published by John Garrett, former publisher of the "Austin Business Journal" has five regional editions and is delivered to every house and business within certain zip codes and all of the news is specific to those zip codes. The most recent entrant on the Austin news scene is "The Texas Tribune", an on-line publication focused on Texas and Austin politics. The "Tribune" is "user-supported" through donations, a business model similar to public radio. The Editor is Evan Smith, former Editor of "Texas Monthly". Smith co-founded the "Texas Tribune", a nonprofit, non-partisan public media organization, with Austin venture capitalist John Thornton and veteran journalist Ross Ramsey.
Commercial radio stations include KASE-FM (country), KVET (sports), KVET-FM (country), KKMJ-FM (adult contemporary), KLBJ (talk), KLBJ-FM (classic rock), KTAE (Christian talk), KFMK (contemporary Christian), KOKE-FM (progressive country) and KPEZ (rhythmic contemporary). KUT is the leading public radio station in Texas and produces the majority of its content locally. KOOP (FM) is a volunteer-run radio station with more than 60 locally produced programs. KVRX is the student-run college radio station of the University of Texas at Austin with a focus on local and non-mainstream music and community programming. Other listener-supported stations include KAZI (urban contemporary), and KMFA (classical)
Network television stations (affiliations in parentheses) include KTBC (Fox), KVUE (ABC), KXAN (NBC), KEYE-TV (CBS), KLRU (PBS), KNVA (The CW), KBVO (My Network TV), and KAKW (Univision). KLRU produces several award winning locally produced programs such as "Austin City Limits".
Alex Jones, journalist, radio show host and filmmaker, produces his talk show The Alex Jones Show in Austin which broadcasts nationally on more than 60 AM and FM radio stations in the United States, WWCR Radio shortwave and XM Radio: Channel 166.
Transportation.
Of all the people who work in Austin, 73% drive alone, 10% carpool, 6% work from home, 5% take the bus, 2% walk, and 1% bicycle.
Highways.
Central Austin lies between two major north-south freeways: Interstate 35 to the east and the Mopac Expressway (Loop 1) to the west. U.S. Highway 183 runs from northwest to southeast, and State Highway 71 crosses the southern part of the city from east to west, completing a rough "box" around central and north-central Austin. Austin is the largest city in the United States to be served by only one Interstate Highway.
U.S. Highway 290 enters Austin from the east and merges into Interstate 35. Its highway designation continues south on I-35 and then becomes part of Highway 71, continuing to the west. Highway 290 splits from Highway 71 in southwest Austin, in an interchange known as "The Y." Highway 71 continues to Brady, Texas, and Highway 290 continues west to intersect Interstate 10 near Junction. Interstate 35 continues south through San Antonio to Laredo on the Texas-Mexico border. Interstate 35 is the highway link to the Dallas-Fort Worth metroplex in northern Texas. There are two links to Houston, Texas (Highway 290 and State Highway 71/Interstate 10). Highway 183 leads northwest of Austin toward Lampasas.
In the mid-1980s, construction was completed on Loop 360, a scenic highway that curves through the hill country from near the 71/Mopac interchange in the south to near the 183/Mopac interchange in the north. The iconic Pennybacker Bridge, also known as the "360 Bridge", crosses Lake Austin to connect the northern and southern portions of Loop 360.
Tollways.
State Highway 130 is a bypass route designed to relieve traffic congestion, starting from Interstate 35 just north of Georgetown and running along a parallel route to the east, where it bypasses Round Rock, Austin, San Marcos and New Braunfels before ending at Interstate 10 east of Seguin, where drivers could drive 30 mi west to return to Interstate 35 in San Antonio. The first segment was opened in November 2006, which was located east of Austin–Bergstrom International Airport at Austin's southeast corner on State Highway 71. Highway 130 runs concurrently with Highway 45 from Pflugerville on the north until it reaches US 183 well south of Austin, where it splits off and goes west. The entire route of State Highway 130 is now complete with last leg, which opened on November 1, 2012. The highway is noted for having the entire route with a speed limit of at least 80 mph. The 41-mile section of the toll road between Mustang Ridge and Seguin has a posted speed limit of 85 mph, the highest posted speed limit in the United States.
State Highway 45 runs east-west from just south of Highway 183 in Cedar Park to 130 inside Pflugerville (just east of Round Rock). A tolled extension of State Highway Loop 1 was also created. A new southeast leg of Highway 45 has recently been completed, running from US 183 and the south end of Segment 5 of TX-130 south of Austin due west to I-35 at the FM 1327/Creedmoor exit between the south end of Austin and Buda. The 183A Toll Road opened March 2007, providing a tolled alternative to U.S. 183 through the cities of Leander and Cedar Park. Currently under construction is a change to East US 290 from US 183 to the town of Manor. Officially, the tollway will be dubbed Tollway 290 with the Manor Expressway as a nickname.
Despite the overwhelming initial opposition to the toll road concept when it was first announced, all three toll roads have exceeded revenue projections.
Airports.
Austin's airport is Austin–Bergstrom International Airport (ABIA) ( AUS), located 5 mi southeast of the city. The airport is on the site of the former Bergstrom Air Force Base, which was closed in 1993 as part of the Base Realignment and Closure process. Previously, Robert Mueller Municipal Airport was the commercial airport of Austin. Austin Executive Airport serves the general aviation coming into the city, as well as other smaller airports outside of the city centre.
Intercity bus service.
Greyhound Lines operates the Austin Station at 916 East Koenig Lane, just east of Airport Boulevard and adjacent to Highland Mall. Turimex Internacional operates bus service from Austin to Nuevo Laredo and on to many destinations in Mexico. The Turimex station is located at 5012 East 7th Street, near Shady Lane.
Megabus offers daily service to San Antonio, Dallas/Fort Worth and Houston from a stop at Dobie Center.
Public transportation.
Capital Metropolitan Transportation Authority Capital Metro provides public transportation to the city, primarily by bus. Capital Metro is planning to change some routes to "Rapid Lines." The lines will feature 60 ft long, train-like, high-tech buses. This addition is going to be implemented to help reduce congestion. Capital Metro opened a 32 mi commuter rail system known as Capital MetroRail on March 22, 2010. The system was built on existing freight rail lines and serves downtown Austin, East Austin, North Central Austin, Northwest Austin, and Leander in its first phase. Future expansion could include a line to Manor and another to Round Rock. Capital Metro is also looking into a light rail system to connect most of Downtown, the University of Texas at Austin, and the 700 acre Mueller Airport Redevelopment. The light rail system would help connect the MetroRail line to key destinations in Central Austin. On August 7, 2014, the Austin City Council unanimously voted to place a $600 million light rail bond proposal on the November 4, 2014 ballot. Implementation of this package is contingent on matching funding from Federal transit grants. If Federal funding is available, then Austin would begin construction of a light rail line that would run from Riverside Drive to the Highland Austin Community College Campus. 
Capital Area Rural Transportation System connects Austin with outlying suburbs.
An Amtrak "Texas Eagle" station is located west of downtown. Segments of the Amtrak route between Austin and San Antonio are under evaluation for a future regional passenger rail corridor as an alternative to the traffic congestion of Interstate 35. This is a multi jurisdictional project called . Austin is also home to Car2Go, a carsharing program. Austin was chosen as the first city in the western hemisphere to host this company's business, which is based in Germany.
Cycling.
Austin is known as the most bike-friendly city in Texas and has a Silver-level rating from the League of American Bicyclists. There are over 80 miles of bike lanes in Austin. Over 2% of commuters get to work by bike and many more Austinites ride for daily transportation needs, according to the American Community Survey. The North Loop neighborhood along with the Manor Road area have the highest bike commuting rates, with over 13% of residents biking to work in 2012. Biking is also very popular recreationally with the extensive network of trails in the city.
The city's bike advocacy organization is Bike Austin. Bike Texas, a state-level advocacy also has its main office in Austin.
Bicycles are a popular transportation choice among students, faculty, and staff at the University of Texas. According to a survey done at UT, 9% of commuters bike to campus.
Walkability.
A 2013 study by Walk Score ranked Austin 35th most walkable of the 50 largest U.S. cities. This is considered a medium low ranking.
Sister cities.
List of sister cities of Austin, Texas, designated by Sister Cities International.
The cities of Belo Horizonte, Brazil and Elche, Spain were formerly sister cities, but upon a vote of the Austin City Council in 1991, their status was de-activated.

</doc>
<doc id="2003" url="http://en.wikipedia.org/wiki?curid=2003" title="Argument from morality">
Argument from morality

The argument from morality is an argument for the existence of God. Arguments from morality tend to be based on moral normativity or moral order. Arguments from moral normativity observe some aspect of morality and argue that God is the best or only explanation for this, concluding that God must exist. Argument from moral order are based on the asserted need for moral order to exist in the universe. They claim that, for this moral order to exist, God must exist to support it. The argument from morality is noteworthy in that one cannot evaluate the soundness of the argument without attending to almost every important philosophical issue in metaethics.
German philosopher Immanuel Kant devised an argument from morality based on practical reason. Kant argued that the goal of humanity is to achieve perfect happiness and virtue (the summum bonum) and believed that an afterlife must exist in order for this to be possible, and that God must exist to provide this. In his book "Mere Christianity", C. S. Lewis argued that "conscience reveals to us a moral law whose source cannot be found in the natural world, thus pointing to a supernatural Lawgiver." Lewis argued that accepting the validity of human reason as a given must include accepting the validity of practical reason, which could not be valid without reference to a higher cosmic moral order which could not exist without a God to create and/or establish it. A related argument is from conscience; John Henry Newman argued that the conscience supports the claim that objective moral truths exist because it drives people to act morally even when it is not in their own interest. Newman argued that, because the conscience suggests the existence of objective moral truths, God must exist to give authority to these truths.
General form.
All variations of the argument from morality begin with an observation about moral thought or experiences and conclude with the existence of God. Some of these arguments propose moral facts which they claim evident through human experience, arguing that God is the best explanation for these. Other versions describe some end which humans should strive to attain, only possible if God exists.
Many arguments from morality are based on moral normativity, which suggests that objective moral truths exist and require God's existence to give them authority. Often, they consider that morality seems to be binding – obligations are seen to convey more than just a preference, but imply that the obligation will stand, regardless of other factors or interests. For morality to be binding, God must exist. In its most general form, the argument from moral normativity is:
Some arguments from moral order suggest that morality is based on rationality and that this can only be the case if there is a moral order in the universe. The arguments propose that only the existence of God as orthodoxly conceived could support the existence of moral order in the universe, so God must exist. Alternative arguments from moral order have proposed that we have an obligation to attain the perfect good of both happiness and moral virtue. They attest that whatever we are obliged to do must be possible, and achieving the perfect good of both happiness and moral virtue is only possible if a natural moral order exists. A natural moral order requires the existence of God as orthodoxly conceived, so God must exist.
Variations.
Practical Reason.
In his "Critique of Pure Reason", German philosopher Immanuel Kant stated that no successful argument for God's existence arises from reason alone. In his "Critique of Practical Reason" he went on to argue that, despite the failure of these arguments, morality requires that God's existence is assumed, owing to practical reason. Rather than proving the existence of God, Kant was attempting to demonstrate that all moral thought requires the assumption that God exists. Kant argued that humans are obliged to bring about the "summum bonum": the two central aims of moral virtue and happiness, where happiness arises out of virtue. As ought implies can, Kant argued, it must be possible for the "summum bonum" to be achieved. He accepted that it is not within the power of humans to bring the "summum bonum" about, because we cannot ensure that virtue always leads to happiness, so there must be a higher power who has the power to create an afterlife where virtue can be rewarded by happiness.
Philosopher G. H. R. Parkinson notes a common objection to Kant's argument: that what ought to be done does not necessarily entail that it is possible. He also argues that alternative conceptions of morality exist which do not rely on the assumptions that Kant makes – he cites utilitarianism as an example which does not require the "summum bonum". Nichola Everitt argues that much moral guidance is unattainable, such as the Biblical command to be Christ-like. She proposes that Kant's first two premises only entail that we must try to achieve the perfect good, not that it is actually attainable.
Argument from objective moral truths.
Both theists and non-theists have accepted that the existence of objective moral truths might entail the existence of God. Atheist philosopher J. L. Mackie accepted that, if objective moral truths existed, they would warrant a supernatural explanation. Scottish philosopher W. R. Sorley presented the following argument:
Many critics have challenged the second premise of this argument, by offering a biological and sociological account of the development of human morality which suggests that it is neither objective nor absolute. This account, supported by biologist E. O. Wilson and philosopher Michael Ruse, proposes that the human experience of morality is a by-product of natural selection, a theory philosopher Mark D. Linville calls evolutionary naturalism. According to the theory, the human experience of moral obligations was the result of evolutionary pressures, which attached a sense of morality to human psychology because it was useful for moral development; this entail that moral values do not exist independently of the human mind. Morality might be better understood as an evolutionary imperative in order to propagate genes and ultimately reproduce. No human society today advocates immorality, such as theft or murder, because it would undoubtedly lead to the end of that particular society and any chance for future survival of offspring. Scottish empiricist David Hume made a similar argument, that belief in objective moral truths is unwarranted and to discuss them is meaningless.
Because evolutionary naturalism proposes an empirical account of morality, it does not require morality to exist objectively; Linville considers the view that this will lead to moral scepticism or antirealism. C. S. Lewis argued that, if evolutionary naturalism is accepted, human morality cannot be described as absolute and objective because moral statements cannot be right or wrong. Despite this, Lewis argued, those who accept evolutionary naturalism still act as if objective moral truths exist, leading Lewis to reject naturalism as incoherent. As an alternative ethical theory, Lewis offered a form of divine command theory which equated God with goodness and treated goodness as an essential part of reality, thus asserting God's existence.
J.C.A. Gaskin challenges the first premise of the argument from moral objectivity, arguing that it must be shown why absolute and objective morality entails that morality is commanded by God, rather than simply a human invention. It could be the consent of humanity that gives it moral force, for example. American philosopher Michael Martin argues that it is not necessarily true that objective moral truths must entail the existence of God, suggesting that there could be alternative explanations: he argues that naturalism may be an acceptable explanation and, even if a supernatural explanation is necessary, it does not have to be God (polytheism is a viable alternative). Martin also argues that a non-objective account of ethics might be acceptable and challenges the view that a subjective account of morality would lead to moral anarchy.
Conscience.
Related to the argument from morality is the argument from conscience, associated with eighteenth-century bishop Joseph Butler and nineteenth-century cardinal John Henry Newman. Newman proposed that the conscience, as well as giving moral guidance, provides evidence of objective moral truths which must be supported by the divine. He argued that emotivism is an inadequate explanation of the human experience of morality because people avoid acting immorally, even when it might be in their interests. Newman proposed that, to explain the conscience, God must exist.
British philosopher John Locke argued that moral rules cannot be established from conscience because the differences in people's consciences would lead to contradictions. Locke also noted that the conscience is influenced by "education, company, and customs of the country", a criticism mounted by J. L. Mackie, who argued that the conscience should be seen as an "introjection" of other people into an agent's mind. Michael Martin challenges the argument from conscience with a naturalistic account of conscience, arguing that naturalism provides an adequate explanation for the conscience without the need for God's existence. He uses the example of the internalisation by humans of social pressures, which leads to the fear of going against these norms. Even if a supernatural cause is required, he argues, it could be something other than God; this would mean that the phenomena of the conscience is no more supportive of monotheism than polytheism.

</doc>
<doc id="2004" url="http://en.wikipedia.org/wiki?curid=2004" title="ASL (disambiguation)">
ASL (disambiguation)

ASL is a common initialism for American Sign Language, the sign language of the United States and Canada, and may also refer to:

</doc>
<doc id="2006" url="http://en.wikipedia.org/wiki?curid=2006" title="Auschwitz concentration camp">
Auschwitz concentration camp

Auschwitz concentration camp (German: "Konzentrationslager Auschwitz" ]) was a network of German Nazi concentration camps and extermination camps built and operated by the Third Reich in Polish areas annexed by Nazi Germany during World War II. It consisted of Auschwitz I (the original camp), Auschwitz II–Birkenau (a combination concentration/extermination camp), Auschwitz III–Monowitz (a labor camp to staff an IG Farben factory), and 45 satellite camps.
Auschwitz I was first constructed to hold Polish political prisoners, who began to arrive in May 1940. The first extermination of prisoners took place in September 1941, and Auschwitz II–Birkenau went on to become a major site of the Nazi "Final Solution to the Jewish question". From early 1942 until late 1944, transport trains delivered Jews to the camp's gas chambers from all over German-occupied Europe, where they were killed with the pesticide Zyklon B. At least 1.1 million prisoners died at Auschwitz, around 90 percent of them Jewish; approximately 1 in 6 Jews killed in the Holocaust died at the camp. Others deported to Auschwitz included 150,000 Poles, 23,000 Romani and Sinti, 15,000 Soviet prisoners of war, 400 Jehovah's Witnesses, homosexuals, and tens of thousands of people of diverse nationalities. Many of those not killed in the gas chambers died of starvation, forced labor, infectious diseases, individual executions, and medical experiments.
In the course of the war, the camp was staffed by 7,000 members of the German "Schutzstaffel" (SS), approximately 12 percent of whom were later convicted of war crimes. Some, including camp commandant Rudolf Höss, were executed. The Allied Powers refused to believe early reports of the atrocities at the camp, and their failure to bomb the camp or its railways remains controversial. One hundred forty-four prisoners are known to have escaped from Auschwitz successfully, and on October 7, 1944, two "Sonderkommando" units—prisoners assigned to staff the gas chambers—launched a brief, unsuccessful uprising.
As Soviet troops approached Auschwitz in January 1945, most of its population was evacuated and sent on a death march. The prisoners remaining at the camp were liberated on January 27, 1945, a day now commemorated as International Holocaust Remembrance Day. In the following decades, survivors, such as Primo Levi, Viktor Frankl, and Elie Wiesel, wrote memoirs of their experiences in Auschwitz, and the camp became a dominant symbol of the Holocaust. In 1947, Poland founded a museum on the site of Auschwitz I and II, and in 1979, it was named a UNESCO World Heritage Site.
History.
Background.
Discrimination against Jews began immediately after the Nazi seizure of power in Germany on January 30, 1933. The Law for the Restoration of the Professional Civil Service, passed on April 7 that year, excluded most Jews from the legal profession and the civil service. Similar legislation soon deprived Jewish members of other professions of the right to practise. Violence and economic pressure were used by the regime to encourage Jews to leave the country voluntarily. Jewish businesses were denied access to markets, forbidden to advertise in newspapers, and deprived of access to government contracts. Citizens were harassed and subjected to violent attacks and boycotts of their businesses.
In September 1935 the Nuremberg Laws were enacted. These laws prohibited marriages between Jews and people of Germanic extraction, extramarital relations between Jews and Germans, and the employment of German women under the age of 45 as domestic servants in Jewish households. The Reich Citizenship Law stated that only those of Germanic or related blood were defined as citizens. Thus Jews and other minority groups were stripped of their German citizenship. By the start of World War II in 1939, around 250,000 of Germany's 437,000 Jews emigrated to the United States, Palestine, Great Britain, and other countries.
The ideology of Nazism brought together elements of antisemitism, racial hygiene, and eugenics, and combined them with pan-Germanism and territorial expansionism with the goal of obtaining more "Lebensraum" (living space) for the Germanic people. Nazi Germany attempted to obtain this new territory by invading Poland and the Soviet Union, intending to deport or kill the Jews and Slavs living there, who were viewed as being inferior to the Aryan master race. After the invasion of Poland in September 1939, German dictator Adolf Hitler ordered that the Polish leadership and intelligentsia should be destroyed. Approximately 65,000 civilians were killed by the end of 1939. In addition to leaders of Polish society, the Nazis killed Jews, prostitutes, Romani, and the mentally ill. SS-"Obergruppenführer" (Senior Group Leader) Reinhard Heydrich, then head of the Gestapo, ordered on September 21 that Jews should be rounded up and concentrated into cities with good rail links. Initially the intention was to deport the Jews to points further east, or possibly to Madagascar.
Auschwitz I.
After this part of Poland was annexed by Nazi Germany, Oświęcim (Auschwitz) was located administratively in Germany, Province of Upper Silesia, Regierungsbezirk Kattowitz, Landkreis Bielitz. It was first suggested as a site for a concentration camp for Polish prisoners by "SS-Oberführer" Arpad Wigand, an aide to Higher SS and Police Leader for Silesia, Erich von dem Bach-Zelewski. Bach-Zelewski had been searching for a site to house prisoners in the Silesia region, as the local prisons were filled to capacity. Richard Glücks, head of the Concentration Camps Inspectorate, sent former Sachsenhausen concentration camp commandant Walter Eisfeld to inspect the site, which already held sixteen dilapidated one-story buildings that had once served as an Austrian and later Polish Army barracks and a camp for transient workers. "Reichsführer-SS" Heinrich Himmler, head of the "Schutzstaffel" (SS), approved the site in April 1940, intending to use the facility to house political prisoners. SS-"Obersturmbannführer" (lieutenant colonel) Rudolf Höss oversaw the development of the camp and served as the first commandant. SS-"Obersturmführer" (senior lieutenant) Josef Kramer was appointed Höss's deputy. Auschwitz I, the original camp, became the administrative center for the whole complex.
Local residents were evicted, including 1,200 people who lived in shacks around the barracks. Around 300 Jewish residents of Oświęcim were brought in to lay foundations. From 1940 to 1941, 17,000 Polish and Jewish residents of the western districts of Oświęcim were expelled from places adjacent to the camp. The Germans also ordered expulsions from the villages of Broszkowice, Babice, Brzezinka, Rajsko, Pławy, Harmęże, Bór, and Budy. German citizens were offered tax concessions and other benefits if they would relocate to the area. By October 1943, more than 6,000 Reich Germans had arrived. The Nazis planned to build a model modern residential area for incoming Germans, including schools, playing fields, and other amenities. Some of the plans went forward, including the construction of several hundred apartments, but many were never fully implemented. Basic amenities such as water and sewage disposal were inadequate, and water-borne illnesses were commonplace.
The first prisoners (30 German criminal prisoners from the Sachsenhausen camp) arrived in May 1940, intended to act as functionaries within the prison system. The first transport of 728 Polish prisoners, which included 20 Jews, arrived on June 14, 1940, from the prison in Tarnów, Poland. They were interned in the former building of the Polish Tobacco Monopoly, adjacent to the site, until the camp was ready. The inmate population grew quickly as the camp absorbed Poland's intelligentsia and dissidents, including the Polish underground resistance. By March 1941, 10,900 were imprisoned there, most of them Poles. By the end of 1940, the SS had confiscated land in the surrounding area to create a 40 km2 "zone of interest" surrounded by a double ring of electrified barbed wire fences and watchtowers. Like other Nazi concentration camps, the gates to Auschwitz I displayed the motto "Arbeit macht frei" ("Work brings freedom").
Auschwitz II-Birkenau.
Construction on Auschwitz II-Birkenau began in October 1941 to ease congestion at the main camp. "Reichsführer-SS" Heinrich Himmler, head of the "Schutzstaffel" (SS), intended the camp to house 50,000 prisoners of war, who would be interned as forced laborers. Plans called for the expansion of the camp first to house 150,000 and eventually as many as 200,000 inmates. An initial contingent of 10,000 Soviet prisoners of war arrived at Auschwitz I in October 1941, but by March 1942 only 945 were still alive, and these were transferred to Birkenau, where most of them died from disease or starvation by May. By this time Hitler had decided to annihilate the Jewish people, so Birkenau was repurposed as a combination labor camp / extermination camp.
The chief of construction of Auschwitz II-Birkenau was Karl Bischoff. Unlike his predecessor, he was a competent and dynamic bureaucrat who, in spite of the ongoing war, carried out the construction deemed necessary. The Birkenau camp, the four crematoria, the technically complicated central sauna, a new reception building, and hundreds of other buildings were planned and realized. Bischoff's plans initially called for each barrack to have an occupancy of 550 prisoners (one-third of the space allotted in other Nazi concentration camps). He later changed this to 744 prisoners per barrack. The SS designed the barracks not so much to house people as to destroy them.
The first gas chamber at Birkenau was the "red house" (called Bunker 1 by SS staff), a brick cottage converted into a gassing facility by tearing out the inside and bricking up the walls. It was operational by March 1942. A second brick cottage, the "white house" or Bunker 2, was converted some weeks later. These structures were in use for mass killings until early 1943. Himmler visited the camp in person on July 17 and 18, 1942. He was given a demonstration of a mass killing using the gas chamber in Bunker 2 and toured the building site of the new IG Farben plant being constructed at the nearby town of Monowitz.
In early 1943, the Nazis decided to increase greatly the gassing capacity of Birkenau. Crematorium II, originally designed as a mortuary, with morgues in the basement and ground-level incinerators, was converted into a killing factory by installing gas-tight doors, vents for the Zyklon B (a highly lethal cyanide-based pesticide) to be dropped into the chamber, and ventilation equipment to remove the gas thereafter. It went into operation in March. Crematorium III was built using the same design. Crematoria IV and V, designed from the start as gassing centers, were also constructed that spring. By June 1943, all four crematoria were operational. Most of the victims were killed using these four structures.
The Gypsy camp.
On December 10, 1942, Himmler issued an order to send all Sinti and Roma (Gypsies) to concentration camps, including Auschwitz. A separate camp for Roma was set up at Auschwitz II-Birkenau known as the "Zigeunerfamilienlager" (Gypsy Family Camp). The first transport of German Gypsies arrived on February 26, 1943, and was housed in Section B-IIe of Auschwitz II. Approximately 23,000 Gypsies had been brought to Auschwitz by 1944, 20,000 of whom died there. One transport of 1,700 Polish Sinti and Roma was killed upon arrival, as they were suspected to be ill with spotted fever.
Gypsy prisoners were used primarily for construction work. Thousands died of typhus and noma due to overcrowding, poor sanitary conditions, and malnutrition. Anywhere from 1,400 to 3,000 prisoners were transferred to other concentration camps before the murder of the remaining population.
On August 2, 1944, the SS cleared the Gypsy camp. A witness in another part of the camp later told of the Gypsies unsuccessfully battling the SS with improvised weapons before being loaded into trucks. The surviving population of 2,897 was then killed en masse in the gas chambers. The murder of the Romani people by the Nazis during World War II is known in the Romani language as the Porajmos (devouring).
Auschwitz III.
After examining several sites for a new plant to manufacture buna, a type of synthetic rubber essential to the war effort, chemicals manufacturer IG Farben chose a site near the towns of Dwory and Monowice (Monowitz in German), about 7 km east of Auschwitz I and 3 km east of the town of Oświęcim. Financial support in the form of tax exemptions was available to corporations prepared to develop industries in the frontier regions under the Eastern Fiscal Assistance Law, passed in December 1940. In addition to its proximity to the concentration camp, which could be used as a source of cheap labor, the site had good railway connections and access to raw materials. In February 1941, Himmler ordered that the Jewish population of Oświęcim should be expelled to make way for skilled laborers that would be brought in to work at the plant. All Poles able to work were to remain in the town and were forced to work building the factory. Himmler visited in person in March and decreed an immediate expansion of the parent camp to house 30,000 persons. Development of the camp at Birkenau began about six months later. Construction of IG Auschwitz began in April, with an initial force of 1,000 workers from Auschwitz I assigned to work on the construction. This number increased to 7,000 in 1943 and 11,000 in 1944. Over the course of its history, about 35,000 inmates in total worked at the plant; 25,000 died as a result of malnutrition, disease, and the physically impossible workload. In addition to the concentration camp inmates, who comprised a third of the work force, IG Auschwitz employed slave laborers from all over Europe.
Initially the laborers walked the seven kilometers from Auschwitz I to the plant each day, but as this meant they had to rise at 3:00 am, many arrived exhausted and unable to work. The camp at Monowitz (also called Monowitz-Buna or Auschwitz III) was constructed and began housing inmates on October 30, 1942, the first concentration camp to be financed and built by private industry. In January 1943 the "ArbeitsausbildungLager" (labor education camp) was moved from the parent camp to Monowitz. These prisoners were also forced to work on the building site. The SS charged IG Farben three Reichsmarks per hour for unskilled workers, four for skilled workers. Although the camp administrators expected the prisoners to work at 75 percent of the capacity of a free worker, the inmates were only able to perform 20 to 50 percent as well. Site managers constantly threatened inmates with transportation to Birkenau for death in the gas chambers as a way to try to increase productivity. Deaths and transfers to the gas chambers at Birkenau reduced the prisoner population of Monowitz by nearly a fifth each month; numbers were made up with new arrivals. Life expectancy of inmates at Monowitz averaged about three months. Though the factory was initially expected to begin production in 1943, shortages of labor and raw materials meant start-up had to be postponed repeatedly. The plant was almost ready to commence production when it was overrun by Soviet troops in 1945.
Subcamps.
Various other German industrial enterprises, such as Krupp and Siemens-Schuckert, built factories with their own subcamps. There were 45 such satellite camps, 28 of which served corporations involved in the armanents industry. Prisoner populations ranged from several dozen to several thousand. Subcamps were built at Blechhammer, Jawiszowice, Jaworzno, Lagisze, Mysłowice, Trzebinia, and other centers as far afield as the Protectorate of Bohemia and Moravia. Satellite camps were designated as "Aussenlager" (external camp), "Nebenlager" (extension or subcamp), or "Arbeitslager" (labor camp). Industries with satellite camps included coal mines, foundries and other metal works, chemical plants, and other industries. Prisoners were also made to work in forestry and farming.
Evacuation, death marches, and liberation.
In November 1944, with the Soviet Red Army approaching through Poland, Himmler ordered gassing operations to cease across the Reich. Crematoria II, III, and IV were dismantled, while Crematorium I was transformed into an air raid shelter. The "Sonderkommando" were ordered to remove other evidence of the killings, including the mass graves. The SS destroyed written records, and in the final week before the camp's liberation, burned or demolished many of its buildings.
Himmler ordered the evacuation of all camps in January 1945, charging camp commanders with "making sure that not a single prisoner from the concentration camps falls alive into the hands of the enemy." On January 17, 58,000 Auschwitz detainees were evacuated under guard, largely on foot; thousands of them died in the subsequent death march west towards Wodzisław Śląski. Approximately 20,000 Auschwitz prisoners made it to Bergen-Belsen concentration camp in Germany, where they were liberated by the British in April 1945.
Those too weak or sick to walk were left behind. When the 322nd Rifle Division of the Red Army arrived at the camp on January 27 they found around 7,500 prisoners and about 600 corpses had been left behind. Among the items found by the Soviet soldiers were 370,000 men's suits, 837,000 women's garments, and 7.7 tonnes of human hair.
The camp's liberation received little press attention at the time. Rees attributes this to three factors: the previous discovery of similar crimes at Majdanek concentration camp, competing news from the Allied summit at Yalta, and the Soviet Union's interest, for propaganda purposes, in minimizing attention to Jewish suffering. Due to the vast extent of the camp area, at least four divisions took part in liberating the camp: 100th Rifle Division (established in Vologda, Russia), 322nd Rifle Division (Gorky, Russia), 286th Rifle Division (Leningrad), and 107th Motor Rifle Division (Tambov, Russia). 
After the war.
After liberation, parts of Auschwitz I served first as a hospital for liberated prisoners. Soviet and Polish investigators worked in the initial months to document the war crimes of the SS. In the two years that followed, the Soviets dismantled and exported the IG Farben factories, and the Birkenau barracks were looted by Polish civilians. Area residents sifted the mass graves and ashes for gold. Until 1947, some of the facilities were used as a prison camp of the Soviet NKVD.
After the site became a museum in 1947, exhumation work lasted for more than a decade. Antoni Dobrowolski, the oldest known survivor of Auschwitz, died aged 108 on October 21, 2012, in Dębno, Poland.
Camp commandant Rudolf Höss was pursued by the British Intelligence Corps, who arrested him at a farm near Flensburg, Germany, on March 11, 1946. Höss confessed to his role in the mass killings at Auschwitz in his memoirs and in his trial before the Supreme National Tribunal in Warsaw, Poland. He was convicted of murder and hanged at the camp on April 16, 1947.
Around 12 percent of Auschwitz's 6,500 staff who survived the war were eventually brought to trial. Poland was more active than other nations in investigating war crimes, prosecuting 673 of the total 789 Auschwitz staff brought to trial. On November 25, 1947, the Auschwitz Trial began in Kraków, when Poland's Supreme National Tribunal brought to court 40 former Auschwitz staff. The trial's defendants included commandant Arthur Liebehenschel, women's camp leader Maria Mandel, and camp leader Hans Aumeier. The trials ended on December 22, 1947, with 23 death sentences, 7 life sentences, and 9 prison sentences ranging from three to fifteen years. Hans Münch, an SS doctor who had several former prisoners testify on his behalf, was the only person to be acquitted.
Other former staff were hanged for war crimes in the Dachau Trials and the Belsen Trial, including camp leaders Josef Kramer, Franz Hössler, and Vinzenz Schöttl; doctor Friedrich Entress; and guards Irma Grese and Elisabeth Volkenrath. The Frankfurt Auschwitz Trials, held in West Germany from December 20, 1963 to August 20, 1965, convicted 17 of 22 defendants, giving them prison sentences ranging from life to three years and three months. Bruno Tesch and Karl Weinbacher, the owner and the chief executive officer of the firm Tesch & Stabenow, one of the suppliers of Zyklon B, were executed for knowingly supplying the chemical for use on humans.
Command and control.
Camp guards were members of the "SS-Totenkopfverbände" (Death's Head Units). Around 7,000 SS personnel in total were posted to Auschwitz during the war. Of these, 4 percent of SS personel were officers and 26 percent were non-commissioned officers, while the remainder were rank-and-file members. Approximately three in four SS personnel worked in security. Others worked in the medical or political departments, in the camp headquarters, or in the economic administration, which was responsible for the property of dead prisoners. SS personnel at the camp included 200 women, who worked as guards, nurses, or messengers. The overall command authority for the entire camp was Department D (the Concentration Camps Inspectorate) of the "SS-Wirtschafts-Verwaltungshauptamt" (SS Economics Main Office; SS-WVHA).
Auschwitz was considered a comfortable posting by many SS members, due to many amenities and the abundance of slave labor. Of the various prisoner groups, SS officers preferred Jehovah's Witnesses for household slaves because of their nonviolent behavior. Höss lived with his wife and children in a villa just outside the camp grounds. Other SS personnel were also initially allowed to bring fiancees, wives, and children to live at the camp, but when the SS camp grew more crowded, Höss restricted further arrivals. Facilities for the SS personnel and their families included a library, swimming pool, coffee house, and a theater that hosted regular performances.
One prisoner in each work detail or prisoner block—usually an Aryan—was appointed as a "Kapo" ("head" or "overseer"). The "Kapos" received better rations and lodging and wielded tremendous power over other prisoners, whom they often abused. Very few "Kapos" were prosecuted after the war, however, due to the difficulty in determining which "Kapo" atrocities had been performed under SS orders and which had been individual actions.
About 120 SS personnel were assigned to the gas chambers and lived on site at the crematoria. Several SS personnel oversaw the killings at each gas chamber, while the bulk of the work was done by the mostly Jewish prisoners known as "Sonderkommando" (special squad). "Sonderkommando" responsibilities included guiding victims to the gas chambers and removing, looting, and cremating the corpses.
The "Sonderkommado" were housed separately from other prisoners, in somewhat better conditions. Their quality of life was further improved by access to the goods taken from murdered prisoners, which "Sonderkommando" were sometimes able to steal for themselves and to trade on Auschwitz's black market. Hungarian doctor Miklós Nyiszli reported that the "Sonderkommando" numbered around 860 prisoners when the Hungarian Jews were being killed in 1944. Many "Sonderkommando" committed suicide due to the horrors of their work; those who did not generally were shot by the SS in a matter of weeks, and new "Sonderkommando" units were then formed from incoming transports. Almost none of the 2,000 prisoners placed in these units survived to the camp's liberation.
Life in the camps.
The prisoners' day began at 4:30 am (an hour later in winter) with morning roll call. Dr. Miklós Nyiszli describes roll call as beginning 3:00 am and lasting four hours. The weather was cold in Auschwitz at that time of day, even in summer. The prisoners were ordered to line up outdoors in rows of five and had to stay there until 7:00 am, when the SS officers arrived. Meanwhile the guards would force the prisoners to squat for an hour with their hands above their heads or levy punishments such as beatings or detention for infractions such as having a missing button or an improperly cleaned food bowl. The inmates were counted and re-counted. Nyiszli describes how even the dead had to be present at roll call, standing supported by their fellow inmates until the ordeal was over. When he was a prisoner in 1944–45, five to ten men were found dead in the barracks each night. The prisoners assigned to Mengele's staff slept in a separate barracks and were awoken at 7:00 am for a roll call that only took a few minutes.
After roll call, the "Kommando", or work details, walked to their place of work, five abreast, wearing striped camp fatigues, no underwear, and ill-fitting wooden shoes without socks. A prisoner's orchestra (such as the Women's Orchestra of Auschwitz) was forced to play cheerful music as the workers left the camp. "Kapos" were responsible for the prisoners' behavior while they worked, as was an SS escort. The working day lasted 12 hours during the summer and a little less in the winter. Much of the work took place outdoors at construction sites, gravel pits, and lumber yards. No rest periods were allowed. One prisoner was assigned to the latrines to measure the time the workers took to empty their bladders and bowels. Sunday was not a work day, but the prisoners did not rest; they were required to clean the barracks and take their weekly shower. Prisoners were allowed to write (in German) to their families on Sundays. Inmates who did not speak German would trade some of their bread to another inmate for help composing their letters. Members of the SS censored the outgoing mail.
A second mandatory roll call took place in the evening. If a prisoner was missing, the others had to remain standing in place until he was either found or the reason for his absence discovered, regardless of the weather conditions, even if it took hours. After roll call, individual and collective punishments were meted out, depending on what had happened during the day, before the prisoners were allowed to retire to their blocks for the night and receive their bread rations and water. Curfew was two or three hours later. The prisoners slept in long rows of wooden bunks, lying in and on their clothes and shoes to prevent them from being stolen.
According to Nyiszli, "Eight hundred to a thousand people were crammed into the superimposed compartments of each barracks. Unable to stretch out completely, they slept there both lengthwise and crosswise, with one man's feet on another's head, neck, or chest. Stripped of all human dignity, they pushed and shoved and bit and kicked each other in an effort to get a few more inches' space on which to sleep a little more comfortably. For they did not have long to sleep".
The types of prisoners were distinguishable by triangular pieces of cloth, called "Winkel", sewn onto on their jackets below their prisoner number. Political prisoners had a red triangle, Jehovah's Witnesses had purple, criminals had green, and so on. The nationality of the inmate was indicated by a letter stitched onto the "Winkel". Jews had a yellow triangle, overlaid by a second "Winkel" if they also fit into a second category. Uniquely at Auschwitz, prisoners were tattooed with their prisoner number, on the chest for Soviet prisoners of war and on the left arm for civilians.
Prisoners received a hot drink in the morning, but no breakfast, and a thin meatless vegetable soup at noon. In the evening they received a small ration of moldy bread. Most prisoners saved some of the bread for the following morning. Nyiszli notes the daily intake did not exceed 700 calories, except for prisoners being subjected to live medical experimentation, who were better fed and clothed. Sanitary arrangements were poor, with inadequate latrines and a lack of fresh water. In Auschwitz II-Birkenau, latrines were not installed until 1943, two years after camp construction began. The camps were infested with vermin such as disease-carrying lice, and the inmates suffered and died in epidemics of typhus and other diseases. Noma, a bacterial infection occurring among the malnourished, was a common cause of death among children in the Gypsy camp.
Block 11 of Auschwitz I was the prison within the prison, where violators of the numerous rules were punished. Some prisoners were made to spend the nights in standing cells. These cells were about 1.5 m2, and held four men; they could do nothing but stand, and were forced during the day to work with the other prisoners. Prisoners sentenced to death for attempting to escape were confined in a dark cell and given neither food nor water until they were dead.
In the basement were the "dark cells", which had only a very tiny window and a solid door. Prisoners placed in these cells gradually suffocated as they used up all the oxygen in the cell; sometimes the SS lit a candle in the cell to use up the oxygen more quickly. Many were subjected to hanging with their hands behind their backs for hours, even days, thus dislocating their shoulder joints.
Selection and extermination process.
On July 31, 1941, Hermann Göring gave written authorization to Heydrich, Chief of the Reich Main Security Office (RSHA), to prepare and submit a plan for "Die Endlösung der Judenfrage" (the Final Solution of the Jewish question) in territories under German control and to coordinate the participation of all involved government organizations. The resulting "Generalplan Ost" (General Plan for the East) called for deporting the population of occupied Eastern Europe and the Soviet Union to Siberia, for use as slave labour or to be murdered. In addition to eliminating Jews, the Nazis also planned to reduce the population of the conquered territories by 30 million people through starvation in an action called the Hunger Plan. Food supplies would be diverted to the German army and German civilians. Cities would be razed and the land allowed to return to forest or resettled by German colonists.
Plans for the total eradication of the Jewish population of Europe—eleven million people—were formalized at the Wannsee Conference on January 20, 1942. Some would be worked to death and the rest would be killed. Initially the victims were killed with gas vans or by "Einsatzgruppen" firing squads, but these methods proved impracticable for an operation of this scale. By 1942, killing centers at Auschwitz, Sobibór, Treblinka, and other Nazi extermination camps replaced "Einsatzgruppen" as the primary method of mass killing.
The first mass exterminations at Auschwitz took place in early September 1941, when 900 inmates were killed by gathering them in the basement of Block 11 and gassing them with Zyklon B. This building proved unsuitable for mass gassings, so the site of the killings was moved to the crematorium at Auschwitz I (Crematorium I, which operated until July 1942). There, more than 700 victims could be killed at once. In order to keep the victims calm, they were told they were to undergo disinfection and de-lousing. They were ordered to undress outside and then were locked in the building and gassed. After its decommissioning as a gas chamber, the building was converted to a storage facility and later served as an air raid shelter for the SS. The gas chamber and crematorium were reconstructed after the war using the original components, which remained on site. Some 60,000 people were killed at Crematorium I.
Mass exterminations were moved to two provisional gas chambers (Bunkers 1 and 2), where the killings continued while the larger Crematoria II, III, IV, and V were under construction. Bunker 2 was temporarily reactivated from May to November 1944, when large numbers of Hungarian Jews were exterminated. In summer 1944 the capacity of the crematoria and outdoor incineration pits was 20,000 bodies per day. A planned sixth facility—Crematorium VI—was never built.
Prisoners were transported from all over German-occupied Europe by rail, arriving in daily convoys. By July 1942, the SS were conducting "selections". Incoming Jews were segregated; those deemed able to work were sent to the right and admitted into the camp, and those deemed unfit for labor were sent to the left and immediately gassed. The group selected to die, about three-quarters of the total, included almost all children, women with small children, all the elderly, and all those who appeared on brief and superficial inspection by an SS doctor not to be completely fit. After the selection process was complete, those too ill or too young to walk to the crematoria were transported there on trucks or killed on the spot with a bullet to the head. The belongings of the arrivals were seized by the SS and sorted in an area of the camp called "Canada", so called because Canada was seen as a land of plenty. Many of the SS at the camp enriched themselves by pilfering the confiscated property.
SS officers told the victims they were to take a shower and undergo delousing. The victims undressed in an outer chamber and walked into the gas chamber, which was disguised as a shower facility. Some were even issued soap and a towel. The Zyklon B was delivered by ambulance to the crematoria by a special SS bureau known as the Hygienic Institute. The actual delivery of the gas to the victims was always handled by the SS, on the order of the supervising SS doctor. After the doors were shut, SS men dumped in the Zyklon B pellets through vents in the roof or holes in the side of the chamber. The victims were dead within 20 minutes. Despite the thick concrete walls, screaming and moaning from within could be heard outside. In one failed attempt to muffle the noise, two motorcycle engines were revved up to full throttle nearby, but the sound of yelling could still be heard over the engines.
"Sonderkommando" wearing gas masks then dragged the bodies from the chamber. The victims' glasses, artificial limbs, jewelry, and hair were removed, and any dental work was extracted so the gold could be melted down. The corpses were burned in the nearby incinerators, and the ashes were buried, thrown in the river, or used as fertilizer.
The gas chambers worked to their fullest capacity from April–July 1944, during the massacre of Hungary's Jews. Hungary was an ally of Germany during the war, but it had resisted turning over its Jews until Germany invaded that March. A rail spur leading directly into Birkenau was completed that May to deliver the victims closer to the gas chambers. From 14 May until early July 1944, 437,000 Hungarian Jews, half of the pre-war population, were deported to Auschwitz, at a rate of 12,000 a day for a considerable part of that period. The incoming volume was so great that the SS resorted to burning corpses in open-air pits as well as in the crematoria. The last selection took place on October 30, 1944.
Medical experiments.
German doctors performed a wide variety of experiments on prisoners at Auschwitz. SS doctors tested the efficacy of X-rays as a sterilization device by administering large doses to female prisoners. Prof. Dr. Carl Clauberg injected chemicals into women's uteruses in an effort to glue them shut. Bayer, then a subsidiary of IG Farben, bought prisoners to use as research subjects for testing new drugs. Prisoners were also deliberately infected with spotted fever for vaccination research and exposed to toxic substances to study the effects.
The most infamous doctor at Auschwitz was Josef Mengele, known as the "Angel of Death". Particularly interested in research on identical twins, Mengele performed cruel experiments on them, such as inducing diseases in one twin and killing the other when the first died to perform comparative autopsies. He also took a special interest in dwarfs, and he deliberately induced noma in twins, dwarfs, and other prisoners to study the effects.
Kurt Heissmeyer took twenty Jewish children from Auschwitz to use in pseudoscientific medical experiments at the Neuengamme concentration camp. In April 1945, the children were killed by hanging to conceal the project.
A skeleton collection was obtained from among a pool of 115 Jewish Auschwitz inmates, chosen for their perceived stereotypical racial characteristics. Rudolf Brandt and Wolfram Sievers, general manager of the "Ahnenerbe" (a Nazi research institute), were responsible for delivering the skeletons to the collection of the Anatomy Institute at the Reich University of Strasbourg in the Alsace region of Occupied France. The collection was sanctioned by Himmler and under the direction of August Hirt. Ultimately 87 of the inmates were shipped to Natzweiler-Struthof and killed in August 1943. Brandt and Sievers were later convicted in the Doctors' Trial in Nuremberg.
Death toll.
The exact number of victims at Auschwitz is difficult to fix with certainty, as many prisoners were never registered and much evidence was destroyed by the SS in the final days of the war. As early as 1942, Himmler visited the camp and ordered that "all mass graves were to be opened and the corpses burned. In addition the ashes were to be disposed of in such a way that it would be impossible at some future time to calculate the number of corpses burned."
Shortly following the camp's liberation, the Soviet government stated that four million people had been killed on the site, a figure now regarded as greatly exaggerated. While under interrogation, Höss said that Adolf Eichmann told him that two and a half million Jews had been killed in gas chambers and about half a million had died of other causes. Later he wrote, "I regard two and a half million far too high. Even Auschwitz had limits to its destructive possibilities". Gerald Reitlinger's 1953 book "The Final Solution" estimated the number killed at 800,000 to 900,000, and Raul Hilberg's 1961 work "The Destruction of the European Jews" estimated the number killed at a maximum of 1,000,000 Jewish victims.
In 1983, French scholar George Wellers was one of the first to use German data on deportations to estimate the number killed at Auschwitz, arriving at a figure of 1,471,595 dead, including 1.35 million Jews and 86,675 Poles. A larger study started by Franciszek Piper used timetables of train arrivals combined with deportation records to calculate at least 960,000 Jewish deaths and at least 1.1 million total deaths, a figure adopted as official by the Auschwitz-Birkenau State Museum in the 1990s. Piper also stated that a figure of as many as 1.5 million total deaths was possible.
By nation, the greatest number of Auschwitz's Jewish victims were from Hungary, accounting for 438,000 deaths, followed by Polish Jews (300,000 deaths), French (69,000), Dutch (60,000), and Greek (55,000). Fewer than one percent of Soviet Jews murdered in the Holocaust were killed in Auschwitz, as German forces had already been driven from Russia when the killing at Auschwitz reached its peak in 1944. Approximately 1 in 6 Jews killed in the Holocaust died at the camp.
The next largest group of victims were non-Jewish Poles, who accounted for 70,000 to 75,000 deaths. Twenty-one thousand Roma and Sinti were killed, along with 15,000 Soviet POWs and 10,000 to 15,000 peoples of other nations. Around 400 Jehovah's Witnesses were imprisoned at Auschwitz, at least 152 of whom died.
Escapes, resistance, and the Allies' knowledge of the camps.
Inmates were at times able to distribute information from the camp via messages and shortwave radio transmissions. The Polish government-in-exile in London first reported the gassing of prisoners on July 21, 1942. However, these reports were for a long time discarded as exaggerated or unreliable by the Allied Powers, Germany's opponents.
Information regarding Auschwitz was also available to the Allies during the years 1940–43 by the accurate and frequent reports of Polish Home Army (Armia Krajowa) Captain Witold Pilecki. Pilecki was the only known person to volunteer to be imprisoned at Auschwitz concentration camp, spending 945 days there. He gathered evidence of genocide and organized resistance structures known as Związek Organizacji Wojskowej (ZOW) at the camp. His first report was smuggled to the outside world in November 1940, through an inmate who was released from the camp. He eventually escaped on April 27, 1943, but his personal report of mass killings was dismissed as exaggeration by the Allies, as were his previous reports.
The first information about Auschwitz concentration camp was published in winter 1940–41 in the Polish underground newspapers "Polska żyje" ("Poland lives") and "Biuletyn Informacyjny" ("Newsletter"). From 1942 members of the Bureau of Information and Propaganda of the Warsaw area Home Army published in occupied Poland a few brochures based on the accounts of escapees. The first of these was a fictional memoir "Oświęcim. Pamiętnik więźnia" ("Auschwitz: Diary of a prisoner"), written by Halina Krahelska and published in April 1942 in Warsaw. Also published in 1942 were the books "Auschwitz: obóz śmierci" ("Auschwitz: camp of death") written by Natalia Zarembina, and "W piekle" ("In Hell") by Zofia Kossak-Szczucka, the Polish writer, social activist, and founder of Żegota.
In 1943, the "Kampfgruppe Auschwitz" (Combat Group Auschwitz) was organized with the aim of sending out information about what was happening. "Sonderkommandos" buried notes in the ground, hoping they would be found by the camp's liberators. The group also took and smuggled out photographs of corpses and preparations for mass killings in mid-1944.
The attitude of the Allies changed with receipt of the detailed, 32-page Vrba–Wetzler report, compiled by two Jewish prisoners, Rudolf Vrba and Alfréd Wetzler, who escaped on April 7, 1944. This report finally convinced Allied leaders that mass killings were taking place in Auschwitz.
Details from the Vrba-Wetzler report were released to the Swiss press by diplomat George Mantello and printed on June 6 by "The New York Times". Auschwitz Plans originating with the Polish government were provided to the U.K foreign ministry in August 1944.
Starting with a plea from the Slovakian rabbi Chaim Michael Dov Weissmandl in May 1944, there was a growing campaign by Jewish organizations to persuade the Allies to bomb Auschwitz or the railway lines leading to it. At one point British Prime Minister Winston Churchill ordered that such a plan be prepared, but he was told that precision bombing the camp to free the prisoners or disrupt the railway was not technically feasible.
In 1978, historian David S. Wyman published an essay titled "Why Auschwitz Was Never Bombed", arguing that the US Air Force had the capability to attack Auschwitz and should have done so; books by Bernard Wasserstein and Martin Gilbert raised similar questions about British inaction. Since the 1990s, other historians have argued that Allied bombing accuracy was not sufficient for Wyman's proposed attack, and that counterfactual history is an inherently problematic endeavor. The controversy over this decision has lasted to the present day in both countries.
Individual escape attempts.
At least 802 prisoners attempted to escape from the Auschwitz camps, mostly Polish or Soviet prisoners fleeing from work sites outside the camp. 144 were successful. The fates of 331 of the escapees are unknown. A common punishment for escape attempts was death by starvation; the families of successful escapees were sometimes arrested and interned in Auschwitz and prominently displayed to deter others. If someone did manage to escape, the SS picked ten people at random from the prisoner's block and starved them to death.
One daring escape from Auschwitz was staged by Ukrainian Eugeniusz Bendera and three Poles, Kazimierz Piechowski, Stanisław Gustaw Jaster, and Józef Lempart, on June 20, 1942. After breaking into a warehouse, the four dressed as members of the "SS-Totenkopfverbände" (the SS units responsible for concentration camps), armed themselves, and stole an SS staff car, which they then drove unchallenged through the main gate.
On June 24, 1944, a Belgian Jewish woman, Mala Zimetbaum, escaped with her Polish boyfriend, Edek Galinski, also in stolen SS uniforms. They were later recaptured, tortured, and executed by the SS.
Birkenau revolt.
The "Sonderkommando" units were aware that as witnesses to the killings, they themselves would eventually be killed to hide Nazi crimes. Though they knew that it would mean their deaths, the "Sonderkommando" of Birkenau "Kommando" III staged an uprising on October 7, 1944, following an announcement that some of them would be selected to be "transferred to another camp"—a common Nazi ruse for the murder of prisoners. The "Sonderkommando" attacked the SS guards with stones, axes, and makeshift hand grenades. As the SS set up machine guns to attack the prisoners in Crematorium IV, the "Sonderkommando" in Crematorium II also revolted, some of them managing to escape the compound. The rebellion was suppressed by nightfall.
Ultimately, three SS guards were killed—one of whom was burned alive by the prisoners in the oven of Crematorium II—and 250 "Sonderkommando" were killed. Hundreds of prisoners escaped, but were all soon captured and executed, along with an additional group who participated in the revolt. Crematorium IV was destroyed in the fighting, and a group of prisoners in the gas chamber of Crematorium V was spared in the chaos.
Legacy.
In the decades since its liberation, Auschwitz has become a primary symbol of the Holocaust. Historian Timothy D. Snyder attributes this to the camp's high death toll as well as its "unusual combination of an industrial camp complex and a killing facility", which left behind far more witnesses than single-purpose killing facilities such as Chełmno or Treblinka. The United Nations General Assembly has designated January 27, the date of the camp's liberation, as International Holocaust Remembrance Day. In a speech on the fiftieth anniversary of the liberation, German chancellor Helmut Kohl described Auschwitz as the "darkest and most horrific chapter of German history".
Notable memoirists of the camp include Primo Levi, Elie Wiesel, and Tadeusz Borowski. In "If This Is a Man", Levi wrote that the concentration camps represented the epitome of the totalitarian system:
[N]ever has there existed a state that was really "totalitarian." ... Never has some form of reaction, a corrective of the total tyranny, been lacking, not even in the Third Reich or Stalin's Soviet Union: in both cases, public opinion, the magistrature, the foreign press, the churches, the feeling for justice and humanity that ten or twenty years of tyranny were not enough to eradicate, have to a greater or lesser extent acted as a brake. Only in the "Lager" [camp] was the restraint from below non-existent, and the power of these small satraps absolute. 
Psychiatrist Viktor Frankl drew on his imprisonment at Auschwitz in composing "Man's Search for Meaning" (1946), one of the most widely read works about the camp. An existentialist work, the book argues that individuals can find purpose even among great suffering, and that this sense of purpose sustains them. Wiesel wrote about his own imprisonment at Auschwitz in "Night" (1960) and other works, and became a prominent spokesman against ethnic violence. In 1986, he was awarded the Nobel Peace Prize.
Camp survivor Simone Veil was later elected President of the European Parliament, serving from 1979–82. Two Auschwitz victims—Maximilian Kolbe, a priest who volunteered to die by starvation in place of a stranger, and Edith Stein, a Jewish convert to Catholicism—were later named saints of the Roman Catholic Church.
Auschwitz-Birkenau State Museum.
On July 2, 1947, the Polish government passed a law establishing a state memorial to the victims of Nazism on the site of the camp. In 1955, an exhibition opened displaying prisoner mug shots; hair, suitcases, and shoes taken from murdered prisoners; canisters of Zyklon B pellets; and other objects related to the killings. UNESCO added the camp to its list of World Heritage Sites in 1979. In 2011, the museum drew 1,400,000 visitors.
Pope John Paul II performed mass over the train tracks leading to the camp on June 7, 1979. In the decades following his visit, controversies erupted over a group of Carmelite nuns founding a convent on the site and erecting a large cross originally used in the pope's mass. Protesters objected to what they saw as Christianization of the site, while others argued that the cross's presence effectively recognized the camp's Catholic victims.
The 5 m, 41 kg wrought-iron "Arbeit macht frei" sign over the entrance to Auschwitz I was stolen on December 18, 2009. Authorities temporarily replaced the stolen sign with a replica. Police found the sign, cut into three parts, in northern Poland two days later. "Aftonbladet" reported that the sign had been stolen by Polish thieves on behalf of a Swedish right-wing extremist group hoping to use proceeds from the proposed sale of the sign to a collector of Nazi memorabilia, to finance a series of terror attacks aimed at influencing voters in upcoming Swedish parliamentary elections. Former Swedish neo-Nazi Anders Högström was convicted in Poland and sentenced to serve two years eight months in a Swedish prison, while five Polish men who had acted on his behalf served prison time in Poland.
On September 4, 2003, three Israeli Air Force F-15 Eagles performed a fly-over of Auschwitz-Birkenau during a ceremony at the camp below. The flight was led by Major-General Amir Eshel, the son of Holocaust survivors.
On January 27, 2015, some 300 Auschwitz survivors and other guests gathered under a giant tent at the entrance to Auschwitz II Birkenau to commemorate the 70th anniversary of the camp's liberation. Attendees included president of the World Jewish Congress Ronald Lauder, film director Steven Spielberg, and world leaders such as Polish president Bronisław Komorowski and King Willem-Alexander of the Netherlands. As the number of remaining survivors decreases each year, the attendance at the event is unlikely to be surpassed at future major anniversaries. Commemorations also took place at Yad Vashem in Israel, Theresienstadt concentration camp, and in Berlin and Moscow.
References.
</dl>
Further reading.
</dl>

</doc>
<doc id="2007" url="http://en.wikipedia.org/wiki?curid=2007" title="Archery">
Archery

Archery is the practice or skill of using a bow to propel arrows. The word comes from the Latin "arcus". Historically, archery has been used for hunting and combat. In modern times, it is mainly a competitive sport and recreational activity. A person who participates in archery is typically called an "archer" or a "bowman"—and a person who is fond of or an expert at archery is sometimes called a toxophilite.
History.
The bow seems to have been invented in the later Paleolithic or early Mesolithic periods. The oldest signs of its use in Europe come from the Stellmoor in the Ahrensburg valley north of Hamburg, Germany and dates from the late Paleolithic, about 10,000–9000 BCE. The arrows were made of pine and consisted of a mainshaft and a 15 – long fore shaft with a flint point. There are no definite earlier bows; previous pointed shafts are known, but may have been launched by spear-throwers rather than bows. The oldest bows known so far come from the Holmegård swamp in Denmark. Bows eventually replaced the spear-thrower as the predominant means for launching shafted projectiles, on every continent except Australasia, though spear-throwers persisted alongside the bow in parts of the Americas, notably Mexico and among the Inuit.
Bows and arrows have been present in Egyptian culture since its predynastic origins. In the Levant, artifacts that could be arrow-shaft straighteners are known from the Natufian culture, (c. 12,800–10,300 BP (before present)) onwards. The Khiamian and PPN A shouldered Khiam-points may well be arrowheads.
Classical civilizations, notably the Assyrians, Armenians, Persians, Parthians, Indians, Koreans, Chinese, and Japanese fielded large numbers of archers in their armies. The English longbow proved its worth for the first time in Continental warfare at the Battle of Crécy. In the Americas archery was widespread at European contact.
Archery was highly developed in Asia. The Sanskrit term for archery, dhanurveda, came to refer to martial arts in general. In East Asia, Goguryeo, one of the Three Kingdoms of Korea was well known for its regiments of exceptionally skilled archers.
Mounted archery.
Central Asian tribesmen (after the domestication of the horse) and American Plains Indians (after gaining access to horses) became extremely adept at archery on horseback. Lightly armoured, but highly mobile archers were excellently suited to warfare in the Central Asian steppes, and they formed a large part of armies that repeatedly conquered large areas of Eurasia. Shorter bows are more suited to use on horseback, and the composite bow enabled mounted archers to use powerful weapons. Empires throughout the Eurasian landmass often strongly associated their respective "barbarian" counterparts with the usage of the bow and arrow, to the point where powerful states like the Han Dynasty referred to their neighbours, the Xiong-nu, as "Those Who Draw the Bow" For example, Xiong-nu mounted bowmen made them more than a match for the Han military, and their threat was at least partially responsible for Chinese expansion into the Ordos region, to create a stronger, more powerful buffer zone against them. It is possible that "barbarian" peoples were responsible for introducing archery or certain types of bows to their "civilized" counterparts—the Xiong-nu and the Han being one example. Similarly, short bows seem to have been introduced to Japan by northeast Asian groups.
Decline of archery.
The development of firearms rendered bows obsolete in warfare. Despite the high social status, ongoing utility, and widespread pleasure of archery in Armenia, China, Egypt, England, America, India, Japan, Korea, Turkey and elsewhere, almost every culture that gained access to even early firearms used them widely, to the neglect of archery. Early firearms were vastly inferior in rate-of-fire, and were very susceptible to wet weather. However, they had longer effective range and were tactically superior in the common situation of soldiers shooting at each other from behind obstructions. They also required significantly less training to use properly, in particular penetrating steel armour without any need to develop special musculature. Armies equipped with guns could thus provide superior firepower, and highly trained archers became obsolete on the battlefield. However, the bow and arrow is still an effective weapon, and archers have seen action in the 21st century. Traditional archery remains in use for sport, and for hunting in many areas.
Eighteenth-century revival.
Early recreational archery societies included the Finsbury Archers and the Kilwinning Papingo, established in 1688. The latter held competitions in which archers had to dislodge a wooden parrot from the top of an abbey tower. The Company of Scottish Archers was formed in 1676 and is one of the oldest sporting bodies in the world. It remained a small and scattered pastime, however, until the late 18th century when it experienced a fashionable revival among the aristocracy. Sir Ashton Lever, an antiquarian and collector, formed the Toxophilite Society in London in 1781, with the patronage of George, the Prince of Wales.
Archery societies were set up across the country, each with its own strict entry criteria and outlandish costumes. Recreational archery soon became extravagant social and ceremonial events for the nobility, complete with flags, music and 21 gun salutes for the competitors. The clubs were "the drawing rooms of the great country houses placed outside" and thus came to play an important role in the social networks of the local upper class. As well as its emphasis on display and status, the sport was notable for its popularity with females. Young women could not only compete in the contests but retain and show off their sexuality while doing so. Thus, archery came to act as a forum for introductions, flirtation and romance. It was often consciously styled in the manner of a Medieval tournament with titles and laurel wreaths being presented as a reward to the victor. General meetings were held from 1789, in which local lodges convened together to standardise the rules and ceremonies. Archery was also co-opted as a distinctively British tradition, dating back to the lore of Robin Hood and it served as a patriotic form of entertainment at a time of political tension in Europe. The societies were also elitist, and the new middle class bourgeoisie were excluded from the clubs due to their lack of social status.
After the Napoleonic Wars, the sport became increasingly popular among all classes, and it was framed as a nostalgic reimagining of the preindustrial rural Britain. Particularly influential was Sir Walter Scott's 1819 novel, "Ivanhoe" that depicted the heroic character Lockseley winning an archery tournament.
A modern sport.
The 1840s saw the first attempts at turning the recreation into a modern sport. The first Grand National Archery Society meeting was held in York in 1844 and over the next decade the extravagant and festive practices of the past were gradually whittled away and the rules were standardised as the 'York Round' - a series of shoots at 60, 80, and 100 yards. Horace A. Ford helped to improve archery standards and pioneered new archery techniques. He won the Grand National 11 times in a row and published a highly influential guide to the sport in 1856.
Towards the end of the 19th century, the sport experienced declining participation as alternative sports such as croquet and tennis became more popular among the middle class. By 1889, just 50 archery clubs were left in Britain, but it was still included as a sport at the 1900 Paris Olympics.
In the United States, primitive archery was revived in the early 20th century. The last of the Yahi Indian tribe, a native known as Ishi, came out of hiding in California in 1911. His doctor, Saxton Pope, learned many of Ishi's traditional archery skills, and popularized them. The Pope and Young Club, founded in 1961 and named in honor of Pope and his friend, Arthur Young, became one of North America's leading bowhunting and conservation organizations. Founded as a nonprofit scientific organization, the Club was patterned after the prestigious Boone and Crockett Club and advocated responsible bowhunting by promoting quality, fair chase hunting, and sound conservation practices.
From the 1920s, professional engineers took an interest in archery, previously the exclusive field of traditional craft experts. They led the commercial development of new forms of bow including the modern recurve and compound bow. These modern forms are now dominant in modern Western archery; traditional bows are in a minority. In the 1980s, the skills of traditional archery were revived by American enthusiasts, and combined with the new scientific understanding. Much of this expertise is available in the "Traditional Bowyer's Bibles" (see Additional reading). Modern game archery owes much of its success to Fred Bear, an American bow hunter and bow manufacturer.
Mythology.
Deities and heroes in several mythologies are described as archers, including the Greek Artemis and Apollo, the Roman Diana and Cupid, the Germanic Agilaz, continuing in legends like those of Wilhelm Tell, Palnetoke, or Robin Hood. Armenian Hayk and Babylonian Marduk, Indian Arjuna, Abhimanyu, Eklavya, Karna, Bhishma, Drona, Parashurama Rama, and Shiva, and Persian Arash were all archers. Earlier Greek representations of Heracles normally depict him as an archer.
The Nymphai Hyperboreioi (Νύμφαι Ὑπερβόρειοι) were worshipped on the Greek island of Delos as attendants of Artemis, presiding over aspects of archery; Hekaerge (Ἑκαέργη), represented distancing, Loxo (Λοξώ), trajectory, and Oupis (Οὖπις), aim.
In East Asia, Yi the archer and his apprentice Feng Meng appear in several early Chinese myths, and the historical character of Zhou Tong features in many fictional forms. Jumong, the first Taewang of the Goguryeo kingdom of the Three Kingdoms of Korea, is claimed by legend to have been a near-godlike archer. Archery features in the story of Oguz Khagan.
In West African Yoruba belief, Osoosi is one of several deities of the hunt who are identified with bow and arrow iconography and other insignia associated with archery.
Equipment.
Types of bows.
While there is great variety in the construction details of bows (both historic and modern), all bows consist of a string attached to elastic limbs that store mechanical energy imparted by the user drawing the string. Bows may be broadly split into two categories: those drawn by pulling the string directly and those that use a mechanism to pull the string.
Directly drawn bows may be further divided based upon differences in the method of limb construction, notable examples being self bows, laminated bows and composite bows. Bows can also be classified by the bow shape of the limbs when unstrung; in contrast to traditional European straight bows, a recurve bow and some types of Longbow have tips that curve away from the archer when the bow is unstrung. The cross-section of the limb also varies; the classic longbow is a tall bow with narrow limbs that are D-shaped in cross section, and the flatbow has flat wide limbs that are approximately rectangular in cross-section. The classic D-shape comes from the use of the wood of the yew tree. The sap-wood is best suited to the tension on the back of the bow, and the heart-wood to the compression on the belly. Hence, a cross-section of a yew longbow shows the narrow, light-coloured sap-wood on the 'straight' part of the D, and the red/orange heartwood forms the curved part of the D, to balance the mechanical tension/compression stress. Cable-backed bows use cords as the back of the bow; the draw weight of the bow can be adjusted by changing the tension of the cable. They were widespread among Inuit who lacked easy access to good bow wood. One variety of cable-backed bow is the Penobscot bow or Wabenaki bow, invented by Frank Loring (Chief Big Thunder) about 1900. It consists of a small bow attached by cables on the back of a larger main bow.
Compound bows are designed to reduce the force required to hold the string at full draw, hence allowing the archer more time to aim with less muscular stress. Most compound designs use cams or elliptical wheels on the ends of the limbs to achieve this. A typical let-off is anywhere from 65%–80%. For example, a 60-pound bow with 80% let-off only requires 12 pounds of force to hold at full draw. Up to 99% let-off is possible. The compound bow was invented by Holless Wilbur Allen in the 1960s (a US patent was filed in 1966 and granted in 1969) and it has become the most widely used type of bow for all forms of archery in North America.
Mechanically drawn bows typically have a stock or other mounting, such as the crossbow. Crossbows typically have shorter draw lengths compared to compound bows. Because of this, heavier draw weights are required to achieve the same energy transfer to the arrow. These mechanically drawn bows also have devices to hold the tension when the bow is fully drawn. They are not limited by the strength of a single archer and larger varieties have been used as siege engines.
Types of arrows and fletchings.
The most common form of arrow consists of a shaft with an arrowhead attached to the front end and with fletchings and a nock attached to the other end. Arrows across time and history are normally carried in a container known as a quiver, which can take many different forms. Shafts of arrows are typically composed of solid wood, bamboo fiberglass, aluminium alloy, carbon fiber, or composite materials. Wooden arrows are prone to warping. Fiberglass arrows are brittle, but can be produced to uniform specifications easily. Aluminium shafts were a very popular high-performance choice in the latter half of the 20th century due to their straightness, lighter weight, and subsequently higher speed and flatter trajectories. Carbon fiber arrows became popular in the 1990s and are very light, flying even faster and flatter than aluminium arrows. Today, arrows made up of composite materials are the most popular tournament arrows at Olympic Events, especially the Easton X10 and A/C/E.
The arrowhead is the primary functional component of the arrow. Some arrows may simply use a sharpened tip of the solid shaft, but separate arrowheads are far more common, usually from metal, stone, or other hard materials. The most commonly used forms are target points, field points, and broadheads, although there are also other types, such as bodkin, judo, and blunt heads.
Fletching is traditionally made from bird feathers. Also solid plastic vanes and thin sheetlike spin vanes are used. They are attached near the nock (rear) end of the arrow with thin double sided tape, glue, or, traditionally, sinew. Three fletches is the most common configuration in all cultures, though as many as six have been used. Two makes the arrow unstable in flight. When "three-fletched" the fletches are equally spaced around the shaft with one placed such that it is perpendicular to the bow when nocked on the string (though with modern equipment, variations are seen especially when using the modern spin vanes). This fletch is called the "index fletch" or "cock feather" (also known as "the odd vane out" or "the nocking vane") and the others are sometimes called the "hen feathers". Commonly, the cock feather is of a different color. However, if archers are using fletching made of feather or similar material, they may use same color vanes, as different dyes can give varying stiffness to vanes, resulting in less precision. When "four-fletched", often two opposing fletches are cock feathers and occasionally the fletches are not evenly spaced.
The fletching may be either "parabolic" (short feathers in a smooth parabolic curve) or "shield" (generally shaped like half of a narrow shield) cut and is often attached at an angle, known as "helical" fletching, to introduce a stabilizing spin to the arrow while in flight. Whether helicial or straight fletched, when natural fletching (bird feathers) are used it is critical that all feathers come from the same side of the bird. Oversized fletchings can be used to accentuate drag and thus limit the range of the arrow significantly; these arrows are called "flu-flus". Misplacement of fletchings can often change the arrow's flight path dramatically.
Bow string.
Dacron and other modern materials offer high strength for their weight and are used on most modern bows. Linen and other traditional materials are still used on traditional bows. Almost any fiber can be made into a bow string. The author of "Arab Archery" suggests the hide of a young, emaciated camel. Njál's saga describes the refusal of a wife, Hallgerður, to cut her hair to make an emergency bowstring for her husband, Gunnar Hámundarson, who is then killed.
Protective equipment.
Most archers wear a bracer (also known as an arm-guard) to protect the inside of the bow arm from being hit by the string and prevent clothing from catching the bow string. The bracer does not brace the arm; the word comes from the armoury term "brassard", meaning an armoured sleeve or badge. The Navajo people have developed highly ornamented bracers as non-functional items of adornment. Some archers (mostly women) also wear protection on their chests, called chestguards or plastrons. The myth of the Amazons was that they had one breast removed to solve this problem. Roger Ascham mentions one archer, presumably with an unusual shooting style, who wore a leather guard for his face.
The drawing digits are normally protected by a leather tab, glove, or thumb ring. A simple tab of leather is commonly used, as is a skeleton glove. Medieval Europeans probably used a complete leather glove.
Eurasiatic archers who used the thumb or Mongolian draw protected their thumbs, usually with leather according to the author of "Arab Archery", but also with special rings of various hard materials. Many surviving Turkish and Chinese examples are works of considerable art. Some are so highly ornamented that the users could not have used them to loose an arrow. Possibly these were items of personal adornment, and hence value, remaining extant whilst leather had virtually no intrinsic value and would also deteriorate with time. In traditional Japanese archery a special glove is used that has a ridge to assist in drawing the string.
Release aids.
A release aid is a mechanical device designed to give a crisp and precise loose of arrows from a compound bow. In the most commonly used, the string is released by a finger-operated trigger mechanism, held in the archer's hand or attached to their wrist. In another type, known as a back-tension release, the string is automatically released when drawn to a pre-determined tension.
Stabilisers.
Stabilisers are mounted usually on the front of the bow below the handle and on the right side, below the handle to help aiming by keeping the bow steady.
Shooting technique and form.
The standard convention on teaching archery is to hold the bow depending upon eye dominance (though in modern Kyudo all archers are trained to hold the bow in the left hand). Therefore, if one is right-eye dominant, they would hold the bow in the left hand and draw the string with the right hand. However, not everyone agrees with this line of thought. A smoother, and more fluid release of the string will produce the most consistently repeatable shots, and therefore may provide greater accuracy of the arrow flight. Some believe that the hand with the greatest dexterity should therefore be the hand that draws and releases the string. Either eye can be used for aiming, and the less dominant eye can be trained over time to become more effective for use. To assist with this, an eye patch can be temporarily worn over the dominant eye.
The hand that holds the bow is referred to as the "bow hand" and its arm the "bow arm". The opposite hand is called the "drawing hand" or "string hand". Terms such as "bow shoulder" or "string elbow" follow the same convention.
If shooting according to eye dominance, right-eye-dominant archers shooting conventionally hold the bow with their left hand. If shooting according to hand dexterity, the archer draws the string with the hand that possesses the greatest dexterity, regardless of eye dominance.
Modern form.
To shoot an arrow, an archer first assumes the correct stance. The body should be at or nearly perpendicular to the target and the shooting line, with the feet placed shoulder-width apart. As an archer progresses from beginner to a more advanced level other stances such as the "open stance" or the "closed stance" may be used, although many choose to stick with a "neutral stance". Each archer has a particular preference, but mostly this term indicates that the leg furthest from the shooting line is a half to a whole foot-length from the other foot, on the ground.
To load, the bow is pointed toward the ground, tipped slightly clockwise of vertical (for a right handed shooter) and the shaft of the arrow is placed on the arrow rest or shelf. The back of the arrow is attached to the bowstring with the nock (a small locking groove located at the proximal end of the arrow). This step is called "nocking the arrow". Typical arrows with three vanes should be oriented such that a single vane, the "cock feather", is pointing away from the bow, to improve the clearance of the arrow as it passes the arrow rest.
A compound bow is fitted with a special type of arrow rest, known as a launcher, and the arrow is usually loaded with the cock feather/vane pointed either up, or down, depending upon the type of launcher being used.
The bowstring and arrow are held with three fingers, or with a mechanical arrow release. Most commonly, for finger shooters, the index finger is placed above the arrow and the next two fingers below, although several other techniques have their adherents around the world, involving three fingers below the arrow, or an arrow pinching technique. "Instinctive" shooting is a technique eschewing sights and is often preferred by traditional archers (shooters of longbows and recurves). In either the split finger or three finger under case, the string is usually placed in either the first or second joint of the fingers.
Another type of string hold, used on traditional bows, is the type favoured by the Mongol warriors, known as the "thumb release", style. This involves using the thumb to draw the string, with the fingers curling around the thumb to add some support. To release the string, the fingers are opened out and the thumb relaxes to allow the string to slide off the thumb. When using this type of release, the arrow should rest on the same side of the bow as the drawing hand i.e. Left hand draw = arrow on left side of bow.
The archer then raises the bow and draws the string, with varying alignments for vertical versus slightly canted bow positions. This is often one fluid motion for shooters of recurves and longbows, which tend to vary from archer to archer. Compound shooters often experience a slight jerk during the drawback, at around midpoint where the draw weight is at its maximum—before relaxing into a comfortable stable full draw position. The archer draws the string hand towards the face, where it should rest lightly at a fixed "anchor point". This point is consistent from shot to shot, and is usually at the corner of the mouth, on the chin, to the cheek, or to the ear, depending on preferred shooting style. The archer holds the bow arm outwards, toward the target. The elbow of this arm should be rotated so that the inner elbow is perpendicular to the ground, though archers with hyper extendable elbows tend to angle the inner elbow toward the ground, as exemplified by the Korean archer Jang Yong-Ho. This keeps the forearm out of the way of the bowstring.
In modern form, the archer stands erect, forming a "T". The archer's lower trapezius muscles are used to pull the arrow to the anchor point. Some modern bows are equipped with a mechanical device, called a clicker, which produces a clicking sound when the archer reaches the correct draw length. In contrast, traditional English Longbow shooters step "into the bow", exerting force with both the bow arm and the string hand arm simultaneously, especially when using bows having draw weights from 100 lbs to over 175 lbs. Heavily-stacked traditional bows (recurves, long bows, and the like) are released immediately upon reaching full draw at maximum weight, whereas compound bows reach their maximum weight in or around mid-draw, dropping holding weight significantly at full draw. Compound bows are often held at full draw for a short time to achieve maximum accuracy.
The arrow is typically released by relaxing the fingers of the drawing hand (see Bow draw), or triggering the mechanical release aid. Usually the release aims to keep the drawing arm rigid, the bow hand relaxed, and the arrow is moved back using the back muscles, as opposed to using just arm motions. An archer should also pay attention to the recoil or "follow through" of his or her body, as it may indicate problems with form (technique) that affect accuracy.
Aiming methods.
There are two main forms of aiming in archery: using a mechanical or fixed sight, or barebow.
Mechanical sights can be affixed to the bow to aid in aiming. They can be as simple as a pin, or may use optics with magnification. They usually also have a peep sight (rear sight) built into the string, which aids in a consistent anchor point. Modern compound bows automatically limit the draw length to give a consistent arrow velocity, while traditional bows allow great variation in draw length. Some bows use mechanical methods to make the draw length consistent. Barebow archers often use a sight picture, which includes the target, the bow, the hand, the arrow shaft and the arrow tip, as seen at the same time by the archer. With a fixed "anchor point" (where the string is brought to, or close to, the face), and a fully extended bow arm, successive shots taken with the sight picture in the same position fall on the same point. This lets the archer adjust aim with successive shots to achieve accuracy.
Modern archery equipment usually includes sights. Instinctive aiming is used by many archers who use traditional bows. The two most common forms of a non-mechanical release are split-finger and three-under. Split-finger aiming requires the archer to place the index finger above the nocked arrow, while the middle and ring fingers are both placed below. Three-under aiming places the index, middle, and ring fingers under the nocked arrow. This technique allows the archer to better look down the arrow since the back of the arrow is closer to the dominant eye, and is commonly called "gun barreling" (referring to common aiming techniques used with firearms).
When using short bows or shooting from horseback, it is difficult to use the sight picture. The archer may look at the target, but without including the weapon in the field of accurate view. Aiming then involves hand-eye coordination—which includes proprioception and motor-muscle memory, similar to that used when throwing a ball. With sufficient practice, such archers can normally achieve good practical accuracy for hunting or for war. Aiming without a sight picture may allow more rapid shooting.
Instinctive shooting is a style of shooting that includes the barebow aiming method that relies heavily upon the subconscious mind, proprioception, and motor/muscle memory to make aiming adjustments; the term used to refer to a general category of archers who did not use a mechanical or fixed sight.
Physics.
When a projectile is thrown by hand, the speed of the projectile is determined by the kinetic energy imparted by the thrower's muscles performing work. However, the energy must be imparted over a limited distance (determined by arm length) and therefore (because the projectile is accelerating) over a limited time, so the limiting factor is not work but rather power, which determined how much energy can be added in the limited time available. Power generated by muscles, however, is limited by force–velocity relationship, and even at the optimal contraction speed for power production, total work by the muscle is less than half of what it would be if the muscle contracted over the same distance at slow speeds, resulting in less than 1/4 the projectile launch velocity possible without the limitations of the force–velocity relationship.
When a bow is used, the muscles are able to perform work much more slowly, resulting in greater force and greater work done. This work is stored in the bow as elastic potential energy, and when the bowstring is released, this stored energy is imparted to the arrow much more quickly than can be delivered by the muscles, resulting in much higher velocity and, hence, greater distance. This same process is employed by frogs, which use elastic tendons to increase jumping distance. In archery, some energy dissipates through elastic hysteresis, reducing the overall amount released when the bow is shot. Of the remaining energy, some is dampened both by the limbs of the bow and the bowstring. Depending on the arrow's elasticity, some of the energy is also absorbed by compressing the arrow, primarily because the release of the bowstring is rarely in line with the arrow shaft, causing it to flex out to one side. This is because the bowstring accelerates faster than the archer's fingers can open, and consequently some sideways motion is imparted to the string, and hence arrow nock, as the power and speed of the bow pulls the string off the opening fingers.
Even with a release aid mechanism some of this effect is usually experienced, since the string always accelerates faster than the retaining part of the mechanism. This makes the arrow oscillate in flight—its center flexing to one side and then the other repeatedly, gradually reducing as the arrow's flight proceeds. This is clearly visible in high-speed photography of arrows at discharge. A direct effect of these energy transfers can clearly be seen when dry firing. Dry firing refers to releasing the bow string without a nocked arrow. Because there is no arrow to receive the stored potential energy, all the energy stays in the bow. Some have suggested that dry firing may cause physical damage to the bow, such as cracks and fractures—and because most bows are not specifically made to handle the high amounts of energy dry firing produces, should never be done.
Modern arrows are made to a specified 'spine', or stiffness rating, to maintain matched flexing and hence accuracy of aim. This flexing can be a desirable feature, since, when the spine of the shaft is matched to the acceleration of the bow(string), the arrow bends or flexes around the bow and any arrow-rest, and consequently the arrow, and fletchings, have an un-impeded flight. This feature is known as the archer's paradox. It maintains accuracy, for if part of the arrow struck a glancing blow on discharge, some inconsistency would be present, and the excellent accuracy of modern equipment would not be achieved.
The accurate flight of an arrow is dependent on its fletching. The arrow's manufacturer (a "fletcher") can arrange fletching to cause the arrow to rotate along its axis. This improves accuracy by evening pressure buildups that would otherwise cause the arrow to "plane" on the air in a random direction after shooting. Even with a carefully made arrow, the slightest imperfection or air movement causes some unbalanced turbulence in air flow. Consequently, rotation creates an equalization of such turbulence, which, overall, maintains the intended direction of flight i.e. accuracy. This rotation is not to be confused with the rapid gyroscopic rotation of a rifle bullet. Fletching that is not arranged to induce rotation still improves accuracy by causing a restoring drag any time the arrow tilts from its intended direction of travel.
The innovative aspect of the invention of the bow and arrow was the amount of power delivered to an extremely small area by the arrow. The huge ratio of length vs. cross sectional area, coupled with velocity, made the arrow more powerful than any other hand held weapon until firearms were invented. Arrows can spread or concentrate force, depending on the application. Practice arrows, for instance, have a blunt tip that spreads the force over a wider area to reduce the risk of injury or limit penetration. Arrows designed to pierce armor in the Middle Ages used a very narrow and sharp tip ("bodkinhead") to concentrate the force. Arrows used for hunting used a narrow tip ("broadhead") that widens further, to facilitate both penetration and a large wound.
Hunting.
Using archery to take game animals is known as "bow hunting". Bow hunting differs markedly from hunting with firearms, as distance between hunter and prey must be much shorter to ensure a humane kill. The skills and practices of bow hunting therefore emphasize very close approach to the prey, whether by still hunting, stalking, or waiting in a blind or tree stand. In many countries, including much of the United States, bow hunting for large and small game is legal. Bow hunters generally enjoy longer seasons than are allowed with other forms of hunting such as black powder, shotgun, or rifle. Usually, compound bows are used for large game hunting and may feature fiber optic sights and other enhancements. Using a bow and arrow to take fish is known as "bow fishing".
Modern competitive archery.
Competitive archery involves shooting arrows at a target for accuracy from a set distance or distances. This is the most popular form of competitive archery worldwide and is called target archery. A form particularly popular in Europe and America is field archery, shot at targets generally set at various distances in a wooded setting. Competitive archery in the United States is governed by USA Archery and National Field Archery Association (NFAA), which also certifies instructors.
Para-Archery is an adaptation of archery for athletes with a disability. It is governed by the World Archery Federation (WA), and is one of the sports in the Summer Paralympic Games. There are also several other lesser-known and historical forms of archery, as well as archery novelty games and flight archery, where the aim is to shoot the greatest distance.
External links.
 at DMOZ

</doc>
<doc id="2009" url="http://en.wikipedia.org/wiki?curid=2009" title="Alvar Aalto">
Alvar Aalto

Hugo Alvar Henrik Aalto (3 February 1898 – 11 May 1976) was a Finnish architect and designer, as well as a sculptor and painter. His work includes architecture, furniture, textiles and glassware. Aalto's early career runs in parallel with the rapid economic growth and industrialization of Finland during the first half of the twentieth century and many of his clients were industrialists; among these were the Ahlström-Gullichsen family. The span of his career, from the 1920s to the 1970s, is reflected in the styles of his work, ranging from Nordic Classicism of the early work, to a rational International Style Modernism during the 1930s to a more organic modernist style from the 1940s onwards. His furniture designs were considered Scandinavian Modern. What is typical for his entire career, however, is a concern for design as a Gesamtkunstwerk, a "total work of art"; whereby he – together with his first wife Aino Aalto – would design not just the building, but give special treatments to the interior surfaces and design furniture, lamps, and furnishings and glassware. The Alvar Aalto Museum, designed by Aalto himself, is located in what is regarded as his home city Jyväskylä.
Biography.
Life.
Hugo Alvar Henrik Aalto was born in Kuortane, Finland. His father, Johan Henrik Aalto, was a Finnish-speaking land-surveyor and his mother, Selly (Selma) Matilda (née Hackstedt) was a Swedish-speaking postmistress. When Aalto was 5 years old, the family moved to Alajärvi, and from there to Jyväskylä in Central Finland. Aalto studied at the Jyväskylä Lyceum school, where he completed his basic education in 1916 and took drawing lessons from a local artist named Jonas Heiska. In 1916 he then enrolled to study architecture at the Helsinki University of Technology. His studies were interrupted by the Finnish Civil War, which he fought in. He fought on the side of the "White Army" and fought at the Battle of Länkipohja and the Battle of Tampere. He built his first piece of architecture while still a student, a house for his parents, at Alajärvi. Afterwards, he continued his education, graduating in 1921. In the summer of 1922 he began his official military service, finishing at the Hamina reserve officer training school, and was promoted to reserve second lieutenant in June 1923.
In 1920, while still a student, Aalto made his first trip abroad, travelling via Stockholm to Gothenburg, where he even briefly found work with the architect Arvid Bjerke. In 1922, he accomplished his first independent piece at the Industrial Exposition in Tampere. In 1923 he returned to Jyväskylä, where he opened his first architectural office, under the name 'Alvar Aalto, Architect and Monumental Artist'. At that same time he also wrote articles for the Jyväskylä newspaper "Sisä-Suomi" under the pseudonym Remus. During this time, he designed a number of small single-family houses in Jyväskylä, and the office's workload steadily increased. In 1925, he married architect Aino Marsio. Their honeymoon journey to Italy was Aalto's first trip there, though Aino had previously made a study trip there. The latter trip together sealed an intellectual bond with the culture of the Mediterranean region that was to remain important to Aalto for the rest of his life. On their return, they continued with a number of local projects, notably the Jyväskylä Worker's Club. However, the Aaltos moved their office to Turku in 1927, and started collaborating with architect Erik Bryggman. The office moved again in 1933 to Helsinki.
The Aaltos designed and built a joint house-office (1935–36) for themselves in Munkkiniemi, Helsinki, but later (1954–56) had a purpose-built office erected in the same neighbourhood – nowadays the former is a "house museum" and the latter the premises of the Alvar Aalto Academy. In 1926 the young Aaltos designed and had built a summer cottage in Alajärvi, Villa Flora. In 1938, the Aaltos visited the United States for the first time, ostensibly to visit the Finnish Pavilion, which they had designed, for the New York World Fair of the following year. Aino Aalto died of cancer in 1949. Aino and Alvar Aalto had 2 children, a daughter Johanna "Hanni" Alanen, born Aalto, 1925, and a son Hamilkar Aalto, 1928. In 1952 Aalto married architect Elissa Mäkiniemi (died 1994), who had been working as an assistant in his office. In 1952 Aalto designed and had built a summer cottage, the so-called Experimental House, for himself and his new wife in Muuratsalo in Central Finland. Alvar Aalto died on 11 May 1976, in Helsinki, and is buried in the Hietaniemi cemetery in Helsinki.
Architecture career.
Early career: classicism.
Although he is sometimes regarded as among the first and most influential architects of Nordic modernism, a closer examination of the historical facts reveals that Aalto (while a pioneer in Finland) closely followed and had personal contacts with other pioneers in Sweden, in particular Gunnar Asplund and Sven Markelius. What they and many others of that generation in the Nordic countries had in common was that they started off from a classical education and were first designing classical architecture, though what historians now call Nordic Classicism – a style that had been a reaction to the previous dominant style of National Romanticism – before moving, in the late 1920s, towards Modernism. On returning to Jyväskylä in 1923 to establish his own architect's office, Aalto busied himself with a number of single-family homes, all designed in the Nordic Classicism style, such as the manor-like house for his mother's cousin Terho Manner in Töysa in 1923, a summer villa for the Jyväskylä chief constable in 1923 and the Alatalo farmhouse in Tarvaala in 1924. During this period he also completed his first public buildings, the Jyväskylä Workers' Club in 1925, the Jyväskylä Defence Corps building in 1926 and the Seinäjoki Defence Corp building in 1924–29. Aalto also entered several architectural competitions for prestigious state public buildings, both in Finland and abroad, including the two competitions for the Finnish Parliament building in 1923 and 1924, the extension to the University of Helsinki in 1931, and the building to house the League of Nations in Geneva, Switzerland, in 1926–27. Furthermore, this was the period when Aalto was most prolific in his writings, with articles for professional journals and newspapers. Among his most well-known essays from this period are "Urban culture" (1924), "Temple baths on Jyväskylä ridge" (1925), "Abbé Coignard's sermon" (1925), and "From doorstep to living room" (1926).
Early career: functionalism.
The shift in Aalto's design approach from classicism to modernism is epitomised by the Viipuri Library (1927–35), which went through a transformation from an originally classical competition entry proposal to the completed high-modernist building. Yet his humanistic approach is in full evidence in the library: the interior displays natural materials, warm colours, and undulating lines. Due to problems over financing and a change of site, the Viipuri Library project lasted eight years, and during that same time he also designed the Turun Sanomat Building (1929–30) and Paimio Sanatorium (1929–32). Thus, the Turun Sanomat Building first heralded Aalto's move towards modernism, and this was then carried forward both in the Paimio Sanatorium and in the ongoing design for the library. Although the Turun Sanomat Building and Paimio Sanatorium are comparatively pure modernist works, they too carried the seeds of his questioning of such an orthodox modernist approach and a move to a more daring, synthetic attitude. It has been said that his work on two of these three buildings (not the Viipuri Library) showed similarities to Walter Gropius' style, in particular his work on the Bauhaus school of design in Dessau. His work on the Viipuri building started to show his individuality in a departure from the European norms.
Through Sven Markelius, Aalto became a member of the Congres Internationaux d'Architecture Moderne (CIAM), attending the second congress in Frankfurt in 1929 and the fourth congress in Athens in 1933, where he established a close friendship with László Moholy-Nagy, Sigfried Giedion and Philip Morton Shand. It was during this time that he followed closely the work of the main driving force behind the new modernism, Le Corbusier, and visited him in his Paris office several times in the following years.
It was not until the completion of the Paimio Sanatorium (1932) and Viipuri Library (1935) that Aalto first achieved world attention in architecture. His reputation grew in the USA following the critical reception of his design for the Finnish Pavilion at the 1939 New York World's Fair, described by Frank Lloyd Wright as a "work of genius". It could be said that Aalto's international reputation was sealed with his inclusion in the second edition of Sigfried Giedion's influential book on Modernist architecture, "Space, Time and Architecture: The growth of a new tradition" (1949), in which Aalto received more attention than any other Modernist architect, including Le Corbusier. In his analysis of Aalto, Giedion gave primacy to qualities that depart from direct functionality, such as mood, atmosphere, intensity of life and even national characteristics, declaring that "Finland is with Aalto wherever he goes".
In 1938, the Museum of Modern Art, in New York organized an exhibit that eventually went on a 12-city tour. Afterwards he visited America for the first time and gave a series of lectures at Yale.
Mid career: experimentation.
During the 1930s Alvar spent some time experimenting with laminated wood, making sculptures, and abstract reliefs, characterized by irregular curved forms. Utilizing this knowledge he was able to solve technical problems concerning the flexibility of wood and also of working out spatial issues in his designs. Aalto's early experiments with wood and his move away from a purist modernism would be tested in built form with the commission to design Villa Mairea (1939) in Noormarkku, the luxury home of the young industrialist couple Harry and Maire Gullichsen. It was Maire Gullichsen who acted as the main client, and she worked closely not only with Alvar but also Aino Aalto on the design, inspiring them to be more daring in their work. The original design was to include a private art gallery, but this was never built. The building forms a U-shape around a central inner "garden" the central feature of which is a kidney-shaped swimming pool. Adjacent to the pool is a sauna executed in a rustic style, alluding to both Finnish and Japanese precedents. The design of the house is a synthesis of numerous stylistic influences, from traditional Finnish vernacular to purist modernism, as well as influences from English and Japanese architecture. While the house is clearly intended for a wealthy family, Aalto nevertheless argued that it was also an experiment that would prove useful in the design of mass housing. It created zones for different activities within the structure.
His increased fame led to offers and commissions outside Finland. In 1941 he accepted an invitation as a visiting professor to Massachusetts Institute of Technology in the USA. This was during the Second World War, and he involved his students in designing low-cost, small-scale housing for the reconstruction of war-torn Finland. While teaching at MIT, Aalto also designed the student dormitory, Baker House, completed in 1948. The dormitory lay along the Charles River and its undulating form provided maximum view and ventilation for each resident. This building was the first building of Aalto's redbrick period. Originally used in Baker House to signify the Ivy League university tradition, on his return to Finland Aalto used it in a number of key buildings, in particular, in several of the buildings in the new Helsinki University of Technology campus (starting in 1950), Säynätsalo Town Hall (1952), Helsinki Pensions Institute (1954), Helsinki House of Culture (1958), as well as in his own summer house, the so-called Experimental House in Muuratsalo (1957).
In the 50's he immersed himself in his sculpting, be it with bronze, marble, or mixed media. This paid off as he produced an outstanding piece for the memorial of the Battle of Suomussalmi (1960), located on the battlefield. It consists of a leaning bronze pillar on a pedestal.
Mature career: monumentalism.
The early 1960s and 1970s (up until his death in 1976) were marked by key works in Helsinki, in particular the huge town plan for the void in centre of Helsinki adjacent to Töölö Bay and the vast railway yards, and marked on the edges by significant buildings such as the National Museum and the main railway station, both by Eliel Saarinen. In his town plan Aalto proposed a line of separate marble-clad buildings fronting the bay which would house various cultural institutions, including a concert hall, opera, museum of architecture and headquarters for the Finnish Academy. The scheme also extended into the Kamppi district with a series of tall office blocks. Aalto first presented his scheme in 1961, but it went through various modifications during the early 1960s. Only two fragments of the overall plan were ever realized: the Finlandia Hall concert hall (1976) fronting Töölö Bay, and an office building in the Kamppi district for the Helsinki Electricity Company (1975). The Miesian formal language of geometric grids employed in the buildings was also used by Aalto for other sites in Helsinki, including the Enso-Gutzeit building (1962), the Academic Bookstore (1962) and the SYP Bank building (1969).
Following Aalto's death in 1976 his office continued to operate under the direction of his widow, Elissa, completing works already to some extent designed. These works include the Jyväskylä City Theatre and Essen opera house. Since the death of Elissa Aalto the office has continued to operate as the Alvar Aalto Academy, giving advice on the restoration of Aalto buildings and organising the vast archive material.
Furniture career.
Whereas Aalto was famous for his architecture, his furniture designs were well thought of and are still popular today. He studied Josef Hoffmann and the Wiener Werkstätte, and for a period of time, worked under Eliel Saarinen. He also gained inspiration from Gebrüder Thonet. During the late 1920s and 1930s he, working closely with Aino Aalto, also focused a lot of his energy on furniture design, partly due to the decision to design much of the individual furniture pieces and lamps for the Paimio Sanatorium. Of particular significance was the experimentation in bent plywood chairs, most notably the so-called Paimio chair, which had been designed for the sitting tuberculosis patient. The Aaltos, together with visual arts promoter Maire Gullichsen and art historian Nils-Gustav Hahl founded the Artek company in 1935, ostensibly to sell Aalto products, but also other imported products. He became the first furniture designer to use the cantilever principle in chair design using wood.
Awards.
Aalto's awards included the Prince Eugen Medal in 1954, the Royal Gold Medal for Architecture from the Royal Institute of British Architects in 1957 and the Gold Medal from the American Institute of Architects in 1963. He was elected a Foreign Honorary Member of the American Academy of Arts and Sciences in 1957. He also was a member of the Academy of Finland, and was its president from 1963 to 1968. From 1925 to 1956 he was a member of the Congrès International d'Architecture Moderne. In 1960 he received an honorary doctorate at the Norwegian University of Science and Technology (NTNU).
Works.
Aalto's career spans the changes in style from (Nordic Classicism) to purist International Style Modernism to a more personal, synthetic and idiosyncratic Modernism. Aalto's wide field of design activity ranges from the large scale of city planning and architecture to interior design, furniture and glassware design and painting. It has been estimated that during his entire career Aalto designed over 500 individual buildings, approximately 300 of which were built, the vast majority of which are in Finland. He also has a few buildings in France, Germany, Italy and the USA.
Aalto's work with wood, was influenced by early Scandinavian architects; however, his experiments and departure from the norm brought attention to his ability to make wood do things not previously done. His techniques in the way he cut the beech tree, for example, and also his ability to use plywood as structural and aesthetic. Other examples include the rough-hewn vertical placement of logs at his pavilion at the Lapua expo, looking similar to a medieval barricade, at the orchestra platform at turku and the Paris expo at the World Fair, he used varying sizes and shapes of planks. Also at Paris and at Villa Mairea he utilized birch boarding in a vertical arrangement. Also his famous undulating walls and ceilings made of red pine. In his roofing, he created massive spans (155-foot at the covered statium at Otaniemi) all without tie rods. His stairway at Villa Mairea, he evokes feelings of a natural forest by binding beech wood with withes into columns.
Aalto claimed that his paintings were not made as individual artworks but as part of his process of architectural design, and many of his small-scale "sculptural" experiments with wood led to later larger architectural details and forms. These experiments also led to a number of patents: for example, he invented a new form of laminated bent-plywood furniture in 1932. His experimental method had been influenced by his meetings with various members of the Bauhaus design school, especially László Moholy-Nagy, whom he first met in 1930. Aalto's furniture was exhibited in London in 1935, to great critical acclaim, and to cope with the consumer demand Aalto, together with his wife Aino, Maire Gullichsen and Nils-Gustav Hahl founded the company Artek that same year. Aalto glassware (Aino as well as Alvar) is manufactured by Iittala. Aalto was one of the first architects outside of Germany, France, and the Netherlands to master modern architecture.
Aalto's 'High Stool' and 'Stool E60' (manufactured by Artek) are currently used in Apple Stores across the world to serve as seating for customers. Finished in black lacquer, the stools are used to seat customers at the 'Genius Bar' and also in other areas of the store at times when seating is required for a product workshop or special event. Aalto was also influential in bringing modern art to the Finnish people, in particular the work of his friends, Alexander Milne Calder and Fernand Léger.
Critique of Aalto's architecture.
As already mentioned, Aalto's international reputation was sealed with his inclusion in the second edition of Sigfried Giedion's influential book on Modernist architecture, "Space, Time and Architecture: The growth of a new tradition" (1949), in which Aalto received more attention than any other Modernist architect, including Le Corbusier. In his analysis of Aalto, Giedion gave primacy to qualities that depart from direct functionality, such as mood, atmosphere, intensity of life and even national characteristics, declaring that "Finland is with Aalto wherever he goes". However, a few more recent architecture critics and historians have questioned Aalto's position of influence in the canonic history. Italian Marxist architecture historians Manfredo Tafuri and Francesco Dal Co put forward the viewpoint that Aalto's "historical significance has perhaps been rather exaggerated; with Aalto we are outside of the great themes that have made the course of contemporary architecture so dramatic. The qualities of his works have a meaning only as masterful distractions, not subject to reproduction outside the remote reality in which they have their roots." Their viewpoint was propounded by their own priority given to urbanism, seeing Aalto as an anti-urban, and thus consequently disparaging what they regarded as peripheral non-urban areas of the world: "Essentially his architecture is not appropriate to urban typologies." Similarly concerned with the appropriateness of Aalto's form language, at the other end of the political spectrum, American postmodernist critic Charles Jencks made a claim for the need for buildings to signify meaning; however, he then lifted out Aalto's Pensions Institute building as an example of what he termed Aalto's 'soft paternalism': "Conceived as a fragmented mass to break up the feeling of bureaucracy, it succeeds all too well in being humane and killing the pensioner with kindness. The forms are familiar red brick and ribbon-strip windows broken by copper and bronze elements – all carried through with a literal-mindedness that borders on the soporific." But also during Aalto's lifetime he faced critique from his fellow architects in Finland, most notably Kirmo Mikkola and Juhani Pallasmaa; by the last decade of his life Aalto's work was seen as idiosyncratic and individualistic, when the opposing tendencies of rationalism and constructivism – often championed under left-wing politics – argued for anonymous virtually non-aesthetic architecture. Mikkola wrote of Aalto's late works: "Aalto has moved to his present baroque line..."
Memorials.
Aalto has been commemorated in a number of ways:
Sources.
</dl>
Further reading.
Göran Schildt has written and edited many books on Aalto, the most well-known being the three-volume biography, usually referred to as the definitive biography on Aalto.

</doc>
