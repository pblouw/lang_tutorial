<doc id="3746" url="http://en.wikipedia.org/wiki?curid=3746" title="Blade Runner">
Blade Runner

Blade Runner is a 1982 American neo-noir dystopian science fiction film directed by Ridley Scott and starring Harrison Ford, Rutger Hauer, Sean Young, and Edward James Olmos. The screenplay, written by Hampton Fancher and David Peoples, is a modified film adaptation of the 1968 novel "Do Androids Dream of Electric Sheep?" by Philip K. Dick.
The film depicts Los Angeles in November 2019, in which genetically engineered replicants, which are visually indistinguishable from adult humans, are manufactured by the powerful Tyrell Corporation as well as by other "mega-corporations" around the world. The use of replicants on Earth is banned and they are exclusively utilized for dangerous or menial work on off-world colonies. Replicants who defy the ban and return to Earth are hunted down and killed ("retired") by special police operatives known as "Blade Runners". The plot focuses on a desperate group of recently escaped replicants hiding in Los Angeles and the burnt-out expert Blade Runner, Rick Deckard (Harrison Ford), who reluctantly agrees to take on one more assignment to hunt them down.
"Blade Runner" initially polarized critics: some were displeased with the pacing, while others enjoyed its thematic complexity. The film performed poorly in North American theaters but has since become a cult film. It has been hailed for its production design, depicting a "retrofitted" future, and remains a leading example of the neo-noir genre. It brought the work of Philip K. Dick to the attention of Hollywood and several later films were based on his work. Ridley Scott regards "Blade Runner" as "probably" his most complete and personal film. In 1993, the film was selected for preservation in the United States National Film Registry by the Library of Congress as being "culturally, historically, or aesthetically significant". "Blade Runner" is now regarded by many critics as one of the best science fiction films ever made.
Seven versions of the film have been shown for various markets as a result of controversial changes made by film executives. A rushed "Director's Cut" was released in 1992 after a strong response to workprint screenings. This, in conjunction with its popularity as a video rental, made it one of the first films released on DVD, resulting in a basic disc with mediocre video and audio quality. In 2007, Warner Bros. released "The Final Cut", a 25th anniversary digitally remastered version which is the only one on which Scott had complete artistic freedom and was shown in select theaters and subsequently released on DVD, HD DVD, and Blu-ray Disc.
Plot.
In Los Angeles, November 2019, retired police officer Rick Deckard (Harrison Ford) is detained by officer Gaff (Edward James Olmos) and brought to his former supervisor, Bryant (M. Emmet Walsh). Deckard, whose job as a "Blade Runner" was to track down bioengineered beings known as replicants and destroy them, is informed that four have come to Earth illegally. As Tyrell Corporation Nexus-6 models, they have only a four-year lifespan and may have come to Earth to try to extend their lives.
Deckard watches a video of a Blade Runner named Holden administering the "Voight-Kampff" test designed to distinguish replicants from humans based on their emotional response to questions. The test subject, Leon (Brion James), shoots Holden after Holden asks about Leon's mother. Bryant wants Deckard to retire Leon and the other replicants: Roy Batty (Rutger Hauer), Zhora (Joanna Cassidy), and Pris (Daryl Hannah). Deckard initially refuses, but after Bryant ambiguously threatens him, he reluctantly agrees.
Deckard begins his investigation at the Tyrell Corporation to ensure that the test works on Nexus-6 models. While there, he discovers that Dr. Eldon Tyrell's (Joe Turkel) assistant Rachael (Sean Young) is an experimental replicant who believes herself to be human. Rachael has been given false memories to provide an "emotional cushion". As a result, a more extensive test is required to determine whether she is a replicant.
Events are then set into motion that pit Deckard's search for the replicants against their search for Tyrell to force him to extend their lives. Roy and Leon investigate a replicant eye-manufacturing laboratory and learn of J.F. Sebastian (William Sanderson), a gifted designer who works closely with Tyrell. Rachael visits Deckard at his apartment to prove her humanity by showing him a family photo, but after Deckard reveals that her memories are only implants taken from a real person, she leaves his apartment in tears. Meanwhile, Pris locates Sebastian and manipulates him to gain his trust.
While searching Leon's hotel room, Deckard finds a photo of Zhora and a synthetic snake scale that leads him to a strip club where Zhora works. Deckard kills Zhora and shortly after is told by Bryant to also kill Rachael, who has absconded from the Tyrell Corporation. Deckard spots Rachael in a crowd but is attacked by Leon. Rachael kills Leon using Deckard's gun, and the two return to Deckard's apartment, where he promises not to hunt her. Later they share an intimate moment; Rachael abruptly tries to leave, but Deckard physically restrains her and forces Rachael to kiss him.
Arriving at Sebastian's apartment, Roy tells Pris the others are dead. Sympathetic to their plight, Sebastian reveals that because of "Methuselah Syndrome", a genetic premature aging disorder, his life will also be cut short. Sebastian and Roy gain entrance into Tyrell's secure penthouse, where Roy demands more life from his maker. Tyrell tells him that it is impossible. Roy confesses that he has done "questionable things" which Tyrell dismisses, praising Roy's advanced design and accomplishments in his short life. Roy kisses Tyrell, then kills him. Sebastian runs for the elevator followed by Roy, who then rides the elevator down alone.
Upon entering Sebastian's apartment, Deckard is ambushed by Pris, but manages to kill her just as Roy returns. As Roy's body begins to shut down, he chases Deckard through the building and ends up on the roof. Deckard tries to jump to another roof, but misses and is left hanging precariously between the buildings. Roy makes the jump with ease, and as Deckard's grip loosens, Roy hoists him onto the roof, saving him. As his life runs out, Roy delivers a monologue about how his memories are about to be lost; Roy dies in front of Deckard, who watches silently. Gaff arrives and shouts to Deckard, "It's too bad she won't live, but then again, who does?" Deckard returns to his apartment to find Rachael sleeping in his bed. As they leave, Deckard finds a small tin-foil origami unicorn, a calling card left by Gaff. Depending on the version, Deckard and Rachael either leave the apartment block to an uncertain future, or drive through an idyllic pastoral landscape.
Themes.
Although "Blade Runner" is ostensibly an action film, it operates on multiple dramatic and narrative levels. It is indebted to film noir conventions: the "femme fatale"; protagonist-narration (removed in later versions); dark and shadowy cinematography; and the questionable moral outlook of the hero – in this case, extended to include reflections upon the nature of his own humanity. It is a literate science fiction film, thematically enfolding the philosophy of religion and moral implications of human mastery of genetic engineering in the context of classical Greek drama and hubris. It also draws on Biblical images, such as Noah's flood, and literary sources, such as "Frankenstein". Linguistically, the theme of mortality is subtly reiterated in the chess game between Roy and Tyrell, based on the famous Immortal Game of 1851, though Scott has said that was coincidental.
"Blade Runner" delves into the implications of technology on the environment and on society by reaching to the past, using literature, religious symbolism, classical dramatic themes, and film noir. This tension between past, present, and future is mirrored in the retrofitted future of "Blade Runner", which is high-tech and gleaming in places but decayed and old elsewhere. Ridley Scott described the film as: "extremely dark, both literally and metaphorically, with an oddly masochistic feel", in an interview by Lynn Barber for the British Sunday newspaper "The Observer" in 2002. Scott "liked the idea of exploring pain" in the wake of his brother's skin cancer death: "When he was ill, I used to go and visit him in London, and that was really traumatic for me."
An aura of paranoia suffuses the film: corporate power looms large; the police seem omnipresent; vehicle and warning lights probe into buildings; and the consequences of huge biomedical power over the individual are explored – especially the consequences for replicants of their implanted memories. Control over the environment is depicted as taking place on a vast scale, hand in hand with the absence of any natural life, with artificial animals substituting for their extinct predecessors. This oppressive backdrop explains the frequently referenced migration of humans to extra-terrestrial ("off-world") colonies.
The dystopian themes explored in "Blade Runner" are an early example of cyberpunk concepts expanding into film. Eyes are a recurring motif, as are manipulated images, calling into question reality and our ability to accurately perceive and remember it.
These thematic elements provide an atmosphere of uncertainty for "Blade Runner"‍ '​s central theme of examining humanity. In order to discover replicants, an empathy test is used, with a number of its questions focused on the treatment of animals – seemingly an essential indicator of someone's "humanity". The replicants appear to show compassion and concern for one another and are juxtaposed against human characters who lack empathy while the mass of humanity on the streets is cold and impersonal. The film goes so far as to put in doubt whether Deckard is human, and forces the audience to re-evaluate what it means to be human.
The question of whether Deckard is intended to be a human or a replicant has been an ongoing controversy since the film's release. Both Michael Deeley and Harrison Ford wanted Deckard to be human while Hampton Fancher preferred ambiguity. Ridley Scott has confirmed that in his vision Deckard is a replicant.
Deckard's unicorn dream sequence, inserted into the "Director's Cut", coinciding with Gaff's parting gift of an origami unicorn is seen by many as showing that Deckard is a replicant – as Gaff could have accessed Deckard's implanted memories. The interpretation that Deckard is a replicant is challenged by others who believe the unicorn imagery shows that the characters, whether human or replicant, share the same dreams and recognize their affinity, or that the absence of a decisive answer is crucial to the film's main theme. The inherent ambiguity and uncertainty of the film, as well as its textual richness, have permitted viewers to see it from their own perspectives.
Production.
Casting.
Casting the film proved troublesome, particularly for the lead role of Deckard. Screenwriter Hampton Fancher envisioned Robert Mitchum as Deckard and wrote the character's dialogue with Mitchum in mind. Director Ridley Scott and the film's producers spent months meeting and discussing the role with Dustin Hoffman, who eventually departed over differences in vision. Harrison Ford was ultimately chosen for several reasons, including his performance in the "Star Wars" films, Ford's interest in the "Blade Runner" story, and discussions with Steven Spielberg who was finishing "Raiders of the Lost Ark" at the time and strongly praised Ford's work in the film. Following his success in films like "Star Wars" (1977) and "Raiders of the Lost Ark" (1981), Ford was looking for a role with dramatic depth. According to production documents, several actors were considered for the role, including Gene Hackman, Sean Connery, Jack Nicholson, Paul Newman, Clint Eastwood, Tommy Lee Jones, Arnold Schwarzenegger, Al Pacino, and Burt Reynolds.
One role that was not difficult to cast was Rutger Hauer as Roy Batty, the violent yet thoughtful leader of the replicants. Scott cast Hauer without having met him, based solely on Hauer's performances in Paul Verhoeven's movies Scott had seen ("Katie Tippel", "Soldier of Orange" and "Turkish Delight"). Hauer's portrayal of Batty was regarded by Philip K. Dick as, "the perfect Batty—cold, Aryan, flawless". Of the many films Hauer has done, "Blade Runner" is his favorite. As he explained in a live chat in 2001, ""Blade Runner" needs no explanation. It just [is]. All of the best. There is nothing like it. To be part of a real masterpiece which changed the world's thinking. It's awesome." Hauer wrote his character's "tears in the rain" speech himself and presented the words to Scott on set prior to filming.
"Blade Runner" used a number of then-lesser-known actors: Sean Young portrays Rachael, an experimental replicant implanted with the memories of Tyrell's niece, causing her to believe she is human; Nina Axelrod auditioned for the role. Daryl Hannah portrays Pris, a "basic pleasure model" replicant; Stacey Nelkin auditioned for the role, but was given another part in the film, which was ultimately cut before filming. Casting Pris and Rachael was challenging, requiring several screen tests, with Morgan Paull playing the role of Deckard. Paull was cast as Deckard's fellow bounty hunter Holden based on his performances in the tests. Brion James portrays Leon Kowalski, a combat replicant, and Joanna Cassidy portrays Zhora, an assassin replicant. 
Edward James Olmos portrays Gaff. Olmos used his diverse ethnic background, and personal research, to help create the fictional "Cityspeak" language his character uses in the film. His initial address to Deckard at the noodle bar is partly in Hungarian and means, "Horse dick [bullshit]! No way. You are the Blade ... Blade Runner." M. Emmet Walsh plays Captain Bryant, a hard-drinking, sleazy, and underhanded police veteran typical of the film noir genre. Joe Turkel portrays Dr. Eldon Tyrell, a corporate mogul who built an empire on genetically manipulated humanoid slaves. William Sanderson was cast as J. F. Sebastian, a quiet and lonely genius who provides a compassionate yet compliant portrait of humanity. J. F. sympathizes with the replicants, whom he sees as companions, and shares their shorter lifespan due to his rapid aging disease; Joe Pantoliano was considered for the role. James Hong portrays Hannibal Chew, an elderly geneticist specializing in synthetic eyes, and Hy Pyke portrays the sleazy bar owner Taffey Lewis with ease and in a single take, something almost unheard-of with Scott whose drive for perfection resulted at times in double-digit takes.
Development.
Interest in adapting Philip K. Dick's novel "Do Androids Dream of Electric Sheep?" developed shortly after its 1968 publication. Director Martin Scorsese was interested in filming the novel, but never optioned it. Producer Herb Jaffe optioned it in the early 1970s, but Dick was unimpressed with the screenplay written by Herb's son Robert: "Jaffe's screenplay was so terribly done ... Robert flew down to Santa Ana to speak with me about the project. And the first thing I said to him when he got off the plane was, 'Shall I beat you up here at the airport, or shall I beat you up back at my apartment?' "
The screenplay by Hampton Fancher was optioned in 1977. Producer Michael Deeley became interested in Fancher's draft and convinced director Ridley Scott to film it. Scott had previously declined the project, but after leaving the slow production of "Dune", wanted a faster-paced project to take his mind off his older brother's recent death. He joined the project on February 21, 1980, and managed to push up the promised Filmways financing from US$13 million to $15 million. Fancher's script focused more on environmental issues and less on issues of humanity and faith, which had featured heavily in the novel, and Scott wanted changes. Fancher found a cinema treatment by William S. Burroughs for Alan E. Nourse's novel "The Bladerunner" (1974), titled "Blade Runner (a movie)". Scott liked the name, so Deeley obtained the rights to the titles. Eventually he hired David Peoples to rewrite the script and Fancher left the job over the issue on December 21, 1980, although he later returned to contribute additional rewrites.
Having invested over $2.5 million in pre-production, as the date of commencement of principal photography neared, Filmways withdrew financial backing. In 10 days Deeley had secured $21.5 million in financing through a three-way deal between The Ladd Company (through Warner Bros.), the Hong Kong-based producer Sir Run Run Shaw, and Tandem Productions.
Philip K. Dick became concerned that no one had informed him about the film's production, which added to his distrust of Hollywood. After Dick criticized an early version of Hampton Fancher's script in an article written for the Los Angeles "Select TV Guide", the studio sent Dick the David Peoples rewrite. Although Dick died shortly before the film's release, he was pleased with the rewritten script, and with a 20-minute special effects test reel that was screened for him when he was invited to the studio. Despite his well known skepticism of Hollywood in principle, Dick enthused to Ridley Scott that the world created for the film looked exactly as he had imagined it. He said, "I saw a segment of Douglas Trumbull's special effects for "Blade Runner" on the KNBC-TV news. I recognized it immediately. It was my own interior world. They caught it perfectly." He also approved of the film's script, saying, "After I finished reading the screenplay, I got the novel out and looked through it. The two reinforce each other, so that someone who started with the novel would enjoy the movie and someone who started with the movie would enjoy the novel." The motion picture was dedicated to Dick. Principal photography of "Blade Runner" began on March 9, 1981, and ended four months later.
In 1992, Ford revealed, ""Blade Runner" is not one of my favorite films. I tangled with Ridley." Apart from friction with the director, Ford also disliked the voiceovers: "When we started shooting it had been tacitly agreed that the version of the film that we had agreed upon was the version without voiceover narration. It was a f**king ["sic"] nightmare. I thought that the film had worked without the narration. But now I was stuck re-creating that narration. And I was obliged to do the voiceovers for people that did not represent the director's interests." "I went kicking and screaming to the studio to record it." 
In 2006 Scott was asked "Who's the biggest pain in the arse you've ever worked with?", he replied: "It's got to be Harrison ... he'll forgive me because now I get on with him. Now he's become charming. But he knows a lot, that's the problem. When we worked together it was my first film up and I was the new kid on the block. But we made a good movie." Ford said of Scott in 2000: "I admire his work. We had a bad patch there, and I'm over it." In 2006 Ford reflected on the production of the film saying: "What I remember more than anything else when I see "Blade Runner" is not the 50 nights of shooting in the rain, but the voiceover ... I was still obliged to work for these clowns that came in writing one bad voiceover after another." Ridley Scott confirmed in the summer 2007 issue of "Total Film" that Harrison Ford contributed to the "Blade Runner" Special Edition DVD, having already done his interviews. "Harrison's fully on board", said Scott.
The Bradbury Building in downtown Los Angeles served as a filming location, and a Warner Bros. backlot housed the LA 2019 streets. Test screenings resulted in several changes including adding a voice over, a happy ending, and the removal of a Holden hospital scene. The relationship between the filmmakers and the investors was difficult, which culminated in Deeley and Scott being fired but still working on the film. Crew members created T-shirts during filming saying, "Yes Guv'nor, My Ass" that mocked Scott's unfavorable comparison of U.S. and British crews; Scott responded with a T-shirt of his own, "Xenophobia Sucks" making the incident known as the T-shirt war.
Design.
Ridley Scott credits Edward Hopper's painting "Nighthawks" and the French science fiction comic magazine "Métal Hurlant" ("Heavy Metal"), to which the artist Moebius contributed, as stylistic mood sources. He also drew on the landscape of "Hong Kong on a very bad day" and the industrial landscape of his one-time home in north east England. The visual style of the movie is influenced by the work of Futurist Italian architect, Antonio Sant'Elia. Scott hired Syd Mead as his concept artist who, like Scott, was influenced by "Métal Hurlant". Moebius was offered the opportunity to assist in the pre-production of "Blade Runner", but he declined so that he could work on René Laloux's animated film "Les Maîtres du temps" – a decision that he later regretted. Production designer Lawrence G. Paull and art director David Snyder realized Scott's and Mead's sketches. Douglas Trumbull and Richard Yuricich supervised the special effects for the film.
"Blade Runner" has numerous deep similarities to Fritz Lang's "Metropolis", including a built-up urban environment, in which the wealthy literally live above the workers, dominated by a huge building – the Stadtkrone Tower in "Metropolis" and the Tyrell Building in "Blade Runner". Special effects supervisor David Dryer used stills from "Metropolis" when lining up "Blade Runner"‍ '​s miniature building shots.
"Spinner" is the generic term for the fictional flying cars used in the film. A Spinner can be driven as a ground-based vehicle, and take off vertically, hover, and cruise using jet propulsion much like vertical take-off and landing (VTOL) aircraft. They are used extensively by the police to patrol and survey the population, and it is clear that despite restrictions wealthy people can acquire spinner licenses. The vehicle was conceived and designed by Syd Mead who described the spinner as an "aerodyne"—a vehicle which directs air downward to create lift, though press kits for the film stated that the spinner was propelled by three engines: "conventional internal combustion, jet, and anti-gravity" Mead's conceptual drawings were transformed into 25 working vehicles by automobile customizer Gene Winfield. A Spinner is on permanent exhibit at the Science Fiction Museum and Hall of Fame in Seattle, Washington.
A very advanced form of lie detector that measures contractions of the iris muscle and the presence of invisible airborne particles emitted from the body. The bellows were designed for the latter function and give the machine the menacing air of a sinister insect. The VK is used primarily by Blade Runners to determine if a suspect is truly human by measuring the degree of his empathic response through carefully worded questions and statements.
—Description from the original 1982 "Blade Runner" press kit.
The Voight-Kampff machine (or device) is a fictional interrogation tool, originating in the book where it is spelled Voigt-Kampff. The Voight-Kampff is a polygraph-like machine used by Blade Runners to assist in the testing of an individual to determine if he or she is a replicant. It measures bodily functions such as respiration, blush response, heart rate, and eye movement in response to emotionally provocative questions. In the film two replicants take the test, Leon and Rachael, and Deckard tells Tyrell that it usually takes 20 to 30 cross-referenced questions to distinguish a replicant; in contrast with the book, where it is stated it only takes "six or seven" questions to make a determination. In the film it takes more than one hundred questions to determine that Rachael is a replicant.
Music.
The "Blade Runner" soundtrack by Vangelis is a dark melodic combination of classic composition and futuristic synthesizers which mirrors the film-noir retro-future envisioned by Ridley Scott. Vangelis, fresh from his Academy Award winning score for "Chariots of Fire", composed and performed the music on his synthesizers. He also made use of various chimes and the vocals of collaborator Demis Roussos. Another memorable sound is the haunting tenor sax solo "Love Theme" by British saxophonist Dick Morrissey, who performed on many of Vangelis's albums. Ridley Scott also used "Memories of Green" from the Vangelis album "See You Later, "an orchestral version of which Scott would later use in his film "Someone To Watch Over Me".
Along with Vangelis' compositions and ambient textures, the film's sound scape also features a track by the Japanese ensemble Nipponia - "Ogi No Mato" or "The Folding Fan as a Target" from the Nonesuch Records release "Traditional Vocal and Instrumental Music -" and a track by harpist Gail Laughton from "Harps of the Ancient Temples" on Laurel Records.
Despite being well received by fans and critically acclaimed and nominated in 1983 for a BAFTA and Golden Globe as best original score, and the promise of a soundtrack album from Polydor Records in the end titles of the film, the release of the official soundtrack recording was delayed for over a decade. There are two official releases of the music from "Blade Runner". In light of the lack of a release of an album, the New American Orchestra recorded an orchestral adaptation in 1982 which bore little resemblance to the original. Some of the film tracks would, in 1989, surface on the compilation "Vangelis: Themes", but not until the 1992 release of the "Director's Cut" version would a substantial amount of the film's score see commercial release.
These delays and poor reproductions led to the production of many bootleg recordings over the years. A bootleg tape surfaced in 1982 at science fiction conventions and became popular given the delay of an official release of the original recordings, and in 1993 "Off World Music, Ltd" created a bootleg CD that would prove more comprehensive than Vangelis' official CD in 1994. A set with three CDs of "Blade Runner"-related Vangelis music was released in 2007. Titled "Blade Runner Trilogy", the first disc contains the same tracks as the 1994 official soundtrack release, the second features previously unreleased music from the movie, and the third disc is all newly composed music from Vangelis, inspired by, and in the spirit of the movie.
Special effects.
The movie's special effects are generally recognised to be among the best of all time, using the available (non-digital) technology to the fullest. In addition to matte paintings and models, the techniques employed included multipass exposures. In some scenes, the set was lit, shot, the film rewound, and then rerecorded over with different lighting. In some cases this was done 16 times in all. The cameras were frequently motion controlled using computers.
Release.
"Blade Runner" was released in 1,290 theaters on June 25, 1982. That date was chosen by producer Alan Ladd, Jr. because his previous highest-grossing films ("" and "Alien") had a similar opening date (May 25) in 1977 and 1979, making the date his "lucky day".
The gross for the opening weekend was a disappointing $6.15 million. A significant factor in the film's rather poor box office performance was that it was released around the same time as other science fiction films, including "The Thing", "", and, most significantly, "E.T. the Extra-Terrestrial", which dominated box office revenues that summer.
Critical reception.
Film critics were polarized as some felt the story had taken a back seat to special effects and that it was not the action/adventure the studio had advertised. Others acclaimed its complexity and predicted it would stand the test of time.
In the United States, a general criticism was its slow pacing that detracts from other strengths; Sheila Benson from the "Los Angeles Times" called it "Blade Crawler", while Pat Berman in "The State" and "Columbia Record" described it as "science fiction pornography". Pauline Kael noted that with its "extraordinary" congested-megalopolis sets, "Blade Runner" "has its own look, and a visionary sci-fi movie that has its own look can't be ignored – it has its place in film history" but "hasn't been thought out in human terms". Roger Ebert praised the visuals of both the original "Blade Runner" and the "Director's Cut" versions and recommended it for that reason; however, he found the human story clichéd and a little thin. In 2007, upon release of "The Final Cut", Ebert somewhat revised his original opinion of the film and added it to his list of Great Movies, while noting, "I have been assured that my problems in the past with "Blade Runner" represent a failure of my own taste and imagination, but if the film was perfect, why has Sir Ridley continued to tinker with it?" "Blade Runner" holds a 91% rating on Rotten Tomatoes with an average score of 8.4 out of 10 from 96 reviews. The site's main consensus reads "Misunderstood when it first hit theaters, the influence of Ridley Scott's mysterious, neo-noir Blade Runner has deepened with time. A visually remarkable, achingly human sci-fi masterpiece."
Accolades.
"Blade Runner" has won and been nominated for the following awards:
Versions.
Several different versions of "Blade Runner" have been shown. The original workprint version (1982, 113 minutes) was shown for audience test previews in Denver and Dallas in March 1982. Negative responses to the previews led to the modifications resulting in the U.S. theatrical version. The workprint was shown as a director's cut without Scott's approval at the Los Angeles Fairfax Theater in May 1990, at an AMPAS showing in April 1991, and in September and October 1991 at the Los Angeles NuArt Theater and the San Francisco Castro Theater. Positive responses pushed the studio to approve work on an official director's cut. A San Diego Sneak Preview was shown only once, in May 1982, and was almost identical to the U.S. theatrical version but contained three extra scenes not shown in any other version, including the 2007 Final Cut.
Two versions were shown in the film's 1982 theatrical release: the U.S. theatrical version (116 minutes), known as the original version or "Domestic Cut", released on Betamax and VHS in 1983 and Laserdisc in 1987; and the "International Cut" (117 minutes), also known as the "Criterion Edition" or "uncut version", which included more violent action scenes than the U.S. version. Although initially unavailable in the U.S., and distributed in Europe and Asia via theatrical and local Warner Home Video Laserdisc releases, it was later released on VHS and Criterion Collection Laserdisc in North America, and re-released in 1992 as a "10th Anniversary Edition".
Scott's "Director's Cut" (1991, 116 minutes) was made available on VHS and Laserdisc in 1993, and on DVD in 1997. Significant changes from the theatrical version include: the removal of Deckard's voice-over; re-insertion of a unicorn sequence; and removal of the studio-imposed happy ending. Scott provided extensive notes and consultation to Warner Bros. through film preservationist Michael Arick, who was put in charge of creating the "Director's Cut". Scott's "The Final Cut" (2007, 117 minutes) was released by Warner Bros. theatrically on October 5, 2007, and subsequently released on DVD, HD DVD, and Blu-ray Disc in December 2007. This is the only version over which Scott had complete editorial control.
Legacy.
Cultural impact.
While not initially a success with North American audiences, the film was popular internationally and garnered a cult following. The film's dark style and futuristic designs have served as a benchmark and its influence can be seen in many subsequent science fiction films, anime, video games, and television programs. For example, Ronald D. Moore and David Eick, the producers of the re-imagining of "Battlestar Galactica", have both cited "Blade Runner" as one of the major influences for the show. "Blade Runner" continues to reflect modern trends and concerns, and an increasing number consider it one of the greatest science fiction films of all time. It was voted the best science fiction film ever made in a poll of 60 eminent world scientists conducted in 2004. "Blade Runner" is also cited as an important influence to both the style and story of the "Ghost in the Shell" film series, which itself has been highly influential to the future-noir genre.
The film was selected for preservation in the United States National Film Registry in 1993 and is frequently used in university courses. In 2007 it was named the second most visually influential film of all time by the Visual Effects Society.
"Blade Runner" is one of the most musically sampled films of the 20th century. The 2009 album, "I, Human", by Singaporean band Deus Ex Machina makes numerous references to the genetic engineering and cloning themes from the film, and even features a track titled "Replicant".
"Blade Runner" has influenced adventure games such as the 2012 graphical text adventure "Cypher", "Rise of the Dragon", "Snatcher", "Beneath a Steel Sky", "", "Bubblegum Crisis" (and its original anime films), the role-playing game "Shadowrun", the first-person shooter "Perfect Dark", and the "Syndicate" series of video games. The film is also cited as a major influence on Warren Spector, designer of the computer-game "Deus Ex", which displays evidence of the film's influence in both its visual rendering and plot. The look of the film, darkness, neon lights and opacity of vision, is easier to render than complicated backdrops, making it a popular choice for game designers.
"Blade Runner" has also been the subject of parody, such as the comics "Blade Bummer" by "Crazy" comics, "Bad Rubber" by Steve Gallacci, and the "Red Dwarf" 2009 three-part miniseries, "".
Among the folklore that has developed around the film over the years has been the belief that the film was a curse to the companies whose logos were displayed prominently as product placements in some scenes. While they were market leaders at the time, Atari, Bell, Cuisinart and Pan Am experienced setbacks after the film's release. The Coca-Cola Company suffered losses during its failed introduction of New Coke in 1985, but soon afterwards regained its market share.
Media recognitions for "Blade Runner" include:
In other media.
Before the film's principal photography began, "Cinefantastique" magazine commissioned Paul M. Sammon to write an article about "Blade Runner"‍ '​s production which became the book "Future Noir: The Making of Blade Runner".
The book chronicles "Blade Runner"‍ '​s evolution, focusing on film-set politics, especially the British director's experiences with his first American film crew; of which producer Alan Ladd, Jr. has said, "Harrison wouldn't speak to Ridley and Ridley wouldn't speak to Harrison. By the end of the shoot Ford was 'ready to kill Ridley', said one colleague. He really would have taken him on if he hadn't been talked out of it."
"Future Noir" has short cast biographies and quotations about their experiences, and photographs of the film's production and preliminary sketches. A second edition of "Future Noir" was published in 2007.
Philip K. Dick refused a $400,000 offer to write a "Blade Runner" novelization, saying: "[I was] told the cheapo novelization would have to appeal to the twelve-year-old audience" and "[it] would have probably been disastrous to me artistically." He added, "That insistence on my part of bringing out the original novel and not doing the novelization – they were just furious. They finally recognized that there was a legitimate reason for reissuing the novel, even though it cost them money. It was a victory not just of contractual obligations but of theoretical principles." "Do Androids Dream of Electric Sheep?" was eventually reprinted as a tie-in, with the film poster as a cover and the original title in parentheses below the "Blade Runner" title.
Archie Goodwin scripted the comic book adaptation, "A Marvel Super Special: Blade Runner", published in September 1982.
There are two video games based on the film, one from 1985 for Commodore 64, Sinclair ZX Spectrum and Amstrad CPC by CRL Group PLC based on the music by Vangelis (due to licensing issues), and another action adventure PC game from 1997 by Westwood Studios. The 1997 video game featured new characters and branching storylines based on the "Blade Runner" world. Eldon Tyrell, Gaff, Leon, Rachael, Chew, and J.F. Sebastian appear, and their voice files are recorded by the original actors. The player assumes the role of McCoy, another replicant-hunter working at the same time as Deckard.
The PC game featured a non-linear plot, non-player characters that each ran in their own independent AI, and an unusual pseudo-3D engine (which eschewed polygonal solids in favor of voxel elements) that did not require the use of a 3D accelerator card to play the game.
The television film "Total Recall 2070" was initially planned as a spin-off of the movie "Total Recall", and would eventually be transformed into a hybrid of "Total Recall" and "Blade Runner". The "Total Recall" film was also based on a Philip K. Dick story, "We Can Remember It for You Wholesale"; many similarities between "Total Recall 2070" and "Blade Runner" were noted, as well as apparent inspiration from Isaac Asimov's "The Caves of Steel" and the TV series "Holmes & Yo-Yo".
The film has been the subject of several documentaries. "On the Edge of Blade Runner" (2000, 55 minutes) was directed by Andrew Abbott and hosted/written by Mark Kermode. Interviews with production staff, including Scott, give details of the creative process and the turmoil during preproduction. Stories from Paul M. Sammon and Hampton Fancher provide insight into Philip K. Dick and the origins of "Do Androids Dream of Electric Sheep?" "Future Shocks" (2003, 27 minutes) is a documentary by TVOntario. It includes interviews with executive producer Bud Yorkin, Syd Mead, and the cast, and commentary by science fiction author Robert J. Sawyer and from film critics. "Dangerous Days: Making Blade Runner" (2007, 213 minutes) is a documentary directed and produced by Charles de Lauzirika for "The Final Cut" version of the film. It was culled from over 80 interviews, including Ford, Young, and Scott. The documentary consists of eight chapters, each covering a portion of the film-making – or in the case of the final chapter, the film's controversial legacy. "All Our Variant Futures: From Workprint to Final Cut" (2007, 29 minutes), produced by Paul Prischman, appears on the "Blade Runner" Ultimate Collector's Edition and provides an overview of the film's multiple versions and their origins, as well as detailing the seven-year-long restoration, enhancement and remastering process behind "The Final Cut".
Sequels.
Dick's friend, K. W. Jeter, wrote three authorized "Blade Runner" novels that continue Deckard's story, attempting to resolve the differences between the film and "Do Androids Dream of Electric Sheep?": ' (1995), ' (1996), and "" (2000). By 1999, Stuart Hazeldine had written a sequel to "Blade Runner" based on "The Edge of Human", titled "Blade Runner Down"; the project was shelved due to rights issues. "Blade Runner" co-author David Peoples wrote the 1998 action film "Soldier", which was referred to by him as a "sidequel" or spiritual successor to the original film.
Scott considered developing a sequel, tentatively titled "Metropolis". At the 2007 Comic-Con Scott again announced that he was considering a sequel to the film. "Eagle Eye" co-writer Travis Wright worked with producer Bud Yorkin for several years on the project. His colleague John Glenn, who left the project by 2008, stated the script explores the nature of the off-world colonies as well as what happens to the Tyrell Corporation in the wake of its founder's death.
In June 2009 "The New York Times" reported that Scott and his brother Tony Scott were working on a "Blade Runner" prequel, set in 2019. The prequel, "Purefold", was planned as a series of 5–10 minute shorts, aimed first at the web and then perhaps television. Due to rights issues the proposed series was not to be linked too closely to the characters or events of the 1982 film. On February 7, 2010, it was announced that production on "Purefold" had ceased, due to funding problems. On March 4, 2011, io9 reported that Yorkin was developing a new "Blade Runner" film. It was also reported that month that director Christopher Nolan was the desired choice to make the film.
It was announced on August 18, 2011, that Scott was to direct a new "Blade Runner" movie, with filming to begin no earlier than 2013. Indications from producer Andrew Kosove were that Ford was unlikely to be involved in the project. Scott later said that the film was "liable to be a sequel" but without the previous cast, and that he was close to finding a writer that "might be able to help [him] deliver". On February 6, 2012, Kosove denied that any casting considerations had been made in response to buzz that Ford might reprise his role, saying, "It is absolutely, patently false that there has been any discussion about Harrison Ford being in "Blade Runner". To be clear, what we are trying to do with Ridley now is go through the painstaking process of trying to break the back of the story ... The casting of the movie could not be further from our minds at this moment." When Scott was asked about the possibility of a sequel in October 2012, he said, "It's not a rumor—it's happening. With Harrison Ford? I don't know yet. Is he too old? Well, he was a Nexus-6 so we don't know how long he can live. And that's all I'm going to say at this stage."
In November 2014, "Variety" magazine reported that Scott was no longer the director for the film and would only fulfill a producer's role. Scott also revealed that Ford's character will only appear in "the third act" of the sequel. In February 2015, Alcon Entertainment confirmed that Scott will not be back to direct, and they were negotiating with "Prisoners" director Denis Villeneuve. Ford, however, will return, as will original writer Hampton Fancher, and the film is expected to enter production in mid-2016.

</doc>
<doc id="3747" url="http://en.wikipedia.org/wiki?curid=3747" title="Bill Gates">
Bill Gates

William Henry "Bill" Gates III (born October 28, 1955) is an American business magnate, philanthropist, investor, computer programmer, and inventor. Gates originally established his reputation as the co-founder of Microsoft, the world’s largest PC software company, with Paul Allen. During his career at Microsoft, Gates held the positions of chairman, CEO and chief software architect, and was also the largest individual shareholder until May 2014. He has also authored and co-authored several books.
Today he is consistently ranked in the Forbes list of the world's wealthiest people and was the wealthiest overall from 1995 to 2014—excluding a few brief periods post-2008. Between 2009 and 2014 his wealth more than doubled from $40 billion to more than $82 billion. Between 2013 and 2014 his wealth increased by $15 billion, or around $1.5 billion more than the entire GDP of Iceland in 2014. Gates is currently the richest man in the world.
Gates is one of the best-known entrepreneurs of the personal computer revolution. Gates has been criticized for his business tactics, which have been considered anti-competitive, an opinion which has in some cases been upheld by numerous court rulings. In the later stages of his career, Gates has pursued a number of philanthropic endeavors, donating large amounts of money to various charitable organizations and scientific research programs through the Bill & Melinda Gates Foundation, established in 2000.
Gates stepped down as chief executive officer of Microsoft in January 2000. He remained as chairman and created the position of chief software architect for himself. In June 2006, Gates announced that he would be transitioning from full-time work at Microsoft to part-time work, and full-time work at the Bill & Melinda Gates Foundation. He gradually transferred his duties to Ray Ozzie (who has since left Microsoft), chief software architect, and Craig Mundie, chief research and strategy officer. Gates's last full-time day at Microsoft was June 27, 2008. He stepped down as chairman of Microsoft in February 2014, taking on a new post as technology advisor to support newly appointed CEO Satya Nadella.
Early life.
Gates was born in Seattle, Washington, in an upper-middle-class family, the son of William H. Gates, Sr. and Mary Maxwell Gates. His ancestral origin includes English, German, and Scots-Irish. His father was a prominent lawyer, and his mother served on the board of directors for First Interstate BancSystem and the United Way. Gates's maternal grandfather was JW Maxwell, a national bank president. Gates has one elder sister, Kristi (Kristianne), and one younger sister, Libby. He was the fourth of his name in his family, but was known as William Gates III or "Trey" because his father had the "II" suffix. Early on in his life, Gates's parents had a law career in mind for him. When Gates was young, his family regularly attended a Protestant Congregational church. The family encouraged competition; one visitor reported that "it didn't matter whether it was hearts or pickleball or swimming to the dock ... there was always a reward for winning and there was always a penalty for losing".
At 13, he enrolled in the Lakeside School, an exclusive preparatory school. When he was in the eighth grade, the Mothers Club at the school used proceeds from Lakeside School's rummage sale to buy a Teletype Model 33 ASR terminal and a block of computer time on a General Electric (GE) computer for the school's students. Gates took an interest in programming the GE system in BASIC, and was excused from math classes to pursue his interest. He wrote his first computer program on this machine: an implementation of tic-tac-toe that allowed users to play games against the computer. Gates was fascinated by the machine and how it would always execute software code perfectly. When he reflected back on that moment, he said, "There was just something neat about the machine." After the Mothers Club donation was exhausted, he and other students sought time on systems including DEC PDP minicomputers. One of these systems was a PDP-10 belonging to Computer Center Corporation (CCC), which banned four Lakeside students—Gates, Paul Allen, Ric Weiland, and Kent Evans—for the summer after it caught them exploiting bugs in the operating system to obtain free computer time.
At the end of the ban, the four students offered to find bugs in CCC's software in exchange for computer time. Rather than use the system via Teletype, Gates went to CCC's offices and studied source code for various programs that ran on the system, including programs in Fortran, Lisp, and machine language. The arrangement with CCC continued until 1970, when the company went out of business. The following year, Information Sciences, Inc. hired the four Lakeside students to write a payroll program in Cobol, providing them computer time and royalties. After his administrators became aware of his programming abilities, Gates wrote the school's computer program to schedule students in classes. He modified the code so that he was placed in classes with "a disproportionate number of interesting girls." He later stated that "it was hard to tear myself away from a machine at which I could so unambiguously demonstrate success." At age 17, Gates formed a venture with Allen, called Traf-O-Data, to make traffic counters based on the Intel 8008 processor. In early 1973, Bill Gates served as a congressional page in the U.S. House of Representatives.
Gates graduated from Lakeside School in 1973 and was a National Merit Scholar. He scored 1590 out of 1600 on the SAT and enrolled at Harvard College in the autumn of 1973. While at Harvard, he met Steve Ballmer, who would later succeed Gates as CEO of Microsoft.
In his sophomore year, Gates devised an algorithm for pancake sorting as a solution to one of a series of unsolved problems presented in a combinatorics class by Harry Lewis, one of his professors. Gates's solution held the record as the fastest version for over thirty years; its successor is faster by only one percent. His solution was later formalized in a published paper in collaboration with Harvard computer scientist Christos Papadimitriou.
Gates did not have a definite study plan while a student at Harvard and spent a lot of time using the school's computers. Gates remained in contact with Paul Allen, and he joined him at Honeywell during the summer of 1974. The following year saw the release of the MITS Altair 8800 based on the Intel 8080 CPU, and Gates and Allen saw this as the opportunity to start their own computer software company. Gates dropped out of Harvard at this time. He had talked this decision over with his parents, who were supportive of him after seeing how much Gates wanted to start a company.
Microsoft.
BASIC.
After reading the January 1975 issue of "Popular Electronics" that demonstrated the Altair 8800, Gates contacted Micro Instrumentation and Telemetry Systems (MITS), the creators of the new microcomputer, to inform them that he and others were working on a BASIC interpreter for the platform. In reality, Gates and Allen did not have an Altair and had not written code for it; they merely wanted to gauge MITS's interest. MITS president Ed Roberts agreed to meet them for a demo, and over the course of a few weeks they developed an Altair emulator that ran on a minicomputer, and then the BASIC interpreter. The demonstration, held at MITS's offices in Albuquerque, was a success and resulted in a deal with MITS to distribute the interpreter as Altair BASIC. Paul Allen was hired into MITS, and Gates took a leave of absence from Harvard to work with Allen at MITS in Albuquerque in November 1975. They named their partnership "Micro-Soft" and had their first office located in Albuquerque. Within a year, the hyphen was dropped, and on November 26, 1976, the trade name "Microsoft" was registered with the Office of the Secretary of the State of New Mexico. Gates never returned to Harvard to complete his studies.
Microsoft's BASIC was popular with computer hobbyists, but Gates discovered that a pre-market copy had leaked into the community and was being widely copied and distributed. In February 1976, Gates wrote an Open Letter to Hobbyists in the MITS newsletter saying that MITS could not continue to produce, distribute, and maintain high-quality software without payment. This letter was unpopular with many computer hobbyists, but Gates persisted in his belief that software developers should be able to demand payment. Microsoft became independent of MITS in late 1976, and it continued to develop programming language software for various systems. The company moved from Albuquerque to its new home in Bellevue, Washington on January 1, 1979.
During Microsoft's early years, all employees had broad responsibility for the company's business. Gates oversaw the business details, but continued to write code as well. In the first five years, Gates personally reviewed every line of code the company shipped, and often rewrote parts of it as he saw fit.
IBM partnership.
IBM approached Microsoft in July 1980 regarding its upcoming personal computer, the IBM PC. The computer company first proposed that Microsoft write the BASIC interpreter. When IBM's representatives mentioned that they needed an operating system, Gates referred them to Digital Research (DRI), makers of the widely used CP/M operating system. IBM's discussions with Digital Research went poorly, and they did not reach a licensing agreement. IBM representative Jack Sams mentioned the licensing difficulties during a subsequent meeting with Gates and told him to get an acceptable operating system. A few weeks later, Gates proposed using 86-DOS (QDOS), an operating system similar to CP/M that Tim Paterson of Seattle Computer Products (SCP) had made for hardware similar to the PC. Microsoft made a deal with SCP to become the exclusive licensing agent, and later the full owner, of 86-DOS. After adapting the operating system for the PC, Microsoft delivered it to IBM as PC DOS in exchange for a one-time fee of $50,000.
Gates did not offer to transfer the copyright on the operating system, because he believed that other hardware vendors would clone IBM's system. They did, and the sales of MS-DOS made Microsoft a major player in the industry. Despite IBM's name on the operating system the press quickly identified Microsoft as being very influential on the new computer. "PC Magazine" asked if Gates were "the man behind the machine?", and "InfoWorld" quoted an expert as stating "it's Gates' computer". Gates oversaw Microsoft's company restructuring on June 25, 1981, which re-incorporated the company in Washington state and made Gates President of Microsoft and the Chairman of the Board.
Windows.
Microsoft launched its first retail version of Microsoft Windows on November 20, 1985, and in August, the company struck a deal with IBM to develop a separate operating system called OS/2. Although the two companies successfully developed the first version of the new system, mounting creative differences caused the partnership to deteriorate.
Management style.
From Microsoft's founding in 1975 until 2006, Gates had primary responsibility for the company's product strategy. He aggressively broadened the company's range of products, and wherever Microsoft achieved a dominant position he vigorously defended it. He gained a reputation for being distant to others; as early as 1981 an industry executive complained in public that "Gates is notorious for not being reachable by phone and for not returning phone calls." Another executive recalled that after he showed Gates a game and defeated him 35 of 37 times, when they met again a month later Gates "won or tied every game. He had studied the game until he solved it. That is a competitor."
As an executive, Gates met regularly with Microsoft's senior managers and program managers. Firsthand accounts of these meetings describe him as verbally combative, berating managers for perceived holes in their business strategies or proposals that placed the company's long-term interests at risk.
He has interrupted presentations with such comments "That's the stupidest thing I've ever heard!" and, "Why don't you just give up your options and join the Peace Corps?" The target of his outburst then had to defend the proposal in detail until, hopefully, Gates was fully convinced. When subordinates appeared to be procrastinating, he was known to remark sarcastically, "I'll do it over the weekend."
Gates's role at Microsoft for most of its history was primarily a management and executive role. However, he was an active software developer in the early years, particularly on the company's programming language products. He has not officially been on a development team since working on the TRS-80 Model 100, but wrote code as late as 1989 that shipped in the company's products. On June 15, 2006, Gates announced that he would transition out of his day-to-day role over the next two years to dedicate more time to philanthropy. He divided his responsibilities between two successors, placing Ray Ozzie in charge of day-to-day management and Craig Mundie in charge of long-term product strategy.
Antitrust litigation.
Many decisions that led to antitrust litigation over Microsoft's business practices have had Gates's approval. In the 1998 "United States v. Microsoft" case, Gates gave deposition testimony that several journalists characterized as evasive. He argued with examiner David Boies over the contextual meaning of words such as, "compete", "concerned", and "we". The judge and other observers in the court room were seen laughing at various points during the deposition. "BusinessWeek" reported:
 Early rounds of his deposition show him offering obfuscatory answers and saying 'I don't recall,' so many times that even the presiding judge had to chuckle. Worse, many of the technology chief's denials and pleas of ignorance were directly refuted by prosecutors with snippets of e-mail that Gates both sent and received.
Gates later said he had simply resisted attempts by Boies to mischaracterize his words and actions. As to his demeanor during the deposition, he said, "Did I fence with Boies? ... I plead guilty. Whatever that penalty is should be levied against me: rudeness to Boies in the first degree." Despite Gates' denials, the judge ruled that Microsoft had committed monopolization and tying, and blocking competition, both in violation of the Sherman Antitrust Act.
Appearance in ads.
Gates appeared in a series of ads to promote Microsoft in 2008. The first commercial, co-starring Jerry Seinfeld, is a 90-second talk between strangers as Seinfeld walks up on a discount shoe store (Shoe Circus) in a mall and notices Gates buying shoes inside. The salesman is trying to sell Mr. Gates shoes that are a size too big. As Gates is buying the shoes, he holds up his discount card, which uses a slightly altered version of his own mugshot of his arrest in New Mexico in 1977 for a traffic violation. As they are walking out of the mall, Seinfeld asks Gates if he has melded his mind to other developers, after getting a "Yes", he then asks if they are working on a way to make computers edible, again getting a "Yes". Some say that this is an homage to Seinfeld's own show about "nothing" ("Seinfeld"). In a second commercial in the series, Gates and Seinfeld are at the home of an average family trying to fit in with normal people.
Post-Microsoft.
Since leaving day-to-day operations at Microsoft, Gates continues his philanthropy and, among other projects, purchased the video rights to the Messenger Lectures series called "The Character of Physical Law", given at Cornell University by Richard Feynman in 1964 and recorded by the BBC. The videos are available online to the public at Microsoft's Project Tuva. In April 2010, Gates was invited to visit and speak at the Massachusetts Institute of Technology (MIT), where he asked the students to take on the difficult problems of the world in their futures.
According to the Bloomberg Billionaires Index, Gates was the world's highest-earning billionaire in 2013, as his fortune increased by US$15.8 billion to US$78.5 billion. As of January 2014, most of Gates’s assets are held in Cascade Investment LLC, an entity through which he owns stakes in numerous businesses, including Four Seasons Hotels and Resorts, and Corbis Corp. On February 4, 2014, Gates stepped down as Chair of Microsoft to become Technology Advisor alongside Satya Nadella.
In a substantial interview with "Rolling Stone" magazine, published in the March 27, 2014 issue, Gates provided his perspective on a range of issues, such as climate change, his charitable activities, various tech companies and people involved in them, and the state of America. In response to a question about his greatest fear when he looks 50 years into the future, Gates stated: "... there'll be some really bad things that'll happen in the next 50 or 100 years, but hopefully none of them on the scale of, say, a million people that you didn't expect to die from a pandemic, or nuclear or bioterrorism." Gates also identified innovation as the "real driver of progress" and pronounced that "America's way better today than it's ever been."
Personal life.
After being named one of "Good Housekeeping"‍ '​s "50 Most Eligible Bachelors" in 1985, Gates married Melinda French on January 1, 1994. They have three children: daughters Jennifer Katharine (b. 1996) and Phoebe Adele (b. 2002), and son Rory John (b. 1999). The family resides in the Gateses' home, an earth-sheltered house in the side of a hill overlooking Lake Washington in Medina. According to King County public records, as of 2006 the total assessed value of the property (land and house) is $125 million, and the annual property tax is $991,000. The 66000 sqft estate has a 60 ft swimming pool with an underwater music system, as well as a 2500 sqft gym and a 1000 sqft dining room.
In an interview with "Rolling Stone", Gates stated in regard to his faith:
The moral systems of religion, I think, are super important. We've raised our kids in a religious way; they've gone to the Catholic church that Melinda goes to and I participate in. I've been very lucky, and therefore I owe it to try and reduce the inequity in the world. And that's kind of a religious belief. I mean, it's at least a moral belief.
In the same interview, Gates said: "I agree with people like Richard Dawkins that mankind felt the need for creation myths. Before we really began to understand disease and the weather and things like that, we sought false explanations for them. Now science has filled in some of the realm – not all – that religion used to fill. But the mystery and the beauty of the world is overwhelmingly amazing, and there's no scientific explanation of how it came about. To say that it was generated by random numbers, that does seem, you know, sort of an uncharitable view [laughs]. I think it makes sense to believe in God, but exactly what decision in your life you make differently because of it, I don't know."
Among Gates's private acquisitions is the Codex Leicester, a collection of writings by Leonardo da Vinci, which Gates bought for $30.8 million at an auction in 1994. Gates is also known as an avid reader, and the ceiling of his large home library is engraved with a quotation from "The Great Gatsby". He also enjoys playing bridge, tennis, and golf.
Gates was number one on the Forbes 400 list from 1993 through to 2007 and number one on "Forbes" list of The World's Richest People from 1995 to 2007 and 2009. In 1999, his wealth briefly surpassed $101 billion, causing the media to call Gates a "centibillionaire". Despite his wealth and extensive business travel Gates usually flew coach until 1997, when he bought a private jet. Since 2000, the nominal value of his Microsoft holdings has declined due to a fall in Microsoft's stock price after the dot-com bubble burst and the multi-billion dollar donations he has made to his charitable foundations. In a May 2006 interview, Gates commented that he wished that he were not the richest man in the world because he disliked the attention it brought. In March 2010, Gates was the second wealthiest person behind Carlos Slim, but regained the top position in 2013 according to the Bloomberg Billionaires List. Carlos Slim retook the position again in June 2014.
Gates has several investments outside Microsoft, which in 2006 paid him a salary of $616,667 and $350,000 bonus totalling $966,667. He founded Corbis, a digital imaging company, in 1989. In 2004 he became a director of Berkshire Hathaway, the investment company headed by long-time friend Warren Buffett.
Around the 1990s, Gates spoke at a high school about "the eleven rules of life," aimed at high school and college graduates. The rules have since been repeated in schools across the world, with the purpose of educating students on how to be successful in their future. Although the rules are commonly attributed to Gates, it is actually originally written by educator Charles Sykes in his book "Dumbing Down on Our Kids," written in 1996.
Philanthropy.
Bill & Melinda Gates Foundation.
Gates studied the work of Andrew Carnegie and John D. Rockefeller, and in 1994 sold some of his Microsoft stock to create the "William H. Gates Foundation." In 2000, Gates and his wife combined three family foundations to create the charitable "Bill & Melinda Gates Foundation," which was identified by the Funds for NGOs company in 2013 as the world's wealthiest charitable foundation, with assets reportedly valued at more than US$34.6 billion. The Foundation allows benefactors to access information that shows how its money is being spent, unlike other major charitable organizations such as the Wellcome Trust.
Gates has credited the generosity and extensive philanthropy of David Rockefeller as a major influence. Gates and his father met with Rockefeller several times, and their charity work is partly modeled on the Rockefeller family's philanthropic focus, whereby they are interested in tackling the global problems that are ignored by governments and other organizations. As of 2007, Bill and Melinda Gates were the second-most generous philanthropists in America, having given over US$28 billion to charity; the couple plan to eventually donate 95 percent of their wealth to charity.
Personal.
Gates's wife urged people to learn a lesson from the philanthropic efforts of the Salwen family, which had sold its home and given away half of its value, as detailed in "The Power of Half". Gates and his wife invited Joan Salwen to Seattle to speak about what the family had done, and on December 9, 2010, Gates, investor Warren Buffett, and Facebook founder and CEO Mark Zuckerberg signed a commitment they called the "Gates-Buffet Giving Pledge." The pledge is a commitment by all three to donate at least half of their wealth over the course of time to charity.
Gates has recently expressed concern about the existential threats of Superintelligence; in a Reddit "ask me anything", he stated that 
"First the machines will do a lot of jobs for us and not be super intelligent. That should be positive if we manage it well. A few decades after that though the intelligence is strong enough to be a concern. I agree with Elon Musk and some others on this and don't understand why some people are not concerned." In a March 2015 interview with Baidu's CEO, Robert Li, Gates claimed he would "highly recommend" Nick Bostrom's recent work, .
Gates has also provided personal donations to educational institutions. In 1999 Gates donated US$20 million to the Massachusetts Institute of Technology (MIT) for the construction of a computer laboratory named the "William H. Gates Building" that was designed by architect Frank O. Gehry. While Microsoft had previously given financial support to the institution, this was the first personal donation received from Gates.
The Maxwell Dworkin Laboratory of the Harvard School of Engineering and Applied Sciences is named after the mothers of both Gates and Microsoft President Steven A. Ballmer, both of whom were students (Ballmer was a member of the School's graduating class of 1977, while Gates left his studies for Microsoft), and donated funds for the laboratory's construction. Gates also donated US$6 million to the construction of the Gates Computer Science Building, completed in January 1996, on the campus of Stanford University. The building contains the Computer Science Department (CSD) and the Computer Systems Laboratory (CSL) of Stanford's Engineering department.
On August 15, 2014, Bill Gates posted a video of himself dumping a bucket of ice water on his head, after Facebook founder Mark Zuckerberg challenged him to do so, in order to raise awareness for the disease ALS (amyotrophic lateral sclerosis).
Bill Gates and his foundation are taking an interest in solving global sanitation problems since about 2005, for example by announcing the "Reinvent the Toilet Challenge" which has received considerable media interest. To raise awareness for the topic of sanitation and possible solutions, Bill Gates drank water which was "produced from human feces" in 2014 - in fact it was produced from a sewage sludge treatment process called the Omni-processor. In early 2015 he also appeared with Jimmy Fallon on Late Night With Jimmy Fallon and challenged him to see if he could taste the difference between this reclaimed water or bottled water.
Criticism.
In 2007 the "Los Angeles Times" criticized the foundation for investing its assets in companies which have been accused of worsening poverty, polluting heavily, and pharmaceutical companies that do not sell into the developing world.
In response to press criticism, the foundation announced a review of its investments to assess social responsibility. It subsequently canceled the review and stood by its policy of investing for maximum return, while using voting rights to influence company practices. The Gates Millennium Scholars program has been criticized by Ernest W. Lefever for its exclusion of Caucasian students. The scholarship program is administered by the United Negro College Fund.
Recognition.
In 1987 Gates was listed as a billionaire in "Forbes" magazine's 400 Richest People in America issue, just days before his 32nd birthday. As the world's youngest self-made billionaire, he was worth $1.25 billion, over $900 million more than he'd been worth the year before, when he'd debuted on the list.
In 2015 Government of India awarded The Padma Bhushan the third highest civilian award of The Republic of India for Gates and his wife Melinda for their contribution in Social work. "Time" magazine named Gates , as well as one of the 100 most influential people of 2004, 2005, and 2006. "Time" also collectively named Gates, his wife Melinda and U2's lead singer Bono as the 2005 Persons of the Year for their humanitarian efforts. In 2006, he was voted eighth in the list of "Heroes of our time". Gates was listed in the "Sunday Times" power list in 1999, named CEO of the year by "Chief Executive Officers magazine" in 1994, ranked number one in the "Top 50 Cyber Elite" by "Time" in 1998, ranked number two in the "Upside" Elite 100 in 1999 and was included in "The Guardian" as one of the "Top 100 influential people in media" in 2001.
According to "Forbes", Gates was ranked as the fourth most powerful person in the world in 2012, up from fifth in 2011.
In 1994, he was honored as the twentieth Distinguished Fellow of the British Computer Society. Gates has received honorary doctorates from Nyenrode Business Universiteit, Breukelen, The Netherlands, in 2000; KTH Royal Institute of Technology, Stockholm, Sweden, in 2002; Waseda University, Tokyo, Japan, in 2005; Tsinghua University, Beijing, China, in April 2007; Harvard University in June 2007; Karolinska Institutet, Stockholm, in 2007, and Cambridge University in June 2009. He was also made an honorary trustee of Peking University in 2007.
Gates was made an honorary Knight Commander of the Order of the British Empire (KBE) by Queen Elizabeth II in 2005. In November 2006, he was awarded the Placard of the Order of the Aztec Eagle, together with his wife Melinda who was awarded the Insignia of the same order, both for their philanthropic work around the world in the areas of health and education, particularly in Mexico, and specifically in the program ""Un país de lectores"". Gates received the 2010 Bower Award for Business Leadership from The Franklin Institute for his achievements at Microsoft and his philanthropic work. Also in 2010 he was honored with the Silver Buffalo Award by the Boy Scouts of America, its highest award for adults, for his service to youth.
Entomologists named Bill Gates' flower fly, "Eristalis gatesi", in his honor in 1997.
In 2002, Bill and Melinda Gates received the Jefferson Award for Greatest Public Service Benefiting the Disadvantaged.
In 2006, Gates received the James C. Morgan Global Humanitarian Award from The Tech Awards.
In recognition for the foundation's philanthropic activities in India, Bill and Melinda Gates jointly received India's third highest civilian honor Padma Bhushan in 2015.
Books, films, and social media.
Books.
To date, Bill Gates has authored two books:
Social media.
In 2013, Gates became a LinkedIn Influencer.

</doc>
<doc id="3748" url="http://en.wikipedia.org/wiki?curid=3748" title="Bourbon">
Bourbon

Bourbon may refer to:

</doc>
<doc id="3751" url="http://en.wikipedia.org/wiki?curid=3751" title="Belgian Blue">
Belgian Blue

Belgian Blue is a breed of beef cattle from Belgium. These cattle are referred to in French as "Race de la Moyenne et Haute Belgique", or, more commonly, "Blanc Bleu Belge". Alternative names for this breed include Belgian Blue-White; Belgian White and Blue Pied; Belgian White Blue; Blue; and Blue Belgian. The Belgian Blue's sculpted, heavily muscled appearance is known as "double-muscling". The double-muscling phenotype is a heritable condition which results in the increased number of muscle fibers (hyperplasia) rather than the normal enlargement of individual muscle fibers (hypertrophy).
This particular trait is shared with another breed of cattle known as Piedmontese. Both of these breeds have an increased ability to convert feed into lean muscle, which causes these particular breeds' meat to have a reduced fat content. The Belgian Blue is named after their typically blue-grey mottled hair colour, however its colour can vary from white to black.
History.
The condition was first documented in 1808 by a livestock observationist named George Culley. The breed originated in central and upper Belgium in the nineteenth century, from crossing local breeds with a Shorthorn breed of cattle from the United Kingdom. It is also possible that Charolais cattle were cross bred as well. Belgian Blue cattle were first used as a dairy and beef breed. The modern beef breed was developed in the 1950s by Professor Hanset, working at an artificial insemination centre in Liege province. The breed's characteristic gene mutation was maintained through linebreeding to the point where the condition was a fixed property in the Belgian Blue breed. In 1978 the Belgian Blue cattle were introduced to the United States through a man by the name of Nick Tutt, a farmer from central Canada who immigrated to west Texas showing surrounding Universities of this cattle.
Breed characteristics.
The Belgian Blue has a natural mutation in the myostatin gene which codes for the protein, myostatin ("myo" meaning muscle and "statin" meaning stop). Myostatin is a protein that acts to inhibit muscle development. This mutation also interferes with fat deposition, resulting in very lean meat. The truncated myostatin gene is unable to function in its normal capacity, resulting in accelerated lean muscle growth. Muscle growth is due primarily to physiological changes in the animal's muscle cells (fibers) from hypertrophy to a hyperplasia mode of growth. This particular type of growth is seen early in the fetus of a pregnant dam, which results in a calf that is born with two times the number of muscle fibers at birth than a calf with no myostatin gene mutation. In addition a new born double-muscled calf’s birth weight compared to a normal calf is significantly greater.
Belgian Blue cattle have improved feed conversion ratio (FCR) due to lower feed intake compared to weight gain. The ability for these animals to have improved FCR is due to an altered composition of body weight gain which includes increased protein and decreased fat deposition. The Belgian Blue’s bone structure is the same as a normal cow, albeit holding a greater amount of muscle, which causes them to have a greater meat to bone ratio. These cattle have a muscle yield of about 20% more on average than cattle without the genetic myostatin mutation. Because of this breed’s increased muscle yield a diet containing higher protein is required to compensate for the altered mode of weight gain. During finishing this breed requires high-energy (concentrated) feeds, and will not yield the same results if put on a high-fiber diet.
The value of the double-muscling breed is due to their superior carcass characteristics. However with decreased fat content there is decreased marbling of meat, which means the meat tenderness is reduced. Conversely, the Belgian Blue's meat tenderness has been argued to be just as tender because there are a large number of smaller muscle fibers. The Belgian Blue's meat cuts also have a lower collagen content, which allows the protein quality to be improved due to a higher yield of amino acids.
Breed issues.
Double-muscled cows can experience dystocia (a difficult birth), even when bred to normal beef bulls or dairy bulls, because of a narrower birth canal. In addition to the dam’s reduced pelvic dimensions, the calf’s birth weight and width are increased, making parturition harder. The neonatal calf is so large that Caesarean sections are routinely scheduled for breeders. The bull’s testicular weight and semen quantity and quality have been observed as reduced, however this seems to be less of an issue when compared to the dam's difficulties in calving.
Economic efficiency.
The economics of breeding and raising Belgian Blue cattle are inconclusive because of complications experienced during parturition and metabolic demand for increased concentrated feeds. The breed's increased need to have Caesarean sections when calving means increased cost and added work, and can become a welfare issue. However, the carcass value of double-muscled animals may be enhanced due to increased dressing yield, lean carcass content, and upgrading of some cuts leading to a higher proportion of higher valued cuts. The slower rate of fat deposition causes slaughtering to be delayed in most cases, which means an increase in maintenance costs in those animals. Belgian Blue cattle require more skilled management and do not thrive in harsh environments. For these reasons and others, the breed's overall production efficiency in an economic sense is still unclear.

</doc>
<doc id="3755" url="http://en.wikipedia.org/wiki?curid=3755" title="Boron">
Boron

Boron is a chemical element with symbol B and atomic number 5. Because boron is produced entirely by cosmic ray spallation and not by stellar nucleosynthesis it is a low-abundance element in both the Solar system and the Earth's crust. Boron is concentrated on Earth by the water-solubility of its more common naturally occurring compounds, the borate minerals. These are mined industrially as evaporites, such as borax and kernite. The largest proven boron deposits are in Turkey, which is also the largest producer of boron minerals.
Chemically uncombined boron, which is classed as a metalloid, is found in small amounts in meteoroids, but is not found naturally on Earth. Industrially, very pure boron is produced with difficulty, as boron tends to form refractory materials containing small amounts of carbon or other elements. Several allotropes of boron exist: amorphous boron is a brown powder, and crystalline boron is black, extremely hard (about 9.5 on the Mohs scale), and a poor conductor at room temperature. The primary use of elemental boron is to make boron filaments, which are used in a similar way to carbon fibers in some high-strength materials.
Almost all boron use is as chemical compounds. About half of global consumption of boron compounds is as additives for glass fibers in boron-containing fiberglass used for insulation or as structural materials. The next leading use is to make boron polymers and ceramics, that play specialized roles as high-strength lightweight structural and refractory materials. Borosilicate glass glassware is used for its greater strength and breakage resistance (thermal shock resistance) than ordinary soda lime glass. Boron compounds are also used as fertilizers in agriculture, and in sodium perborate bleaches. In minor uses, boron is an important dopant for semiconductors, and boron-containing reagents are used as intermediates in the synthesis of organic fine chemicals. A few boron-containing organic pharmaceuticals are used, or are in study. Natural boron is composed of two stable isotopes, one of which (boron-10) has a number of uses as a neutron-capturing agent.
In biology, borates have low toxicity in mammals (similar to table salt), but are more toxic to arthropods and are used as insecticides. Boric acid is mildly antimicrobial, and a natural boron-containing organic antibiotic is known. Boron is essential to life. Small amounts of boron compounds play a strengthening role in the cell walls of all plants, making boron necessary in soils. Experiments indicate a role for boron as an ultratrace element in animals, but its role in animal physiology is unknown.
History.
The word "boron" was coined from "borax", the mineral from which it was isolated, by analogy with "carbon", which it resembles chemically. For the etymology of "borax", see that article. 
Borax glazes were used in China from AD 300, and some tincal (crude borax) reached the West, where the Persian alchemist Jābir ibn Hayyān seems to mention it in AD 700. Marco Polo brought some glazes back to Italy in the 13th century. Agricola, around 1600, reports the use of borax as a flux in metallurgy. In 1777, boric acid was recognized in the hot springs (soffioni) near Florence, Italy, and became known as "sal sedativum", with mainly medical uses. The rare mineral is called sassolite, which is found at Sasso, Italy. Sasso was the main source of European borax from 1827 to 1872, at which date American sources replaced it. Boron compounds were relatively rarely used chemicals until the late 1800s when Francis Marion Smith's Pacific Coast Borax Company first popularized these compounds and made them in volume and hence cheap.
Boron was not recognized as an element until it was isolated by Sir Humphry Davy and by Joseph Louis Gay-Lussac and Louis Jacques Thénard. In 1808 Davy observed that electric current sent through a solution of borates produced a brown precipitate on one of the electrodes. In his subsequent experiments, he used potassium to reduce boric acid instead of electrolysis. He produced enough boron to confirm a new element and named the element "boracium". Gay-Lussac and Thénard used iron to reduce boric acid at high temperatures. By oxidizing boron with air, they showed that boric acid is an oxidation product of boron.
Jöns Jakob Berzelius identified boron as an element in 1824. Pure boron was arguably first produced by the American chemist Ezekiel Weintraub in 1909.
Preparation of elemental boron in the laboratory.
The earliest routes to elemental boron involved reduction of boric oxide with metals such as magnesium or aluminium. However the product is almost always contaminated with metal borides. Pure boron can be prepared by reducing volatile boron halides with hydrogen at high temperatures. Ultrapure boron for use in the semiconductor industry is produced by the decomposition of diborane at high temperatures and then further purified with the zone melting or Czochralski processes.
The production of boron compounds does not involve formation of elemental boron, but exploits the convenient availability of borates.
Characteristics.
Allotropes.
Boron is similar to carbon in its capability to form stable covalently bonded molecular networks. Even nominally disordered (amorphous) boron contains regular boron icosahedra which are, however, bonded randomly to each other without long-range order. Crystalline boron is a very hard, black material with a high melting point of above 2000 °C. It exists in four major polymorphs: α-rhombohedral and β-rhombohedral (α-R and β-R), γ and β-tetragonal (β-T); α-tetragonal phase also exists (α-T), but is very difficult to produce without significant contamination. Whereas α, β and T phases are based on B12 icosahedra, the γ-phase can be described as a rocksalt-type arrangement of the icosahedra and B2 atomic pairs. It can be produced by compressing other boron phases to 12–20 GPa and heating to 1500–1800 °C; it remains stable after releasing the temperature and pressure. The T phase is produced at similar pressures, but higher temperatures of 1800–2200 °C. As to the α and β phases, they might both coexist at ambient conditions with the β phase being more stable. Compressing boron above 160 GPa produces a boron phase with an as yet unknown structure, and this phase is a superconductor at temperatures 6–12 K. Borospherene (fullerene-like B40) molecules) and borophene (proposed graphene-like structure) have been described in 2014.
Chemistry of the element.
See also: .
Elemental boron is rare and poorly studied because the material is extremely difficult to prepare. Most studies on "boron" involve samples that contain small amounts of carbon. Chemically, boron behaves more similarly to silicon than to aluminium. Crystalline boron is chemically inert and resistant to attack by boiling hydrofluoric or hydrochloric acid. When finely divided, it is attacked slowly by hot concentrated hydrogen peroxide, hot concentrated nitric acid, hot sulfuric acid or hot mixture of sulfuric and chromic acids.
The rate of oxidation of boron depends upon the crystallinity, particle size, purity and temperature. Boron does not react with air at room temperature, but at higher temperatures it burns to form boron trioxide:
Boron undergoes halogenation to give trihalides, for example,
The trichloride in practice is usually made from the oxide.
Chemical compounds.
In its most familiar compounds, boron has the formal oxidation state III. These include oxides, sulfides, nitrides, and halides.
The trihalides adopt a planar trigonal structure. These compounds are Lewis acids in that they readily form adducts with electron-pair donors, which are called Lewis bases. For example, fluoride (F−) and boron trifluoride (BF3) combined to give the tetrafluoroborate anion, BF4−. Boron trifluoride is used in the petrochemical industry as a catalyst. The halides react with water to form boric acid.
Boron is found in nature on Earth entirely as various oxides of B(III), often associated with other elements. More than one hundred borate minerals contain boron in oxidation state +3. These minerals resemble silicates in some respect, although boron is often found not only in a tetrahedral coordination with oxygen, but also in a trigonal planar configuration. Unlike silicates, the boron minerals never contain boron with coordination number greater than four. A typical motif is exemplified by the tetraborate anions of the common mineral borax, shown at left. The formal negative charge of the tetrahedral borate centers is balanced by metal cations in the minerals, such as the sodium (Na+) in borax.
Boranes are chemical compounds of boron and hydrogen, with the generic formula of BxHy. These compounds do not occur in nature. Many of the boranes readily oxidise on contact with air, some violently. The parent member BH3 is called borane, but it is known only in the gaseous state, and dimerises to form diborane, B2H6. The larger boranes all consist of boron clusters that are polyhedral, some of which exist as isomers. For example, isomers of B20H26 are based on the fusion of two 10-atom clusters.
The most important boranes are diborane B2H6 and two of its pyrolysis products, pentaborane B5H9 and decaborane B10H14. A large number of anionic boron hydrides are known, e.g. [B12H12]2−.
The formal oxidation number in boranes is positive, and is based on the assumption that hydrogen is counted as −1 as in active metal hydrides. The mean oxidation number for the borons is then simply the ratio of hydrogen to boron in the molecule. For example, in diborane B2H6, the boron oxidation state is +3, but in decaborane B10H14, it is 7/5 or +1.4. In these compounds the oxidation state of boron is often not a whole number.
The boron nitrides are notable for the variety of structures that they adopt. They adopt structures analogous to various allotropes of carbon, including graphite, diamond, and nanotubes. In the diamond-like structure called cubic boron nitride (tradename Borazon), boron atoms exist in the tetrahedral structure of carbons atoms in diamond, but one in every four B-N bonds can be viewed as a coordinate covalent bond, wherein two electrons are donated by the nitrogen atom which acts as the Lewis base to a bond to the Lewis acidic boron(III) centre. Cubic boron nitride, among other applications, is used as an abrasive, as it has a hardness comparable with diamond (the two substances are able to produce scratches on each other). In the BN compound analogue of graphite, hexagonal boron nitride (h-BN), the positively charged boron and negatively charged nitrogen atoms in each plane lie adjacent to the oppositely charged atom in the next plane. Consequently graphite and h-BN have very different properties, although both are lubricants, as these planes slip past each other easily. However, h-BN is a relatively poor electrical and thermal conductor in the planar directions.
Organoboron chemistry.
A large number of organoboron compounds are known and many are useful in organic synthesis. Many are produced from hydroboration, which employs diborane, B2H6, a simple borane chemical. Organoboron(III) compounds are usually tetrahedral or trigonal planar, for example, tetraphenylborate, [B(C6H5)4]− vs. triphenylborane, B(C6H5)3. However, multiple boron atoms reacting with each other have a tendency to form novel dodecahedral (12-sided) and icosahedral (20-sided) structures composed completely of boron atoms, or with varying numbers of carbon heteroatoms.
Organoboron chemicals have been employed in uses as diverse as boron carbide (see below), a complex very hard ceramic composed of boron-carbon cluster anions and cations, to carboranes, carbon-boron cluster chemistry compounds that can be halogenated to form reactive structures including carborane acid, a superacid. As one example, carboranes form useful molecular moieties that add considerable amounts of boron to other biochemicals in order to synthesize boron-containing compounds for boron neutron capture therapy of cancer.
Compounds of B(I) and B(II).
Although these are not found on Earth naturally, boron forms a variety of stable compounds with formal oxidation state less than three. As for many covalent compounds, formal oxidation states are often of little meaning in boron hydrides and metal borides. The halides also form derivatives of B(I) and B(II). BF, isoelectronic with N2, is not isolable in condensed form, but B2F4 and B4Cl4 are well characterized.
Binary metal-boron compounds, the metal borides, contain boron in negative oxidation states. Illustrative is magnesium diboride (MgB2). Each boron atom has a formal −1 charge and magnesium is assigned a formal charge of 2+.
In this material, the boron centers are trigonal planar, with an extra double bond for each boron, with the boron atoms forming sheets akin to the carbon in graphite. However, unlike the case with hexagonal boron nitride which by comparison lacks electrons in the plane of the covalent atoms, the delocalized electrons in the plane of magnesium diboride allow it to conduct electricity similar to isoelectronic graphite. In addition, in 2001 this material was found to be a high-temperature superconductor.
Certain other metal borides find specialized applications as hard materials for cutting tools. Often boron in borides has fractional oxidation states, such as −1/3 in calcium hexaboride (CaB6).
From the structural perspective, the most distinctive chemical compounds of boron are the hydrides. Included in this series are the cluster compounds dodecaborate (B12H122−), decaborane (B10H14), and the carboranes such as C2B10H12. Characteristically such compounds contain boron with coordination numbers greater than four.
Isotopes.
Boron has two naturally occurring and stable isotopes, 11B (80.1%) and 10B (19.9%). The mass difference results in a wide range of δ11B values, which are defined as a fractional difference between the 11B and 10B and traditionally expressed in parts per thousand, in natural waters ranging from −16 to +59. There are 13 known isotopes of boron, the shortest-lived isotope is 7B which decays through proton emission and alpha decay. It has a half-life of 3.5×10−22 s. Isotopic fractionation of boron is controlled by the exchange reactions of the boron species B(OH)3 and [B(OH)4]−. Boron isotopes are also fractionated during mineral crystallization, during H2O phase changes in hydrothermal systems, and during hydrothermal alteration of rock. The latter effect results in preferential removal of the [10B(OH)4]− ion onto clays. It results in solutions enriched in 11B(OH)3 and therefore may be responsible for the large 11B enrichment in seawater relative to both oceanic crust and continental crust; this difference may act as an isotopic signature. The exotic 17B exhibits a nuclear halo, i.e. its radius is appreciably larger than that predicted by the liquid drop model.
The 10B isotope is good at capturing thermal neutrons (see neutron cross section#Typical cross sections). Natural boron is about 20% 10B and 80% 11B. The nuclear industry enriches natural boron to nearly pure 10B. The less-valuable by-product, depleted boron, is nearly pure 11B.
Commercial isotope enrichment.
Because of its high neutron cross-section, boron-10 is often used to control fission in nuclear reactors as a neutron-capturing substance. Several industrial-scale enrichment processes have been developed, however only the fractionated vacuum distillation of the dimethyl ether adduct of boron trifluoride (DME-BF3) and column chromatography of borates are being used.
Enriched boron (boron-10).
Enriched boron or 10B is used in both radiation shielding and is the primary nuclide used in neutron capture therapy of cancer. In the latter ("boron neutron capture therapy" or BNCT), a compound containing 10B is incorporated into a pharmaceutical which is selectively taken up by a malignant tumor and tissues near it. The patient is then treated with a beam of low energy neutrons at a relatively low neutron radiation dose. The neutrons, however, trigger energetic and short-range secondary alpha particle and lithium-7 heavy ion radiation that are products of the boron + neutron nuclear reaction, and this ion radiation additionally bombards the tumor, especially from inside the tumor cells.
In nuclear reactors, 10B is used for reactivity control and in emergency shutdown systems. It can serve either function in the form of borosilicate control rods or as boric acid. In pressurized water reactors, boric acid is added to the reactor coolant when the plant is shut down for refueling. It is then slowly filtered out over many months as fissile material is used up and the fuel becomes less reactive.
In future manned interplanetary spacecraft, 10B has a theoretical role as structural material (as boron fibers or BN nanotube material) which would also serve a special role in the radiation shield. One of the difficulties in dealing with cosmic rays, which are mostly high energy protons, is that some secondary radiation from interaction of cosmic rays and spacecraft materials is high energy spallation neutrons. Such neutrons can be moderated by materials high in light elements such as polyethylene, but the moderated neutrons continue to be a radiation hazard unless actively absorbed in the shielding. Among light elements that absorb thermal neutrons, 6Li and 10B appear as potential spacecraft structural materials which serve both for mechanical reinforcement and radiation protection.
Depleted boron (boron-11).
Radiation-hardened semiconductors.
Cosmic radiation will produce secondary neutrons if it hits spacecraft structures. Those neutrons will be captured in 10B, if it is present in the spacecraft's semiconductors, producing a gamma ray, an alpha particle, and a lithium ion. These resultant decay products may then irradiate nearby semiconductor "chip" structures, causing data loss (bit flipping, or single event upset). In radiation-hardened semiconductor designs, one countermeasure is to use "depleted boron", which is greatly enriched in 11B and contains almost no 10B. It is useful because 11B is largely immune to radiation damage. Depleted boron is a byproduct of the nuclear industry.
Proton-boron fusion.
11B is also a candidate as a fuel for aneutronic fusion. When struck by a proton with energy of about 500 keV, it produces three alpha particles and 8.7 MeV of energy. Most other fusion reactions involving hydrogen and helium produce penetrating neutron radiation, which weakens reactor structures and induces long-term radioactivity, thereby endangering operating personnel. However, the alpha particles from 11B fusion can be turned directly into electric power, and all radiation stops as soon as the reactor is turned off.
NMR spectroscopy.
Both 10B and 11B possess nuclear spin. The nuclear spin of 10B is 3 and that of 11B is 3⁄2. These isotopes are, therefore, of use in nuclear magnetic resonance spectroscopy; and spectrometers specially adapted to detecting the boron-11 nuclei are available commercially. The 10B and 11B nuclei also cause splitting in the resonances of attached nuclei.
Occurrence.
See also: .
Boron is rare in the Universe and solar system due to trace formation in the Big Bang and in stars. It is formed in minor amounts in cosmic ray spallation nucleosynthesis and may be found uncombined in cosmic dust and meteoroid materials. In the high oxygen environment of Earth, boron is always found fully oxidized to borate. Boron does not appear on Earth in elemental form.
Although boron is a relatively rare element in the Earth's crust, representing only 0.001% of the crust mass, it can be highly concentrated by the action of water, in which many borates are soluble. 
It is found naturally combined in compounds such as borax and boric acid (sometimes found in volcanic spring waters). About a hundred borate minerals are known.
Production.
Economically important sources of boron are the minerals colemanite, rasorite (kernite), ulexite and tincal. Together these constitute 90% of mined boron-containing ore. The largest global borax deposits known, many still untapped, are in Central and Western Turkey, including the provinces of Eskişehir, Kütahya and Balıkesir. Global proven boron mineral mining reserves exceed one billion metric tonnes, against a yearly production of about four million tonnes.
Turkey and the United States are the largest producers of boron products. Turkey produces about half of the global yearly demand, though Eti Mine Works (Turkish: "Eti Maden İşletmeleri") a Turkish state-owned mining and chemicals company focusing on boron products. It holds a government monopoly on the mining of borate minerals in Turkey, which possesses 72% of the world's known deposits. In 2012, it held a 47% share of production of global borate minerals, ahead of its main competitor, Rio Tinto Group.
Almost a quarter (23%) of global boron production comes from the single Rio Tinto Borax Mine (also known as the U.S. Borax Boron Mine) near Boron, California.
Market trend.
The average cost of crystalline boron is $5/g. Free boron is chiefly used in making boron fibers, where it is deposited by chemical vapor deposition on a tungsten core (see below). Boron fibers are used in lightweight composite applications, such as high strength tapes. This use is a very small fraction of total boron use. Boron is introduced into semiconductors as boron compounds, by ion implantation.
Estimated global consumption of boron (almost entirely as boron compounds) was about 4 million tonnes of B2O3 in 2012. Boron mining and refining capacities are considered to be adequate to meet expected levels of growth through the next decade.
The form in which boron is consumed has changed in recent years. The use of ores like colemanite has declined following concerns over arsenic content. Consumers have moved toward the use of refined borates and boric acid that have a lower pollutant content.
Increasing demand for boric acid has led a number of producers to invest in additional capacity. Turkey's state-owned Eti Mine Works opened a new boric acid plant with the production capacity of 100,000 tonnes per year at Emet in 2003. Rio Tinto Group increased the capacity of its boron plant from 260,000 tonnes per year in 2003 to 310,000 tonnes per year by May 2005, with plans to grow this to 366,000 tonnes per year in 2006. Chinese boron producers have been unable to meet rapidly growing demand for high quality borates. This has led to imports of sodium tetraborate (borax) growing by a hundredfold between 2000 and 2005 and boric acid imports increasing by 28% per year over the same period.
The rise in global demand has been driven by high growth rates in glass fiber, fiberglass and borosilicate glassware production. A rapid increase in the manufacture of reinforcement-grade boron-containing fiberglass in Asia, has offset the development of boron-free reinforcement-grade fiberglass in Europe and the USA. The recent rises in energy prices may lead to greater use of insulation-grade fiberglass, with consequent growth in the boron consumption. Roskill Consulting Group forecasts that world demand for boron will grow by 3.4% per year to reach 21 million tonnes by 2010. The highest growth in demand is expected to be in Asia where demand could rise by an average 5.7% per year.
Applications.
Nearly all boron ore extracted from the Earth is destined for refinement into boric acid and sodium tetraborate pentahydrate. In the United States, 70% of the boron is used for the production of glass and ceramics.
The major global industrial-scale use of boron compounds (about 46% of end-use) is in production of glass fiber for boron-containing insulating and structural fiberglasses, especially in Asia. Boron is added to the glass as borax pentahydrate or boron oxide, to influence the strength or fluxing qualities of the glass fibers. Another 10% of global boron production is for borosilicate glass as used in high strength glassware. About 15% of global boron is used in boron ceramics, including super-hard materials discussed below. Agriculture consumes 11% of global boron production, and bleaches and detergents about 6%.
Elemental boron fiber.
Boron fibers (boron filaments) are high-strength, lightweight materials that are used chiefly for advanced aerospace structures as a component of composite materials, as well as limited production consumer and sporting goods such as golf clubs and fishing rods. The fibers can be produced by chemical vapor deposition of boron on a tungsten filament.
Boron fibers and sub-millimeter sized crystalline boron springs are produced by laser-assisted chemical vapor deposition. Translation of the focused laser beam allows to produce even complex helical structures. Such structures show good mechanical properties (elastic modulus 450 GPa, fracture strain 3.7%, fracture stress 17 GPa) and can be applied as reinforcement of ceramics or in micromechanical systems.
Boronated fiberglass.
Fiberglass is a fiber reinforced polymer made of plastic reinforced by glass fibers, commonly woven into a mat. The glass fibers used in the material are made of various types of glass depending upon the fiberglass use. These glasses all contain silica or silicate, with varying amounts of oxides of calcium, magnesium, and sometimes boron. The boron is present as borosilicate, borax, or boron oxide, and is added to increase the strength of the glass, or as a fluxing agent to decrease the melting temperature of silica, which is too high to be easily worked in its pure form to make glass fibers.
The highly boronated glasses used in fiberglass are E-glass (named for "Electrical" use, but now the most common fiberglass for general use). E-glass is alumino-borosilicate glass with less than 1% w/w alkali oxides, mainly used for glass-reinforced plastics. Other common high-boron glasses include C-glass, an alkali-lime glass with high boron oxide content, used for glass staple fibers and insulation, and D-glass, a borosilicate glass, named for its low Dielectric constant).
Not all fiberglasses contain boron, but on a global scale, most of the fiberglass used does contain it. Because the ubiquitous use of fiberglass in construction and insulation, boron-containing fiberglasses consume half the global production of boron, and are the single largest commercial boron market.
Borosilicate glass.
Borosilicate glass, which is typically 12–15% B2O3, 80% SiO2, and 2% Al2O3, has a low coefficient of thermal expansion giving it a good resistance to thermal shock. Schott AG's "Duran" and Owens-Corning's trademarked Pyrex are two major brand names for this glass, used both in laboratory glassware and in consumer cookware and bakeware, chiefly for this resistance.
Boron carbide ceramic.
Several boron compounds are known for their extreme hardness and toughness.
Boron carbide is a ceramic material which is obtained by decomposing B2O3 with carbon in the electric furnace:
Boron carbide's structure is only approximately B4C, and it shows a clear depletion of carbon from this suggested stoichiometric ratio. This is due to its very complex structure. The substance can be seen with empirical formula B12C3 (i.e., with B12 dodecahedra being a motif), but with less carbon, as the suggested C3 units are replaced with C-B-C chains, and some smaller (B6) octahedra are present as well (see the boron carbide article for structural analysis). The repeating polymer plus semi-crystalline structure of boron carbide gives it great structural strength per weight. It is used in tank armor, bulletproof vests, and numerous other structural applications.
Boron carbide's ability to absorb neutrons without forming long-lived radionuclides (especially when doped with extra boron-10) makes the material attractive as an absorbent for neutron radiation arising in nuclear power plants. Nuclear applications of boron carbide include shielding, control rods and shut-down pellets. Within control rods, boron carbide is often powdered, to increase its surface area.
High-hardness and abrasive compounds.
Boron carbide and cubic boron nitride powders are widely used as abrasives. Boron nitride is a material isoelectronic to carbon. Similar to carbon, it has both hexagonal (soft graphite-like h-BN) and cubic (hard, diamond-like c-BN) forms. h-BN is used as a high temperature component and lubricant. c-BN, also known under commercial name borazon, is a superior abrasive. Its hardness is only slightly smaller than, but its chemical stability is superior, to that of diamond. Heterodiamond (also called BCN) is another diamond-like boron compound.
Boron metal coatings.
Metal borides are used for coating tools through chemical vapor deposition or physical vapor deposition. Implantation of boron ions into metals and alloys, through ion implantation or ion beam deposition, results in a spectacular increase in surface resistance and microhardness. Laser alloying has also been successfully used for the same purpose. These borides are an alternative to diamond coated tools, and their (treated) surfaces have similar properties to those of the bulk boride.
For example, rhenium diboride can be produced at ambient pressures, but is rather expensive because of rhenium. The hardness of ReB2 exhibits considerable anisotropy because of its hexagonal layered structure. Its value is comparable to that of tungsten carbide, silicon carbide, titanium diboride or zirconium diboride.
Similarly, AlMgB14 + TiB2 composites possess high hardness and wear resistance and are used in either bulk form or as coatings for components exposed to high temperatures and wear loads.
Detergent formulations and bleaching agents.
Borax is used in various household laundry and cleaning products, including the "20 Mule Team Borax" laundry booster and "Boraxo" powdered hand soap. It is also present in some tooth bleaching formulas.
Sodium perborate serves as a source of active oxygen in many detergents, laundry detergents, cleaning products, and laundry bleaches. However, despite its name, "Borateem" laundry bleach no longer contains any boron compounds, using sodium percarbonate instead as a bleaching agent.
Insecticides.
Boric acid is used as an insecticide, notably against ants, fleas, and cockroaches.
Semiconductors.
Boron is a useful dopant for such semiconductors as silicon, germanium, and silicon carbide. Having one fewer valence electron than the host atom, it donates a hole resulting in p-type conductivity. Traditional method of introducing boron into semiconductors is via its atomic diffusion at high temperatures. This process uses either solid (B2O3), liquid (BBr3), or gaseous boron sources (B2H6 or BF3). However, after the 1970s, it was mostly replaced by ion implantation, which relies mostly on BF3 as a boron source. Boron trichloride gas is also an important chemical in semiconductor industry, however not for doping but rather for plasma etching of metals and their oxides. Triethylborane is also injected into vapor deposition reactors as a boron source. Examples are the plasma deposition of boron-containing hard carbon films, silicon nitride-boron nitride films, and for doping of diamond film with boron.
Magnets.
Boron is a component of neodymium magnets (Nd2Fe14B), which are among the strongest type of permanent magnet. These magnets are found in a variety of electromechanical and electronic devices, such as magnetic resonance imaging (MRI) medical imaging systems, in compact and relatively small motors and actuators. As examples, computer HDDs (hard disk drives), CD (compact disk) and DVD (digital versatile disk) players rely on neodymium magnet motors to deliver intense rotary power in a remarkably compact package. In mobile phones 'Neo' magnets provide the magnetic field which allows tiny speakers to deliver appreciable audio power.
Shielding in nuclear reactors.
Boron shielding is used as a control for nuclear reactors, taking advantage of its high cross-section for neutron capture.
Pharmaceutical and biological applications.
Boric acid has antiseptic, antifungal, and antiviral properties and for these reasons is applied as a water clarifier in swimming pool water treatment. Mild solutions of boric acid have been used as eye antiseptics.
Bortezomib ("Velcade"). Boron appears as an active element in its first-approved organic pharmaceutical in the novel pharmaceutical bortezomib, a new class of drug called the proteasome inhibitors, which are active in myeloma and one form of lymphoma (it is in currently in experimental trials against other types of lymphoma). The boron atom in bortezomib binds the catalytic site of the 26S proteasome with high affinity and specificity.
Research areas.
Magnesium diboride is an important superconducting material with the transition temperature of 39 K. MgB2 wires are produced with the powder-in-tube process and applied in superconducting magnets.
Amorphous boron is used as a melting point depressant in nickel-chromium braze alloys.
Hexagonal boron nitride forms atomically thin layers, which have been used to enhance the electron mobility in graphene devices. It also forms nanotubular structures (BNNTs), which have with high strength, high chemical stability, and high thermal conductivity, among its list of desirable properties.
Biological role.
Boron is needed by life. In 2013, a hypothesis suggested it was possible that boron and molybdenum catalyzed the production of RNA on Mars with life being transported to Earth via a meteorite around 3 billion years ago.
There exists one known boron-containing natural antibiotic, boromycin, isolated from streptomyces.
Boron is an essential plant nutrient, required primarily for maintaining the integrity of cell walls. However, high soil concentrations of > 1.0 ppm lead to marginal and tip necrosis in leaves as well as poor overall growth performance. Levels as low as 0.8 ppm produce these same symptoms in plants that are particularly sensitive to boron in the soil. Nearly all plants, even those somewhat tolerant of soil boron, will show at least some symptoms of boron toxicity when soil boron content is greater than 1.8 ppm. When this content exceeds 2.0 ppm, few plants will perform well and some may not survive. When boron levels in plant tissue exceed 200 ppm, symptoms of boron toxicity are likely to appear.
As an ultratrace element, boron is necessary for the optimal health of rats. Boron deficiency in rats is not easy to produce, since boron is needed by rats in such small amounts that ultrapurified foods and dust filtration of air are required. Boron deficiency manifests in rats as poor coat or hair quality. Presumably boron is necessary to other mammals. No deficiency syndrome in humans has been described. Small amounts of boron occur widely in the diet, and the amounts needed in the diet would, by extension from rodent studies, be very small. The exact physiological role of boron in the animal kingdom is poorly understood.
Boron occurs in all foods produced from plants. Since 1989 its nutritional value has been argued. It is thought that boron plays several biochemical roles in animals, including humans.
The United States Department of Agriculture conducted an experiment in which postmenopausal women took 3 mg of boron a day. The results showed that supplemental boron reduced excretion of calcium by 44%, and activated estrogen and vitamin D, suggesting a possible role in the suppression of osteoporosis. However, whether these effects were conventionally nutritional, or medicinal, could not be determined. The U.S. National Institutes of Health states that "Total daily boron intake in normal human diets ranges from 2.1–4.3 mg boron/day."
Congenital endothelial dystrophy type 2, a rare form of corneal dystrophy, is linked to mutations in SLC4A11 gene that encodes a transporter reportedly regulating the intracellular concentration of boron.
Analytical quantification.
For determination of boron content in food or materials the colorimetric "curcumin method" is used. Boron is converted to boric acid or borates and on reaction with curcumin in acidic solution, a red colored boron-chelate complex, rosocyanine, is formed.
Health issues and toxicity.
Elemental boron, boron oxide, boric acid, borates, and many organoboron compounds are nontoxic to humans and animals (with toxicity similar to that of table salt). The LD50 (dose at which there is 50% mortality) for animals is about 6 g per kg of body weight. Substances with LD50 above 2 g are considered nontoxic. The minimum lethal dose for humans has not been established. An intake of 4 g/day of boric acid was reported without incident, but more than this is considered toxic in more than a few doses. Intakes of more than 0.5 grams per day for 50 days cause minor digestive and other problems suggestive of toxicity. Single medical doses of 20 g of boric acid for neutron capture therapy have been used without undue toxicity. Fish have survived for 30 min in a saturated boric acid solution and can survive longer in strong borax solutions. Boric acid is more toxic to insects than to mammals, and is routinely used as an insecticide.
The boranes (boron hydrogen compounds) and similar gaseous compounds are quite poisonous. As usual, it is not an element that is intrinsically poisonous, but their toxicity depends on structure.
The boranes are toxic as well as highly flammable and require special care when handling. Sodium borohydride presents a fire hazard owing to its reducing nature and the liberation of hydrogen on contact with acid. Boron halides are corrosive.

</doc>
<doc id="3756" url="http://en.wikipedia.org/wiki?curid=3756" title="Bromine">
Bromine

Bromine (from Greek: βρῶμος, "brómos", meaning "strong-smelling" or "stench") is a chemical element with symbol Br, and atomic number 35. It is a halogen. The element was isolated independently by two chemists, Carl Jacob Löwig and Antoine Jerome Balard, in 1825–1826. Elemental bromine is a fuming red-brown liquid at room temperature, corrosive and toxic, with properties between those of chlorine and iodine. Free bromine does not occur in nature, but occurs as colorless soluble crystalline mineral halide salts, analogous to table salt.
Bromine is rarer than about three-quarters of elements in the Earth's crust. The high solubility of bromide ions has caused its accumulation in the oceans, and commercially the element is easily extracted from brine pools, mostly in the United States, Israel and China. About 556,000 tonnes were produced in 2007, an amount similar to the far more abundant element magnesium.
At high temperatures, organobromine compounds readily convert to free bromine atoms, a process which has the effect of stopping free radical chemical chain reactions. This effect makes organobromine compounds useful as fire retardants; more than half the bromine produced industrially worldwide each year is put to this use. Unfortunately, the same property causes sunlight to convert volatile organobromine compounds to free bromine atoms in the atmosphere, and an unwanted side effect of this process is ozone depletion. As a result, many organobromide compounds that were formerly in common use—such as the pesticide, methyl bromide—have been abandoned. Bromine compounds are still used for purposes such as in well drilling fluids, in photographic film, and as an intermediate in the manufacture of organic chemicals.
Bromine has been long believed to have no essential function in mammals, but recent research suggests that bromine is necessary for tissue development. In addition, bromine is used preferentially over chlorine by one antiparasitic enzyme in the human immune system. Organobromides are needed and produced enzymatically from bromide by some lower life forms in the sea, particularly algae, and the ash of seaweed was one source of bromine's discovery. As a pharmaceutical, the simple bromide ion, Br−, has inhibitory effects on the central nervous system, and bromide salts were once a major medical sedative, before being replaced by shorter-acting drugs. They retain niche uses as antiepileptics.
Characteristics.
Physical.
The element bromine exists as a diatomic molecule, Br2. It is a dense, mobile, slightly transparent reddish-brown liquid, that evaporates easily at standard temperature and pressures to give an orange vapor (its color resembles nitrogen dioxide) that has a strongly disagreeable odor resembling that of chlorine. It is one of only two elements on the periodic table that are known to be liquids at room temperature (mercury is the other, although caesium, gallium, and rubidium melt just above room temperature).
At a pressure of 55 GPa (roughly 540,000 times atmospheric pressure) bromine converts to a metal. At 75 GPa it converts to a face centered orthorhombic structure. At 100 GPa it converts to a body centered orthorhombic monatomic form.
Chemical.
Being less reactive than chlorine but more reactive than iodine, bromine reacts vigorously with metals, especially in the presence of water, to give bromide salts. It is also reactive toward most organic compounds, especially upon illumination, conditions that favor the dissociation of the diatomic molecule into bromine radicals:
It bonds easily with many elements and has a strong bleaching action.
Bromine is slightly soluble in water, but it is highly soluble in organic solvents such as carbon disulfide, carbon tetrachloride, aliphatic alcohols, and acetic acid.
Isotopes.
Bromine has two stable isotopes, 79Br (50.69%) and 81Br (49.31%). At least 23 radioisotopes are known. Many of the bromine isotopes are fission products. Several of the heavier bromine isotopes from fission are delayed neutron emitters, important to the controllability of a nuclear reactor. All of the radioactive bromine isotopes are relatively short lived. The longest half-life is the neutron deficient 77Br at 2.376 days. The longest half-life on the neutron rich side is 82Br at 1.471 days. A number of the bromine isotopes exhibit metastable isomers. Stable 79Br exhibits a radioactive isomer, with a half-life of 4.86 seconds. It decays by isomeric transition to the stable ground state.
The isotopes of bromine range from 67Br to 98Br. One of these, 67Br has an unknown half life. Six isotopes, 95Br to 98Br, 68Br, and 69Br have half-lives under a microsecond. The isotopes 91Br to 94Br and 70Br have half lives of a microsecond to a second. All but two of the remaining isotopes of bromine have half lives of 1 second to 16.2(2) hours. The other two, 79Br and 81Br, are stable.
The three lightest isotopes of bromine (67Br to 69Br) decay via proton emission. The isotopes 70Br through 78Br decay via electron capture or positron emission. 80Br and 82Br to 97Br decay via electron emission. 98Br decays via neutron emission.
History.
Bromine was discovered independently by two chemists, Carl Jacob Löwig and Antoine Balard, in 1825 and 1826, respectively.
Balard found bromine chemicals in the ash of seaweed from the salt marshes of Montpellier. The seaweed was used to produce iodine, but also contained bromine. Balard distilled the bromine from a solution of seaweed ash saturated with chlorine. The properties of the resulting substance resembled that of an intermediate of chlorine and iodine; with those results he tried to prove that the substance was iodine monochloride (ICl), but after failing to do so he was sure that he had found a new element and named it muride, derived from the Latin word "muria" for brine.
Löwig isolated bromine from a mineral water spring from his hometown Bad Kreuznach in 1825. Löwig used a solution of the mineral salt saturated with chlorine and extracted the bromine with diethyl ether. After evaporation of the ether a brown liquid remained. With this liquid as a sample for his work he applied for a position in the laboratory of Leopold Gmelin in Heidelberg. The publication of the results was delayed and Balard published his results first.
After the French chemists Louis Nicolas Vauquelin, Louis Jacques Thénard, and Joseph-Louis Gay-Lussac approved the experiments of the young pharmacist Balard, the results were presented at a lecture of the Académie des Sciences and published in "Annales de Chimie et Physique". In his publication Balard states that he changed the name from "muride" to "brôme" on the proposal of M. Anglada. ("Brôme" (bromine) derives from the Greek βρωμος (stench).) Other sources claim that the French chemist and physicist Joseph-Louis Gay-Lussac suggested the name "brôme" for the characteristic smell of the vapors. Bromine was not produced in large quantities until 1860.
The first commercial use, besides some minor medical applications, was the use of bromine for the daguerreotype. In 1840 it was discovered that bromine had some advantages over the previously used iodine vapor to create the light sensitive silver halide layer used for daguerreotypy.
Potassium bromide and sodium bromide were used as anticonvulsants and sedatives in the late 19th and early 20th centuries, until they were gradually superseded by chloral hydrate and then the barbiturates. In the early years of the First World War, bromine compounds such as xylyl bromide were used as poison gas.
Occurrence.
The diatomic element Br2 does not occur naturally. Instead, bromine exists exclusively as bromide salts in diffuse amounts in crustal rock. Owing to leaching, bromide salts have accumulated in sea water at 65 parts per million (ppm), which is less than chloride. Bromine may be economically recovered from bromide-rich brine wells and from the Dead Sea waters (up to 50,000 ppm). It exists in the Earth's crust at an average concentration of 0.4 ppm, making it the 62nd most abundant element. The bromine concentration in soils varies normally between 5 and 40 ppm, but some volcanic soils can contain up to 500 ppm. The concentration of bromine in the atmosphere is extremely low, at only a few ppt. A large number of organobromine compounds are found in small amounts in nature.
China's bromine reserves are located in the Shandong Province and Israel's bromine reserves are contained in the waters of the Dead Sea. The largest bromine reserve in the United States is located in Columbia County and Union County, Arkansas, U.S.
Production.
Bromine production has increased sixfold since the 1960s. Approximately 556,000 tonnes (worth around US $2.5 billion) were produced in 2007 worldwide, with the predominant contribution from the United States (226,000 t) and Israel (210,000 t). US production was excluded from the United States Geological Survey after 2007, and from the 380,000 tonnes mined by other countries in 2010, 140,000 t were produced by China, 130,000 t by Israel and 80,000 t by Jordan.
Bromide-rich brines are treated with chlorine gas, flushing through with air. In this treatment, bromide anions are oxidized to bromine by the chlorine gas.
Laboratory methods of production.
In the laboratory, because of its commercial availability and long shelf-life, bromine is not typically prepared. Small amounts of bromine can however be generated through the reaction of solid sodium bromide with concentrated sulfuric acid (H2SO4). The first stage is formation of hydrogen bromide (HBr), which is a gas, but under the reaction conditions some of the HBr is oxidized further by the sulfuric acid to form bromine (Br2) and sulfur dioxide (SO2).
Non oxidizing acid alternatives, such as the use of dilute hydrobromic acid with sodium hypobromite, are also available, as the hypobromous acid formed from them is unstable in the presence of bromide, being reduced by it according to the reaction:
The reactions are the reverse of disproportionation reactions of elemental bromine in base, and are called comproportionation. A similar reaction happens with sodium hypochlorite, acid, and chloride, leading to elemental chlorine.
Reactions involving an oxidizing agent, such as potassium permanganate or manganese dioxide, on bromide ions in the presence of an acid, also give bromine in the reactions analogous to the formation of elemental chlorine and iodine from an acid and oxidant.
Like iodine, bromine is soluble in chloroform but only slightly soluble in water. In water, the solubility can be increased by the presence of bromide ions. Concentrated solutions of bromine are rarely prepared in the lab because of hazards. As is the case with chlorine solutions or iodine solutions, sodium thiosulphate (or any soluble thiosulphate) is an effective reagent for reducing bromine to colorless odorless bromide, thus dealing with stains and odor from the element in unwanted places. For the same reason, thiosulfate ("fixer's hypo") is used in photography to deal with free bromine in silver bromide film emulsions.
Compounds and chemistry.
Organic chemistry.
As with other halogens, bromine substitutes for hydrogen in hydrocarbons, bonding covalently to carbon. As with other halogens, the C-Br product of this substitution is generally colorless if the corresponding C-H compound is colorless. Addition of covalently bonded bromine tends to increase the density and raise the melting and boiling points of organic compounds.
Organic compounds are brominated by either addition or substitution reactions. Bromine undergoes electrophilic addition to the double-bonds of alkenes, via a cyclic bromonium intermediate. In non-aqueous solvents such as carbon disulfide, this yields the di-bromo product. For example, reaction with ethylene will produce 1,2-dibromoethane. Bromine also undergoes electrophilic substitution to phenols and anilines. When used as bromine water, a small amount of the corresponding bromohydrin is formed as well as the dibromo compound. So reliable is the reactivity of bromine that bromine water is employed as a reagent to test for the presence of alkenes, phenols, and anilines. Like the other halogens, bromine participates in free radical reactions. For example, hydrocarbons are brominated upon treatment with bromine in the presence of light.
Bromine, sometimes with a catalytic amount of phosphorus, easily brominates carboxylic acids at the α-position. This method, the Hell-Volhard-Zelinsky reaction, is the basis of the commercial route to bromoacetic acid. "N"-Bromosuccinimide is commonly used as a substitute for elemental bromine, being easier to handle, and reacting more mildly and thus more selectively. Organic bromides are often preferable relative to the less reactive chlorides and more expensive iodide-containing reagents. Thus, Grignard and organolithium compound are most often generated from the corresponding bromides.
Certain bromine-related compounds have been evaluated to have an ozone depletion potential or bioaccumulate in living organisms. As a result, many industrial bromine compounds are no longer manufactured, are being restricted, or scheduled for phasing out. The Montreal Protocol mentions several organobromine compounds for this phase out.
Inorganic chemistry.
Inorganic bromine compounds adopt a variety of oxidation states from −1 to +7. In nature, bromide (Br−) is by far the most common state, and departures from this -1 oxidation state are entirely due to living organisms and bromide's interaction with biologically produced oxidants, such as free oxygen.
Like other halogens, bromide ion is colorless, and forms a number of transparent ionic mineral salts, analogous to chloride. Bromide ion is highly soluble in water.
Examples of compounds for bromine's various oxidation states are shown below:
Bromine is an oxidizer, and it will oxidize iodide ions to iodine, being itself reduced to bromide:
Bromine will also oxidize metals and metalloids to the corresponding bromides. Anhydrous bromine is less reactive toward many metals than hydrated bromine, however. Dry bromine reacts vigorously with aluminium, titanium, mercury as well as alkaline earths and alkali metals.
Dissolving bromine in alkaline solution gives a mixture of bromide and hypobromite:
This hypobromite is responsible for the bleaching abilities of bromide solutions. Warming of these solutions causes the disproportion reaction of the hypobromite to give bromate, a strong oxidising agent very similar to chlorate.
In contrast to the route to perchlorates, perbromates are not accessible through electrolysis but only by reacting bromate solutions with fluorine or ozone.
Bromine reacts violently and explosively with aluminium metal, forming aluminium bromide:
Bromine reacts with hydrogen in gaseous form and gives hydrogen bromide:
Bromine reacts with alkali metal iodides in a displacement reaction. This reaction forms alkali metal bromides and produces elemental iodine:
Applications.
A wide variety of organobromine compounds are used in industry. Some are prepared from bromine and others are prepared from hydrogen bromide, which is obtained by burning hydrogen in bromine.
Illustrative of the addition reaction is the preparation of 1,2-dibromoethane, the organobromine compound produced in the largest amounts:
Flame retardant.
Brominated flame retardants represent a commodity of growing importance, and represent the largest commercial use of bromine. When the brominated material burns, the flame retardant produces hydrobromic acid which interferes in the radical chain reaction of the oxidation reaction of the fire. The mechanism is that the highly reactive hydrogen radicals, oxygen radicals, and hydroxy radicals react with hydrobromic acid to form less reactive bromine radicals (i.e., free bromine atoms). Bromine atoms may also react directly with other radicals to help terminate the free radical chain-reactions that characterize combustion.
To make brominated polymers and plastics, bromine-containing compounds can be incorporated into the polymer during polymerization. One method is to include a relatively small amount of brominated monomer during the polymerization process. For example, vinyl bromide can be used in the production of polyethylene, polyvinylchloride or polypropylene. Specific highly brominated molecules can also be added that participate in the polymerization process For example, tetrabromobisphenol A can be added to polyesters or epoxy resins, where it becomes part of the polymer. Epoxys used in printed circuit boards are normally made from such flame retardant resins, indicated by the FR in the abbreviation of the products (FR-4 and FR-2). In some cases the bromine containing compound may be added after polymerization. For example, decabromodiphenyl ether can be added to the final polymers.
A number of gaseous or highly volatile brominated halomethane compounds are non-toxic and make superior fire suppressant agents by this same mechanism, and are particular effective in enclosed spaces such as submarines, airplanes, and spacecraft. However, they are expensive and their production and use has been greatly curtailed due to their effect as ozone-depleting agents. They are no longer used in routine fire extinguishers, but retain niche uses in aerospace and military automatic fire-suppression applications. They include bromochloromethane (Halon 1011, CH2BrCl), bromochlorodifluoromethane (Halon 1211, CBrClF2), and bromotrifluoromethane (Halon 1301, CBrF3).
Gasoline additive.
Ethylene bromide was an additive in gasolines containing lead anti-engine knocking agents. It scavenges lead by forming volatile lead bromide, which is exhausted from the engine. This application accounted for 77% of the bromine use in 1966 in the US. This application has declined since the 1970s due to environmental regulations (see below).
Pesticide.
Poisonous methyl bromide was widely used as pesticide to fumigate soil and to fumigate housing, by the tenting method. Ethylene bromide was similarly used. These volatile organobromine compounds are all now regulated as ozone depletion agents. The Montreal Protocol on Substances that Deplete the Ozone scheduled the phase out for the ozone depleting chemical by 2005, and organobromide pesticides are no longer used (in housing fumagation they have been replaced by such compounds as sulfuryl fluoride, which contain neither the chlorine or bromine organics which harm ozone). Prior to the Montreal protocol in 1991 (for example) an estimated 35,000 tonnes of the chemical were used to control nematodes, fungi, weeds and other soil-borne diseases.
Medical and veterinary use.
Bromide compounds, especially potassium bromide, were frequently used as general sedatives in the 19th and early 20th century. Bromides in the form of simple salts are still used as anticonvulsants in both veterinary and human medicine, although the latter use varies from country to country. For example, the U.S. Food and Drug Administration (FDA) does not approve bromide for the treatment of any disease, and it was removed from over-the-counter sedative products like Bromo-Seltzer, in 1975. Thus, bromide levels are not routinely measured by medical laboratories in the U.S. However, U.S. veterinary medical diagnostic testing laboratories will measure blood bromide levels on request, as an aid to treatment of epilepsy in dogs.
Toxicity. Long-term use of potassium bromide (or any bromide salt) can lead to bromism. This state of central nervous system depression causes the moderate toxicity of bromide in multi-gram doses for humans and other mammals. The very long half-life of bromide ion in the body (~12 days) also contributes to toxicity from bromide build-up in body fluids. Bromide ingestion may also cause a skin eruption resembling acne.
Biological role.
Recently, bromine (as bromide) was shown to be an essential cofactor for the peroxidasin catalyzed formation of sulfilimine crosslinks in collagen IV. Since this is a post-translational modification that occurs in all animals, bromine is therefore an essential trace element for humans. Inorganic bromine and organobromine compounds also perform other biological functions, which may be essential, or at least optimal and preferred. For example, in the presence of H2O2 formed by the eosinophil, and either chloride or bromide ions, eosinophil peroxidase provides a potent mechanism by which eosinophils kill multicellular parasites (such as, for example, the nematode worms involved in filariasis); and also certain bacteria (such as tuberculosis bacteria). Eosinophil peroxidase is a haloperoxidase that preferentially uses bromide over chloride for this purpose, generating hypobromite (hypobromous acid). A brominated ester (2-octyl γ-bromoacetoacetate) has been shown to be produced endogenously by mammalian (including human) tissue and has been shown to prolong REM sleep in cats.
Marine organisms are the main source of organobromine compounds. Over 1600 compounds were identified by 1999. The most abundant one is methyl bromide (CH3Br) with an estimated 56,000 tonnes produced by marine algae each year. The essential oil of the Hawaiian alga "Asparagopsis taxiformis" consists of 80% tribromomethane (bromoform). Most of such organobromine compounds in the sea are made via the action of a unique algal enzyme, vanadium bromoperoxidase. Though this enzyme is the most prolific creator of organic bromides by living organisms, other bromoperoxidases exist in nature that do not use vanadium.
A famous example of a bromine-containing organic compound that has been used by humans since ancient times is the fabric dye Tyrian purple. The brominated indole indigo dye is produced by a medium-sized predatory sea snail, the marine gastropod "Murex brandaris". The organobromine nature of the compound was not discovered until 1909 (see Paul Friedländer).
Safety.
Elemental bromine is toxic and causes burns. As an oxidizing agent, it is incompatible with most organic and inorganic compounds. Care needs to be taken when transporting bromine; it is commonly carried in steel tanks lined with lead, supported by strong metal frames.
When certain ionic compounds containing bromine are mixed with potassium permanganate (KMnO4) and an acidic substance, they will form a pale brown cloud of bromine gas.
This gas smells like bleach and is very irritating to the mucous membranes. Upon exposure, one should move to fresh air immediately. If symptoms of bromine poisoning arise, medical attention is needed.

</doc>
<doc id="3757" url="http://en.wikipedia.org/wiki?curid=3757" title="Barium">
Barium

Barium is a chemical element with symbol Ba and atomic number 56. It is the fifth element in Group 2, a soft silvery metallic alkaline earth metal. Because of its high chemical reactivity barium is never found in nature as a free element. Its hydroxide was known in pre-modern history as baryta; this substance does not occur as a mineral, but can be prepared by heating barium carbonate.
The most common naturally occurring minerals of barium are barite (barium sulfate, BaSO4) and witherite (barium carbonate, BaCO3), both being insoluble in water. Barium's name originates from the alchemical derivative "baryta", which itself comes from Greek βαρύς ("barys"), meaning "heavy." Barium was identified as a new element in 1774, but not reduced to a metal until 1808, shortly after electrolytic isolation techniques became available.
Barium has only a few industrial applications. The metal has been historically used to scavenge air in vacuum tubes. It is a component of YBCO (high-temperature superconductors) and electroceramics, and is added to steel and cast iron to reduce the size of carbon grains within the microstructure of the metal. Barium compounds are added to fireworks to impart a green color. Barium sulfate is used as an insoluble heavy additive to oil well drilling fluid, as well as in purer form, as X-ray radiocontrast agents for imaging the human gastrointestinal tract. Soluble barium compounds are poisonous due to release of the soluble barium ion, and therefore have been used as rodenticides.
Characteristics.
Physical properties.
Barium is a soft, silvery-white metal, with a slight golden shade when ultrapure.:2 The silvery-white color of barium metal rapidly vanishes upon oxidation in air yielding a dark gray oxide layer. Barium has a medium specific weight and good electrical conductivity. Ultrapure barium is very hard to prepare, and therefore many properties of barium have not been accurately measured yet.:2
At room temperature and pressure, barium has a body-centered cubic structure, with a barium–barium distance of 503 picometers, expanding with heating at a rate of approximately 1.8×10-5/°C.:2 It is a very soft metal with a Mohs hardness of 1.25.:2 Its melting temperature of 1000 K:4–43 is intermediate between those of the lighter strontium (1050 K):4–86 and heavier radium (973 K);:4–78 however, its boiling point of 2170 K exceeds that of strontium (1655 K).:4–86 The density (3.62 g·cm−3):4–43 is again intermediate between those of strontium (2.36 g·cm−3):4–86 and radium (~5 g·cm−3).:4–78
Chemical reactivity.
Barium is chemically similar to magnesium, calcium, and strontium, being even more reactive. It always exhibits the oxidation state of +2.:2 Reactions with chalcogens are highly exothermic (release energy); the reaction with oxygen or air occurs at room temperature, and therefore barium is stored under oil or inert gas atmosphere.:2 Reactions with other nonmetals, such as carbon, nitrogen, phosphorus, silicon, and hydrogen, are generally exothermic and proceed upon heating.:2–3 Reactions with water and alcohols are also very exothermic and release hydrogen gas::3
Additionally, barium reacts with ammonia to form complexes such as Ba(NH3)6.:3
The metal is readily attacked by most acids. Sulfuric acid is a notable exception, as passivation stops the reaction by forming the insoluble barium sulfate. Barium combines with several metals, including aluminium, zinc, lead, and tin, forming intermetallic phases and alloys.
Compounds.
Barium salts are typically white when solid and colorless when dissolved, as barium ions provide no specific coloring. They are also denser than their strontium or calcium analogs, except for the halides (see table; zinc is given for comparison).
Barium hydroxide ("baryta") was known to alchemists who produced it by heating barium carbonate. Unlike calcium hydroxide, it absorbs very little CO2 in aqueous solutions and is therefore insensitive to atmospheric fluctuations. This property is used in calibrating pH equipment.
Volatile barium compounds burn with a green to pale green flame, which is an efficient test to detect a barium compound. The color results from spectral lines at 455.4, 493.4, 553.6, and 611.1 nm.:3 
Organobarium compounds are a growing class of compounds: for example, dialkylbariums are known, as are alkylhalobariums.:3
Isotopes.
Barium occurs naturally on Earth as a mixture of seven primordial nuclides, barium-130, 132, and 134 through 138. Barium-130 undergoes very slow radioactive decay to xenon-130 via double beta plus decay, and barium-132 is expected to decay similarly to xenon-132. The corresponding half-lives should exceed the age of the Universe by at least thousand times. Their abundances are ~0.1% relative to that of natural barium. Their radioactivity is so weak that they pose no danger to life. Out of the stable isotopes, barium-138 makes up 71.7% of all barium, and the lighter the isotope, the less abundant it is. In total, barium has about 50 known isotopes, ranging in mass between 114 and 153. The most stable metastable isotope is barium-133, which has a half-life of approximately 10.51 years, and five more isotopes have their half-lives longer than a day. Barium also has 10 meta states, out of which barium-133m1 is the most stable, having a half-life of about 39 hours.
History.
Alchemists in the early Middle Ages knew about some barium minerals. Smooth pebble-like stones of mineral barite found in Bologna, Italy, were known as "Bologna stones." Witches and alchemists were attracted to them because after exposure to light they would glow for years. The phosphorescent properties of barite heated with organics were described by V. Casciorolus in 1602.:5
Carl Scheele identified barite as containing a new element in 1774, but could not isolate barium, only barium oxide. Johan Gottlieb Gahn also isolated barium oxide two years later in similar studies. Oxidized barium was at first called "barote," by Guyton de Morveau, a name that was changed by Antoine Lavoisier to "baryta." Also in the 18th century, English mineralogist William Withering noted a heavy mineral in the lead mines of Cumberland, now known to be witherite. Barium was first isolated by electrolysis of molten barium salts in 1808, by Sir Humphry Davy in England. Davy, by analogy with calcium named "barium" after baryta, with the "-ium" ending signifying a metallic element. Robert Bunsen and Augustus Matthiessen obtained pure barium by electrolysis of a molten mixture of barium chloride and ammonium chloride.
The production of pure oxygen in the Brin process was a large-scale application of barium peroxide in the 1880s, before it was replaced by electrolysis and fractional distillation of liquefied air in the early 1900s. In this process barium oxide reacts at 500 - with air to form barium peroxide, which decomposes at above 700 C by releasing oxygen:
In 1908, barium sulfate was first applied as a radiocontrast agent in X-ray imaging of the digestive system.
Occurrence and production.
The abundance of barium is 0.0425% in the Earth's crust and 13 µg/L in sea water. The main commercial source of barium is barite (also called barytes or heavy spar), which is a barium sulfate mineral.:5 Its deposits are spread all over the world. The only other commercial source is far less important than barite; it is witherite, a barium carbonate mineral. Its main deposits are located in England, Romania, and the former USSR.:5
The barite reserves are estimated between 0.7 and 2 billion tonnes. The maximum production was achieved in 1981, at 8.3 million tonnes, and only 7–8% of it was used to make barium or its compounds.:5 The barite production has again risen since the second half of the 1990s: from 5.6 million tonnes in 1996 to 7.6 in 2005 and 7.8 in 2011. China accounts for more than 50% of this output, followed by India (14% in 2011), Morocco (8.3%), US (8.2%), Turkey (2.5%), Iran and Kazakhstan (2.6% each).
The mined ore is washed, crushed, classified, and separated from quartz. If the quartz penetrates too deep into the ore, or the iron, zinc, or lead content is abnormally high, then froth flotation methods are applied. The product is a 98% pure barite (by mass); the purity should be no less than 95%, with a minimal content of iron and silicon dioxide.:7 It is then reduced by carbon to barium sulfide::6
The water-soluble barium sulfide is the starting point for other compounds: dissolved BaS upon reaction with oxygen gives the hydroxide, with nitric acid the nitrate, with carbon dioxide the carbonate, and so on.:6 The nitrate can be thermally decomposed to yield the oxide.:6 Barium metal is produced by reduction with aluminium at 1100 C. The intermetallic compound BaAl4 is produced first::3
It is an intermediate, which reacts with barium oxide to give the metal. Note that not all barium is reduced.:3
The remaining barium oxide reacts with the formed aluminium oxide::3
and the overall reaction is:3
The thus produced barium vapor is collected at the cooler part of the apparatus and then packed into molds under argon atmosphere.:3 This method is used commercially and can yield ultrapure barium.:3 Commonly sold barium is about 99% pure, with main impurities being strontium and calcium (up to 0.8% and 0.25%) and other contaminants contributing less than 0.1%.:4
A similar reaction with silicon at 1200 C yields barium and barium metasilicate.:3 Electrolysis is not used because barium readily dissolves in molten halides and is rather impure when isolated with this method.:3
Gemstone.
A barium-containing mineral benitoite (barium titanium silicate) occurs as a very rare blue fluorescent gemstone, and is the official state gem of California.
Applications.
Metal and alloys.
Barium, as a metal or when alloyed with aluminium, is used to remove unwanted gases (gettering) from vacuum tubes, such as TV picture tubes.:4 Barium is suitable for this purpose because of its low vapor pressure and reactivity towards oxygen, nitrogen, carbon dioxide, and water; it can even partly remove noble gases by dissolving them in the crystal lattice. This application is gradually disappearing due to the rising popularity of the tubeless LCD and plasma sets.:4
Other uses of elemental barium are minor and include an additive to silumin (aluminium–silicon alloys) that refines their structure, as well as:4
Barium sulfate and barite.
Barium sulfate (the mineral barite, BaSO4) is important to the petroleum industry, for example, as a drilling fluid in oil and gas wells.:4–5 The precipitate of the compound (called "blanc fixe", from a French expression meaning "permanent white") is used in paints and varnishes, and also as a filler in ringing ink, plastics, and rubbers.:9 It is also a paper coating pigment.:9 In the form of nanoparticles, it can improve physical properties of some polymers, such as epoxies.:9
Barium sulfate has a low toxicity and relatively high density of ca. 4.5 g·cm−3 (and thus opacity to X-rays). For this reason it is used as a radiocontrast agent in X-ray imaging of the digestive system ("barium meals" and "barium enemas").:4–5 Lithopone, a pigment that contains barium sulfate and zinc sulfide, is a permanent white that has good covering power, and does not darken when exposed to sulfides.
Other barium compounds.
Aside from the sulfate, other compounds of barium find only niche applications. Applications are limited by the toxicity of Ba2+ ions (barium carbonate is a rat poison), which is not a problem for the insoluble BaSO4.
Biological dangers and precautions.
Because of the high reactivity of the metal, toxicological data are available only for compounds. Water-soluble barium compounds are poisonous. At low doses, barium ions act as a muscle stimulant, whereas higher doses affect the nervous system, causing cardiac irregularities, tremors, weakness, anxiety, dyspnea and paralysis. This may be due to the ability of Ba2+ to block potassium ion channels, which are critical to the proper function of the nervous system. Other target organs for water-soluble barium compounds (i.e., barium ions) are eyes, immune system, heart, respiratory system, and skin. They affect the body strongly, causing, for example, blindness and sensitization.
Barium is not carcinogenic, and it does not bioaccumulate. However, inhaled dust containing insoluble barium compounds can accumulate in the lungs, causing a benign condition called baritosis. For comparison to the soluble poisons, the insoluble sulfate is nontoxic and is thus not classified as a dangerous good.:9
To avoid a potentially vigorous chemical reaction, barium metal is kept under argon or mineral oils. Contact with air is dangerous, as it may cause ignition. Moisture, friction, heat, sparks, flames, shocks, static electricity, reactions with oxidizers and acids should be avoided. Everything that may make contact with barium should be grounded. Those who work with the metal should wear pre-cleaned non-sparking shoes, flame-resistant rubber clothes, rubber gloves, apron, goggles, and a gas mask; they are not allowed to smoke in the working area and must wash themselves after handling barium.

</doc>
<doc id="3758" url="http://en.wikipedia.org/wiki?curid=3758" title="Berkelium">
Berkelium

Berkelium is a transuranic radioactive chemical element with symbol Bk and atomic number 97. It is a member of the actinide and transuranium element series. It is named after the city of Berkeley, California, the location of the University of California Radiation Laboratory where it was discovered in December 1949. This was the fifth transuranium element discovered after neptunium, plutonium, curium and americium.
The major isotope of berkelium, 249Bk, is synthesized in minute quantities in dedicated high-flux nuclear reactors, mainly at the Oak Ridge National Laboratory in Tennessee, USA, and at the Research Institute of Atomic Reactors in Dimitrovgrad, Russia. The production of the second-important isotope 247Bk involves the irradiation of the rare isotope 244Cm with high-energy alpha particles.
Just over one gram of berkelium has been produced in the United States since 1967. There is no practical application of berkelium outside of scientific research which is mostly directed at the synthesis of heavier transuranic elements and transactinides. A 22 milligram batch of berkelium-249 was prepared during a 250-day irradiation period and then purified for a further 90 days at Oak Ridge in 2009. This sample was used to synthesize the element ununseptium for the first time in 2009 at the Joint Institute for Nuclear Research, Russia, after it was bombarded with calcium-48 ions for 150 days. This was a culmination of the Russia–US collaboration on the synthesis of elements 113 to 118.
Berkelium is a soft, silvery-white, radioactive metal. The berkelium-249 isotope emits low-energy electrons and thus is relatively safe to handle. It decays with a half-life of 330 days to californium-249, which is a strong emitter of ionizing alpha particles. This gradual transformation is an important consideration when studying the properties of elemental berkelium and its chemical compounds, since the formation of californium brings not only chemical contamination, but also free-radical effects and self-heating from the emitted helium nuclei.
Characteristics.
Physical.
Berkelium is a soft, silvery-white, radioactive actinide metal. In the periodic table, it is located to the right of the actinide curium, to the left of the actinide californium and below the lanthanide terbium with which it shares many similarities in physical and chemical properties. Its density of 14.78 g/cm3 lies between those of curium (13.52 g/cm3) and californium (15.1 g/cm3), as does its melting point of 986 °C, below that of curium (1340 °C) but higher than that of californium (900 °C). Berkelium is relatively soft and has one of the lowest bulk moduli among the actinides, at about 20 GPa (2×1010 Pa).
Berkelium(III) ions shows two sharp fluorescence peaks at 652 nanometers (red light) and 742 nanometers (deep red – near infrared) due to internal transitions at the f-electron shell. The relative intensity of these peaks depends on the excitation power and temperature of the sample. This emission can be observed, for example, after dispersing berkelium ions in a silicate glass, by melting the glass in presence of berkelium oxide or halide.
Between 70 K and room temperature, berkelium behaves as a Curie–Weiss paramagnetic material with an effective magnetic moment of 9.69 Bohr magnetons (µB) and a Curie temperature of 101 K. This magnetic moment is almost equal to the theoretical value of 9.72 µB calculated within the simple atomic L-S coupling model. Upon cooling to about 34 K, berkelium undergoes a transition to an antiferromagnetic state. Enthalpy of dissolution in hydrochloric acid at standard conditions is −600 kJ/mol−1, from which the standard enthalpy change of formation (Δf"H"°) of aqueous Bk3+ ions is obtained as −601 kJ/mol−1. The standard potential Bk3+/Bk0 is −2.01 V. The ionization potential of a neutral berkelium atom is 6.23 eV.
Allotropes.
At ambient conditions, berkelium assumes its most stable α form which has a hexagonal symmetry, space group "P63/mmc", lattice parameters of 341 pm and 1107 pm. The crystal has a double-hexagonal close packing structure with the layer sequence ABAC and so is isotypic (having a similar structure) with α-lanthanum and α-forms of actinides beyond curium. This crystal structure changes with pressure and temperature. When compressed at room temperature to 7 GPa, α-berkelium transforms to the beta modification, which has a face-centered cubic ("fcc") symmetry and space group "Fm3m". This transition occurs without change in volume, but the enthalpy increases by 3.66 kJ/mol. Upon further compression to 25 GPa, berkelium transforms to an orthorhombic γ-berkelium structure similar to that of α-uranium. This transition is accompanied by a 12% volume decrease and delocalization of the electrons at the 5f electron shell. No further phase transitions are observed up to 57 GPa.
Upon heating, α-berkelium transforms into another phase with an "fcc" lattice (but slightly different from β-berkelium), space group "Fm3m" and the lattice constant of 500 pm; this "fcc" structure is equivalent to the closest packing with the sequence ABC. This phase is metastable and will gradually revert to the original α-berkelium phase at room temperature. The temperature of the phase transition is believed to be quite close to the melting point.
Chemical.
Like all actinides, berkelium dissolves in various aqueous inorganic acids, liberating gaseous hydrogen and converting into the berkelium(III) state. This trivalent oxidation state (+3) is the most stable, especially in aqueous solutions, but tetravalent (+4) and possibly divalent (+2) berkelium compounds are also known. The existence of divalent berkelium salts is uncertain and has only been reported in mixed lanthanum chloride-strontium chloride melts. A similar behavior is observed for the lanthanide analogue of berkelium, terbium. Aqueous solutions of Bk3+ ions are green in most acids. The color of Bk4+ ions is yellow in hydrochloric acid and orange-yellow in sulfuric acid. Berkelium does not react rapidly with oxygen at room temperature, possibly due to the formation of a protective oxide layer surface. However, it reacts with molten metals, hydrogen, halogens, chalcogens and pnictogens to form various binary compounds.
Isotopes.
About twenty isotopes and six nuclear isomers (excited states of an isotope) of berkelium have been characterized with the mass numbers ranging from 235 to 254. All of them are radioactive. The longest half-lives are observed for 247Bk (1,380 years), 248Bk (9 years) and 249Bk (330 days); the half-lives of the other isotopes range from microseconds to several days. The isotope which is the easiest to synthesize is berkelium-249. This emits mostly soft β-particles which are inconvenient for detection. Its alpha radiation is rather weak – 1.45×10-3% with respect to the β-radiation – but is sometimes used to detect this isotope. The second important berkelium isotope, berkelium-247, is an alpha-emitter, as are most actinide isotopes.
Occurrence.
All berkelium isotopes have a half-life far too short to be primordial. Therefore, any primordial berkelium, that is, berkelium present on the Earth during its formation, has decayed by now.
On Earth, berkelium is mostly concentrated in certain areas, which were used for the atmospheric nuclear weapons tests between 1945 and 1980, as well as at the sites of nuclear incidents, such as the Chernobyl disaster, Three Mile Island accident and 1968 Thule Air Base B-52 crash. Analysis of the debris at the testing site of the first U.S. hydrogen bomb, Ivy Mike, (1 November 1952, Enewetak Atoll), revealed high concentrations of various actinides, including berkelium. For reasons of military secrecy, this result was published only in 1956.
Nuclear reactors produce mostly, among the berkelium isotopes, berkelium-249. During the storage and before the fuel disposal, most of it beta decays to californium-249. The latter has a half-life of 351 years, which is relatively long when compared to the other isotopes produced in the reactor, and is therefore undesirable in the disposal products.
A few atoms of berkelium can be produced by neutron capture reactions and beta decay in very highly concentrated uranium-bearing deposits, thus making it the rarest naturally occurring element.
History.
Although very small amounts of berkelium were possibly produced in previous nuclear experiments, it was first intentionally synthesized, isolated and identified in December 1949 by Glenn T. Seaborg, Albert Ghiorso and Stanley G. Thompson. They used the 60-inch cyclotron at the University of California, Berkeley. Similar to the nearly simultaneous discovery of americium (element 95) and curium (element 96) in 1944, the new elements berkelium and californium (element 98) were both produced in 1949–1950.
The name choice for element 97 followed the previous tradition of the Californian group to draw an analogy between the newly discovered actinide and the lanthanide element positioned above it in the periodic table. Previously, americium was named after a continent as its analogue europium, and curium honored scientists Marie and Pierre Curie as the lanthanide above it, gadolinium, was named after the explorer of the rare earth elements Johan Gadolin. Thus the discovery report by the Berkeley group reads: "It is suggested that element 97 be given the name berkelium (symbol Bk) after the city of Berkeley in a manner similar to that used in naming its chemical homologue terbium (atomic number 65) whose name was derived from the town of Ytterby, Sweden, where the rare earth minerals were first found." This tradition ended on berkelium, though, as the naming of the next discovered actinide, californium, was not related to its lanthanide analogue dysprosium, but after the discovery place.
The most difficult steps in the synthesis of berkelium were its separation from the final products and the production of sufficient quantities of americium for the target material. First, americium (241Am) nitrate solution was coated on a platinum foil, the solution was evaporated and the residue converted by annealing to americium dioxide (AmO2). This target was irradiated with 35 MeV alpha particles for 6 hours in the 60-inch cyclotron at the Lawrence Radiation Laboratory, University of California, Berkeley. The (α,2n) reaction induced by the irradiation yielded the 243Bk isotope and two free neutrons:
After the irradiation, the coating was dissolved with nitric acid and then precipitated as the hydroxide using concentrated aqueous ammonia solution. The product was centrifugated and re-dissolved in nitric acid. To separate berkelium from the unreacted americium, this solution was added to a mixture of ammonium and ammonium sulfate and heated to convert all the dissolved americium into the oxidation state +6. Unoxidized residual americium was precipitated by the addition of hydrofluoric acid as americium(III) fluoride (AmF3). This step yielded a mixture of the accompanying product curium and the expected element 97 in form of trifluorides. The mixture was converted to the corresponding hydroxides by treating it with potassium hydroxide, and after centrifugation, was dissolved in perchloric acid.
Further separation was carried out in the presence of a citric acid/ammonium buffer solution in a weakly acidic medium (pH≈3.5), using ion exchange at elevated temperature. The chromatographic separation behavior was then unknown for the element 97, but was anticipated by analogy with terbium (see elution curves). First results were disappointing as no alpha-particle emission signature could be detected from the elution product. Only the further search for characteristic X-rays and conversion electron signals resulted in the identification of a berkelium isotope. Its mass number was uncertain between 243 and 244 in the initial report, but was later established as 243.
Synthesis and extraction.
Preparation of isotopes.
Berkelium is produced by bombarding lighter actinides uranium (238U) or plutonium (239Pu) with neutrons in a nuclear reactor. In a more common case of uranium fuel, plutonium is produced first by neutron capture (the so-called (n,γ) reaction or neutron fusion) followed by beta-decay:
Plutonium-239 is further irradiated by a source that has a high neutron flux, several times higher than a conventional nuclear reactor, such as the 85-megawatt High Flux Isotope Reactor (HFIR) at the Oak Ridge National Laboratory in Tennessee, USA. The higher flux promotes fusion reactions involving not one but several neutrons, converting 239Pu to 244Cm and then to 249Cm:
Curium-249 has a short half-life of 64 minutes, and thus its further conversion to 250Cm has a low probability. Instead, it transforms by beta-decay into 249Bk:
The thus-produced 249Bk has a long half-life of 330 days and thus can capture another neutron. However, the product, 250Bk, again has a relatively short half-life of 3.212 hours and thus, does not yield any heavier berkelium isotopes. Instead decays to the californium isotope 250Cf:
Although 247Bk is the most stable isotope of berkelium, its production in nuclear reactors is very inefficient due to the long half-life of its potential progenitor curium-247, which does not allow it sufficient time to beta decay before capturing another neutron. Thus, 249Bk is the most accessible isotope of berkelium, which still, is available only in small quantities (only 0.66 grams have been produced in the US over the period 1967–1983) at a high price of the order 185 USD per microgram.
The isotope 248Bk was first obtained in 1956 by bombarding a mixture of curium isotopes with 25 MeV α-particles. Although its direct detection was hindered by strong signal interference with 245Bk, the existence of a new isotope was proven by the growth of the decay product 248Cf which had been previously characterized. The half-life of 248Cf was estimated as 23 ± 5 hours and a more reliable value still is not known. Berkelium-247 was produced during the same year by irradiating 244Cm with alpha-particles:
Berkelium-242 was synthesized in 1979 by bombarding 235U with 11B, 238U with 10B, 232Th with 14N or 232Th with 15N. It converts by electron capture to 242Cm with a half-life of 7.0 ± 1.3 minutes. A search for an initially suspected isotope 241Bk was then unsuccessful; 241Bk has since been synthesized.
Separation.
The fact that berkelium readily assumes oxidation state +4 in solids, and is relatively stable in this state in liquids greatly assists separation of berkelium away from many other actinides. These are inevitably produced in relatively large amounts during the nuclear synthesis and often favor the +3 state. This fact was not yet known in the initial experiments, which used a more complex separation procedure. Various oxidation agents can be applied to the berkelium(III) solutions to convert it to the +4 state, such as bromates (BrO3-), bismuthates (BiO3-), chromates (CrO42- and CrO), silver(I) thiolate (Ag2S2O8), lead(IV) oxide (PbO2), ozone (O3), or photochemical oxidation procedures. Berkelium(IV) is then extracted with ion exchange, extraction chromatography or liquid-liquid extraction using HDEHP (bis-(2-ethylhexyl) phosphoric scid), amines, tributyl phosphate or various other reagents. These procedures separate berkelium from most trivalent actinides and lanthanides, except for the lanthanide cerium (lanthanides are absent in the irradiation target but are created in various nuclear fission decay chains).
A more detailed procedure adopted at the Oak Ridge National Laboratory was as follows: the initial mixture of actinides is processed with ion exchange using lithium chloride reagent, then precipitated as hydroxides, filtered and dissolved in nitric acid. It is then treated with high-pressure elution from cation exchange resins, and the berkelium phase is oxidized and extracted using one of the procedures described above. Reduction of the thus-obtained berkelium(IV) to the +3 oxidation state yields a solution, which is nearly free from other actinides (but contains cerium). Berkelium and cerium are then separated with another round of ion-exchange treatment.
Bulk metal preparation.
In order to characterize chemical and physical properties of solid berkelium and its compounds, a program was initiated in 1952 at the Material Testing Reactor, Arco, Idaho, US. It resulted in preparation of an eight-gram plutonium-239 target and in the first production of macroscopic quantities (0.6 micrograms) of berkelium by Burris B. Cunningham and Stanley G. Thompson in 1958, after a continuous reactor irradiation of this target for six years. This irradiation method was and still is the only way of producing weighable amounts of the element, and most solid-state studies of berkelium have been conducted on microgram or submicrogram-sized samples.
The world's major irradiation sources are the 85-megawatt High Flux Isotope Reactor at the Oak Ridge National Laboratory in Tennessee, USA, and the SM-2 loop reactor at the Research Institute of Atomic Reactors (NIIAR) in Dimitrovgrad, Russia, which are both dedicated to the production of transcurium elements (atomic number greater than 96). These facilities have similar power and flux levels, and are expected to have comparable production capacities for transcurium elements, although the quantities produced at NIIAR are not publicly reported. In a "typical processing campaign" at Oak Ridge, tens of grams of curium are irradiated to produce decigram quantities of californium, milligram quantities of berkelium-249 and einsteinium, and picogram quantities of fermium. In total, just over one gram of berkelium-249 has been produced at Oak Ridge since 1967.
The first berkelium metal sample weighing 1.7 micrograms was prepared in 1971 by the reduction of berkelium(III) fluoride with lithium vapor at 1000 °C; the fluoride was suspended on a tungsten wire above a tantalum crucible containing molten lithium. Later, metal samples weighting up to 0.5 milligrams were obtained with this method.
Similar results are obtained with berkelium(IV) fluoride. Berkelium metal can also be produced by the reduction of berkelium(IV) oxide with thorium or lanthanum.
Compounds.
Oxides.
Two oxides of berkelium are known, with the berkelium oxidation state of +3 (Bk2O3) and +4 (BkO2). Berkelium(IV) oxide is a brown solid, while berkelium(III) oxide is a yellow-green solid with a melting point of 1920 °C and is formed from BkO2 by reduction with molecular hydrogen:
Upon heating to 1200 °C, the oxide Bk2O3 undergoes a phase change; it undergoes another phase change at 1750 °C. Such three-phase behavior is typical for the actinide sesquioxides. Berkelium(II) oxide, BkO, has been reported as a brittle gray solid but its exact chemical composition remains uncertain.
Halides.
In halides, berkelium assumes the oxidation states +3 and +4. The +3 state is the most stable, especially in solutions, while the tetravalent halides BkF4 and Cs2BkCl6 are only known in the solid phase. The coordination of berkelium atom in its trivalent fluoride and chloride is tricapped trigonal prismatic, with the coordination number of 9. In trivalent bromide, it is bicapped trigonal prismatic (coordination 8) or octahedral (coordination 6), and in the iodide it is octahedral.
Berkelium(IV) fluoride (BkF4) is a yellow-green ionic solid and is isotypic with uranium tetrafluoride or zirconium(IV) fluoride. Berkelium(III) fluoride (BkF3) is also a yellow-green solid, but it has two crystalline structures. The most stable phase at low temperatures is isotypic with yttrium(III) fluoride, while upon heating to between 350 and 600 °C, it transforms to the structure found in lanthanum(III) fluoride.
Visible amounts of berkelium(III) chloride (BkCl3) were first isolated and characterized in 1962, and weighed only 3 billionths of a gram. It can be prepared by introducing hydrogen chloride vapors into an evacuated quartz tube containing berkelium oxide at a temperature about 500 °C. This green solid has a melting point of 600 °C, and is isotypic with uranium(III) chloride. Upon heating to nearly melting point, BkCl3 converts into an orthorhombic phase.
Two forms of berkelium(III) bromide are known: one with berkelium having coordination 6, and one with coordination 8. The latter is less stable and transforms to the former phase upon heating to about 350 °C. An important phenomenon for radioactive solids has been studied on these two crystal forms: the structure of fresh and aged 249BkBr3 samples was probed by X-ray diffraction over a period longer than 3 years, so that various fractions of berkelium-249 had beta decayed to californium-249. No change in structure was observed upon the 249BkBr3—249CfBr3 transformation. However, other differences were noted for 249BkBr3 and 249CfBr3. For example, the latter could be reduced with hydrogen to 249CfBr2, but the former could not – this result was reproduced on individual 249BkBr3 and 249CfBr3 samples, as well on the samples containing both bromides. The intergrowth of californium in berkelium occurs at a rate of 0.22% per day and is an intrinsic obstacle in studying berkelium properties. Beside a chemical contamination, 249Cf, being an alpha emitter, brings undesirable self-damage of the crystal lattice and the resulting self-heating. The chemical effect however can be avoided by performing measurements as a function of time and extrapolating the obtained results.
Other inorganic compounds.
The pnictides of berkelium-249 of the type BkX are known for the elements nitrogen, phosphorus, arsenic and antimony. They crystallize in the rock-salt structure and are prepared by the reaction of either berkelium(III) hydride (BkH3) or metallic berkelium with these elements at elevated temperature (about 600 °C) under high vacuum.
Berkelium(III) sulfide, Bk2S3, is prepared by either treating berkelium oxide with a mixture of hydrogen sulfide and carbon disulfide vapors at 1130 °C, or by directly reacting metallic berkelium with elemental sulfur. These procedures yield brownish-black crystals.
Berkelium(III) and berkelium(IV) hydroxides are both stable in 1 molar solutions of sodium hydroxide. Berkelium(III) phosphate (BkPO4) has been prepared as a solid, which shows strong fluorescence under excitation with a green light. Berkelium hydrides are produced by reacting metal with hydrogen gas at temperatures about 250 °C. They are non-stoichiometric with the nominal formula BkH2+x (0 < x < 1). Several other salts of berkelium are known, including an oxysulfide (Bk2O2S), and hydrated nitrate (Bk(NO3)3·4H2O), chloride (BkCl3·6H2O), sulfate (Bk2(SO4)3·12H2O) and oxalate (Bk2(C2O4)3·4H2O). Thermal decomposition at about 600 °C in an argon atmosphere (to avoid oxidation to BkO2) of Bk2(SO4)3·12H2O yields the crystals of berkelium(III) oxysulfate (Bk2O2SO4). This compound is thermally stable to at least 1000 °C in inert atmosphere.
Organoberkelium compounds.
Berkelium forms a trigonal (η5–C5H5)3Bk metallocene complex with three cyclopentadienyl rings, which can be synthesized by reacting berkelium(III) chloride with the molten beryllocene (Be(C5H5)2) at about 70 °C. It has an amber color and a density of 2.47 g/cm3. The complex is stable to heating to at least 250 °C, and sublimates without melting at about 350 °C. The high radioactivity of berkelium gradually destroys the compound (within a period of weeks). One cyclopentadienyl ring in (η5–C5H5)3Bk can be substituted by chlorine to yield [Bk(C5H5)2Cl]2. The optical absorption spectra of this compound are very similar to those of (η5–C5H5)3Bk.
Applications.
There is currently no use for any isotope of berkelium outside of basic scientific research. Berkelium-249 is a common target nuclide to prepare still heavier transuranic elements and transactinides, such as lawrencium, rutherfordium and bohrium. It is also useful as a source of the isotope californium-249, which is used for studies on the chemistry of californium in preference to the more radioactive californium-252 that is produced in neutron bombardment facilities such as the HFIR.
A 22 milligram batch of berkelium-249 was prepared in a 250-day irradiation and then purified for 90 days at Oak Ridge in 2009. This target yielded the first 6 atoms of ununseptium at the Joint Institute for Nuclear Research (JINR), Dubna, Russia, after bombarding it with calcium ions in the U400 cyclotron for 150 days. This synthesis was a culmination of the Russia—US collaboration between JINR and Lawrence Livermore National Laboratory on the synthesis of elements 113 to 118 which was initiated in 1989.
Nuclear fuel cycle.
The nuclear fission properties of berkelium are different from those of the neighboring actinides curium and californium, and they suggest berkelium to perform poorly as a fuel in a nuclear reactor. Specifically, berkelium-249 has a moderately large neutron capture cross section of 710 barns for thermal neutrons, 1200 barns resonance integral, but very low fission cross section for thermal neutrons. In a thermal reactor, much of it will therefore be converted to berkelium-250 which quickly decays to californium-250. In principle, berkelium-249 can sustain a nuclear chain reaction in a fast breeder reactor. Its critical mass is relatively high at 192 kg; it can be reduced with a water or steel reflector but would still exceed the world production of this isotope.
Berkelium-247 can maintain chain reaction both in a thermal-neutron and in a fast-neutron reactor, however, its production is rather complex and thus the availability is much lower than its critical mass, which is about 75.7 kg for a bare sphere, 41.2 kg with a water reflector and 35.2 kg with a steel reflector (30 cm thickness).
Health issues.
Little is known about the effects of berkelium on human body, and analogies with other elements may not be drawn because of different radiation products (electrons for berkelium and alpha particles, neutrons, or both for most other actinides). The low energy of electrons emitted from berkelium-249 (less than 126 keV) hinders its detection, due to signal interference with other decay processes, but also makes this isotope relatively harmless to humans as compared to other actinides. However, berkelium-249 transforms with a half-life of only 330 days to the strong alpha-emitter californium-249, which is rather dangerous and has to be handled in a glove box in a dedicated laboratory.
Most available berkelium toxicity data originate from research on animals. Upon ingestion by rats, only about 0.01% berkelium ends in the blood stream. From there, about 65% goes to the bones, where it remains for about 50 years, 25% to the lungs (biological half-life about 20 years), 0.035% to the testicles or 0.01% to the ovaries where berkelium stays indefinitely. The balance of about 10% is excreted. In all these organs berkelium might promote cancer, and in the skeletal system its radiation can damage red blood cells. The maximum permissible amount of berkelium-249 in the human skeleton is 0.4 nanograms.

</doc>
<doc id="3760" url="http://en.wikipedia.org/wiki?curid=3760" title="Bauxite">
Bauxite

Bauxite, an aluminium ore, is the world's main source of aluminium. It consists mostly of the minerals gibbsite Al(OH)3, boehmite γ-AlO(OH) and diaspore α-AlO(OH), mixed with the two iron oxides goethite and haematite, the clay mineral kaolinite and small amounts of anatase TiO2. Bauxite was named by the French geologist Pierre Berthier in 1821 after the village of Les Baux in Provence, southern France, where he discovered it and was the first to recognize that it contained aluminium.
Formation.
Lateritic bauxites (silicate bauxites) are distinguished from karst bauxite ores (carbonate bauxites). The carbonate bauxites occur predominantly in Europe and Jamaica above carbonate rocks (limestone and dolomite), where they were formed by lateritic weathering and residual accumulation of intercalated clay layers – dispersed clays which were concentrated as the enclosing limestones gradually dissolved during chemical weathering.
The lateritic bauxites are found mostly in the countries of the tropics. They were formed by lateritization of various silicate rocks such as granite, gneiss, basalt, syenite, and shale. In comparison with the iron-rich laterites, the formation of bauxites depends even more on intense weathering conditions in a location with very good drainage. This enables the dissolution of the kaolinite and the precipitation of the gibbsite. Zones with highest aluminium content are frequently located below a ferruginous surface layer. The aluminium hydroxide in the lateritic bauxite deposits is almost exclusively gibbsite.
In the case of Jamaica, recent analysis of the soils showed elevated levels of cadmium, suggesting that the bauxite originates from recent Miocene ash deposits from episodes of significant volcanism in Central America.
Production trends.
In 2009, Australia was the top producer of bauxite with almost one-third of the world's production, followed by China, Brazil, India, and Guinea. Although aluminium demand is rapidly increasing, known reserves of its bauxite ore are sufficient to meet the worldwide demands for aluminium for many centuries. Increased aluminium recycling, which has the advantage of lowering the cost in electric power in producing aluminium, will considerably extend the world's bauxite reserves.
In November 2010, Nguyen Tan Dung, the prime minister of Vietnam, announced that Vietnam's bauxite reserves might total 11000 Mt; this would be the largest in the world.
Processing.
Bauxite is usually strip mined because it is almost always found near the surface of the terrain, with little or no overburden. Approximately 70% to 80% of the world's dry bauxite production is processed first into alumina, and then into aluminium by electrolysis as of 2010. Bauxite rocks are typically classified according to their intended commercial application: metallurgical, abrasive, cement, chemical, and refractory.
Usually, bauxite ore is heated in a pressure vessel along with a sodium hydroxide solution at a temperature of 150 to 200 °C. At these temperatures, the aluminium is dissolved as an aluminate (the Bayer process). After separation of ferruginous residue (red mud) by filtering, pure gibbsite is precipitated when the liquid is cooled, and then seeded with fine-grained aluminium hydroxide. The gibbsite is usually converted into aluminium oxide, Al2O3, by heating. This mineral is dissolved at a temperature of about 960 °C in molten cryolite. Next, this molten substance can yield metallic aluminium by passing an electric current through it in the process of electrolysis, which is called the Hall–Héroult process, named after its American and French discoverers.
Prior to the invention of this process in 1886, elemental aluminium was made by heating ore along with elemental sodium or potassium in a vacuum. The method was complicated and consumed materials that were themselves expensive at that time. This made early elemental aluminium more expensive than gold.

</doc>
<doc id="3764" url="http://en.wikipedia.org/wiki?curid=3764" title="Bavaria">
Bavaria

The Free State of Bavaria (German: "Freistaat Bayern", ], Alemannic German: "Freistaat Bayre", Bavarian: "Freistood Boajan/Baijaan", Main-Franconian: "Freischdood Bayan"; Czech: "Bavorsko") is a federal state of Germany. In the southeast of the country with an area of 70,548 square kilometres (27,200 sq mi), it is the largest state, making up almost a fifth of the total land area of Germany, and with 12.5 million inhabitants is Germany's second most populous state. Munich, Bavaria's capital and largest city, is the third largest city in Germany.
The history of Bavaria stretches from its earliest settlement and formation as a duchy in the 6th century through the Holy Roman Empire to becoming an independent kingdom and finally a state of the Federal Republic of Germany.
The Duchy of Bavaria dates back to the year 555. In the 17th century, the Duke of Bavaria became a Prince-elector of the Holy Roman Empire. The Kingdom of Bavaria existed from 1806 to 1918, when Bavaria became a republic. In 1946, the Free State of Bavaria re-organised itself on democratic lines.
Modern Bavaria also includes parts of the historical regions of Franconia, Upper Palatinate and Swabia.
History.
The Bavarians emerged in a region north of the Alps, previously inhabited by Celts, which had been part of the Roman provinces of Raetia and Noricum. The Bavarians spoke Old High German but, unlike other Germanic groups, probably did not migrate from elsewhere. Rather, they seem to have coalesced out of other groups left behind by Roman withdrawal late in the 5th century. These peoples may have included the Celtic Boii, some remaining Romans, Marcomanni, Allemanni, Quadi, Thuringians, Goths, Scirians, Rugians, Heruli. The name "Bavarian" ("Baiuvarii") means "Men of Baia" which may indicate Bohemia, the homeland of the Celtic Boii and later of the Marcomanni. They first appear in written sources circa 520. Saint Boniface completed the people's conversion to Christianity in the early-8th century. Bavaria was, for the most part, unaffected by the Protestant Reformation that happened centuries later.
From about 554 to 788, the house of Agilolfing ruled the Duchy of Bavaria, ending with Tassilo III who was deposed by Charlemagne.
Three early dukes are named in Frankish sources: Garibald I may have been appointed to the office by the Merovingian kings and married the Lombard princess Walderada when the church forbade her to King Chlothar I in 555. Their daughter, Theodelinde, became Queen of the Lombards in northern Italy and Garibald was forced to flee to her when he fell out with his Frankish overlords. Garibald's successor, Tassilo I, tried unsuccessfully to hold the eastern frontier against the expansion of Slavs and Avars around 600. Tassilo's son Garibald II seems to have achieved a balance of power between 610 and 616.
After Garibald II little is known of the Bavarians until Duke Theodo I, whose reign may have begun as early as 680. From 696 onwards he invited churchmen from the west to organize churches and strengthen Christianity in his duchy (it is unclear what Bavarian religious life consisted of before this time). His son, Theudebert, led a decisive Bavarian campaign to intervene in a succession dispute in the Lombard Kingdom in 714, and married his sister Guntrud to the Lombard King Liutprand. At Theodo's death the duchy was divided among his sons, but reunited under his grandson Hugbert.
At Hugbert's death (735) the duchy passed to a distant relative named Odilo, from neighbouring Alemannia (modern southwest Germany and northern Switzerland). Odilo issued a law code for Bavaria, completed the process of church organisation in partnership with St. Boniface (739), and tried to intervene in Frankish succession disputes by fighting for the claims of the Carolingian Grifo. He was defeated near Augsburg in 743 but continued to rule until his death in 748.
Middle Ages.
Tassilo III (b. 741 - d. after 796) succeeded his father at the age of eight after an unsuccessful attempt by Grifo to rule Bavaria. He initially ruled under Frankish oversight but began to function independently from 763 onwards. He was particularly noted for founding new monasteries and for expanding eastwards, fighting Slavs in the eastern Alps and along the River Danube and colonising these lands. After 781, however, his cousin Charlemagne began to pressure Tassilo to submit and finally deposed him in 788. The deposition was not entirely legitimate. Dissenters attempted a coup against Charlemagne at Tassilo's old capital of Regensburg in 792, led by his own son Pépin the Hunchback. The king had to drag Tassilo out of imprisonment to formally renounce his rights and titles at the Assembly of Frankfurt in 794. This is the last appearance of Tassilo in the sources, and he probably died a monk. As all of his family were also forced into monasteries, this was the end of the Agilolfing dynasty.
For the next 400 years numerous families held the duchy, rarely for more than three generations. With the revolt of duke Henry the Quarrelsome in 976, Bavaria lost large territories in the south and south east. The territory of "Ostarrichi" was elevated to a duchy out of own right and given to the Babenberger family. This event marks the founding of Austria.
The last, and one of the most important, of the dukes of Bavaria was Henry the Lion of the house of Welf, founder of Munich, and "de facto" the second most powerful man in the empire as the ruler of two duchies. When in 1180, Henry the Lion was deposed as Duke of Saxony and Bavaria by his cousin, Frederick I, Holy Roman Emperor (a.k.a. "Barbarossa" for his red beard), Bavaria was awarded as fief to the Wittelsbach family, counts palatinate of Schyren ("Scheyern" in modern German). They ruled for 738 years, from 1180 to 1918. The Electorate of the Palatinate by Rhine ("Kurpfalz" in German) was also acquired by the House of Wittelsbach in 1214.
The first of several divisions of the duchy of Bavaria occurred in 1255. With the extinction of the Hohenstaufen in 1268, Swabian territories were acquired by the Wittelsbach dukes. Emperor Louis the Bavarian acquired Brandenburg, Tirol, Holland and Hainaut for his House but released the Upper Palatinate for the Palatinate branch of the Wittelsbach in 1329. In 1506 with the Landshut War of Succession, the other parts of Bavaria were reunited, and Munich became the sole capital.
17th and 18th centuries.
In 1623 the Bavarian duke replaced his relative of the Palatinate branch, the Electorate of the Palatinate in the early days of the Thirty Years' War and acquired the powerful prince-electoral dignity in the Holy Roman Empire, determining its Emperor thence forward, as well as special legal status under the empire's laws. The country became one of the Jesuit supported counter-reformation centers. During the early and mid-18th century the ambitions of the Bavarian prince electors led to several wars with Austria as well as occupations by Austria (Spanish succession, election of a Wittelsbach emperor instead of a Habsburger). From 1777 onwards and after the younger Bavarian branch of the family had died out with elector Max III Joseph, Bavaria and the Electorate of the Palatinate were governed once again in personal union, now by the Palatinian lines. The new state also comprised the Duchies of Jülich and Berg as these on their part were in personal union with the Palatinate.
Kingdom of Bavaria.
When Napoleon abolished the Holy Roman Empire, Bavaria became a kingdom in 1806. Its area doubled after the Duchy of Jülich was ceded to France, as the Electorate Palatinate was divided between France and Grand Duchy of Baden. The Duchy of Berg was given to Jerome Bonaparte. The Tirol was temporarily reunited Salzburg with Bavaria but finally ceded to Austria. In return Bavaria was allowed to annex the Rhenish Palatinate and Franconia in 1815. Between 1799 and 1817, the leading minister, Count Montgelas, followed a strict policy of modernisation; he laid the foundations of administrative structures that survived the monarchy and retain core validity in the 21st century. In 1808 a first constitution was passed, being modernized in 1818. This second version established a bicameral Parliament with a House of Lords ("Kammer der Reichsräte") and a House of Commons ("Kammer der Abgeordneten"). The constitution would be followed until the collapse of the monarchy at the end of World War I.
As a part of the German Empire.
After the rise of Prussia to power, Bavaria preserved its independence by playing off the rivalries of Prussia and Austria. Allied to Austria, it was defeated in the 1866 Austro-Prussian War and did not belong to the North German Federation of 1867, but the question of German unity was still alive. When France attacked Prussia in 1870, the south German states Baden, Württemberg, Hessen-Darmstadt and Bavaria joined the Prussian forces and ultimately joined the Federation, which was renamed "Deutsches Reich" (German Empire) in 1871. Bavaria continued as a monarchy, and it had some special rights within the federation (such as an army, railways and a postal service of its own).
When Bavaria became part of the newly formed German Empire, this action was considered controversial by Bavarian nationalists who had wanted to retain independence. As Bavaria had a majority-Catholic population, many people resented being ruled by the mostly Protestant northerners of Prussia. As a direct result of the Bavarian-Prussian feud, political parties formed to encourage Bavaria to break away and regain its independence. Although the idea of Bavarian separatism was popular in the late 19th and early 20th century, apart from a small minority such as the Bavaria Party, most Bavarians have accepted that Bavaria is part of Germany.
In the early-20th century, Wassily Kandinsky, Paul Klee, Henrik Ibsen, and other notable artists were drawn to Bavaria, notably to the Schwabing district of Munich, a center of international artistic activity. This area was devastated by bombing and invasion during World War II.
20th century.
On 12 November 1918, Ludwig III signed a document, the Anif declaration, releasing both civil and military officers from their oaths; the newly formed republican government of Socialist premier Kurt Eisner interpreted this as an abdication. To date, however, no member of the house of Wittelsbach has ever formally declared renunciation of the throne. On the other hand, none has ever since officially called upon their Bavarian or Stuart claims. Family members are active in cultural and social life, including the head of the house, HRH Duke Franz in Bavaria. They step back from any announcements on public affairs, showing approval or disapproval solely by HRH's presence or absence.
Eisner was assassinated in February 1919 ultimately leading to a Communist revolt and the short-lived Bavarian Soviet Republic being proclaimed 6 April 1919. After violent suppression by elements of the German Army and notably the Freikorps, the Bavarian Socialist Republic fell in May 1919. The Bamberg Constitution ("Bamberger Verfassung") was enacted on 12 or 14 August 1919 and came into force on 15 September 1919 creating the Free State of Bavaria within the Weimar Republic. Extremist activity further increased, notably the 1923 Beer Hall Putsch led by the National Socialists, and Munich and Nuremberg became Nazi strongholds under the Third Reich. As a manufacturing centre, Munich was heavily bombed during World War II and occupied by U.S. troops.
The Rhenish Palatinate was detached from Bavaria in 1946 and made part of the new state Rhineland-Palatinate. During the Cold War, Bavaria was part of West Germany. In 1949, the Landtag of Bavaria rejected ratifying the Basic Law of Germany, mainly because it was seen as not granting sufficient powers to the individual "Länder", but at the same time decided that it would still come into force in Bavaria if two-thirds of the other Länder ratified it. All of the other Ländern ratified it, and it became law.
Bavarian identity.
Bavarians have often emphasized a separate national identity and considered themselves as "Bavarians" first, "Germans" second. This feeling started to come about more strongly among Bavarians when the Kingdom of Bavaria joined the Protestant Prussian-dominated German Empire while the Bavarian nationalists wanted to keep Bavaria as Catholic and an independent state. Aside from the minority Bavaria Party, most Bavarians accept that Bavaria is part of Germany. Another consideration is that Bavarians foster different cultural identities: Franconia in the north, speaking East Franconian German, Bavarian Swabia in the south west, speaking Swabian German and Altbayern (so-called "Old Bavaria", the regions forming the "historic", pentagon-shaped Bavaria before the acquisitions through the Vienna Congress, at present the districts of the Upper Palatinate, Lower and Upper Bavaria). In Munich, the Old Bavarian dialect was spoken, but nowadays mainly High German.
Coat of arms.
The modern coat of arms of Bavaria was designed by Eduard Ege in 1946, following heraldic traditions.
Geography.
Bavaria shares international borders with Austria and the Czech Republic as well as with Switzerland (across Lake Constance). Because all of these countries are part of the Schengen Area, the border is completely open. Neighbouring states within Germany are Baden-Württemberg, Hesse, Thuringia and Saxony. Two major rivers flow through the state, the Danube ("Donau") and the Main. The Bavarian Alps define the border with Austria, (including the Austrian federal-states of Vorarlberg, Tyrol and Salzburg) and within the range is the highest peak in Germany, the Zugspitze. The Bavarian Forest and the Bohemian Forest form the vast majority of the frontier with the Czech Republic and Bohemia.
The major cities in Bavaria are Munich ("München"), Nuremberg ("Nürnberg"), Augsburg, Regensburg, Würzburg, Ingolstadt, Fürth and Erlangen.
Major cities.
Source: Bayerisches Landesamt für Statistik und Datenverarbeitung
Administrative divisions.
Regierungsbezirke (administrative districts).
Bavaria is divided into 7 administrative districts called "Regierungsbezirke" (singular "Regierungsbezirk").
Bezirke.
"Bezirke" (districts) are the third communal layer in Bavaria; the others are the "Landkreise" and the "Gemeinden" or "Städte". The "Bezirke" in Bavaria are territorially identical with the "Regierungsbezirke" (e.g. Regierung von Oberbayern), but are a different form of administration, having their own parliaments, etc.
In the larger states of Germany (including Bavaria), there are "Regierungsbezirke" which are only administrative divisions and not self-governing entities as the "Bezirke" in Bavaria.
Landkreise/kreisfreie cities.
These administrative regions consist of 71 administrative districts (called "Landkreise", singular "Landkreis", e.g. rural districts) and 25 independent cities ("kreisfreie Städte", singular "kreisfreie Stadt", e.g. urban districts).
Landkreise:
Kreis-free Cities:
Gemeinden (municipalities).
The 71 administrative districts are on the lowest level divided into 2031 municipalities (called "Gemeinden", singular "Gemeinde"). Together with the 25 independent cities ("kreisfreie Städte", which are in effect municipalities independent of "Landkreis" administrations), there are a total of 2056 municipalities in Bavaria.
In 44 of the 71 administrative districts, there are a total of 215 unincorporated areas (as of January 1, 2005, called "gemeindefreie Gebiete", singular "gemeindefreies Gebiet"), not belonging to any municipality, all uninhabited, mostly forested areas, but also four lakes (Chiemsee-without islands, Starnberger See-without island Roseninsel, Ammersee, which are the three largest lakes of Bavaria, and Waginger See).
Government and politics.
Government.
The Constitution of Bavaria of the free state of Bavaria was enacted on 8 December 1946. The new Bavarian Constitution became the basis for the Bavarian State after the Second World War.
Bavaria has a unicameral "Landtag", or state parliament, elected by universal suffrage. Until December 1999, there was also a "Senat", or Senate, whose members were chosen by social and economic groups in Bavaria, but following a referendum in 1998, this institution was abolished.
The Bavarian State Government consists of the Minister-President of Bavaria, 11 Ministers and 6 Secretaries of State. The Minister-President is elected for a period of five years by the State Parliament and is head of state. With the approval of the State Parliament he appoints the members of the State Government. The State Government is composed of the:
Political processes also take place in the 7 regions ("Regierungsbezirke" / "Bezirke") in Bavaria, in the 71 administrative districts ("Landkreise") and the 25 towns and cities forming their own districts ("kreisfreie Städte"), and in the 2,031 local authorities ("Gemeinden").
In 1995 Bavaria introduced direct democracy on the local level in a referendum. This was initiated bottom-up by an association called "Mehr Demokratie" (More Democracy). This is a grass-roots organization which campaigns for the right to citizen-initiated referendums. In 1997 the Bavarian Supreme Court aggravated the regulations considerably (e.g. by introducing a turn-out quorum). Nevertheless, Bavaria has the most advanced regulations on local direct democracy in Germany. This has led to a spirited citizens' participation in communal and municipal affairs—835 referenda took place from 1995 through 2005.
Bavarian politics.
The last state elections were held on 15 September 2013, where the Christian Social Union (CSU) won an absolute majority in the state parliament in spite of bad press surrounding a cronyism affair. The CSU's former coalition partner Free Democrats (FDP) failed to gain caucus recognition amidst a downward trend in all of Germany. 
The 17th parliamentary term comprises 180 mandates of which the CSU won 101, the Social Democrats (SPD) 42, the Free Voters 19 and the Alliance '90/ The Greens 18.
Bavaria has a multi-party system dominated by the conservative CSU, which has won every election since 1945, and the center-left SPD. Thus far Wilhelm Hoegner has been the only SPD candidate to ever become Minister-President; notable successors in office include multi-term Federal Minister Franz Josef Strauss, a key figure of the West Germany conservatives of the Cold War years and Edmund Stoiber, who both failed with their bids for Chancellorship. The German Greens and the center-right Free Voters have been represented in the state parliament since 1986 and 2008 respectively.
In the 2003 elections the CSU won a 2/3 supermajority — something no party had ever achieved in post-war Germany. However, in the subsequent 2008 elections the CSU lost the absolute majority for the first time in 46 years. The losses were purportedly attributed to political affairs and the CSU's stance towards an anti-smoking bill later overruled by a public referendum.
Designation as a "free state".
Unlike most German states ("Länder"), which simply designate themselves as "State of X" ("Land X"), Bavaria uses the style of "Free State of Bavaria" ("Freistaat Bayern"). The difference to other states is purely terminological - German constitutional law does not draw a distinction between "States" and "Free States". The situation is thus analogous to the United States, where some states use the style "Commonwealth" rather than "State". The choice of "Free State", a creation of the early 20th century and intended to be a German alternative to the Latin loanword "republic", has historical reasons, Bavaria having been styled that way even before the current 1946 constitution was enacted. Two other states, Saxony and Thuringia, also use the style "Free State"; unlike Bavaria, however, these were not part of the original states when the Grundgesetz was enacted but joined the federation later on, in 1990, as a result of German reunification.
Economy.
Bavaria has long had one of the largest economies of any region in Germany, or Europe for that matter. Its GDP in 2007 exceeded 434 billion Euros (about 600 bn US$). This makes Bavaria itself one of the largest economies in Europe and only 17 countries in the world have higher GDP. Some large companies headquartered in Bavaria include BMW, Siemens, Rohde & Schwarz, Audi, Munich Re, Allianz, Infineon, MAN, Wacker Chemie, Puma, and Adidas. Bavaria has a GDP per capita of over $48 000 US, meaning that if it were its own independent country it would rank 7th or 8th in the world.
Company names.
The motorcycle and automobile makers BMW ("Bayerische Motoren-Werke", or Bavarian Motor Works) and Audi, Allianz, Grundig (consumer electronics), Siemens (electricity, telephones, informatics, medical instruments), Continental (Automotive Tire and Electronics), Adidas, Puma, HypoVereinsbank (UniCredit Group), Infineon and Krauss-Maffei Wegmann have (or had) a Bavarian industrial base.
Culture.
Some features of the Bavarian culture and mentality are remarkably distinct from the rest of Germany. Noteworthy differences (especially in rural areas, less significant in the major cities) can be found with respect to:
Religion.
Bavarian culture ("Altbayern") has a long and predominant tradition of Roman Catholic faith. Pope emeritus, Benedict XVI (Joseph Alois Ratzinger), was born in Marktl am Inn in Upper Bavaria and was Cardinal-Archbishop of Munich and Freising. Otherwise, the culturally Franconian and Swabian regions of the modern State of Bavaria are historically more diverse in religiosity, with both Catholic and Protestant traditions.
As of 2010 54.4% of Bavarians still adhere to Roman Catholicism though the number is on the decline (they were 70.4% in 1970, 56.3% in 2007). 20.4% of the population adheres to the Evangelical Lutheran Church in Bavaria, and their number is declining too. Muslims make up 4.0% of the population of Bavaria. 21.2% of Bavarians are irreligious or adhere to other religions, and this number is increasing.
Traditions.
Bavarians commonly emphasize pride in their traditions. Traditional costumes collectively known as Tracht are worn on special occasions and include in Altbayern Lederhosen for males and Dirndl for females. Centuries-old folk music is performed. The Maibaum, or Maypole (which in the Middle Ages served as the community's yellow pages, as figurettes on the pole represent the trades of the village), and the bagpipes in the Upper Palatinate region bear witness to the ancient Celtic and Germanic remnants of cultural heritage of the region. There are a lot of traditional Bavarian sports disciplines, e.g. the Aperschnalzen is an old tradition of competitive whipcracking.
Whether actually in Bavaria, overseas or full of citizens from other nations they continue to cultivate their traditions. They hold festivals and dances to keep their traditions alive. In New York the German American Cultural Society is a larger umbrella group for others such as the Bavarian organizations, which represent a specific part of Germany. They proudly put forth a German Parade called Steuben Parade each year. Various affiliated events take place amongst its groups, one of which is the Bavarian Dancers.
Food and drink.
Bavarians tend to place a great value on food and drink. In addition to their renowned dishes, Bavarians also consume many items of food and drink which are unusual elsewhere in Germany; for example Weißwurst ("white sausage") or in some instances a variety of entrails. At folk festivals and in many beer gardens, beer is traditionally served by the litre (in a Maß). Bavarians are particularly proud of the traditional Reinheitsgebot, or purity law, initially established by the Duke of Bavaria for the City of Munich (e.g. the court) in 1487 and the duchy in 1516. According to this law, only three ingredients were allowed in beer: water, barley, and hops. In 1906 the Reinheitsgebot made its way to all-German law, and remained a law in Germany until the EU struck it down recently as incompatible with the European common market. German breweries, however, cling to the principle. Bavarians are also known as some of the world's most beer-loving people with an average annual consumption of 170 litres per person, although figures have been declining in recent years.
Bavaria is also home to the Franconia wine region, which is situated along the Main River in Franconia. The region has produced wine ("Frankenwein") for over 1,000 years and is famous for its use of the Bocksbeutel wine bottle. The production of wine forms an integral part of the regional culture, and many of its villages and cities hold their own wine festivals (Weinfeste) throughout the year.
Language and dialects.
Three German dialects are spoken in Bavaria: Austro-Bavarian in Old Bavaria (South-East and East), Swabian German (an Alemannic German dialect) in the Bavarian part of Swabia (South West) and East Franconian German in Franconia (North). In the 20th century an increasing part of the population began to speak Standard German, mainly in the cities.
Ethnography.
Bavarians consider themselves to be egalitarian and informal. Their sociability can be experienced at the annual Oktoberfest, the world's largest beer festival, which welcomes around six million visitors every year, or in the famous beer gardens. In traditional Bavarian beer gardens, patrons may bring their own food but buy beer only from the brewery that runs the beer garden. 
In the United States, particularly among German Americans, Bavarian culture is viewed somewhat nostalgically, and several "Bavarian villages" have been founded, most notably Frankenmuth, Michigan; Helen, Georgia; and Leavenworth, Washington. Since 1962, the latter has been styled with a Bavarian theme and is home to an Oktoberfest celebration it claims is among the most attended in the world outside of Munich.
Sports.
Football.
Bavaria is home to several football clubs including FC Bayern Munich, 1. FC Nuremberg, FC Augsburg, TSV 1860 München, FC Ingolstadt 04 and SpVgg Greuther Fürth. Bayern Munich is the most popular and successful football team in Germany having won a record 25 German titles. They are followed by 1.FC Nuremberg who have won 9 titles. SpVgg Greuther Fürth have won 3 championships while TSV 1860 München have been champions once. FC Bayern Munich are 2013/2014 FIFA Club World Cup Champions.
Famous people.
There are many famous people who were born or lived in present-day Bavaria:

</doc>
<doc id="3765" url="http://en.wikipedia.org/wiki?curid=3765" title="Brandenburg">
Brandenburg

Brandenburg (  ; Low German: "Brannenborg", Lower Sorbian: "Bramborska"; Upper Sorbian: "Braniborska"; Polish: "Brandenburgia") is one of the sixteen federated states of Germany. The capital is Potsdam.
It lies in the east of the country and is one of the federal states that was re-created in 1990 upon the reunification of the former West Germany and East Germany. Brandenburg surrounds but does not include the national capital and city-state Berlin.
Originating in the medieval Northern March, the Margraviate of Brandenburg grew to become the core of the Kingdom of Prussia, which would later become the Free State of Prussia. The eastern third of historic Brandenburg ("Ostbrandenburg/Neumark") was ceded to Poland in 1945.
History.
In late medieval and early modern times, Brandenburg was one of seven electoral states of the Holy Roman Empire, and, along with Prussia, formed the original core of the German Empire, the first unified German state. Governed by the Hohenzollern dynasty from 1415, it contained the future German capital Berlin. After 1618 the Margraviate of Brandenburg and the Duchy of Prussia were combined to form Brandenburg-Prussia, which was ruled by the same branch of the House of Hohenzollern. In 1701 the state was elevated as the Kingdom of Prussia. Franconian Nuremberg and Ansbach, Swabian Hohenzollern, the eastern European connections of Berlin, and the status of Brandenburg's ruler as prince-elector together were instrumental in the rise of that state.
Early Middle Ages.
Brandenburg is situated in territory known in antiquity as Magna Germania, which reached to the Vistula river. By the 7th century, Slavic peoples are believed to have settled in the Brandenburg area. The Slavs expanded from the east, possibly driven from their homelands in present-day Ukraine and perhaps Belarus by the invasions of the Huns and Avars. They relied heavily on river transport. The two principal Slavic groups in the present-day area of Brandenburg were the Hevelli in the west and the Sprevane in the east.
Beginning in the early 10th century, Henry the Fowler and his successors conquered territory up to the Oder River. Slavic settlements such as Brenna (Brandenburg an der Havel), Budusin (Bautzen), and Chośebuz (Cottbus) came under imperial control through the installation of margraves. Their main function was to defend and protect the eastern marches. In 948 Emperor Otto I established margraves to exert imperial control over the pagan Slavs west of the Oder River. Otto founded the Bishoprics of Brandenburg and Havelberg. The Northern March was founded as a northeastern border territory of the Holy Roman Empire. However, a great uprising of Wends drove imperial forces from the territory of present-day Brandenburg in 983. The region returned to the control of Slavic leaders.
12th century.
During the 12th century, the Ottonian German kings and emperors re-established control over the mixed Slav-inhabited lands of present-day Brandenburg, although some Slavs like the Sorbs in Lusatia adapted to Germanization while retaining their distinctiveness. The Roman Catholic Church brought bishoprics which, with their walled towns, afforded protection from attacks for the townspeople. With the monks and bishops, the history of the town of Brandenburg an der Havel, which was the first center of the state of Brandenburg, began. In 1134, in the wake of a German crusade against the Wends, the German magnate, Albert the Bear, was granted the Northern March by the Emperor Lothar III. He formally inherited the town of Brandenburg and the lands of the Hevelli from their last Wendish ruler, Pribislav, in 1150. After crushing a force of Sprevane who occupied the town of Brandenburg in the 1150s, Albert proclaimed himself ruler of the new Margraviate of Brandenburg. Albert, and his descendants the Ascanians, then made considerable progress in conquering, colonizing, Christianizing, and cultivating lands as far east as the Oder. Within this region, Slavic and German residents intermarried. During the 13th century, the Ascanians began acquiring territory east of the Oder, later known as the Neumark (see also Altmark).
Late Middle Ages.
In 1320, the Brandenburg Ascanian line came to an end, and from 1323 up until 1415 Brandenburg was under the control of the Wittelsbachs of Bavaria, followed by the Luxembourg Dynasties. Under the Luxembourgs, the Margrave of Brandenburg gained the status of a prince-elector of the Holy Roman Empire. In the years 1373-1415, Brandenburg has been a part of the Lands of the Bohemian Crown. In 1415, the Electorate of Brandenburg was granted by Emperor Sigismund to the House of Hohenzollern, which would rule until the end of World War I. The Hohenzollerns established their capital in Berlin, by then the economic center of Brandenburg.
16th and 17th centuries.
Brandenburg converted to Protestantism in 1539 in the wake of the Protestant Reformation, and generally did quite well in the 16th century, with the expansion of trade along the Elbe, Havel, and Spree Rivers. The Hohenzollerns expanded their territory by acquiring the Duchy of Prussia in 1618, the Duchy of Cleves (1614) in the Rhineland, and territories in Westphalia. The result was a sprawling, disconnected country known as Brandenburg-Prussia that was in poor shape to defend itself during the Thirty Years' War.
Beginning near the end of that devastating conflict, however, Brandenburg enjoyed a string of talented rulers who expanded their territory and power in Europe. The first of these was Frederick William, the so-called "Great Elector", who worked tirelessly to rebuild and consolidate the nation. He moved the royal residence to Potsdam. At the Treaty of Westphalia, his envoy Joachim Friedrich von Blumenthal negotiated the acquisition of several important territories such as Halberstadt. Under the Treaty of Oliva Christoph Caspar von Blumenthal(son of the above) negotiated the incorporation of the Duchy of Prussia into the Hohenzollern inheritance.
Kingdom of Prussia and united Germany.
When Frederick William died in 1688, he was followed by his son Frederick, third of that name in Brandenburg. As the lands that had been acquired in Prussia were outside the boundaries of the Holy Roman Empire, Frederick assumed (as Frederick I) the title of "King in Prussia" (1701). Although his self-promotion from margrave to king relied on his title to the Duchy of Prussia, Brandenburg was still the most important portion of the kingdom. However, this combined state is known as the Kingdom of Prussia.
Brandenburg remained the core of the Kingdom of Prussia, and it was the site of the kingdom's capitals, Berlin and Potsdam. When Prussia was subdivided into provinces in 1815, the territory of the Margraviate of Brandenburg became the Province of Brandenburg, again subdivided into the government regions of Frankfurt and of Potsdam. In 1881, the City of Berlin was separated from the Province of Brandenburg. However, industrial towns ringing Berlin lay within Brandenburg, and the growth of the region's industrial economy brought an increase in the population of the province. The Province of Brandenburg had an area of 39039 km2 and a population of 2.6 million (1925). After World War II, the Neumark, the part of Brandenburg east of the Oder-Neisse Line, was transferred to Poland; and its native German population expelled. The remainder of the province became a state in the Soviet Zone of occupation in Germany when Prussia was dissolved in 1947.
East Germany and reunified Germany.
Since the foundation of East Germany in 1949 Brandenburg formed one of its component states. The State of Brandenburg was completely dissolved in 1952 by the Socialist government of East Germany, doing away with all component states. The East German government then divided Brandenburg among several "Bezirke" or districts. (See Administrative division of the German Democratic Republic). Most of Brandenburg lay within the Bezirke of Cottbus, Frankfurt, or Potsdam, but parts of the former province passed to the Schwerin, Neubrandenburg and Magdeburg districts (town Havelberg). East Germany relied heavily on lignite (the lowest grade of coal) as an energy source, and lignite strip mines marred areas of southeastern Brandenburg. The industrial towns surrounding Berlin were important to the East German economy, while rural Brandenburg remained mainly agricultural.
The present State of Brandenburg was re-established on 3 October 1990 upon German re-unification. The newly elected Landtag of Brandenburg first met on 26 October 1990. As in other former parts of East Germany, the lack of modern infrastructure and exposure to West Germany's competitive market economy brought widespread unemployment and economic difficulty. In the recent years, however, Brandenburg's infrastructure has been modernized and unemployment has slowly declined.
In 1995, the governments of Berlin and Brandenburg proposed to merge the states in order to form a new state with the name of "Berlin-Brandenburg", though some suggested calling the proposed new state "Prussia". The merger was rejected in a plebiscite in 1996 – while West Berliners voted for a merger, East Berliners and Brandenburgers voted against it.
Geography.
Brandenburg is bordered by Mecklenburg-Vorpommern in the north, Poland in the east, the Freistaat Sachsen in the south, Saxony-Anhalt in the west, and Lower Saxony in the northwest.
The Oder River forms a part of the eastern border, the Elbe River a portion of the western border. The main rivers in the state itself are the Spree and the Havel. In the southeast, there is a wetlands region called the Spreewald; it is the northernmost part of Lusatia, where the Sorbs, a Slavic people, still live. These areas are bilingual, i.e., German and Sorbian are both used.
Protected areas.
Brandenburg is known for its well-preserved natural environment and its ambitious natural protection policies which began in the 1990s. 15 large protected areas were designated following Germany's reunification. Each of them is provided with state-financed administration and a park ranger staff, who guide visitors and work to ensure nature conservation. Most protected areas have visitor centers.
National parks
Biosphere reserves
Nature parks
Religion.
17.1% of the Brandenburgers adhere to the local Evangelical Church in Germany (mostly the Evangelical Church in Berlin, Brandenburg and Silesian Upper Lusatia), while 3.1% are Roman Catholic (mostly the Archdiocese of Berlin, and a minority in the Diocese of Görlitz). The majority (79.8%) of Brandenburgers can be defined as non-religious, adherents of non-Christian religions or not adherents of the larger Christian denominations. Compared to Berlin and other parts of Germany the proportion of Muslims is very small.
Political subdivisions.
Brandenburg is divided into 14 rural districts ("Landkreise") and four urban districts ("kreisfreie Städte"), shown with their population in 2011:
Government.
September 2014 state election.
The most recent election took place on 14 September 2014. The coalition government formed by the Social Democrats and the Left Party led by Dietmar Woidke (SPD) was re-elected. The next ordinary state election is scheduled for 2019.
Music.
The "Brandenburg concerti" by Johann Sebastian Bach (original title: "Six Concerts à plusieurs instruments") are a collection of six instrumental works presented by Bach to Christian Ludwig, Margrave of Brandenburg-Schwedt, in 1721 (though probably composed earlier). They are widely regarded as among the finest musical compositions of the Baroque era and are among the composer's best known works.

</doc>
<doc id="3768" url="http://en.wikipedia.org/wiki?curid=3768" title="Bundestag">
Bundestag

The Bundestag (], "Federal Diet") is a constitutional and legislative body in Germany.
The Bundestag was established by the Basic Law for the Federal Republic of Germany in 1949 as the successor to the earlier Reichstag. It meets in the Reichstag Building in Berlin. Norbert Lammert is the current President of the Bundestag. Representatives of the Bundestag are directly elected, usually every four years although earlier if the Chancellor loses a vote of no confidence and asks the President to dissolve the Bundestag and hold an election.
History.
With the dissolution of the German Confederation in 1866 and the founding of the German Empire (Deutsches Reich) in 1871, the Reichstag was established as the German parliament in Berlin, which was the capital of the then Kingdom of Prussia (the largest and most influential state in both the Confederation and the empire). Two decades later, the current parliament building was erected. The Reichstag delegates were elected by direct and equal male suffrage (and not the three-class electoral system prevailing in Prussia until 1918). The Reichstag did not participate in the appointment of the Chancellor until the parliamentary reforms of October 1918. After the Revolution of November 1918 and the establishment of the Weimar Constitution, women were given the right to vote for (and serve in) the Reichstag, and the parliament could use the no-confidence vote to force the chancellor or any cabinet member to resign. In March 1933, one month after the Reichstag fire, the then president, Paul von Hindenburg, a retired war hero, gave Hitler ultimate power through the Decree for the Protection of People and State and the Enabling Act of 1933, although Hitler remained at the post of Federal Government Chancellor (though he called himself the Führer). After this the Reichstag met only rarely, usually at the following the Reichstag fire starting in 1933 to unanimously rubber-stamp the decisions of the government. It last convened on 26 April 1942.
With the new constitution of 1949, the Bundestag was established as the new (West) German parliament. Because West Berlin was not officially under the jurisdiction of the Constitution and because of the Cold War, the Bundestag met in Bonn in several different buildings, including (provisionally) a former water works facility. In addition, owing to the city's legal status, citizens of West Berlin were unable to vote in elections to the Bundestag, and were instead represented by 20 non-voting delegates, indirectly elected by the city's House of Representatives.
The Bundeshaus in Bonn is the former Parliament Building of Germany. The sessions of the German Bundestag were held there from 1949 until its move to Berlin in 1999. Today it houses the International Congress Centre Bundeshaus Bonn and in the north areas the branch office of the Bundesrat (upper house). The southern areas became part of German offices for the United Nations in 2008.
The former Reichstag building housed a history exhibition (Fragen an die deutsche Geschichte) and served occasionally as a conference center. The Reichstag building was also occasionally used as a venue for sittings of the Bundestag and its committees and the Bundesversammlung, the body which elects the German Federal President. However, the Soviets harshly protested against the use of the Reichstag building by institutions of the Federal Republic of Germany and tried to disturb the sittings by flying supersonic jets close to the building.
Since 1999, the German parliament has again assembled in Berlin in its original Reichstag building, which dates from the 1890s and underwent a significant renovation under the lead of British architect Sir Norman Foster. Parliamentary committees and subcommittees, public hearings and faction meetings take place in three auxiliary buildings, which surround the Reichstag building: the Jakob-Kaiser-Haus, Paul-Löbe-Haus and Marie-Elisabeth-Lüders-Haus.
In 2005, a small aircraft crashed close to the German parliament. It was then decided to ban private air traffic over Central Berlin.
Tasks.
Together with the Bundesrat, the Bundestag is the legislative branch of the German political system.
Although most legislation is initiated by the executive branch, the Bundestag considers the legislative function its most important responsibility, concentrating much of its energy on assessing and amending the government's legislative program. The committees (see below) play a prominent role in this process. Plenary sessions provide a forum for members to engage in public debate on legislative issues before them, but they tend to be well attended only when significant legislation is being considered.
The Bundestag members are the only federal officials directly elected by the public; the Bundestag in turn elects the Chancellor and, in addition, exercises oversight of the executive branch on issues of both substantive policy and routine administration. This check on executive power can be employed through binding legislation, public debates on government policy, investigations, and direct questioning of the chancellor or cabinet officials. For example, the Bundestag can conduct a question hour "(Fragestunde)," in which a government representative responds to a previously submitted written question from a member. Members can ask related questions during the question hour. The questions can concern anything from a major policy issue to a specific constituent's problem. Use of the question hour has increased markedly over the past forty years, with more than 20,000 questions being posed during the 1987-90 term. Understandably, the opposition parties are active in exercising the parliamentary right to scrutinize government actions.
One striking difference when comparing the Bundestag with the British Parliament is the lack of time spent on serving constituents in Germany. This is in part due to Germany's electoral system. A practical constraint on the expansion of constituent service is the limited personal staff of Bundestag deputies. Despite these constraints, especially those deputies that are elected directly usually try to keep close contact with their constituents, and to help them with their problems, particularly when they are related to federal policies or agencies.
Constituent service does also take place in the form of the Petition Committee. In 2004, the Petition Committee received over 18,000 complaints from citizens and was able to negotiate a mutually satisfactory solution to more than half of them. In 2005, as a pilot of the potential of internet petitions, a version of e-Petitioner was produced for the Bundestag. This was a collaborative project involving The Scottish Parliament, International Teledemocracy Centre and the Bundestag ‘Online Services Department’. The system was formally launched on 1 September 2005, and in 2008 the Bundestag moved to a new system based on its evaluation.
Election.
Members serve four-year terms, with elections held every four years, or earlier in the relatively rare case that the Bundestag is dissolved prematurely by the president. The Bundestag can be dissolved by the president on the recommendation of the chancellor if the latter has lost a vote of confidence in the Bundestag, if the recommendation is made and accepted before the Bundestag acts to elect a new Chancellor. This has happened three times: 1972 under Chancellor Willy Brandt, 1983 under Chancellor Helmut Kohl and 2005 under Chancellor Gerhard Schröder. The procedures for these situations are governed by Articles 67 and 68 of the Basic Law of the Federal Republic of Germany.
All candidates must be at least eighteen years old; there are no term limits. The election uses the MMP electoral system. In addition, the Bundestag has a minimum threshold of either 5% of the national party vote or three (directly elected) constituency representatives for a party to gain additional representation through the system of proportional representation.
Thus, small minority parties cannot easily enter the Bundestag and prevent the formation of stable majority governments as they could under the Weimar constitution. Since 1961, only two new parties (Bündnis 90/Die Grünen and PDS/The Left) have entered the Bundestag.
The most recent election, the German federal election, 2013, was held on 22 September 2013.
Distribution of seats in the Bundestag.
Half of the Members of the Bundestag are elected directly from 299 constituencies (first-past-the-post system), the other half are elected from the parties’ Land lists in such a way as to achieve proportional representation for the total Bundestag (if possible).
Accordingly, each voter has two votes in the elections to the Bundestag. The first vote, allowing voters to elect their local representatives to the Bundestag, decides which candidates are sent to Parliament from the constituencies.
The second vote is cast for a party list; it determines the relative strengths of the parties represented in the Bundestag.
At least 598 Members of the Bundestag are elected in this way. Parties that gain more than 5% of the second votes or win at least 3 direct mandates are allocated seats in the Bundestag in proportion to the number of votes it has received (d'Hondt method until 1987, largest remainder method until the 2005 election, now Sainte-Laguë method).
In addition to this, there are certain circumstances in which some candidates win what are known as overhang seats when the seats are being distributed. If a party has gained more direct mandates in a Land than it is entitled to according to the results of the second vote, it does not forfeit these mandates because all directly elected candidates are guaranteed a seat in the Bundestag.
Distribution of seats by party in the 17th Bundestag (2009 to 2013).
See the List of Bundestag Members for lists of changes and current members.
List of Bundestag by session.
  Parties in the ruling coalition
1 1983 to 1994 The Greens and 1990 to 1994 Alliance 90, since 1994 Alliance 90/The Greens
2 1990 to 2005 PDS (Party of Democratic Socialism), 2005 to 2007 The Left Party.PDS, since 2007 The Left
3 BP 17, KPD 15, WAV 12, Centre Party 10, DKP-DRP 5, SSW 1, Independents 3
4 GB-BHE 27, Centre Party 3
Organisation.
Parliamentary groups.
The most important organisational structures within the Bundestag are parliamentary groups ("Fraktionen"; sing. "Fraktion"), which are formed by political parties represented in the chamber which incorporate more than 5% of the Bundestag legislators; CDU and CSU have always formed a single united "Fraktion". The size of a party's "Fraktion" determines the extent of its representation on legislative committees, the time slots allotted for speaking, the number of committee chairs it can hold, and its representation in executive bodies of the Bundestag. The "Fraktionen," not the members, receive the bulk of government funding for legislative and administrative activities.
The leadership of each "Fraktion" consists of a parliamentary party leader, several deputy leaders, and an executive committee. The leadership's major responsibilities are to represent the "Fraktion," enforce party discipline, and orchestrate the party's parliamentary activities. The members of each "Fraktion" are distributed among working groups focused on specific policy-related topics such as social policy, economics, and foreign policy. The "Fraktion" meets every Tuesday afternoon in the weeks in which the Bundestag is in session to consider legislation before the Bundestag and formulate the party's position on it.
Parties which do not fulfill the criterion for being a "Fraktion" but have at least three seats by direct elections (i.e. which have at least three MPs representing a certain electoral district) in the Bundestag can be granted the status of a "group" of the Bundestag. This applied to the Party of Democratic Socialism (PDS) from 1990-1998. This status entails some privileges which are in general less than those of a "Fraktion". In the current Bundestag, there are no such groups (the PDS had only two MPs in parliament until 2005 and could thus not even considered a group anymore; the party has now returned to the Bundestag with full "Fraktion" status).
Executive bodies.
The Bundestag's executive bodies include the Council of Elders and the Presidium. The council consists of the Bundestag leadership, together with the most senior representatives of each "fraktion", with the number of these representatives tied to the strength of the Parliamentary groups in the chamber. The council is the coordination hub, determining the daily legislative agenda and assigning committee chairpersons based on Parliamentary group representation. The council also serves as an important forum for interparty negotiations on specific legislation and procedural issues. The Presidium is responsible for the routine administration of the Bundestag, including its clerical and research activities. It consists of the chamber's president (usually elected from the largest "fraktion") and vice presidents (one from each "fraktion").
Committees.
Most of the legislative work in the Bundestag is the product of standing committees, which exist largely unchanged throughout one legislative period. The number of committees approximates the number of federal ministries, and the titles of each are roughly similar (e.g., defense, agriculture, and labor). Between 1987 and 1990, the term of the eleventh Bundestag, there were twenty-one standing committees. The distribution of committee chairs and the membership of each committee reflect the relative strength of the various Parliamentary groups in the chamber. In the eleventh Bundestag, the CDU/CSU chaired eleven committees, the SPD eight, the FDP one, and the environmentalist party, the Greens (Die Grünen), one. Members of the opposition party can chair a significant number of standing committees (e.g. The finance committee is always chaired by the biggest opposition party). These committees have either a small staff or no staff at all.
Principle of discontinuation.
As is the case with some other parliaments, the Bundestag is subject to the "principle of discontinuation", meaning that a newly elect Bundestag is legally regarded to be a body and entity completely different from the previous Bundestag. This leads to the result, that any motion, application or action submitted to the previous Bundestag, e.g. a bill referred to the Bundestag by the Federal Government, is regarded as completed by non-decision (German terminology: "Die Sache fällt der Diskontinuität anheim"). Thus any bill that has not been decided upon by the beginning of the new electoral period must be brought up by the government again, if it aims to uphold the motion, this procedure in effect delaying the passage of the bill. Furthermore, any newly elected Bundestag will have to freshly decide on the rules of procedure ("Geschäftsordnung"), which is done by a formal decision of taking over such rules from the preceding "Bundestag" by reference.
Any Bundestag is considered dissolved only once a newly elected Bundestag has actually gathered in order to constitute itself (Article 39 sec. 1 sentence 2 of the Basic Law), which has to happen within 30 days of its election (Article 39 sec. 2 of the Basic Law). Thus, it may happen (and has happened) that the old Bundestag gathers and makes decisions even after the election of a new Bundestag that has not gathered in order to constitute itself. For example, elections to the 16th Bundestag took place on 18 September 2005, but the 15th Bundestag still convened after election day to make some decisions on German military engagement abroad, and was entitled to do so, as the newly elected 16th Bundestag did not convene for the first time until 18 October 2005.

</doc>
<doc id="3769" url="http://en.wikipedia.org/wiki?curid=3769" title="Bundesrat">
Bundesrat

Bundesrat is a German word that means "federal council" and may refer to:

</doc>
<doc id="3772" url="http://en.wikipedia.org/wiki?curid=3772" title="BMW">
BMW

Bayerische Motoren Werke AG (  ; English: Bavarian Motor Works), commonly known as BMW or BMW AG, is a German automobile, motorcycle and engine manufacturing company founded in 1916.
BMW is headquartered in Munich, Bavaria, Germany. It also owns and produces Mini cars, and is the parent company of Rolls-Royce Motor Cars. BMW produces motorcycles under BMW Motorrad. In 2014, the BMW Group produced 2,117,965 automobiles and approximately 120,000 motorcycles across all of its brands. BMW is part of the "German Big 3" luxury automakers, along with Audi and Mercedes-Benz, which are the three best-selling luxury automakers in the world.
History.
BMW was established as a business entity following a restructuring of the Rapp Motorenwerke aircraft manufacturing firm in 1917. After the end of World War I in 1918, BMW was forced to cease aircraft-engine production by the terms of the Versailles Armistice Treaty. The company consequently shifted to motorcycle production in 1923, once the restrictions of the treaty started to be lifted, followed by automobiles in 1928–29.
The first car which BMW successfully produced and the car which launched BMW on the road to automobile production was the Dixi, it was based on the Austin 7 and licensed from the Austin Motor Company in Birmingham, England.
BMW's first significant aircraft engine (and commercial product of any sort) was the BMW IIIa inline-six liquid-cooled engine of 1918, much preferred for its high-altitude performance. With German rearmament in the 1930s, the company again began producing aircraft engines for the Luftwaffe. Among its successful World War II engine designs were the BMW 132 and BMW 801 air-cooled radial engines, and the pioneering BMW 003 axial-flow turbojet, which powered the tiny, 1944–1945–era jet-powered "emergency fighter", the Heinkel He 162 "Spatz". The BMW 003 jet engine was first tested as a prime powerplant in the first prototype of the Messerschmitt Me 262, the Me 262 V1, but in 1942 tests the BMW prototype engines failed on takeoff with only the standby Junkers Jumo 210 nose-mounted piston engine powering it to a safe landing. The few Me 262 A-1b test examples built used the more developed version of the 003 jet, recording an official top speed of 800 km/h (497 mph). The first-ever four-jet aircraft ever flown, the sixth and eighth prototypes of the Arado Ar 234 jet reconnaissance-bomber, used BMW 003 jets for power. The improving reliability of the 003 as 1944 progressed, earmarked it as the required powerplant for airframe designs competing for the "Jägernotprogramm"'s light fighter production contract, won by the Heinkel He 162 "Spatz" design. The BMW 003 aviation turbojet also found itself under consideration as the basic starting point for a pioneering turboshaft powerplant for German armored fighting vehicles in 1944–45, as the GT 101. Towards the end of the Third Reich BMW developed some military aircraft projects for the "Luftwaffe", the BMW Strahlbomber, the BMW Schnellbomber and the BMW Strahljäger, but none of them were built.
By the year 1958, the automotive division of BMW was in financial difficulties and a shareholders meeting was held to decide whether to go into liquidation or find a way of carrying on. It was decided to carry on and to try to cash in on the current economy car boom enjoyed so successfully by some of Germany's ex-aircraft manufacturers such as Messerschmitt and Heinkel. The rights to manufacture the Italian Iso Isetta were bought; the tiny cars themselves were to be powered by a modified form of BMW's own motorcycle engine. This was moderately successful and helped the company get back on its feet. The controlling majority shareholder of the BMW Aktiengesellschaft since 1959 is the Quandt family, which owns about 46% of the stock. The rest is in public float.
BMW acquired the Hans Glas company based in Dingolfing, Germany, in 1966. Glas vehicles were briefly badged as BMW until the company was fully absorbed. It was reputed that the acquisition was mainly to gain access to Glas' development of the timing belt with an overhead camshaft in automotive applications, although some saw Glas' Dingolfing plant as another incentive. However, this factory was outmoded and BMW's biggest immediate gain was, according to themselves, a stock of highly qualified engineers and other personnel. The Glas factories continued to build a limited number of their existing models, while adding the manufacture of BMW front and rear axles until they could be closer incorporated into BMW.
In 1992, BMW acquired a large stake in California based industrial design studio DesignworksUSA, which they fully acquired in 1995. In 1994, BMW bought the British Rover Group (which at the time consisted of the Rover, Land Rover and MG brands as well as the rights to defunct brands including Austin and Morris), and owned it for six years. By 2000, Rover was incurring huge losses and BMW decided to sell the combine. The MG and Rover brands were sold to the Phoenix Consortium to form MG Rover, while Land Rover was taken over by Ford. BMW, meanwhile, retained the rights to build the new Mini, which was launched in 2001.
Chief designer Chris Bangle announced his departure from BMW in February 2009, after serving on the design team for nearly seventeen years. He was replaced by Adrian van Hooydonk, Bangle's former right-hand man. Bangle was known for his radical designs such as the 2002 7-Series and the 2002 Z4. In July 2007, the production rights for Husqvarna Motorcycles was purchased by BMW for a reported 93 million euros. BMW Motorrad plans to continue operating Husqvarna Motorcycles as a separate enterprise. All development, sales and production activities, as well as the current workforce, have remained in place at its present location at Varese.
In June 2012, BMW was listed as the #1 most reputable company in the world by Forbes.com. Rankings are based upon aspects such as "people's willingness to buy, recommend, work for, and invest in a company is driven 60% by their perceptions of the company and only 40% by their perceptions of their products."
Shareholder structure.
"by ownership"
"by types"
Production.
In 2006, the BMW group (including Mini and Rolls-Royce) produced 1,366,838 four-wheeled vehicles, which were manufactured in five countries. In 2010, it manufactured 1,481,253 four-wheeled vehicles and 112,271 motorcycles (under both the BMW and Husqvarna brands).
The BMW X3 (E83) was made by Magna Steyr, a subsidiary of Magna of Canada, in Graz, Austria under license from BMW until 2010. More than 45,973 were produced in 2009.
Starting October 2010, the new BMW X3 (F25) is produced at BMW US Manufacturing Company, Greer, Spartanburg County, South Carolina.
It is reported that about 56% of BMW-brand vehicles produced are powered by petrol engines and the remaining 44% are powered by diesel engines. Of those petrol vehicles, about 27% are four-cylinder models and about nine percent are eight-cylinder models. BMW also has local assembly operation using complete knock down components in Thailand, Russia, Egypt, Indonesia, Malaysia, and India, for 3, 5, 7 series and X3.
Worldwide sales.
Vehicles sold in all markets according to BMW's annual reports.
China sales.
BMW sells vehicles in China through "more than 440 BMW sales outlets and 100 Mini stores," delivering 415,200 units to this network between January and November 2014.
Motorcycles.
BMW began production of motorcycle engines and then motorcycles after World War I. Its motorcycle brand is now known as BMW Motorrad. Their first successful motorcycle, after the failed Helios and Flink, was the "R32" in 1923. This had a "boxer" twin engine, in which a cylinder projects into the air-flow from each side of the machine. Apart from their single-cylinder models (basically to the same pattern), all their motorcycles used this distinctive layout until the early 1980s. Many BMWs are still produced in this layout, which is designated the R Series.
During the Second World War, BMW produced the BMW R75 motorcycle with a sidecar attached. Having a unique design copied from the Zündapp KS750, its sidecar wheel was also motor-driven. Combined with a lockable differential, this made the vehicle very capable off-road, an equivalent in many ways to the Jeep.
In 1982, came the K Series, shaft drive but water-cooled and with either three or four cylinders mounted in a straight line from front to back. Shortly after, BMW also started making the chain-driven F and G series with single and parallel twin Rotax engines.
In the early 1990s, BMW updated the airhead Boxer engine which became known as the oilhead. In 2002, the oilhead engine had two spark plugs per cylinder. In 2004 it added a built-in balance shaft, an increased capacity to 1,170 cc and enhanced performance to 100 hp for the R1200GS, compared to 85 hp of the previous R1150GS. More powerful variants of the oilhead engines are available in the R1100S and R1200S, producing 98 and, respectively.
In 2004, BMW introduced the new K1200S Sports Bike which marked a departure for BMW. It had an engine producing 167 hp, derived from the company's work with the Williams F1 team, and is lighter than previous K models. Innovations include electronically adjustable front and rear suspension, and a Hossack-type front fork that BMW calls Duolever.
BMW introduced anti-lock brakes on production motorcycles starting in the late 1980s. The generation of anti-lock brakes available on the 2006 and later BMW motorcycles pave the way for the introduction of electronic stability control, or anti-skid technology later in the 2007 model year.
BMW has been an innovator in motorcycle suspension design, taking up telescopic front suspension long before most other manufacturers. Then they switched to an Earles fork, front suspension by swinging fork (1955 to 1969). Most modern BMWs are truly rear swingarm, single sided at the back (compare with the regular swinging fork usually, and wrongly, called swinging arm).
Some BMWs started using yet another trademark front suspension design, the Telelever, in the early 1990s. Like the Earles fork, the Telelever significantly reduces dive under braking.
BMW Group, on 31 January 2013 announced that Pierer Industrie AG has bought Husqvarna for an undisclosed amount, which will not be revealed by either party in the future. The company is headed by Stephan pierer (CEO of KTM). Pierer Industrie AG is 51% owner of KTM and 100% owner of Husqvarna.
Automobiles.
New Class.
The "New Class" (German: "Neue Klasse") was a line of compact sedans and coupes starting with the 1962 1500 and continuing through the last 2002s in 1977. Powered by BMW's celebrated four-cylinder M10 engine, the New Class models had a fully independent suspension, MacPherson struts in front, and front disc brakes. Initially a family of four-door sedans and two-door coupes, the New Class line was broadened to two-door sports sedans with the addition of the "02 Series" 1600 and 2002 in 1966.
Sharing little in common with the rest of the line beyond power train, the sporty siblings caught auto enthusiasts' attention and established BMW as an international brand. Precursors to the famed BMW 3 Series, the two-doors' success cemented the firm's future as an upper tier performance car maker. New Class four-doors with numbers ending in "0" were replaced by the larger BMW 5 Series in 1972. The upscale 2000C and 2000CS coupes were replaced by the six-cylinder BMW E9, introduced in 1969 with the 2800CS. The 1600 two-door was discontinued in 1975, and the 2002 was replaced by the 320i in 1975.
Current models.
The 1 Series, originally launched in 2004, is BMW's smallest car. Currently available are the second generation hatchback (F20) and first generation coupe/convertible (E82/E88). The 3 Series, a compact executive car manufactured since model year 1975, is currently in its sixth generation (F30); models include the sport sedan (F30), and fourth generation station wagon (F30), and convertible (E93), and the Gran Turismo. In 2014, the 4 Series has been released and replaced the 3 Series Coupe and Convertible. The 5 Series is a mid-size executive car, available in sedan (F10) and station wagon (F11) forms. The 5 Series Gran Turismo (F07), which debuted in 2010, created a segment between station wagons and crossover SUV.
BMW's full-size flagship executive sedan is the 7 Series. Typically, BMW introduces many of their innovations first in the 7 Series, such as the iDrive system. The 7 Series Hydrogen, having one of the world's first hydrogen fueled internal combustion engines, is fueled by liquid hydrogen and emits only clean water vapor. The latest generation (F01) debuted in 2009. Based on the 5 Series' platform, the 6 Series is BMW's grand touring luxury sport coupe/convertible (F12/F13). A 2-seater roadster and coupe which succeeded the Z3, the Z4 has been sold since 2002.
The X3 (F25), BMW's second crossover SUV (called SAV or "Sports Activity Vehicle" by BMW) debuted in 2010 and replaced the X3 (E83), which was based on the E46 3 Series' platform, and had been in production since 2003. Marketed in Europe as an off-roader, it benefits from BMW's xDrive all-wheel drive system. The all-wheel drive X5 (E53) was BMW's first crossover SUV (SAV), based on the 5 Series, and is a mid-size luxury SUV (SAV) sold by BMW since 2000. A 4-seat crossover SUV released by BMW in December 2007, the X6 is marketed as a "Sports Activity Coupe" (SAC) by BMW. The X1 extends the BMW Sports Activity Series model lineup.
In 2013, the company announced that it was to launch its first fully electric car range. This would begin with the launch of the BMW i3 in the second quarter of 2014.
M models.
BMW produce a number of high-performance derivatives of their cars developed by their BMW M GmbH (previously BMW Motorsport GmbH) subsidiary.
The current M models are:
Motorsport.
BMW has been engaged in motorsport activities since the dawn of the first BMW motorcycle in 1923.
Formula One – F1.
BMW has a history of success in Formula One. BMW powered cars have won 20 races. In 2006 BMW took over the Sauber team and became Formula One constructors. In 2007 and 2008 the team enjoyed some success. The most recent win is a lone constructor team's victory by BMW Sauber F1 Team, on 8 June 2008, at the Canadian Grand Prix with Robert Kubica driving. Achievements include:
BMW was an engine supplier to Williams, Benetton, Brabham, and Arrows. Notable drivers who have started their Formula One careers with BMW include Jenson Button, Juan Pablo Montoya, Robert Kubica and Sebastian Vettel.
In July 2009, BMW announced that it would withdraw from Formula One at the end of the 2009 season. The team was sold back to the previous owner, Peter Sauber, who kept the BMW part of the name for the 2010 season due to issues with the Concorde Agreement. The team has since dropped BMW from their name starting in 2011.
Touring cars.
BMW has a long and successful history in touring car racing.
BMW announced on 15 October 2010 that it will return to touring car racing during the 2012 season. Dr. Klaus Draeger, director of research and development of the BMW Group, who was in charge of the return to DTM racing (Deutsche Tourenwagen Masters), commented that "The return of BMW to the DTM is a fundamental part of the restructuring of our motorsport activities. With its increased commitment to production car racing, BMW is returning to its roots. The race track is the perfect place to demonstrate the impressive sporting characteristics of our vehicles against our core competitors in a high-powered environment. The DTM is the ideal stage on which to do this."
Sponsorships.
In football, BMW sponsors Bundesliga club Eintracht Frankfurt.
It is an official sponsor of the London 2012 olympics providing 4000 BMWs and Minis in a deal made in November 2009. The company also made a six-year sponsorship deal with the United States Olympic Committee (USOC) in July 2010.
BMW has sponsored various European golf events such as the PGA Championship at Wentworth, the BMW Italian Open and the BMW International Open in Germany.
Environmental record.
The company is a charter member of the U.S. Environmental Protection Agency's (EPA) National Environmental Achievement Track, which recognizes companies for their environmental stewardship and performance. It is also a member of the South Carolina Environmental Excellence Program.
In 2012, BMW was named the world's most sustainable automotive company for the eighth consecutive year by the Dow Jones Sustainability Indexes. The BMW Group is the only automotive enterprise in the index since its inception in 1999. In 2001, the BMW Group committed itself to the United Nations Environment Programme, the UN Global Compact and the Cleaner Production Declaration. It was also the first company in the automotive industry to appoint an environmental officer, in 1973. BMW is a member of the World Business Council for Sustainable Development.
BMW is industry leader in the Carbon Disclosure Project's Global 500 ranking and 3rd place in Carbon Disclosure Leadership Index across all industries.
BMW is listed in the FTSE4GoodIndex.
The BMW Group was rated the most sustainable DAX 30 company by Sustainalytics in 2012.
BMW has taken measures to reduce the impact the company has on the environment. It is trying to design less-polluting cars by making existing models more efficient, as well as developing environmentally friendly fuels for future vehicles. Possibilities include: electric power, hybrid power (combustion engines and electric motors) hydrogen engines.
BMW offers 49 models with EU5/6 emissions norm and nearly 20 models with CO2 output less than 140 g/km, which puts it on the lowest tax group and therefore could provide the future owner with eco-bonus offered from some European countries.
However, there have been some criticisms directed at BMW, and in particular, accusations of greenwash in reference to their BMW Hydrogen 7. Some critics claim that the emissions produced during hydrogen fuel production outweigh the reduction of tailpipe emissions, and that the Hydrogen 7 is a distraction from more immediate, practical solutions for car pollution.
Bicycles.
BMW has created a range of high-end bicycles sold online and through dealerships. They range from the Kid's Bike to the EUR 4,499 Enduro Bike. In the United States, only the Cruise Bike and Kid's Bike models are sold.
BMW nomenclature.
BMW vehicles follow a certain nomenclature; usually a 3 digit number is followed by 1 or 2 letters. The first number represents the series number. The next two numbers traditionally represent the engine displacement in cubic centimeters divided by 100. However, more recent cars use those two numbers as a performance index, as e.g. the 116i, 118i and 120i (all 2,0L gas-powered), just like the 325d and 330d (both 3,0L diesel) share the same motor block while adjusting engine power through setup and turbocharging. A similar nomenclature is used by BMW Motorrad for their motorcycles.
The system of letters can be used in combination, and is as follows:
† historic nomenclature indicating "td" refers to "Turbo Diesel", not a diesel hatchback or touring model (524td, 525td)
†† typically includes sport seats, spoiler, aerodynamic body kit, upgraded wheels and Limit Slip Differential on pre-95 model etc.
For example, the BMW 750iL is a fuel-injected 7 Series with a long wheelbase and 5.4 litres of displacement. This badge was used for successive generations, E65 and F01, except the "i" and "L" switched places, so it read "Li" instead of "iL".
When 'L' supersedes the series number (e.g. L6, L7, etc.) it identifies the vehicle as a special luxury variant, having extended leather and special interior appointments. The L7 is based on the E23 and E38, and the L6 is based on the E24.
When 'X' is capitalised and supersedes the series number (e.g. X3, X5, etc.) it identifies the vehicle as one of BMW's Sports Activity Vehicles (SAV), their brand of crossovers, with BMW's xDrive. The second number in the 'X' series denotes the platform that it is based upon, for instance the X5 is derived from the 5 Series. Unlike BMW cars, the SAV's main badge does not denote engine size; the engine is instead indicated on side badges.
The 'Z' identifies the vehicle as a two-seat roadster (e.g. Z1, Z3, Z4, etc.). 'M' variants of 'Z' models have the 'M' as a suffix or prefix, depending on country of sale (e.g. 'Z4 M' is 'M Roadster' in Canada).
Previous X & Z vehicles had 'i' or 'si' following the engine displacement number (denoted in litres). BMW is now globally standardising this nomenclature on X & Z vehicles by using 'sDrive' or 'xDrive' (simply meaning rear or all-wheel drive, respectively) followed by two numbers which vaguely represent the vehicle's engine (e.g. Z4 sDrive35i is a rear-wheel-drive Z4 roadster with a 3.0 L twin-turbo fuel-injected engine).
BMW last used the 's' for the E36 328is, which ceased production in 1999. However, the 's' nomenclature was brought back on the 2011 model year BMW 335is and BMW Z4 sDrive35is. The 335is is a sport-tuned trim with more performance and an optional dual clutch transmission that slots between the regular 335i and top-of-the-line M3.
The 'M' – for Motorsport – identifies the vehicle as a high-performance model of a particular series (e.g. M3, M5, M6, etc.). For example, the M6 is the highest performing vehicle in the 6 Series lineup. Although 'M' cars should be separated into their respective series platforms, it is very common to see 'M' cars grouped together as its own lineup on the official BMW website.
Exceptions.
There are exceptions to the numbering nomenclature.
The M version of the BMW 1 Series was named the BMW 1 Series M Coupe rather than the traditional style "M1" due to the possible confusion with BMW's former BMW M1 homologation sports car.
The M versions of the Sports Activity Vehicles, such as the X5 M, could not follow the regular naming convention since MX5 was used for Mazda's MX-5 Miata.
For instance in the 2008 model year, the BMW 125i/128i, 328i, and 528i all had 3.0 naturally aspirated engines (N52), not a 2,500 cc or 2,800 cc engine as the series designation number would lead one to believe. The '28' is to denote a detuned engine in the 2008 cars, compared to the 2006 model year '30' vehicles (330i and 530i) whose 3.0 naturally aspirated engines are from the same N52 family but had more output.
The 2008 BMW 335i and 535i also have 3.0-litre engine; however the engines are twin-turbocharged (N54) which is not identified by the nomenclature. Nonetheless the '35' indicates a more powerful engine than previous '30' models that have the naturally aspirated N52 engine. The 2011 BMW 740i and 335is shares the same twin-turbo 3.0 engine from the N54 family but tuned to higher outputs, although the badging is not consistent ('40' and 's'). The 2013 BMW 640i Gran Coupe's twin-scroll single turbo 3.0L inline-6 engine makes similar output to the older twin turbo inline-6 engines.
The E36 and E46 323i and E39 523i had 2.5-litre engines. The E36 318i made after 1996 has a 1.9 L engine (M44) as opposed to the 1.8 L (M42) used in the 1992 to 1995 models. The E39 540i had a 4.4 L M62 engine, instead of a 4.0 L as the designation would suggest.
The badging for recent V8 engines (N62 and N63) also does not indicate displacement, as the 2006 750i and 2009 750i have 4800 cc (naturally aspirated) and 4400 cc (twin-turbocharged) engines, respectively.
Carsharing services.
In June 2011, BMW and Sixt launched Drivenow, a joint-venture that provides carsharing services in several cities in Europe and North America. As of December 2012, DriveNow operates over 1,000 vehicles, which serve five cities worldwide and over 60,000 customers.
Light and Charge.
BMW has developed street lights equipped with sockets to charge electric cars, called Light and Charge.
Community.
From the summer of 2001 until October 2005, BMW hosted The Hire, showcasing sporty models being driven to extremes. These videos are still popular within the enthusiast community and proved to be a ground-breaking online advertising campaign.
Annually since 1999, BMW enthusiasts have met in Santa Barbara, CA to attend Bimmerfest. One of the largest brand-specific gatherings in the U.S., over 3,000 people attended in 2006, and over 1,000 BMW cars were present. In 2007, the event was held on 5 May.
BMW slang.
The initials BMW are pronounced ] in German. The model series are referred to as "Einser" ("One-er" for 1 series), "Dreier" ("Three-er" for 3 series), "Fünfer" ("Five-er" for the 5 series), "Sechser" ("Six-er" for the 6 series), "Siebener" ("Seven-er" for the 7 series). These are not actually slang, but are the normal way that such letters and numbers are pronounced in German.
The English slang terms Beemer, Bimmer and Bee-em are variously used for BMWs of all kinds, cars and motorcycles.
In the US, specialists have been at pains to prescribe that a distinction must be made between using Beemer exclusively to describe BMW motorcycles, and using Bimmer only to refer to BMW cars, in the manner of a "true aficionado" and avoid appearing to be "uninitiated."
The Canadian "Globe and Mail" prefers Bimmer and calls Beemer a "yuppie abomination," while the "Tacoma News Tribune" says it is a distinction made by "auto snobs." Using the wrong slang risks offending BMW enthusiasts. An editor of "Business Week" was satisfied in 2003 that the question was resolved in favor of Bimmer by noting that a Google search yielded 10 times as many hits compared to Beemer.
The arts.
Manufacturers employ designers for their cars, but BMW has made efforts to gain recognition for exceptional contributions to and support of the arts, including art beyond motor vehicle design. These efforts typically overlap or complement BMW's marketing and branding campaigns. BMW Headquarters designed in 1972 by Karl Schwanzer has become a European icon, and artist Gerhard Richter created his "Red, Yellow, Blue" series of paintings for the building's lobby. In 1975, Alexander Calder was commissioned to paint the 3.0CSL driven by Hervé Poulain at the 24 Hours of Le Mans. This led to more BMW Art Cars, painted by artists including David Hockney, Jenny Holzer, Roy Lichtenstein, and others. The cars, currently numbering 17, have been shown at the Louvre, Guggenheim Museum Bilbao, and, in 2009, at the Los Angeles County Museum of Art and New York's Grand Central Terminal. BMW was the principal sponsor of the 1998 "The Art of the Motorcycle" exhibition at the Solomon R. Guggenheim Museum and other Guggenheim museums, though the financial relationship between BMW and the Guggenheim was criticised in many quarters.
In 2012, BMW brought out the BMW Art Guide by Independent Collectors, which had, amongst others, the Dikeou Collection.
It is the first global guide to private and publicly accessible collections of contemporary art world wide.
The 2006 "BMW Performance Series" was a marketing event geared to attract black car buyers, and included the "BMW Pop-Jazz Live Series," a tour headlined by jazz musician Mike Phillips, and the "BMW Blackfilms.com Film Series" highlighting black filmmakers.
April Fools.
BMW has garnered a reputation over the years for its April Fools pranks, which are printed in the British press every year. In 2010, they ran an advert announcing that customers would be able to order BMWs with different coloured badges to show their affiliation with the political party they supported.
Overseas subsidiaries.
Brazil.
On October 9, 2014, BMW's new South American automobile plant in Araquari, Santa Catarina produced its first car. BMW intend to increase its production capacity to over 30,000 vehicles a year. The new site is intended to create around 1,300 new jobs, of which 500 have already been filled.
Canada.
In October 2008, BMW Group Canada was named one of Greater Toronto's Top Employers by Mediacorp Canada Inc., which was announced by the Toronto Star newspaper.
China.
Signing a deal in 2003 for the production of sedans in China, May 2004 saw the opening of a factory in the North-eastern city of Shenyang where Brilliance Auto produces BMW-branded automobiles in a joint venture with the German company.
Egypt.
Bavarian Auto Group is a multinational group of companies established in March 2003 when it was appointed as the sole importer of BMW and Mini in Egypt, with monopoly rights for import, assembly, distribution, sales and after-sales support of BMW products in Egypt. Since that date, BAG invested a total amount of US$100 million distributed on seven companies and 11 premises in addition to three stores.
India.
BMW India was established in 2006 as a sales subsidiary in Gurgaon (National Capital Region). A state-of-the-art assembly plant for BMW 3 and 5 Series started operation in early 2007 in Chennai. Construction of the plant started in January 2006 with an initial investment of more than one billion Indian Rupees. The plant started operation in the first quarter of 2007 and produces the different variants of BMW 3 Series, BMW 5 Series, BMW 7 Series, BMW X1, BMW X3, Mini Cooper S, Mini Cooper D and Mini Countryman.
Japan.
Yanase Co., Ltd. is the exclusive retailer of all imported BMW (passenger cars and motorcycles) products to Japanese consumers, and has had the exclusive rights to do so since the end of World War II.
Mexico.
In July 2014 BMW announced it was establishing a plant in Mexico, in the city and state of San Luis Potosi involving an investment of $1 billion. Taking advantage of lower wages in the country, and the terms of free trade agreements Mexico has with a host of other countries, were the motivating factors the company said. The plant will employ 1,500 people, and produce 150,000 cars annually, commencing in 2019.
South Africa.
BMWs have been assembled in South Africa since 1968, when Praetor Monteerders' plant was opened in Rosslyn, near Pretoria. BMW initially bought shares in the company, before fully acquiring it in 1975; in so doing, the company became BMW South Africa, the first wholly owned subsidiary of BMW to be established outside Germany. Three unique models that BMW Motorsport created for the South African market were the E23 M745i (1983), which used the M88 engine from the BMW M1, the BMW 333i (1986), which added a six-cylinder 3.2-litre M30 engine to the E30, and the E30 BMW 325is (1989) which was powered by an Alpina-derived 2.7-litre engine.
Unlike U.S. manufacturers, such as Ford and GM, which divested from the country in the 1980s, BMW retained full ownership of its operations in South Africa. Following the end of apartheid in 1994, and the lowering of import tariffs, BMW South Africa ended local production of the 5-Series and 7-Series, in order to concentrate on production of the 3-Series for the export market. South African–built BMWs are now exported to right hand drive markets including Japan, Australia, New Zealand, the United Kingdom, Indonesia, Malaysia, Singapore, and Hong Kong, as well as Sub-Saharan Africa. Since 1997, BMW South Africa has produced vehicles in left-hand drive for export to Taiwan, the United States and Iran, as well as South America.
BMWs with a VIN starting with "NC0" are manufactured in South Africa.
United States.
The BMW Manufacturing Company opened in 1994 and has been manufacturing all Z4 and X5 models, and more recently the X6 and X3, including those for export to Europe, on the same assembly line in Greer near Spartanburg. In an average work day the company builds 600 vehicles: 500 X5s and 100 Z4s. The engines for these vehicles are built in Munich, Germany. BMWs with a VIN starting with "4US and 5US" are manufactured at Spartanburg.
In 2010 BMW announced that it would spend $750 million to expand operations at the Greer plant. This expansion will allow production of 240,000 vehicles a year and will make the plant the largest car factory in the United States by number of employees. BMW's largest single market is the United States.
Currently, the facility produces all BMW X3, X5 and X6 models. The X4, which will be launched in 2014, will also be produced in Spartanburg.
Marketing.
BMW began using the slogan 'The Ultimate Driving Machine' in the 1970s. In 2010, this long-lived campaign was mostly supplanted by 'Joy', a campaign intended to make the brand more "approachable" and to better appeal to women, but by 2012 they had returned to "The Ultimate Driving Machine", which has a strong public association with BMW.
Audio logo.
In 2013, BMW replaced the 'double-gong' sound used in TV and Radio advertising campaigns since 1998. The new sound, developed to represent the future identity of BMW, was described as "introduced by a rising, resonant sound and underscored by two distinctive bass tones that form the sound logo's melodic and rhythmic basis." The new sound was first used in BMW 4 Series Concept Coupe TV commercial. The sound was produced by Thomas Kisser of HASTINGS media music.
Roundel logo.
The circular blue and white BMW logo or roundel evolved from the circular Rapp Motorenwerke company logo, from which the BMW company grew, combined with the blue and white colors of the flag of Bavaria. The logo has been portrayed as the movement of an aircraft propeller with the white blades cutting through a blue sky—first used in a BMW advertisement in 1929, twelve years after the roundel was created—but this is not the origin of the logo itself.
Theft using OBD.
In 2012, BMW vehicles were stolen by programming a blank key fob to start the car through the on-board diagnostics (OBD) connection. The primary causes of this vulnerability lie in the lack of appropriate authentication and authorization in the OBD specifications, which rely largely on security through obscurity.

</doc>
<doc id="3774" url="http://en.wikipedia.org/wiki?curid=3774" title="Bisexual (disambiguation)">
Bisexual (disambiguation)

Bisexual may refer to:

</doc>
<doc id="3776" url="http://en.wikipedia.org/wiki?curid=3776" title="Bornholm">
Bornholm

Bornholm (]; Old Norse: "Burgundaholmr") is a Danish island in the Baltic Sea, to the east of most of Denmark, south of Sweden, northeast of Germany and northwest of Poland. The main industries on the island include fishing, arts and crafts such as glass production and pottery using locally worked clay, and dairy farming. Tourism is important during the summer. The topography of the island consists of dramatic rock formations in the north (unlike the rest of Denmark, which is mostly gentle rolling hills) sloping down towards pine and deciduous forests (greatly damaged by storms in the 1950s) and farmland in the middle and sandy beaches in the south.
Bornholm Regional Municipality covers the entire island. Bornholm was one of the three last Danish municipalities (Danish: "kommune") not belonging to a county— the others were Copenhagen and Frederiksberg. On 1 January 2007, the municipality lost its short-lived (2003 until 2006) county status and became part of the Capital Region of Denmark.
The Ertholmene archipelago is located 18 km to the northeast of Bornholm. These islands, which do not belong to a municipality or region, are administered by the Ministry of Defence.
Strategically located in the Baltic Sea, Bornholm has been fought over for centuries. It has usually been ruled by Denmark, but also by Lübeck and Sweden. The ruin of Hammershus, at the northwestern tip of the island, is the largest medieval fortress in northern Europe, testament to the importance of its location.
Language.
Many inhabitants speak the Bornholmsk dialect, which is officially a dialect of Danish. Bornholmsk retains three grammatical genders, like Icelandic and most dialects of Norwegian, but unlike standard Danish. Its phonology includes archaisms (unstressed [a] and internal [d̥, ɡ̊], where other dialects have [ə] and [ð̞, ʊ / ɪ]) and innovations ([tɕ, dʝ] for [kʰ, ɡ̊] before and after front-tongue vowels). This makes the dialect difficult to understand for some Danish speakers. However, Swedish speakers often consider Bornholmian to be easier to understand than standard Danish. The intonation resembles the Scanian dialect spoken in nearby Scania, the southernmost province of Sweden.
Municipality.
Bornholm Regional Municipality is the local authority (Danish, "kommune") covering the entire island. It comprises the five former (1 April 1970 until 2002) municipalities on the island (Allinge-Gudhjem, Hasle, Nexø, Rønne and Aakirkeby) and the former Bornholm County. The island had 22 municipalities until March 1970, of which 6 were market towns and 16 parishes. The market town municipalities were supervised by the county and not by the Interior Ministry as was the case in the rest of Denmark. The seat of the municipal council is the island's main town, Rønne. The voters decided to merge the county with the municipalities in a referendum May 29, 2001, effective from January 1, 2003. The first regional mayor was Thomas Thors, a physician and member of the Social Democrats and previously mayor of Rønne Municipality. Bjarne Kristiansen, representing the local Borgerlisten political party, served as mayor from January 1, 2006 until 2009. From January 1, 2010 the mayor has been Winni Grosbøll, a high school teacher and a member of the Social Democrats ("Socialdemokraterne") political party.
Ferry services connect Rønne to Świnoujście (Poland), Sassnitz (Germany), Køge (near Copenhagen, Denmark); and catamaran services to Ystad (Sweden). Simrishamn (Sweden) has a ferry connection during the summer. There are also regular catamaran services between Nexø and the Polish ports of Kołobrzeg, Łeba and Ustka. There are direct train and bus connections Ystad-Copenhagen, coordinated with the catamaran. There are also flights from Bornholm Airport to Copenhagen and other locations.
Bornholm Regional Municipality was not merged with other municipalities on 1 January 2007 in the nationwide Municipal Reform of 2007.
Towns and villages.
The larger towns on the island are located on the coast and have a harbour. There is however one exception, centrally placed Aakirkeby. The largest town is Rønne, the seat, in the southwest. The other main towns (clockwise round the island) are Hasle, Allinge-Sandvig, Gudhjem, Svaneke and Nexø. Monday morning 22 September 2014 it was documented by "Folkeregistret" in the municipality that the number of people living in the municipality that day were 39,922, the lowest number in over 100 years.
As of 2012, the Danish statistical office gave the populations as follows:
Other localities (with approximate populations) include Aarsballe (86), Arnager (151), Nylars (228), Olsker (67), Rutsker (64), Rø (181), Stenseby (?) and Vang (92).
History.
In Old Norse the island was known as "Borgundarholm", and in ancient Danish especially the island's name was "Borghand" or "Borghund"; these names were related to Old Norse "borg" "height" and "bjarg/berg" "mountain, rock", as it is an island that rises high from the sea. Other names known for the island include "Burgendaland" (9th century), "Hulmo" / "Holmus" ("Gesta Hammaburgensis ecclesiae pontificum"), "Burgundehulm" (1145), and "Borghandæholm" (14th century). Alfred the Great uses the form "Burgenda land". Some scholars believe that the Burgundians are named after Bornholm; the Burgundians were Germanic peoples who moved west when the Western Roman Empire collapsed and occupied and named Burgundy in France.
Bornholm formed part of the historical Lands of Denmark when the nation united out of a series of petty chiefdoms. It was originally administratively part of the province of Scania and was administered by the Scanian Law after this was codified in the 13th century. Control over the island evolved into a long-raging dispute between the See of Lund and the Danish crown culminating in several battles. The first fortress on the island was "Gamleborg" which was replaced by "Lilleborg", built by the king in 1150. In 1149, the king accepted the transfer of three of the island's four herreder (districts) to the archbishop. In 1250, the archbishop constructed his own fortress, Hammershus. A campaign launched from it in 1259 conquered the remaining part of the island including Lilleborg. The island's status remained a matter of dispute for an additional 200 years.
Bornholm was pawned to Lübeck for 50 years starting 1525. Its first militia, Bornholms Milits, was formed in 1624.
Swedish forces conquered the island in 1645, but returned the island to Denmark in the following peace settlement. After the war in 1658, Denmark ceded the island to Sweden under the Treaty of Roskilde along with the rest of the Skåneland, Bohuslän and Trøndelag, and it was occupied by Swedish forces.
A revolt broke out the same year, culminating in Villum Clausen's shooting of the Swedish commander Johan Printzensköld on 8 December 1658. Following the revolt, a deputation of islanders presented the island as a gift to King Frederick III on the condition that the island would never be ceded again. This status was confirmed in the Treaty of Copenhagen in 1660.
Swedes, notably from Småland and Skania, emigrated to the island during the 19th century, seeking work and better conditions. Most of the migrants did not remain.
German Occupation 1940–1945.
Bornholm, as a part of Denmark, was captured by Germany on April 10, 1940, and served as a lookout post and listening station during the war, as it was a part of the Eastern Front. The island's perfect central position in the Baltic Sea meant that it was an important "natural fortress" between Germany and Sweden, effectively keeping submarines and destroyers away from Nazi-occupied waters. Several concrete coastal installations were built during the war, and several coastal batteries had tremendous range. However, none of them were ever used, and only a single test shot was fired during the occupation. These remnants of Nazi rule have since fallen into disrepair and are mostly regarded today as historical curiosities. Many tourists visit the ruins each year, however, providing supplemental income to the tourist industry.
On 22 August 1943 a V-1 flying bomb (numbered V83, probably launched from a Heinkel He 111) crashed on Bornholm during a test – the warhead was a dummy made of concrete. This was photographed or sketched by the Danish Naval Officer-in-Charge on Bornholm, Lieutenant Commander Hasager Christiansen. This was the first sign British Intelligence saw of Germany's aspirations to develop flying bombs and rockets which were to become known as V-1 and V-2.
Soviet Occupation 1945–1946.
Bornholm was heavily bombarded by the Soviet Air Force in May 1945. German garrison commander, German Navy Captain Gerhard von Kamptz, refused to surrender to the Soviets, as his orders were to surrender to the Western Allies. The Germans sent several telegrams to Copenhagen requesting that at least one British soldier should be transferred to Bornholm, so that the Germans could surrender to the western allied forces instead of the Russians. When von Kamptz failed to provide a written capitulation as demanded by the Soviet commanders, Soviet aircraft relentlessly bombed and destroyed more than 800 civilian houses in Rønne and Nexø and seriously damaged roughly 3,000 more on 7–8 May 1945.
During the Russian bombing of the two main towns on 7 and 8 May, Danish radio was not allowed to broadcast the news because it was thought it would spoil the liberation festivities in Denmark. On 9 May Soviet troops landed on the island, and after a short fight, the German garrison (about 12,000 strong) surrendered. Soviet forces left the island on 5 April 1946.
After the evacuation of their forces from Bornholm, the Soviets took the position that the stationing of foreign troops on Bornholm would be considered a declaration of war against the Soviet Union, and that Denmark should keep troops on it at all times to protect it from such foreign aggression. This policy remained in force after NATO was formed, with Denmark as a founding member. The Soviets accepted the stationing there of Danish troops, which were part of NATO but viewed as military inferior elements of the alliance, but they strongly objected to the presence of other NATO troops on Bornholm, in particular of US troops.
This situation caused diplomatic problems at least twice: Once when an American helicopter landed outside the town of Svaneke due to engine problems in a NATO exercise across the Baltic Sea, and once again (circa 2000), when the "Bornholms Værn" (Bornholm Guard) was disbanded, becoming part of the Guard Hussar Regiment. The Danish government suggested shutting down "Almegårds Kaserne", the local barracks, since "the island could quickly be protected by troops from surrounding areas and has no strategic importance after the fall of the Iron Curtain".
Sights and landmarks.
The island's varied geography and seascapes attract visitors to its many beauty spots from the Hammeren promontory in the northwest to the Almindingen forest in the centre and the Dueodde beaches in the southeast. Of special interest are the rocky sea cliffs at Jons Kapel and Helligdomsklipperne, the varied topography of Paradisbakkerne and rift valleys such as Ekkodalen and Døndalen. Bornholm's numerous windmills include the post mill of Egeby and the well-kept Dutch mill at Aarsdale. The lighthouse at Dueodde is Denmark's tallest, while Hammeren Lighthouse stands at a height of 85 m above sea level and Rønne Lighthouse rises over the waterfront.
The island hosts examples of 19th- and early-20th-century architecture, and about 300 wooden houses in Rønne and Nexø, donated by Sweden after World War II, when the island was repairing damage caused by the war.
The island is home to 15 medieval churches, four of which are round churches that display unique artwork and architecture. The ancient site of Rispebjerg has remains of sun temples from the Neolithic and earthworks from the Iron Age.
Famous people.
The Danish painter Oluf Høst was born in Svaneke in 1884.
The Danish writer and painter Gustaf Munch-Petersen moved to Bornholm in 1935 and married Lisbeth Hjorth while living on the island.
At age 8, socialist writer Martin Andersen Nexø moved to the island, and took his last name after the city of Nexø on its east coast.
M.P. Möller (1854–1937), a pipe-organ builder and manufacturer, was born in Østermarie before moving to the United States.
Gertrud Vasegaard (1913–2007), a ceramist remembered for her stoneware, was born in Rønne and established a workshop in Gudhjem.
Bornholm also attracted many famous artists at the beginning of the 20th century, forming a group now known as the Bornholm school of painters. In addition to Oluf Høst, they include Karl Isaksson (1878–1922), from Sweden, and the Danes Edvard Weie (1879-1943), Olaf Rude (1886–1957), Niels Lergaard (1893–1982), and Kræsten Iversen (1886–1955).
In the mid 1940s Donald (decedent of Ben Sayers) and Mavis Sayers arrived in Bornholm on a boat from Copenhagen. On their arrival they were greeted by a band and were paraded into the town centre. They were celebrated by the locals as the first English people to visit Bornholm since the end of the war and were treated like royalty during their stay. Following their stay they were frequently invited back as they were seen as a symbol of the restoration of Bornholm.
Electricity supply.
Bornholm is connected to the Swedish electricity grid by a submarine 60 kV AC cable, which is among the longest AC cables in Europe. This cable is capable of delivering all the electrical energy consumed on Bornholm. However Bornholm also generates its own electricity at small thermal power plants and especially wind turbines.
Bornholm is also home to a large internationally funded demonstration project to test the viability of novel energy market mechanisms to regulate energy networks with a high prevalence of renewables (such as wind turbines and photovoltaics). 50% of the project is EU-funded, with the remainder coming mainly from large corporations. See http://www.eu-ecogrid.net/ecogrid-eu for more.
Sports.
Bornholm's geography as an island and moderate climate makes Bornholm an ideal location for sailing and other water based sports.
Bornholm has also become an internationally recognised venue for 'match racing', a sailing sport where two identical (or one design) supplied racing yachts are raced in one on one dogfights on the water. The Danish Open event was held in Bornholm in September 2010 at the port town of Ronne on the Western coast of Bornholm. The racing yachts used for the Danish Open event are Danish designed DS37 racing yachts. These highly manoeuvrable and versatile boats are also used in the Match Cup Sweden event.
The five-day Danish Open is a key event in the World Match Racing Tour calendar which is one of only 3 events awarded 'special event' status by the International Sailing Federation. The Tour is the world's leading professional 'match racing' series and features a 9 event calendar which crosses 3 continents during the series.
Points accrued during the Danish Open contribute directly to the World Match Racing Tour championship with the winner of the season finale at the Monsoon Cup in Malaysia claiming the ultimate match racing title ISAF World Match Racing Champion.
Match racing unlike other sailing sports is suited to locations like Bornholm due to the racing taking place in close proximity to the shore which provides spectacular heat of the action viewing for the on-shore audience.
Religion.
Various Christian denominations have become established on the island, most during the 19th century.
References.
General.
</dl>

</doc>
<doc id="3777" url="http://en.wikipedia.org/wiki?curid=3777" title="Bay (disambiguation)">
Bay (disambiguation)

A bay is an area of water bordered by land on three sides.
Bay or baying may also refer to:

</doc>
<doc id="3778" url="http://en.wikipedia.org/wiki?curid=3778" title="Book">
Book

A book is a set of written, printed, illustrated, or blank sheets, made of ink, paper, parchment, or other materials, fastened together to hinge at one side. A single sheet within a book is a leaf, and each side of a leaf is a page. A set of text-filled or illustrated pages produced in electronic format is known as an electronic book, or e-book.
Books may also refer to works of literature, or a main division of such a work. In library and information science, a book is called a monograph, to distinguish it from serial periodicals such as magazines, journals or newspapers. The body of all written works including books is literature. In novels and sometimes other types of books (for example, biographies), a book may be divided into several large sections, also called books (Book 1, Book 2, Book 3, and so on). An avid reader of books is a bibliophile or colloquially, "bookworm".
A shop where books are bought and sold is a bookshop or bookstore. Books can also be borrowed from libraries. Google has estimated that as of 2010, approximately 130,000,000 unique titles had been published.
Etymology.
The word book comes from Old English "bōc" which (itself) comes from the Germanic root "*bōk-", cognate to beech. Similarly, in Slavic languages (for example, Russian, Bulgarian, Macedonian) "буква" (bukva—"letter") is cognate with "beech". In Russian and in Serbian and Macedonian, another Slavic languages, the words "букварь" (bukvar') and "буквар" (bukvar), respectively, refer specifically to a primary school textbook that helps young children master the techniques of reading and writing.
It is thus conjectured that the earliest Indo-European writings may have been carved on beech wood. Similarly, the Latin word "codex", meaning a book in the modern sense (bound and with separate leaves), originally meant "block of wood".
History of books.
Antiquity.
When writing systems were invented/created in ancient civilizations, nearly everything that could be written upon—stone, clay, tree bark, metal sheets—was used for writing.The study of such inscriptions forms a major part of history. The study of inscriptions is known as epigraphy. Alphabetic writing emerged in Egypt . The Ancient Egyptians would often write on papyrus, a plant grown along the Nile River. At first the words were not separated from each other ("scriptura continua") and there was no punctuation. Texts were written from right to left, left to right, and even so that alternate lines read in opposite directions. The technical term for this type of writing is 'boustrophedon,' which means literally 'ox-turning' for the way a farmer drives an ox to plough his fields.
Tablet.
A tablet might be defined as a physically robust writing medium, suitable for casual transport and writing. See also stylus.
Clay tablets were just what they sound like: flattened and mostly dry pieces of clay that could be easily carried, and impressed with a ( possible dampened) stylus. They were used as a writing medium, especially for writing in cuneiform, throughout the Bronze Age and well into the Iron Age.
Wax tablets were wooden planks covered in a thick enough coating of wax to record the impressions of a stylus. They were the normal writing material in schools, in accounting, and for taking notes. They had the advantage of being reusable: the wax could be melted, and reformed into a blank. The custom of binding several wax tablets together (Roman "pugillares") is a possible precursor for modern books (i.e. codex). The etymology of the word codex (block of wood) also suggests that it may have developed from wooden wax tablets.
Scroll.
Papyrus, a thick paper-like material made by weaving the stems of the papyrus plant, then pounding the woven sheet with a hammer-like tool, was used for writing in Ancient Egypt, perhaps as early as the First Dynasty, although the first evidence is from the account books of King Nefertiti Kakai of the Fifth Dynasty (about 2400 BC). Papyrus sheets were glued together to form a scroll. Tree bark such as lime and other materials were also used.
According to Herodotus (History 5:58), the Phoenicians brought writing and papyrus to Greece around the 10th or 9th century BC. The Greek word for papyrus as writing material ("biblion") and book ("biblos") come from the Phoenician port town Byblos, through which papyrus was exported to Greece. From Greek we also derive the word tome (Greek: τόμος), which originally meant a slice or piece and from there began to denote "a roll of papyrus". "Tomus" was used by the Latins with exactly the same meaning as "volumen" (see also below the explanation by Isidore of Seville).
Whether made from papyrus, parchment, or paper, scrolls were the dominant form of book in the Hellenistic, Roman, Chinese, Hebrew, and Macadonian cultures. The more modern codex book format form took over the Roman world by late antiquity, but the scroll format persisted much longer in Asia.
Codex.
In the 5th century, Isidore of Seville explained the then-current relation between codex, book and scroll in his Etymologiae (VI.13): "A codex is composed of many books; a book is of one scroll. It is called codex by way of metaphor from the trunks ("codex") of trees or vines, as if it were a wooden stock, because it contains in itself a multitude of books, as it were of branches." Modern usage differs.
A codex (in modern usage) is the first information repository that modern people would recognize as a "book": leaves of uniform size bound in some manner along one edge, and typically held between two covers made of some more robust material. The first written mention of the codex as a form of book is from Martial, in his Apophoreta at the end of the first century, where he praises its compactness. However, the codex never gained much popularity in the pagan Hellenistic world, and only within the Christian community did it gain widespread use. This change happened gradually during the 3rd and 4th centuries, and the reasons for adopting the codex form of the book are several: the format is more economical, as both sides of the writing material can be used; and it is portable, searchable, and easy to conceal. A book is much easier to read, to find a page that you want, and to flip through. A scroll is more awkward to use. The Christian authors may also have wanted to distinguish their writings from the pagan and Judaic texts written on scrolls. In addition, some metal books were made, that required smaller pages of metal, instead of an impossibly long, unbending scroll of metal. A book can also be easily stored in more compact places, or side by side in a tight library or shelf space.
Manuscripts.
The fall of the Roman Empire in the 5th century A.D. saw the decline of the culture of ancient Rome. Papyrus became difficult to obtain due to lack of contact with Egypt, and parchment, which had been used for centuries, became the main writing material.
Monasteries carried on the Latin writing tradition in the Western Roman Empire. Cassiodorus, in the monastery of Vivarium (established around 540), stressed the importance of copying texts. St. Benedict of Nursia, in his "Rule of Saint Benedict" (completed around the middle of the 6th century) later also promoted reading. The "Rule of Saint Benedict" (Ch. ), which set aside certain times for reading, greatly influenced the monastic culture of the Middle Ages and is one of the reasons why the clergy were the predominant readers of books. The tradition and style of the Roman Empire still dominated, but slowly the peculiar medieval book culture emerged.
Before the invention and adoption of the printing press, almost all books were copied by hand, which made books expensive and comparatively rare. Smaller monasteries usually had only a few dozen books, medium-sized perhaps a few hundred. By the 9th century, larger collections held around 500 volumes and even at the end of the Middle Ages, the papal library in Avignon and Paris library of the Sorbonne held only around 2,000 volumes.
The "scriptorium" of the monastery was usually located over the chapter house. Artificial light was forbidden for fear it may damage the manuscripts. There were five types of scribes:
The bookmaking process was long and laborious. The parchment had to be prepared, then the unbound pages were planned and ruled with a blunt tool or lead, after which the text was written by the scribe, who usually left blank areas for illustration and rubrication. Finally, the book was bound by the bookbinder.
Different types of ink were known in antiquity, usually prepared from soot and gum, and later also from gall nuts and iron vitriol. This gave writing a brownish black color, but black or brown were not the only colors used. There are texts written in red or even gold, and different colors were used for illumination. For very luxurious manuscripts the whole parchment was colored purple, and the text was written on it with gold or silver (for example, Codex Argenteus).
Irish monks introduced spacing between words in the 7th century. This facilitated reading, as these monks tended to be less familiar with Latin. However, the use of spaces between words did not become commonplace before the 12th century. It has been argued that the use of spacing between words shows the transition from semi-vocalized reading into silent reading.
The first books used parchment or vellum (calfskin) for the pages. The book covers were made of wood and covered with leather. Because dried parchment tends to assume the form it had before processing, the books were fitted with clasps or straps. During the later Middle Ages, when public libraries appeared, up to the 18th century, books were often chained to a bookshelf or a desk to prevent theft. These chained books are called "libri catenati".
At first, books were copied mostly in monasteries, one at a time. With the rise of universities in the 13th century, the Manuscript culture of the time led to an increase in the demand for books, and a new system for copying books appeared. The books were divided into unbound leaves ("pecia"), which were lent out to different copyists, so the speed of book production was considerably increased. The system was maintained by secular stationers guilds, which produced both religious and non-religious material.
Judaism has kept the art of the scribe alive up to the present. According to Jewish tradition, the Torah scroll placed in a synagogue must be written by hand on parchment and a printed book would not do, though the congregation may use printed prayer books and printed copies of the Scriptures are used for study outside the synagogue. A sofer "scribe" is a highly respected member of any observant Jewish community.
Middle East.
People of various religious (Jews, Christians, Zoroastrians, Muslims) and ethnic backgrounds (Syriac, Coptic, Persian, Arab etc.) in the Middle East also produced and bound books in the Islamic Golden Age (mid 8th century to 1258), developing advanced techniques in Islamic calligraphy, miniatures and bookbinding. A number of cities in the medieval Islamic world had book production centers and book markets. Yaqubi (d. 897) says that in his time Baghdad had over a hundred booksellers. Booksshops were often situated around the town's principal mosque as in Marrakesh, Morocco, that has a street named "Kutubiyyin" or book sellers in English and the famous Koutoubia Mosque is named so because of its location in this street.
The medieval Muslim world also used a method of reproducing reliable copies of a book in large quantities known as check reading, in contrast to the traditional method of a single scribe producing only a single copy of a single manuscript. In the check reading method, only "authors could authorize copies, and this was done in public sessions in which the copyist read the copy aloud in the presence of the author, who then certified it as accurate." With this check-reading system, "an author might produce a dozen or more copies from a single reading," and with two or more readings, "more than one hundred copies of a single book could easily be produced."
By using as writing material the relatively cheap paper instead of parchment or papyrus the Muslims, in the words of Pedersen "accomplished a feat of crucial significance not only to the history of the Islamic book, but also to the whole world of books"
Wood block printing.
In woodblock printing, a relief image of an entire page was carved into blocks of wood, inked, and used to print copies of that page. This method originated in China, in the Han dynasty (before 220 AD), as a method of printing on textiles and later paper, and was widely used throughout East Asia. The oldest dated book printed by this method is "The Diamond Sutra" (868 AD).
The method (called "Woodcut" when used in art) arrived in Europe in the early 14th century. Books (known as block-books), as well as playing-cards and religious pictures, began to be produced by this method. Creating an entire book was a painstaking process, requiring a hand-carved block for each page; and the wood blocks tended to crack, if stored for long. The monks or people who wrote them were paid highly.
Movable type and incunabula.
The Chinese inventor Bi Sheng made movable type of earthenware circa 1045, but there are no known surviving examples of his printing. Around 1450, in what is commonly regarded as an independent invention, Johannes Gutenberg invented movable type in Europe, along with innovations in casting the type based on a matrix and hand mould. This invention gradually made books less expensive to produce, and more widely available.
Early printed books, single sheets and images which were created before 1501 in Europe are known as incunables or "incunabula". "A man born in 1453, the year of the fall of Constantinople, could look back from his fiftieth year on a lifetime in which about eight million books had been printed, more perhaps than all the scribes of Europe had produced since Constantine founded his city in A.D. 330."
Modern world.
Steam-powered printing presses became popular in the early 19th century. These machines could print 1,100 sheets per hour, but workers could only set 2,000 letters per hour.
Monotype and linotype typesetting machines were introduced in the late 19th century. They could set more than 6,000 letters per hour and an entire line of type at once.
The centuries after the 15th century were thus spent on improving both the printing press and the conditions for freedom of the press through the gradual relaxation of restrictive censorship laws. See also intellectual property, public domain, copyright. In mid-20th century, European book production had risen to over 200,000 titles per year.
Book manufacture in modern times.
The methods used for the printing and binding of books continued fundamentally unchanged from the 15th century into the early 20th century. While there was more mechanization, a book printer in 1900 had much in common with Gutenberg.
Gutenberg's invention was the use of movable metal types, assembled into words, lines, and pages and then printed by letterpress to create multiple copies.
Modern paper books are printed on papers designed specifically for printed books. Traditionally, book papers are off-white or low-white papers (easier to read), are opaque to minimise the show-through of text from one side of the page to the other and are (usually) made to tighter caliper or thickness specifications, particularly for case-bound books. Different paper qualities are used depending on the type of book: Machine finished coated papers, woodfree uncoated papers, coated fine papers and special fine papers are common paper grades.
Today, the majority of books are printed by offset lithography. When a book is printed the pages are laid out on the plate so that after the printed sheet is folded the pages will be in the correct sequence. Books tend to be manufactured nowadays in a few standard sizes. The sizes of books are usually specified as "trim size": the size of the page after the sheet has been folded and trimmed. The standard sizes result from sheet sizes (therefore machine sizes) which became popular 200 or 300 years ago, and have come to dominate the industry. British conventions in this regard prevail throughout the English-speaking world, except for the USA. The European book manufacturing industry works to a completely different set of standards.
Current processes.
Some books, particularly those with shorter runs (i.e. fewer copies) will be printed on sheet-fed offset presses, but most books are now printed on web presses, which are fed by a continuous roll of paper, and can consequently print more copies in a shorter time. As the production line circulates, a complete "book" is collected together in one stack, next to another, and another.
A web press carries out the folding itself, delivering bundles of signatures (sections) ready to go into the gathering line. Notice that when the book is being printed it is being printed one (or two) signatures at a time, not one complete book at a time. Excess numbers are printed to make up for any spoilage due to "make-readies" or test pages to assure final print quality.
A make-ready is the preparatory work carried out by the pressmen to get the printing press up to the required quality of impression. Included in make-ready is the time taken to mount the plate onto the machine, clean up any mess from the previous job, and get the press up to speed. As soon as the pressman decides that the printing is correct, all the make-ready sheets will be discarded, and the press will start making books. Similar make readies take place in the folding and binding areas, each involving spoilage of paper.
After the signatures are folded and gathered, they move into the bindery. In the middle of last century there were still many trade binders – stand-alone binding companies which did no printing, specializing in binding alone. At that time, because of the dominance of letterpress printing, typesetting and printing took place in one location, and binding in a different factory. When type was all metal, a typical book's worth of type would be bulky, fragile and heavy. The less it was moved in this condition the better: so printing would be carried out in the same location as the typesetting. Printed sheets on the other hand could easily be moved. Now, because of increasing computerization of preparing a book for the printer, the typesetting part of the job has flowed upstream, where it is done either by separately contracting companies working for the publisher, by the publishers themselves, or even by the authors. Mergers in the book manufacturing industry mean that it is now unusual to find a bindery which is not also involved in book printing (and vice versa).
If the book is a hardback its path through the bindery will involve more points of activity than if it is a paperback.
Unsewn binding, is now increasingly common. The signatures of a book can also be held together by "Smyth sewing" using needles, "McCain sewing", using drilled holes often used in schoolbook binding, or "notch binding", where gashes about an inch long are made at intervals through the fold in the spine of each signature. The rest of the binding process is similar in all instances. Sewn and notch bound books can be bound as either hardbacks or paperbacks.
Finishing.
"Making cases" happens off-line and prior to the book's arrival at the binding line. In the most basic case-making, two pieces of cardboard are placed onto a glued piece of cloth with a space between them into which is glued a thinner board cut to the width of the spine of the book. The overlapping edges of the cloth (about 5/8" all round) are folded over the boards, and pressed down to adhere. After case-making the stack of cases will go to the foil stamping area for adding decorations and type.
Digital printing.
Recent developments in book manufacturing include the development of digital printing. Book pages are printed, in much the same way as an office copier works, using toner rather than ink. Each book is printed in one pass, not as separate signatures. Digital printing has permitted the manufacture of much smaller quantities than offset, in part because of the absence of make readies and of spoilage. One might think of a web press as printing quantities over 2000, quantities from 250 to 2000 being printed on sheet-fed presses, and digital presses doing quantities below 250. These numbers are of course only approximate and will vary from supplier to supplier, and from book to book depending on its characteristics. Digital printing has opened up the possibility of print-on-demand, where no books are printed until after an order is received from a customer.
E-book.
The term e-book is a contraction of "electronic book"; it refers to a book-length publication in digital form. An e-book is usually made available through the internet, but also on CD-ROM and other forms. E-Books may be read either via a computer or by means of a portable book display device known as an e-book reader, such as the Sony Reader, Barnes & Noble Nook, Kobo eReader, or the Amazon Kindle. These devices attempt to mimic the experience of reading a print book.
Information explosion.
Throughout the 20th century, libraries have faced an ever-increasing rate of publishing, sometimes called an information explosion. The advent of electronic publishing and the internet means that much new information is not printed in paper books, but is made available online through a digital library, on CD-ROM, or in the form of e-books. An on-line book is an e-book that is available online through the internet.
Though many books are produced digitally, most digital versions are not available to the public, and there is no decline in the rate of paper publishing. There is an effort, however, to convert books that are in the public domain into a digital medium for unlimited redistribution and infinite availability. This effort is spearheaded by Project Gutenberg combined with Distributed Proofreaders.
There have also been new developments in the process of publishing books. Technologies such as POD or "print on demand", which make it possible to print as few as one book at a time, have made self-publishing much easier and more affordable. On-demand publishing has allowed publishers, by avoiding the high costs of warehousing, to keep low-selling books in print rather than declaring them out of print.
Book design.
Book design is the art of incorporating the content, style, format, design, and sequence of the various components of a book into a coherent whole.
In the words of Jan Tschichold, book design "though largely forgotten today, methods and rules upon which it is impossible to improve have been developed over centuries. To produce perfect books these rules have to be brought back to life and applied." Richard Hendel describes book design as "an arcane subject" and refers to the need for a context to understand what that means.
Sizes.
The size of a modern book is based on the printing area of a common flatbed press. The pages of type were arranged and clamped in a frame, so that when printed on a sheet of paper the full size of the press, the pages would be right side up and in order when the sheet was folded, and the folded edges trimmed.
The most common book sizes are:
Sizes smaller than 16mo are:
Small books can be called booklets.
Sizes larger than quarto are:
The largest extant medieval manuscript in the world is Codex Gigas 92 × 50 × 22 cm. The world's largest book is made of stone and is in Kuthodaw Pagoda (Burma).
The longest book title in the world is 670 words long .
Types.
By content.
A common separation by content are fiction and non-fiction books. This simple separation can be found in most collections, libraries, and bookstores.
Fiction.
Many of the books published today are fiction, meaning that they are in-part or completely untrue. Historically, paper production was considered too expensive to be used for entertainment. An increase in global literacy and print technology led to the increased publication of books for the purpose of entertainment, and allegorical social commentary. Most fiction is additionally categorized by genre.
The novel is the most common form of fiction book. Novels are stories that typically feature a plot, setting, themes and characters. Stories and narrative are not restricted to any topic; a novel can be whimsical, serious or controversial. The novel has had a tremendous impact on entertainment and publishing markets. A novella is a term sometimes used for fiction prose typically between 17,500 and 40,000 words, and a novelette between 7,500 and 17,500. A Short story may be any length up to 10,000 words, but these word lengths vary.
Comic books or graphic novels are books in which the story is illustrated. The characters and narrators use speech or thought bubbles to express verbal language.
Non-fiction.
In a library, a reference book is a general type of non-fiction book which provides information as opposed to telling a story, essay, commentary, or otherwise supporting a point of view. An almanac is a very general reference book, usually one-volume, with lists of data and information on many topics. An encyclopedia is a book or set of books designed to have more in-depth articles on many topics. A book listing words, their etymology, meanings, and other information is called a dictionary. A book which is a collection of maps is an atlas. A more specific reference book with tables or lists of data and information about a certain topic, often intended for professional use, is often called a handbook. Books which try to list references and abstracts in a certain broad area may be called an index, such as "Engineering Index", or abstracts such as chemical abstracts and biological abstracts.
Books with technical information on how to do something or how to use some equipment are called instruction manuals. Other popular how-to books include cookbooks and home improvement books.
Students typically store and carry textbooks and schoolbooks for study purposes. Elementary school pupils often use workbooks, which are published with spaces or blanks to be filled by them for study or homework. In US higher education, it is common for a student to take an exam using a blue book.
There is a large set of books that are made only to write private ideas, notes, and accounts. These books are rarely published and are typically destroyed or remain private. Notebooks are blank papers to be written in by the user. Students and writers commonly use them for taking notes. Scientists and other researchers use lab notebooks to record their notes. They often feature spiral coil bindings at the edge so that pages may easily be torn out.
Address books, phone books, and calendar/appointment books are commonly used on a daily basis for recording appointments, meetings and personal contact information.
Books for recording periodic entries by the user, such as daily information about a journey, are called logbooks or simply logs. A similar book for writing the owner's daily private personal events, information, and ideas is called a diary or personal journal.
Businesses use accounting books such as journals and ledgers to record financial data in a practice called bookkeeping.
Other types.
There are several other types of books which are not commonly found under this system. Albums are books for holding a group of items belonging to a particular theme, such as a set of photographs, card collections, and memorabilia. One common example is stamp albums, which are used by many hobbyists to protect and organize their collections of postage stamps. Such albums are often made using removable plastic pages held inside in a ringed binder or other similar smolder.
Picture books are books for children with little text and pictures on every page.
Hymnals are books with collections of musical hymns that can typically be found in churches. Prayerbooks or missals are books that contain written prayers and are commonly carried by monks, nuns, and other devoted followers or clergy.
By physical format.
Hardcover books have a stiff binding. Paperback books have cheaper, flexible covers which tend to be less durable. An alternative to paperback is the glossy cover, otherwise known as a dust cover, found on magazines, and comic books. Spiral-bound books are bound by spirals made of metal or plastic. Examples of spiral-bound books include teachers' manuals and puzzle books (crosswords, sudoku).
Publishing is a process for producing pre-printed books, magazines, and newspapers for the reader/user to buy.
Publishers may produce low-cost, pre-publication copies known as galleys or 'bound proofs' for promotional purposes, such as generating reviews in advance of publication. Galleys are usually made as cheaply as possible, since they are not intended for sale.
Libraries.
Private or personal libraries made up of non-fiction and fiction books, (as opposed to the state or institutional records kept in archives) first appeared in classical Greece. In ancient world the maintaining of a library was usually (but not exclusively) the privilege of a wealthy individual. These libraries could have been either private or public, i.e. for people who were interested in using them. The difference from a modern public library lies in the fact that they were usually not funded from public sources. It is estimated that in the city of Rome at the end of the 3rd century there were around 30 public libraries. Public libraries also existed in other cities of the ancient Mediterranean region (for example, Library of Alexandria). Later, in the Middle Ages, monasteries and universities had also libraries that could be accessible to general public. Typically not the whole collection was available to public, the books could not be borrowed and often were chained to reading stands to prevent theft.
The beginning of modern public library begins around 15th century when individuals started to donate books to towns. The growth of a public library system in the United States started in the late 19th century and was much helped by donations from Andrew Carnegie. This reflected classes in a society: The poor or the middle class had to access most books through a public library or by other means while the rich could afford to have a private library built in their homes. In the United States the Boston Public Library 1852 "Report of the Trustees" established the justification for the public library as a tax-supported institution intended to extend educational opportunity and provide for general culture.
The advent of paperback books in the 20th century led to an explosion of popular publishing. Paperback books made owning books affordable for many people. Paperback books often included works from genres that had previously been published mostly in pulp magazines. As a result of the low cost of such books and the spread of bookstores filled with them (in addition to the creation of a smaller market of extremely cheap used paperbacks) owning a private library ceased to be a status symbol for the rich.
In library and booksellers' catalogues, it is common to include an abbreviation such as "Crown 8vo" to indicate the paper size from which the book is made.
When rows of books are lined on a book holder, bookends are sometimes needed to keep them from slanting.
Identification and classification.
During the 20th century, librarians were concerned about keeping track of the many books being added yearly to the Gutenberg Galaxy. Through a global society called the International Federation of Library Associations and Institutions (IFLA), they devised a series of tools including the International Standard Bibliographic Description (ISBD).
Each book is specified by an International Standard Book Number, or ISBN, which is unique to every edition of every book produced by participating publishers, world wide. It is managed by the ISBN Society. An ISBN has four parts: the first part is the country code, the second the publisher code, and the third the title code. The last part is a check digit, and can take values from 0–9 and X (10). The EAN Barcodes numbers for books are derived from the ISBN by prefixing 978, for Bookland, and calculating a new check digit.
Commercial publishers in industrialized countries generally assign ISBNs to their books, so buyers may presume that the ISBN is part of a total international system, with no exceptions. However, many government publishers, in industrial as well as developing countries, do not participate fully in the ISBN system, and publish books which do not have ISBNs.
A large or public collection requires a catalogue. Codes called "call numbers" relate the books to the catalogue, and determine their locations on the shelves. Call numbers are based on a Library classification system. The call number is placed on the spine of the book, normally a short distance before the bottom, and inside.
Institutional or national standards, such as ANSI/NISO Z39.41 - 1997, establish the correct way to place information (such as the title, or the name of the author) on book spines, and on "shelvable" book-like objects, such as containers for DVDs, video tapes and software.
One of the earliest and most widely known systems of cataloguing books is the Dewey Decimal System. Another widely known system is the Library of Congress Classification system. Both systems are biased towards subjects which were well represented in US libraries when they were developed, and hence have problems handling new subjects, such as computing, or subjects relating to other cultures.
Information about books and authors can be stored in databases like online general-interest book databases.
Metadata about a book may include its ISBN or other classification number (see above), the names of contributors (author, editor, illustrator) and publisher, its date and size, and the language of the text.
Uses.
Aside from the primary purpose of reading them, books are also used for other ends:
Paper and conservation.
Paper was first made in China as early as 200 BC, and reached Europe through Muslim territories. At first made of rags, the industrial revolution changed paper-making practices, allowing for paper to be made out of wood pulp. Papermaking in Europe began in the 11th century, although vellum was also common there as page material up until the beginning of 16th century, vellum being the more expensive and durable option. Printers or publishers would often issue the same publication on both materials, to cater to more than one market.
Paper made from wood pulp became popular in the early 20th century, because it was cheaper than linen or abaca cloth-based papers. Pulp-based paper made books less expensive to the general public. This paved the way for huge leaps in the rate of literacy in industrialised nations, and enabled the spread of information during the Second Industrial Revolution.
Pulp paper, however, contains acid which eventually destroys the paper from within. Earlier techniques for making paper used limestone rollers, which neutralized the acid in the pulp. Books printed between 1850 and 1950 are primarily at risk; more recent books are often printed on acid-free or alkaline paper. Libraries today have to consider mass deacidification of their older collections in order to prevent decay.
Stability of the climate is critical to the long-term preservation of paper and book material. Good air circulation is important to keep fluctuation in climate stable. The HVAC system should be up to date and functioning efficiently. Light is detrimental to collections. Therefore, care should be given to the collections by implementing light control. General housekeeping issues can be addressed, including pest control. In addition to these helpful solutions, a library must also make an effort to be prepared if a disaster occurs, one that they cannot control. Time and effort should be given to create a concise and effective disaster plan to counteract any damage incurred through "acts of god" therefore an emergency management plan should be in place.

</doc>
<doc id="3779" url="http://en.wikipedia.org/wiki?curid=3779" title="B52 (disambiguation)">
B52 (disambiguation)

B-52 is the common name of the Boeing B-52 Stratofortress, a strategic bomber aircraft designed and built by Boeing for the U.S. Air Force.
B-52 or B52 may also refer to:

</doc>
<doc id="3780" url="http://en.wikipedia.org/wiki?curid=3780" title="Bal Thackeray">
Bal Thackeray

Bal Keshav Thackeray (]; 23 January 1926 – 17 November 2012) was an Indian politician who founded the Shiv Sena, a right-wing Marathi ethnocentric party active mainly in the Western Indian state of Maharashtra. He was called as 'Balasaheb' by his supporters. His followers called him the "Hindu Hruday Samraat" ("Emperor of Hindu Hearts").
Thackeray began his professional career as a cartoonist with the English language daily "The Free Press Journal" in Mumbai, but left it in 1960 to form his own political weekly "Marmik". His political philosophy was largely shaped by his father Keshav Sitaram Thackeray, a leading figure in the "Samyukta Maharashtra" (United Maharashtra) movement, which advocated the creation of a separate linguistic state of Maharashtra. Through "Marmik", he campaigned against the growing influence of non-Marathis in Mumbai. In 1966, Thackeray formed the Shiv Sena party to advocate for the interests of Maharashtrians in Mumbai's political and professional landscape. In the late 1960s and early 1970s, Thackeray built the party by forming temporary alliances with nearly all of Maharashtra's political parties. Thackeray was also the founder of the Marathi-language newspaper "Saamana" and the Hindi-language newspaper "Dopahar ka saamana". He was the subject of numerous controversies. Upon his death, he was accorded a state funeral with a large number of mourners present.
Early and personal life.
Thackeray was born in the city of Pune on 23 January 1926 to Ramabai and Keshav Sitaram Thackeray (also known as 'Prabodhankar' Thackeray). He was the eldest of nine siblings. His family belonged to the Marathi Chandraseniya Kayastha Prabhu (CKP) community. Keshav Thackeray was a progressive social activist and writer involved with the Samyukta Maharashtra Chalwal (literally, "United Maharashtra Movement") in the 1950s, arguing for Maharashtra to become an independent Marathi-speaking state with Mumbai as its capital. His father was said to have supported the use of strategic violence and the reason his father left the movement was his stance against communists.
Thackeray was married to Meena Thackeray (née Vaidya) and had three sons, Bindumadhav Thackeray, Jaidev Thackeray and Uddhav Thackeray. Meena and Bindumadhav died in 1996.
Early career.
Thackeray began his career as a cartoonist in the "Free Press Journal" in Mumbai. His cartoons were also published in the Sunday edition of "The Times of India". In 1960, he launched the cartoon weekly "Marmik" with his brother. He used it to campaign against the growing numbers and influence of non-Marathi people in Mumbai, targeting Gujaratis and South Indians. After Thackeray's differences with the "Free Press Journal", he and four or five people, including George Fernandes, left the paper and started their own daily "News Day". The paper survived for one or two months.
Politics.
He formed the Shiv Sena on 19 June 1966 with the intent of fighting for the rights of the natives of the state of Maharashtra. The Party grew in power when in the early 1970s he was joined by senior leaders such as Marathi Literature Historian Babasaheb Purandare, Chief Attorney for Trade Union of Maharashtra Madhav Mehere as a party Attorney and experienced Trade Union Chartered Accountant Madhav Deshpande to back him up for various aspects of the party operations. The early objective of the Shiv Sena was to ensure job security for Maharashtrians competing against immigrants from southern India, Gujaratis and Marwaris. In 1989, the Sena's newspaper "Saamna" was launched.
Politically, the Shiv Sena was anti-communist, and wrested control of trade unions in Mumbai from the Communist Party of India. It later allied itself with the Bharatiya Janata Party (BJP). The BJP-Shiv Sena alliance won the 1995 Maharashtra State Assembly elections and came to power. During the tenure of the government from 1995 to 1999, Thackeray declared himself to the 'remote control' chief minister.
On 28 July 1999 Thackeray was banned from voting and contesting in any election for six years from 11 December 1999 till 10 December 2005 on the recommendations of the Election Commission for indulging in corrupt practice by seeking votes in the name of religion. After the six-year voting ban on Thackeray was lifted in 2005, he voted for the first time in the 2007 BMC elections.
Thackeray claimed that the Shiv Sena had helped the "Marathi manoos" (Marathi person) in Mumbai. Thackeray believed that Hindus must be organised to struggle against those who oppose their identity and religion. especially in the public sector.<ref name="Public sector, Threats against other communities, businesses owned especially by Guj and Marwaris/"> </ref> Opposition leftist parties allege that the Shiv Sena has done little to solve the problem of unemployment facing a large proportion of Maharashtrian youth during its tenure, in contradiction to its ideological foundation of 'sons of the soil.'
Factionalism.
In 2006, Thackeray's nephew Raj Thackeray broke away from Shiv Sena to form the Maharashtra Navnirman Sena (MNS). This was when Bal Thackeray on his retirement from active politics appointed his son, Uddhav rather than Raj as the leader of Shiv Sena. Raj continues to maintain that Thackeray was his ideologue.
Issues and actions.
On 14 February 2006, Thackeray condemned and apologised for the violent attacks by Shiv Sainiks upon a private Valentine's Day celebration in Mumbai. "It is said that women were beaten up in the Nallasopara incident. If that really happened, then it is a symbol of cowardice. I have always instructed Shiv Sainiks that in any situation women should not be humiliated and harassed." Thackeray and the Shiv Sena remained opposed to Valentine's Day celebrations, although they indicated support for an "Indian alternative." However, in some cases, the SS has been more tolerant during Valentine's Day celebrations.
Thackeray was criticised for his praise of Adolf Hitler. He was quoted by "Asiaweek" as saying: "I am a great admirer of Hitler, and I am not ashamed to say so! I do not say that I agree with all the methods he employed, but he was a wonderful organiser and orator, and I feel that he and I have several things in common...What India really needs is a dictator who will rule benevolently, but with an iron hand." However, "Indian Express" published an interview 29 January 2007: "Hitler did very cruel and ugly things. But he was an artist, I love him [for that]. He had the power to carry the whole nation, the mob with him. You have to think what magic he had. He was a miracle...The killing of Jews was wrong. But the good part about Hitler was that he was an artist. He was a daredevil. He had good qualities and bad. I may also have good qualities and bad ones."
He later told the "Star Talk" talk show on "Star Plus" that he did not admire Hitler.
Accusations of Xenophobia.
Thackeray and the Shiv Sena were blamed for inciting violence against Muslims during the 1992–1993 Mumbai riots in an inquiry ordered by the government of India - the Srikrishna Commission Report. Following the riots, Thackeray took stances viewed as anti-Muslim. In 2002, Thackeray issued a call to form Hindu suicide bomber squads to take on the menace of terrorism. In response, the Maharashtra government registered a case against him for inciting enmity between different groups. At least two organisations founded and managed by retired Indian Army officers, Lt Col Jayant Rao Chitale and Lt Gen. P.N. Hoon (former commander-in-chief of the Western Command), responded to the call with such statements as not allowing Pakistanis to work in India due to accusations against Pakistan for supporting attacks in India by militants.
Thackeray also declared that he was "not against every Muslim, but only those who reside in this country but do not obey the laws of the land...I consider such people [to be] traitors." The Shiv Sena is viewed by the liberal media as being anti-Muslim, though Shiv Sainiks officially reject this accusation. When explaining his views on Hindutva, he conflated Islam with violence and called on Hindus to "fight terrorism and fight Islam." In an interview with Suketu Mehta, he called for the mass expulsion of illegal Bangladeshi Muslim migrants from India and for a visa system to enter Mumbai, the Indian National Congress state government had earlier during the Indira Gandhi declared national emergency considered a similar measure.
He told "India Today" "[Muslims] are spreading like a cancer and should be operated on like a cancer. The...country should be saved from the Muslims and the police should support them [Hindu Maha Sangh] in their struggle just like the police in Punjab were sympathetic to the Khalistanis." However, in an interview in 1998, he said that his stance had changed on many issues that the Shiv Sena had with Muslims, particularly regarding the Babri Mosque or Ram Janmabhoomi issue: "We must look after the Muslims and treat them as part of us." He also expressed admiration for Muslims in Mumbai in the wake of the 11 July 2006 Mumbai train bombings perpetrated by Islamic fundamentalists. In response to threats made by Abu Azmi, a leader of the Samajwadi Party, that accusations of terrorism directed at Indian Muslims would bring about communal strife, Thackeray said that the unity of Mumbaikars (residents of Mumbai) in the wake of the attacks was "a slap to fanatics of Samajwadi Party leader Abu Asim Azmi" and that Thackeray "salute[s] those Muslims who participated in the two minutes' silence on July 18 to mourn the blast victims." Again in 2008 he wrote: "Islamic terrorism is growing and Hindu terrorism is the only way to counter it. We need suicide bomb squads to protect India and Hindus." He also reiterated a desire for Hindus to unite across linguistic barriers to see "a Hindustan for Hindus" and to "bring Islam in this country down to its knees."
In 2008, following agitation against Biharis and other north Indians travelling to Maharashtra to take civil service examinations for the Indian Railways due to an overlimit of the quota in their home provinces, Thackeray also said of Bihari MPs that they were "spitting in the same plate from which they ate" when they criticised Mumbaikars and Maharashtrians. He wrote: "They are trying to add fuel to the fire that has been extinguished, by saying that Mumbaikars have rotten brains." He also criticised Chhath Puja, a holiday celebrated by Biharis and those from eastern Uttar Pradesh, which occurs six days after the Hindu New Year. He said that it was not a real holiday. This was reportedly a response to MPs from Bihar who had disrupted the proceedings of the Lok Sabha in protest to the attacks on North Indians. Bihar Chief Minister Nitish Kumar, upset with the remarks, called on the prime minister and the central government to intervene in the matter. A "Saamna" editorial prompted at least 16 MPs from Bihar and Uttar Pradesh, belonging to the Rashtriya Janata Dal, Janata Dal (United), Samajwadi Party and the Indian National Congress, to give notice for breach of privilege proceedings against Thackeray. After the matter was raised in the Lok Sabha, Speaker Somnath Chatterjee said: "If anybody has made any comment on our members' functioning in the conduct of business in the House, not only do we treat that with the contempt that it deserves, but also any action that may be necessary will be taken according to procedure and well established norms. Nobody will be spared."
On 27 March 2008, in protest to Thackeray's editorial, leaders of Shiv Sena in Delhi resigned, citing its "outrageous conduct" towards non-Marathis in Maharashtra and announced that they would form a separate party. Addressing a press conference, Shiv Sena's North India chief Jai Bhagwan Goyal said the decision to leave the party was taken because of the "partial attitude" of the party high command towards Maharashtrians. Goyal further said "Shiv Sena is no different from Khalistan and Jammu and Kashmir militant groups which are trying to create a rift between people along regional lines. The main aim of these forces is to split our country. Like the Maharashtra Navnirman Sena, the Shiv Sena too has demeaned North Indians and treated them inhumanely."
The 2011 documentary "Jai Bhim Comrade" depicted a speech by Thackeray at a public rally, in which he articulated "genocidal sentiments" about Muslims, stating that they were the "species to be exterminated." The documentary followed this by showing several Dalit leaders criticizing Thackeray for his beliefs. Following Thackeray's death in 2012, the Pakistani newspaper "Express Tribune" published an editorial defining him as a "bigot" who instigated racism and violence against Muslims.
Death.
Thackeray died on 17 November 2012 as a consequence of a cardiac arrest. Mumbai came to a virtual halt immediately as the news broke out about his death, with shops and commercial establishments downing their shutters. The entire state of Maharashtra was put on high alert. The police appealed for calm and 20,000 Mumbai police officers, 15 units of the State Reserve Police Force and three contingents of the Rapid Action Force were deployed.
Prime Minister Manmohan Singh called for calm in the city and praised Thackeray's "strong leadership", while there were also statements of praise and condolences from other senior politicians such as the chief minister of Gujarat (Now Prime Minister of India), Narendra Modi, and the BJP leader and MP, L. K. Advani.
He was accorded a state funeral at Shivaji Park, which generated some controversy and resulted from demands made by Shiv Sena. It was the first public funeral in the city since that of Bal Gangadhar Tilak in 1920. Thackeray's body was moved to the park on 18 November. Many mourners attended his funeral, although there were no official figures. The range reported in media sources varied from around 1 million, to 1.5 million and as many as nearly 2 million. His cremation took place the next day, where his son Uddhav lit the pyre. Among those present at his cremation were senior representatives of the Maharashtra government and the event was broadcast live on national television channels. The Parliament of India opened for its winter session on 21 November 2012. Thackeray was the only non-member to be noted in its traditional list of obituaries. He is one of few people to have been recorded thus without being a member of either the Lok Sabha or the Rajya Sabha. Despite having not held any official position, he was given the 21-gun salute, which was again a rare honour. Both houses of Bihar Assembly also paid tribute.
The funeral expenses created further controversies when media reports claimed that the BMC had used taxpayers' money. In response to these reports, the party later sent a cheque of Rs. 500,000 to the Corporation.
Cultural references.
Thackeray is satirised in Salman Rushdie's 1995 novel "The Moor's Last Sigh" as 'Raman Fielding'. Suketu Mehta interviewed Thackeray in his critically acclaimed, Pulitzer-nominated, non-fiction 2004 book "Maximum City".

</doc>
<doc id="3784" url="http://en.wikipedia.org/wiki?curid=3784" title="BSE">
BSE

BSE may refer to:

</doc>
<doc id="3785" url="http://en.wikipedia.org/wiki?curid=3785" title="Bille August">
Bille August

Bille August (born 9 November 1948) is a Danish Academy Award-winning film and television director. His film "Pelle the Conqueror" from 1987 won the Palme d'Or, Academy Award and Golden Globe. He is one of only seven directors to win the Palme d'Or twice, winning the prestigious award again in 1992 for "The Best Intentions", based on the autobiographical script by Ingmar Bergman. He was married to Swedish actress Pernilla August from 1991 to 1997. His son Anders August is a screenwriter.
Career.
On 23 September 2011, Bille August announced that he had opened his studio in Hangzhou, China and taken a position as Tianpeng Media's Art Director, aiming to produce Chinese films for Tianpeng Media over the next few years. Tianpeng Media is a new media company established in 2010. The company so far has produced two films, "The Women Knight of Mirror" (竞雄女侠秋瑾) and "The Years of Qi Xiao Fu" (七小福之燃情岁月), which will be released later this year. Bille is the first foreign director to be hired by the Chinese film company. Recently, August also accepted the invitation from the Hangzhou government to serve as a "culture consultant" for the city.
His 2013 film "Night Train to Lisbon" premiered out of competition at the 63rd Berlin International Film Festival.

</doc>
<doc id="3788" url="http://en.wikipedia.org/wiki?curid=3788" title="Body">
Body

Body or BODY may refer to:

</doc>
<doc id="3789" url="http://en.wikipedia.org/wiki?curid=3789" title="Bitola">
Bitola

Bitola (Macedonian: Битола ] known also by several alternative names) is a city in the southwestern part of the Republic of Macedonia. The city is an administrative, cultural, industrial, commercial, and educational centre. It is located in the southern part of the Pelagonia valley, surrounded by the Baba, Nidže and Kajmakčalan mountain ranges, 14 km north of the Medžitlija-Níki border crossing with Greece. It is an important junction connecting the south of the Adriatic Sea with the Aegean Sea and Central Europe. It has been known since the Ottoman period as "the city of the consuls", since many European countries have consulates in Bitola. According to the 2002 census, Bitola is the second largest city in the country. Bitola is also the seat of the Bitola Municipality. Bitola is one of the oldest cities on the territory of the Republic of Macedonia. It was founded as Heraclea Lyncestis in the middle of the 4th century BC by Philip II of Macedon. During the Ottoman rule the city was the last capital of Ottoman Rumelia.
Etymology.
According to Adrian Room, the name "Bitola" is derived from the Old Church Slavonic word ѡ҆би́тѣл҄ь ("obitěĺь", meaning "monastery, cloister") as the city was formerly noted for its monastery. When the meaning of the name was no longer understood, it lost its prefix "o-". The name "Bitola" is mentioned in the Bitola inscription, related to the old city fortress built in 1015. Modern Slavic variants include the Macedonian "Bitola" (Битола), the Serbian "Bitolj" (Битољ) and Bulgarian "Bitolya" (Битоля). In Byzantine times, the name was Hellenized to "Voutélion" (Βουτέλιον) or "Vitólia" (Βιτώλια), hence the names "Butella" used by William of Tyre and "Butili" by the Arab geographer al-Idrisi. The Aromanian name is "Bituli".
The Greek name for the city ("Monastíri", Μοναστήρι), also meaning "monastery", is a calque of the Slavic name. The Turkish name "Manastır" (Ottoman Turkish: مناستر‎) is derived from the Greek name, like the Albanian name ("Manastir").
Geography.
Bitola is located in the southwestern part of Macedonia. The Dragor River flows through the city. Bitola lies at an elevation of 615 meters above sea level, at the foot of Baba Mountain. Its magnificent Pelister mountain (2601 m) is a national park with exquisite flora and fauna, among which is the rarest species of pine, known as Macedonian pine or pinus peuce, as well as a well-known ski resort.
Covering an area of 1798 km². and with a population of 122,173 (1991), Bitola is an important industrial, agricultural, commercial, educational, and cultural center. It represents an important junction that connects the Adriatic Sea to the south with the Aegean Sea and Central Europe.
Panorama of Bitola from Krkardaš.
History.
Prehistory.
Bitola is very rich in monuments from the prehistoric period. Two important ones are Veluška Tumba, and Bara Tumba near the village of Porodin. From the Copper Age there are the settlements of Tumba near the village of Crnobuki, Šuplevec near the village of Suvodol, and Visok Rid near the village of Bukri. The Bronze Age is represented by the settlements of Tumba near the village of Kanino and the settlement with the same name near the village of Karamani.
Ancient and early Byzantine periods.
The area of the town is located in ancient Lynkestis, a region of Upper Macedonia, which was ruled by semi-independent chieftains till the later Argead rulers of Macedon. The tribes of Lynkestis were known as "Lynkestai". They were a Greek tribe and belonged to the Molossian group of the Epirotes. There are important metal artifacts from the ancient period at the necropolis of Crkvishte near the village of Beranci. A golden earring dating from the 4th century BC is depicted on the obverse of the Macedonian 10 denar banknote, issued in 1996.
Heraclea Lyncestis (Greek: Ηράκλεια Λυγκηστίς - "City of Hercules upon the Land of the Lynx") was an important settlement from the Hellenistic period till the early Middle Ages. It was founded by Philip II of Macedon by the middle of the 4th century BC, and named after the Greek demigod Heracles, whom Philip considered his ancestor. With its strategic location, it became a prosperous city. The Romans conquered this part of Macedon in 148 BC and destroyed the political power of the city. However, its prosperity continued mainly due to the Roman Via Egnatia road which passed near the city. Several monuments from the Roman times remain in Heraclea, including a portico, thermae (baths), an amphitheater and a number of basilicas. The theatre was once capable of housing an audience of around 3,000 people.
In the early Byzantine period (4th to 6th centuries AD) Heraclea was an important episcopal centre. Some of its bishops were mentioned in the acts of the Church Councils, including Bishop Evagrius of Heraclea in the Acts of the Sardica Council of 343 AD. A small and a great (Large) basilica, the bishop's residence, and a funeral basilica near the necropolis are some of the remains of this period. Three naves in the Great Basilica are covered with mosaics of very rich floral and figurative iconography; these well preserved mosaics are often regarded as fine examples of the early Christian art period. During the 4th and 6th centuries, the names of other bishops from Heraclea were recorded. The city was sacked by Ostrogothic forces, commanded by Theodoric the Great in 472 and, despite a large gift to him from the city's bishop, it was sacked again in 479. It was restored in the late 5th and early 6th centuries. In the late 6th century the city suffered successive attacks by Slavic tribes and was gradually abandoned.
Middle Ages.
In the 6th and 7th centuries, the region around Bitola experienced a demographic shift as more and more Slavic tribes settled in the area. In place of the deserted theater, several houses were built during that time. The Slavs also built a fortress around their settlement. Bitola became a part of the First Bulgarian Empire from late in the 8th to early 11th centuries. The spread of Christianity was assisted by St. Clement of Ohrid and Naum of Preslav in the 9th and early 10th centuries. Many monasteries and churches were built in the city.
In the 10th century, Bitola was under the rule of the Bulgarian Tsar Samuil. He built a castle in the town, later used by his successor Gavril Radomir of Bulgaria. The town is mentioned in several medieval sources. John Skylitzes's 11th-century chronicle mentions that Emperor Basil II burned Gavril's castles in Bitola, when passing through and ravaging Pelagonia. The second chrysobull (1019) of Basil II mentioned that the Bishop of Bitola depended on the Archbishopric of Ohrid. During the reign of Samuil, the city was an important centre in the Bulgarian state and the seat of the Bitola Bishopric. In many medieval sources, especially Western, the name "Pelagonia" was synonymous with the Bitola Bishopric, and in some of them Bitola was known under the name of Heraclea due to the church tradition that turned the Heraclea Bishopric into the Pelagonian Metropolitan's Diocese. In 1015, Tsar Gavril Radomir was killed by his cousin Ivan Vladislav, who declared himself tsar and rebuilt the city fortress. To celebrate the occasion, a stone inscription written in the Cyrillic alphabet was set in the fortress; in it the Slavic name of the city is mentioned: Bitol.
Following battles with the Tsar Ivan Vladislav, Byzantine emperor Basil II recaptured Monastiri in 1015. The town is mentioned as an episcopal centre in 1019 in a record by Basil II. Two important uprisings against Byzantine rule took place in the Bitola area in 1040 and 1072. After the Bulgarian state was restored in the late 12th century, Bitola was incorporated under the rule of Tsar Kaloyan of Bulgaria. It was conquered again by Byzantium at the end of the 13th century, but it became part of Serbia in the first half of the 14th century, after the conquests of Stefan Dušan.
As a military, political and cultural center, Bitola played a very important role in the life of the medieval society in the region, prior to the Ottoman conquest in the mid-14th century. On the eve of the Ottoman conquest, Bitola (Monastir in Ottoman Turkish) experienced great growth with its well-established trading links all over the Balkan Peninsula, especially with big economic centers like Constantinople, Thessalonica, Ragusa and Tarnovo. Caravans of various goods moved to and from Bitola.
Ottoman rule.
From 1382 to 1912, Manastır (now Bitola) was part of the Ottoman Empire. Fierce battles took place near the city during the arrival of Ottoman forces. Ottoman rule was completely established after the death of Prince Marko in 1395 when Ottoman Empire established the Sanjak of Ohrid as a part of the Rumelia Eyalet and one of earliest established sanjaks in Europe. Before it became part of the Ottoman Empire in 1395 its initial territory belonged to the realm of Prince Marko. Initially its county town was Bitola and later it was Ohrid, so it was initially sometimes referred to as Sanjak of Monastir or Sanjak of Bitola.
For several centuries, Turks were a majority in this city, while the villages were populated mostly with Slavs. Evliya Çelebi says in his "Book of Travels" that the city had 70 mosques, several coffee-tea rooms, a bazaar (market) with iron gates and 900 shops. Manastır became a sanjak centre in the Rumelia Eyalet (Ottoman province).
After the Austro-Ottoman wars, the trade development and the overall thriving of the city was stifled. But in late 19th century, it again it became the second-biggest city in the wider southern Balkan region after Salonica. The city is also known as "city of consuls", because 12 diplomatic consuls resided here from 1878 to 1913.
In 1874, Manastır became the center of Monastir Vilayet which included the sanjaks of Debra, Serfidze, Elbasan, Manastır (Bitola), Görice and towns of Kırcaova, Pirlepe, Florina, Kesriye and Grevena.
Traditionally a strong trading center, Bitola is also known as "the city of the consuls". At one time during the Ottoman rule, Bitola had consulates from twelve countries. During the same period, there were a number of prestigious schools in the city, including a military academy that, among others, was attended by the famous Turkish reformer Kemal Atatürk. Bitola was also the headquarters of many cultural organizations that were established at that time.
There are opposing ethnographic data from that period, but it appears that no specific ethnic or religious group could claim an absolute majority of the population. According to the , Greeks were the largest Christian population in the vilayet, with 740,000 Greeks, 517,000 Bulgarians and 1,061,000 Muslims in the vilayets of Selanik (Thessaloniki) and Manastır. However, it should be noted that the basis of the Ottoman censuses was the millet system. People were assigned an ethnicity according to their religion. So all Sunni Muslims were categorised as Turks, although many of them were Albanians, and all members of the Greek Orthodox church as Greeks, although their numbers included a vast majority of Aromanians, South Albanians, and some Macedonian Slavs. The rest were divided between Bulgarian and Serb Orthodox churches.
Bitola’s population itself was very various. It numbered some 50,000 at the end of the 19th century. There were around 7,000 Aromanians most of whom fully embraced the Hellenist ideas, although many of them joined the Romanian idea. Bitola also had significant Muslim population - 11,000 (Turks, Roma, and Albanians) as well Jewish commununity - 5,200. The Slavic-speakers were divided between the Bulgarian Exarchate - 8,000, and the Greek Patriarchate - 6,300.
In 1894, Manastır was connected with Selanik by train. The first motion picture made in the Balkans was recorded by the Aromanian Manakis brothers in Manastır in 1903. In their honour, the annual Manaki Brothers International Film Camera Festival is held in modern Bitola.
In November 1905, the Secret Committee for the Liberation of Albania, a secret organization to fight for the liberation of Albania from the Ottoman Empire, was founded by Bajo Topulli and other Albanian nationalists and intellectuals. Three years later, the Congress of Manastir of 1908 which defined the modern Albanian alphabet was held in the city.
Ilinden Uprising.
The Bitola region was a stronghold of the Ilinden Uprising. The uprising was started as decided in 1903 in Thessaloniki by the Internal Macedonian Revolutionary Organization (IMRO). The uprising in the Bitola region was planned in Smilevo village in May 1903. The battles were fought in the villages of Bistrica, Rakovo, Buf, Skocivir, Paralovo, Brod, Novaci, Smilevo, Gjavato, Capari and others. Smilevo was defended by 600 rebels led by Dame Gruev and Georgi Sugarev, but when they were defeated, villages were burned.
Balkan wars.
In 1912, Montenegro, Serbia, Bulgaria and Greece fought the Ottomans in the First Balkan War. According to the Treaty of Bucharest, 1913, the region of Macedonia was divided in 3 parts among Greeks, Serbs and Bulgarians. Bitola was to be in Bulgaria, according to a pre-war alliance agreement between Bulgaria and Serbia. But the Serbian army entered the city in 19 November 1912 and refused to hand it to Bulgaria. From that moment, the city started to lose its importance and the population started rapidly decreasing, emigrating outside Macedonia and to the New World.
World War I.
During World War I Bitola was on the Thessaloniki front line. In 1915 Bulgarian forces took the city in 21 November 1915and the Serb forces were forced to either surrender or try a dangerous escape through the Albanian mountains. In 19 November 1916, Bitola was occupied by the Allied Powers, which entered the city from the South, fighting the Bulgarian army. Bitola was divided into French, Russian, Italian and Serbian regions, under the command of French general Maurice Sarrail. Until Bulgaria's surrender in late autumn 1918, Bitola remained a front line city and was almost every day bombarded by airplanes and battery and suffered almost total destruction.
World War II.
During the World War II (1941–1945), the Germans (in 9 April 1941) and later Bulgarians (in 18 April 1941) took control of the city. But in September 1944, Bulgaria switched sides in the war and withdrew from Yugoslavia, and Bitola was freed by Macedonian pro-Titoist Partisans. On 4 November, the 7th Macedonian Liberation Brigade entered Bitola victoriously. After the end of the war, a Macedonian state was established for the first time in modern history, within Yugoslavia. This had cost about 25,000 human lives. In 1945, the first Gymnasium (named "Josip Broz Tito") to use the Macedonian language, was opened in Bitola.
Jewish community.
After the Expulsion of 1492, Spanish-speaking Jews, harassed and persecuted by the Inquisition, were invited by Sultan Bayezid II to the Ottoman territories and arrived in waves from the Iberian peninsula (Spain and Portugal). A majority settled in Salonika, but a large community grew in Monastir and made up over ten percent of the city's population in 1900. The local Jewish population referred to themselves as "Monastirli", and a Monastirli synagogue exists to this day in modern Thessaloniki.
There was little evidence of anti-Semitism among other local communities. The Jews and the Aromanians were the only communities who did not make a national claim on Macedonian territory and were generally seen as neutral in these disputes.
Most Jews of Monastir were murdered during the Holocaust, and at present none remain in the city.
Many descendants of the Jewish community of Monastir made their way during the 20th century to the United States in Indianapolis, Indiana, and Rochester, New York, and to Santiago de Chile, Chile.
Main sights.
The city has many historical building dating from many historical periods. The most notable ones are from the Ottoman age, but there are some from the more recent past.
Širok Sokak.
Širok Sokak (Macedonian: Широк Сокак, meaning "Wide Alley") is a long pedestrian street that runs from Magnolia Square to the City Park.
Clock Tower.
It is unknown when Bitola's clock tower was built. Written sources from the 16th century mention a clock tower, but it is not clear if it is the same one. Some believe it was built at the same time as St. Dimitrija Church, in 1830. Legend says that the Ottoman authorities collected around 60,000 eggs from nearby villages and mixed them in the mortar to make the walls stronger.
The tower has a rectangular base and is about 30 meters high. Near the top is a rectangular terrace with an iron fence. On each side of the fence is an iron console construction which holds the lamps for lighting the clock. The clock is on the highest of three levels. The original clock was replaced during World War II with a working one, given by the Nazis because the city had maintained German graves from World War I.
The massive tower is composed of walls, massive spiral stairs, wooden mezzanine constructions, pendentives and the dome. During the construction of the tower, the façade was simultaneously decorated with simple stone plastic.
Church of Saint Demetrius.
The Church of Saint Demetrius was built in 1830 with voluntary contributions of local merchants and craftsmen. It is plain on the outside, as all churches in the Ottoman Empire had to be, but of rare beauty inside, lavishly decorated with chandeliers, a carved bishop throne and an engraved iconostasis. According to some theories, the iconostasis is a work of the Mijak engravers. Its most impressive feature is the arc above the imperial quarters with modeled figures of Jesus and the apostles.
Other engraved wood items include the bishop's throne made in the spirit of Mijak engravers, several icon frames and five more-recent pillars shaped like thrones. The frescoes originate from two periods: the end of the 19th century, and the end of World War I to the present. The icons and frescoes were created thanks to voluntary contributions of local businessmen and citizens. The authors of many of the icons had a vast knowledge of iconography schemes of the New Testament. The icons show a great sense of color, dominated by red, green and ochra shades. The abundance of golden ornaments is noticeable and points to the presence of late-Byzantine artwork and baroque style. The icon of Saint Demetrius is signed with the initials "D. A. Z.", showing that it was made by iconographer Dimitar Andonov the zograph in 1889. There are many other items, including the chalices made by local masters, a darohranilka of Russian origin, and several paintings of scenes from the New Testament, brought from Jerusalem by pilgrims.
The opening scenes of the film "The Peacemaker" were shot in the "St. Dimitrija" church in Bitola, as well as some "Welcome to Sarajevo" scenes.
Heraclea Lyncestis.
Heraclea Lyncestis (Macedonian: Хераклеа Линкестис) was an important ancient settlement from the Hellenistic period till the early Middle Ages. It was founded by Philip II of Macedon by the middle of the 4th century BC. Today, its ruins fall in the southern part of Bitola, 2 km from the city center.
The covered bazaar.
Situated near the city centre, the covered bazaar (Macedonian: Bezisten, Безистен) is one of the most impressive and oldest buildings in Bitola from the Turkish period. With its numerous cupolas that look like a fortress, with its tree-branch-like inner streets and four big metal doors it is one of the biggest covered markets in the region.
It was built in the 15th century by the Rumelian Beglerbey, the Grand Vizier and the famous donor Kara Daut Pasha Uzuncarsili. Although this object looks very secure, many times during its existence it was robbed and set on fire, but it managed to endure. The Bezisten, from the 15th to the 19th centuries, was rebuilt and many stores, often changing over time, were located there. Most of them were selling textile and other luxurious fabrics. In the same time the Bezisten was a treasury, where in specially made small rooms the money from the whole Rumelian Vilaet was kept, before it was transferred in the royal treasury. In the Bezisten in the 19th century there were a total of 84 stores. Today most of them are contemporary and they sell different types of products, but no matter what the internal transformations, the outer appearance stayed unchanged.
Ajdar-kadi mosque.
The Ajdar-kadi (Turkish judge) Mosque is one of the most attractive monuments of Islamic architecture in Bitola. It was built in the early 1560s, as the project of the famous architect Mimar Sinan, ordered by the Bitola kadija Ajdar-kadi. Over time, it was abandoned and heavily damaged, but recent restoration and conservation has restored to some extent its original appearance.
Jeni Mosque.
The Jeni Mosque is located in the center of the city. It has a square base, topped with a dome. Near the mosque is a minaret, 40 m high. Today, the mosque's rooms house permanent and temporary art exhibitions. Recent archaeological excavations have revealed that it has been built upon an old church.
Ishak Çelebi Mosque.
The Ishak Çelebi Mosque is the inheritance of the kadi Ishak Çelebi. In its spacious yard are several tombs, attractive because of the soft, molded shapes of the sarcophagi.
The old bazaar.
The old bazaar (Macedonian: "Стара Чаршија") is mentioned in a description of the city from the 16th and the 17th centuries. The present bezisten does not differ much in appearance from the original one. The bezisten had eighty-six shops and four large iron gates. The shops used to sell textiles, and today sell food products.
Deboj Bath.
The Deboj Bath is a Turkish bath (hamam). It is not known when exactly it was constructed. At one point, it was heavily damaged, but after repairs it regained its original appearance: a beautiful façade, two large domes and several minor ones.
Bitola today.
Bitola is the economic and industrial center of southwestern Macedonia. Many of the largest companies in the country are based in the city. The Pelagonia agricultural combine is the largest producer of food in the country. The Streževo water system is the largest in the Republic of Macedonia and has the best technological facilities. The three thermoelectric power stations of REK Bitola produce nearly 80% of electricity in the state. The Frinko refrigerate factory was a leading electrical and metal company. Bitola also has significant capacity in the textile and food industries.
Bitola is also home to twelve consulates, which gives the city the nickname "the city of consuls."
 (since 2007)
Also, Albania and Italy expressed interest in opening a consulate in Bitola.
Media.
There are two Bitola Television Stations: Tera and Orbis, two regional radio stations: the private Radio 105, Aktuel Bombarder and Radio Delfin as well as a local weekly newspaper — Bitolski Vesnik.
City Council.
The Bitola Municipality Council (Macedonian: Совет на Општина Битола) is the governing body of the city and municipality of Bitola. The city council approves and rejects projects that would have place inside the municipality given by its members and the Mayor of Bitola. The Council consists of representatives of citizens elected by direct and free elections. The number of members of the council shall be determined according to the number of residents in the community and can not be less than nine nor more than 33 members. Accordingly the number of residents, the Council of Bitola municipality is composed of 31 councilors. Council members are elected for a term of four years.
Council members represent the citizens and the council makes decisions on their own beliefs.
Member of the council may not be revoked.
Led by the Municipal Council president. President of the Council shall be elected from among the members of the council, for a term of four years.
Council president is the candidate who receives a majority of votes of the total number of council members.
Council of the Municipality of Bitola in office 2009 - 2013 consists of representatives of five political parties: VMRO-DPMNE - 17, SDSM - 9, VMRO-NP - 2, PODEM - 1 and Liberal Democratic Party of Macedonia - 2.
Examining matters within its competence, the Council set up committees. Council committees are formed as permanent and temporary.
Permanent committees of the Council:
Sports.
The most popular sports in Bitola are football and handball.
The main football team is FK Pelister and they play at the Tumbe Kafe Stadium which has a capacity of 8,000. Gjorgji Hristov, Dragan Kanatlarovski, Toni Micevski, Nikolče Noveski, Toni Savevski and Mitko Stojkovski are some of the famous Bitola natives to start their careers with the club.
The main handball club and most famous sports team from Bitola is RK Pelister. RK Bitola is the second club from the city and both teams play their games at the Sports Hall Mladost.
In the Macedonian Second League, FK Novaci are competing, which is located in the Region of Bitola.
All the sports teams under the name "Pelister" are supported by the fans known as Čkembari.
Demography.
According to the 1948 census Bitola had 30,761 inhabitants. 77.2% (or 23,734 inhabitants) were Macedonians, 11.5% (or 3,543 inhabitants) were Turks, 4.3% (or 1,327 inhabitants) were Albanians, 3% (or 912 inhabitants) were Serbs and 1.3% (or 402 inhabitants) were Aromanians. As of 2002, the city of Bitola has 74,550 inhabitants and the ethnic composition is the following:
According to the 2002 census the most common languages in the city are the following:
Bitola is a bishopric city and the seat of the Diocese of Prespa- Pelagonia. In World War II the diocese was named Ohrid - Bitola. With the restoration of the autocephaly of the Macedonian Orthodox Church in 1967, it got its present name Prespa- Pelagonia diocese which covers the following regions and cities: Bitola, Resen, Prilep, Krusevo and Demir Hisar.
The first bishop of the diocese (1958 - 1979) was Mr. Kliment. The second and current bishop and administrator of the diocese, responsible as bishop since 1981 is Mr. Petar. The Prespa- Pelagonia diocese has about 500 churches and monasteries. In the last ten years in the diocese have been built or are being built about 40 churches and 140 church buildings. The diocese has two church museums- the cathedral "St. Martyr Demetrius" in Bitola and at the Church "St. John" in Krusevo and permanent exhibition of icons and libraries in the building of the seat of the diocese. The seat building was built between 1901 and 1902 and is one of the most beautiful examples of the baroque architecture. Besides the dominant Macedonian Orthodox Church in Bitola there are other major religious groups such as the Islamic community, the Roman Catholic Church and others.
According to the 2002 census the religious composition of the city is the following:
Culture.
Is the oldest film festival in the world who value the work of cinematographers.In memories of the first cameramen on the Balkans, Milton Manaki, every September the Film and Photo festival "Brothers Manaki" takes place. It is a combination of documentary and full-length films that are being shown. The festival is a world class event and it is a must see.Every year the festival brings world recognized actors including Catherine Deneuve, Isabelle Huppert, Victoria Abril, Predrag Manojlovic, Michael York.
Every year, the traditional folk festival "Ilinden Days" takes place in Bitola. It is a 4-5 day festival of music, songs, and dances that is dedicated to the Ilinden Uprising against the Turks, where the main concentration is placed on the folk culture of Macedonia. Folk dances and songs are presented with many folklore groups and organizations taking part in it.
In the last few years, the Art manifestation "Small Monmartre of Bitola" that is organized by the art studio "Kiril and Metodij" has turned into a successful children's art festival. Children from all over the world come to express their imagination through art, creating important and priceless art that is presented in the country and around the world. "Small Monmartre of Bitola" is a winner of numerous awards and nominations.
Bitolino is an annual children's theater festival held in August with the Babec Theater. Every year professional children's theaters from all over the world participate in the festival. The main prize is the grand prix for best performance.
Every May, Bitola hosts the international children's song festival Si-Do, which in recent years has gained much popularity. Children from all over Europe participate in this event which usually consists of about 20 songs. This festival is supported by ProMedia which organizes the event with a new topic each year. Many Macedonian musicians have participated in the festival including: Next Time and Karolina Goceva who both represented Macedonia at the Eurovision Song Contest.
It is an international festival dedicated mainly to classical music where many creative and reproductive artist from all over the world take place. In addition to the classical music concerts, there are also few nights for pop-modern music, theater plays, art exhibitions, and a day for literature presentation during the event. In the last few years there have been artists from Russia, Slovakia, Poland,and many other countries.
For the reason of Bitola being called the city with most pianos, there is one night of the festival dedicated to piano competitions. One award is given for the best young piano player, and another for competitors under 30.
The Akto Festival for Contemporary Arts is a regional festival. The festival includes visual arts, performing arts, music and theory of culture. The first Akto festival was held in 2006. The aim of the festival is to open the cultural frameworks of a modern society through "recomposing" and redefining them in a new context. In the past, the festival featured artists from regional countries like Slovenia, Greece or Bulgaria, but also from Germany, Italy, France and Austria.
Is annual festival of monodrama held in April in organization of Centre of Culture of Bitola every year many actors from all over the world come in Bitola to play monodramas.
Is authentic cultural and tourist event which has existed since 2007. Founder and organizer of the festival is the Association of Citizens Center for Cultural Decontamination Bitola. The festival is held every year in mid-July in the heart of the old Turkish bazaar in Bitola, as part of Bitola Cultural Summer Bit Fest.
Education.
St. Clement of Ohrid University of Bitola (Macedonian: Универзитет Св. Климент Охридски — Битола) was founded in 1979, as a result of dispersed processes that occurred in education in the 1970s, and increasing demand of highly skilled professionals outside the country's capital. Since 1994, it has carried the name of the Slavic educator St. Clement of Ohrid. The university has institutes in Bitola, Ohrid, and Prilep, and headquarters in Bitola. With its additions in education and science, it has established itself, and cooperates with University of St. Cyril and Methodius from Skopje and other universities in the Balkans and Europe. The following institutes and scientific organizations are part of the university:
There are seven high schools in Bitola:
Ten Primary Schools in Bitola are:
People from Bitola.
Some notable people born in Bitola are:
Twin towns — Sister cities.
Bitola participates in town twinning to foster good international relations. Its current partners include:

</doc>
<doc id="3793" url="http://en.wikipedia.org/wiki?curid=3793" title="Battle of Bosworth Field">
Battle of Bosworth Field

The Battle of Bosworth (or Bosworth Field) was the last significant battle of the Wars of the Roses, the civil war between the Houses of Lancaster and York that raged across England in the latter half of the 15th century. Fought on 22 August 1485, the battle was won by the Lancastrians. Their leader Henry Tudor, Earl of Richmond, by his victory became the first English monarch of the Tudor dynasty. His opponent, Richard III, the last king of the House of York, was killed in the battle. Historians consider Bosworth Field to mark the end of the Plantagenet dynasty, making it a defining moment of English and Welsh history.
Richard's reign began in 1483 when he was handed the throne after his twelve-year-old nephew Edward V, for whom he was acting as Lord Protector, was declared illegitimate and ineligible for the throne. Richard lost popularity after the boy and his younger brother disappeared after Richard incarcerated them in the Tower of London, and Richard's support was further eroded by the popular belief he was implicated in the death of his wife. Across the English Channel in Brittany, Henry Tudor, a descendant of the greatly diminished House of Lancaster, seized on Richard's difficulties so that he could challenge Richard's claim to the throne. Henry's first attempt to invade England was frustrated by a storm in 1483, but at his second attempt he arrived unopposed on 7 August 1485 on the southwest coast of Wales. Marching inland, Henry gathered support as he made for London. Richard mustered his troops and intercepted Henry's army south of Market Bosworth in Leicestershire. Thomas, Lord Stanley, and Sir William Stanley brought a force to the battlefield, but held back while they decided which side it would be more advantageous to support.
Richard divided his army, which outnumbered Henry's, into three groups (or "battles"). One was assigned to the Duke of Norfolk and another to the Earl of Northumberland. Henry kept most of his force together and placed it under the command of the experienced Earl of Oxford. Richard's vanguard, commanded by Norfolk, attacked but struggled against Oxford's men, and some of Norfolk's troops fled the field. Northumberland took no action when signalled to assist his king, so Richard gambled everything on a charge across the battlefield to kill Henry and end the fight. Seeing the king's knights separated from his army, the Stanleys intervened; Sir William led his men to Henry's aid, surrounding and killing Richard. After the battle, Henry was crowned king below an oak tree in nearby Stoke Golding, now a residential garden.
Henry hired chroniclers to portray his reign favourably; the Battle of Bosworth was popularised to represent the Tudor dynasty as the start of a new age. From the 15th to 18th centuries the battle was glamorised as a victory of good over evil. The climax of William Shakespeare's play "Richard III" provides a focal point for critics in later film adaptations. The exact site of the battle is disputed because of the lack of conclusive data, and memorials have been erected at different locations. In 1974, the Bosworth Battlefield Heritage Centre was built on a site that has since been challenged by several scholars and historians. In October 2009, a team of researchers, who had performed geological surveys and archaeological digs in the area from 2003, suggested a location two miles (3.2 km) southwest of Ambion Hill.
Background.
During the 15th century, civil war raged across England as the Houses of York and Lancaster fought each other for the English throne. In 1471, the Yorkists defeated their rivals in the battles of Barnet and Tewkesbury. The Lancastrian King Henry VI and his only son, Edward of Lancaster, died in the aftermath of the Battle of Tewkesbury. Their deaths left the House of Lancaster with no direct claimants to the throne. The Yorkist king, Edward IV, was in complete control of England. He attainted those who refused to submit to his rule, such as Jasper Tudor and his nephew Henry, naming them traitors and confiscating their lands. The Tudors tried to flee to France but strong winds forced them to land in Brittany, then a semi-independent duchy, where they were taken into the custody of Duke Francis II. Henry's mother, Lady Margaret Beaufort, was a great-granddaughter of John of Gaunt, uncle of King Richard II and father of King Henry IV. The Beauforts were originally bastards, but Henry IV legitimised them on the condition that their descendants were not eligible to inherit the throne. Henry Tudor, the only remaining Lancastrian noble with a trace of the royal bloodline, had a weak claim to the throne, and Edward regarded him as "a nobody". The Duke of Brittany, however, viewed Henry as a valuable tool to bargain for England's aid in conflicts with France and kept the Tudors under his protection.
Edward IV died twelve years after Tewkesbury on 9 April 1483. His twelve-year-old elder son succeeded him as King Edward V; the younger son, nine-year-old Richard of Shrewsbury, was next in line to the throne. Edward V was too young to rule and a Royal Council was established to rule the country until the king's coming of age. The royal court was worried when they learned that the Woodvilles, relatives of Edward IV's widow Elizabeth, were plotting to seize control of the council. Having offended many in their quest for wealth and power, the Woodville family was not popular. To frustrate the Woodvilles' ambitions, Lord Hastings and other members of the council turned to the new king's uncle—Richard, Duke of Gloucester, brother of Edward IV. The courtiers urged Gloucester to assume the role of Protector quickly, as had been previously requested by his now dead brother. On 29 April, Gloucester, accompanied by a contingent of guards and Henry Stafford, 2nd Duke of Buckingham, took Edward V into custody and arrested several prominent members of the Woodville family. After bringing the young king to London, Gloucester had two of the Woodvilles (Anthony Woodville, 2nd Earl Rivers and Richard Grey) executed, without trial, on charges of treason.
On 13 June, Gloucester accused Hastings of plotting with the Woodvilles and had him beheaded. Nine days later, Gloucester convinced Parliament to declare the marriage between Edward IV and Elizabeth illegal, rendering their children illegitimate and disqualifying them from the throne. With his brother's children out of the way, he was next in the line of succession and was proclaimed King Richard III on 26 June. The timing and extrajudicial nature of the deeds done to obtain the throne for Richard won him no popularity, and rumours that spoke ill of the new king spread throughout England. After they were declared bastards, the two princes were confined in the Tower of London and never seen in public again.
Discontent with Richard's actions manifested itself in the summer after he took control of the country, as a conspiracy emerged to displace him from the throne. The rebels were mostly loyalists to Edward IV, who saw Richard as a usurper. Their plans were co-ordinated by a Lancastrian, Henry's mother Lady Margaret, who was promoting her son as a candidate for the throne. The highest-ranking conspirator was Buckingham. No chronicles tell of the duke's motive in joining the plot, although historian Charles Ross proposes that Buckingham was trying to distance himself from a king who was becoming increasingly unpopular with the people. Michael Jones and Malcolm Underwood suggest that Margaret deceived Buckingham into thinking the rebels supported him to be king.
The plan was to stage uprisings within a short time in southern and western England, overwhelming Richard's forces. Buckingham would support the rebels by invading from Wales, while Henry came in by sea. Bad timing and weather wrecked the plot. An uprising in Kent started 10 days prematurely, alerting Richard to muster the royal army and take steps to put down the insurrections. Richard's spies informed him of Buckingham's activities, and the king's men captured and destroyed the bridges across the River Severn. When Buckingham and his army reached the river, they found it swollen and impossible to cross because of a violent storm that broke on 15 October. Buckingham was trapped and had no safe place to retreat; his Welsh enemies seized his home castle after he had set forth with his army. The duke abandoned his plans and fled to Wem, where he was betrayed by his servant and arrested by Richard's men. On 2 November 1483, he was executed. Henry had attempted a landing on 10 October (or 19 October), but his fleet was scattered by a storm. He reached the coast of England (at either Plymouth or Poole), and a group of soldiers hailed him to come ashore. They were, in truth, Richard's men, prepared to capture Henry once he set foot on English soil. Henry was not deceived and returned to Brittany, abandoning the invasion. Without Buckingham or Henry, the rebellion was easily crushed by Richard.
The survivors of the failed uprisings fled to Brittany, where they openly supported Henry's claim to the throne. At Christmas, Henry Tudor swore an oath to marry Edward IV's daughter, Elizabeth of York, to unite the warring houses of York and Lancaster. Henry's rising prominence made him a great threat to Richard, and the Yorkist king made several overtures to the Duke of Brittany to surrender the young Lancastrian. Francis refused, holding out for the possibility of better terms from Richard. In mid-1484, Francis was incapacitated by illness and while recuperating, his treasurer, Pierre Landais, took over the reins of government. Landais reached an agreement with Richard to send back Henry and his uncle in exchange for military and financial aid. John Morton, a bishop of Flanders, learned of the scheme and warned the Tudors, who fled to France. The French court allowed them to stay; the Tudors were useful pawns to ensure that Richard's England did not interfere with French plans to annexe Brittany. On 16 March 1485, Richard's queen, Anne Neville, died, and rumours spread across the country that she was murdered to pave the way for Richard to marry his niece, Elizabeth. The gossip alienated Richard from some of his northern supporters, and upset Henry across the English Channel. The loss of Elizabeth's hand in marriage could unravel the alliance between Henry's supporters who were Lancastrians and those who were loyalists to Edward IV. Anxious to secure his bride, Henry assembled approximately 2,000 men and set sail from France on 1 August.
Factions.
By the 15th century, English chivalric ideas of selfless service to the king had been corrupted. Armed forces were mostly raised through musters in individual estates; every able-bodied man had to respond to their lord's call to arms, and each noble had exclusive authority over his militia. Although a king could raise personal militia from his lands, he could only muster a significantly large army through the support of his nobles. Richard, like his predecessors, had to win over these men by granting gifts and maintaining cordial relationships. Powerful nobles could demand greater incentives to remain on the liege's side or else they might turn against him. Three groups, each with its own agenda, stood on Bosworth Field: Richard III and his Yorkist army; his challenger, Henry Tudor, who championed the Lancastrian cause; and the fence-sitting Stanleys.
Yorkist.
Small and slender, Richard III did not have the robust physique associated with many of his Plantagenet predecessors. However, he enjoyed very rough sports and activities that were considered manly. His performances on the battlefield impressed his brother greatly, and he became Edward's right-hand man. During the 1480s, Richard defended the northern borders of England. In 1482, Edward charged him to lead an army into Scotland with the aim of replacing King James III with the Duke of Albany. Richard's army broke through the Scottish defences and occupied the capital, Edinburgh, but Albany decided to give up his claim to the throne in return for the post of Lieutenant General of Scotland. As well as obtaining a guarantee that the Scottish government would concede territories and diplomatic benefits to the English crown, Richard's campaign retook the town of Berwick-upon-Tweed, which the Scots had conquered in 1460. Edward was not satisfied by these gains, which, according to Ross, could have been greater if Richard had been resolute enough to capitalise on the situation while in control of Edinburgh. In her analysis of Richard's character, Christine Carpenter sees him as a soldier who was more used to taking orders than giving them. However, he was not averse to displaying his militaristic streak; on ascending the throne he made known his desire to lead a crusade against "not only the Turks, but all [his] foes".
Richard's most loyal subject was John Howard, 1st Duke of Norfolk. The duke served Richard's brother for many years and was one of Edward IV's closer confidantes. He was a military veteran, having fought in the Battle of Towton in 1461 and served as Hastings' deputy at Calais in 1471. Ross speculates that he may have borne a grudge against Edward for depriving him of a fortune. Norfolk was due to inherit a share of the wealthy Mowbray estate on the death of eight-year-old Anne de Mowbray, the last of her family. However, Edward convinced Parliament to circumvent the law of inheritance and transfer the estate to his younger son, who was married to Anne. Consequently, Howard supported Richard III in deposing Edward's sons, for which he received the dukedom of Norfolk and his original share of the Mowbray estate.
Henry Percy, 4th Earl of Northumberland, also supported Richard's seizure of the throne of England. The Percys were loyal Lancastrians, but Edward IV eventually won the earl's allegiance. Northumberland had been captured and imprisoned by the Yorkists in 1461, losing his titles and estates; however, Edward released him eight years later and restored his earldom. From that time Northumberland served the Yorkist crown, helping to defend northern England and maintain its peace. Initially the earl had issues with Richard III as Edward groomed his brother to be the leading power of the north. Northumberland was mollified when he was promised he would be the Warden of the East March, a position that was formerly hereditary for the Percys. He served under Richard during the 1482 invasion of Scotland, and the allure of being in a position to dominate the north of England if Richard went south to assume the crown was his likely motivation for supporting Richard's bid for kingship. However, after becoming king, Richard began moulding his nephew, John de la Pole, 1st Earl of Lincoln, to manage the north, passing over Northumberland for the position. According to Carpenter, although the earl was amply compensated he despaired of any possibility of advancement under Richard.
Lancastrian.
Henry Tudor was unfamiliar with the arts of war and a stranger to the land he was trying to conquer. He spent the first fourteen years of his life in Wales and the next fourteen in Brittany and France. Slender but strong and decisive, Henry lacked a penchant for battle and was not much of a warrior; chroniclers such as Polydore Vergil and ambassadors like Pedro de Ayala found him more interested in commerce and finance. Having not fought in any battles, Henry recruited several experienced veterans on whom he could rely for military advice and the command of his armies. 
John de Vere, 13th Earl of Oxford, was Henry's principal military commander. He was adept in the arts of war. At the Battle of Barnet, he commanded the Lancastrian right wing and routed the division opposing him. However, as a result of confusion over identities, Oxford's group came under friendly fire from the Lancastrian main force and retreated from the field. The earl fled abroad and continued his fight against the Yorkists, raiding shipping and eventually capturing the island fort of St Michael's Mount in 1473. He surrendered after receiving no aid or reinforcement, but in 1484 escaped from prison and joined Henry's court in France, bringing along his erstwhile gaoler Sir James Blount. Oxford's presence raised morale in Henry's camp and troubled Richard III.
Stanleys.
In the early stages of the Wars of the Roses the Stanleys of Cheshire were predominantly Lancastrians. Sir William Stanley, however, was a staunch Yorkist supporter, fighting in the Battle of Blore Heath in 1459 and helping Hastings to put down uprisings against Edward IV in 1471. When Richard took the crown, Sir William showed no inclination to turn against the new king, refraining from joining Buckingham's rebellion, for which he was amply rewarded. Sir William's elder brother, Thomas Stanley, 2nd Baron Stanley, was not as steadfast. By 1485, he had served three kings, namely Henry VI, Edward IV, and Richard III. Lord Stanley's skilled political manoeuvrings—vacillating between opposing sides until it was clear who would be the winner—gained him high positions; he was Henry's chamberlain and Edward's steward. His non-committal stance, until the crucial point of a battle, earned him the loyalty of his men, who felt he would not needlessly send them to their deaths.
Even though Lord Stanley had served as Edward IV's steward, his relations with the king's brother, the eventual Richard III, were not cordial. The two had conflicts with each other that erupted into violence around March 1470. Furthermore, having taken Lady Margaret as his second wife in June 1472, Stanley was Henry Tudor's stepfather, a relationship which did nothing to win him Richard's favour. Despite these differences, Stanley did not join Buckingham's revolt in 1483. When Richard executed those conspirators who were unable to flee England, he spared Lady Margaret. However, he declared her titles forfeit, and transferred her estates to Stanley's name, to be held in trust for the Yorkist crown. Richard's act of mercy was calculated to reconcile him with Stanley, but it may have been to no avail—Carpenter has identified a further cause of friction in Richard's intention to reopen an old land dispute that involved Thomas Stanley and the Harrington family. Edward IV had ruled the case in favour of Stanley in 1473, but Richard planned to overturn his brother's ruling and give the wealthy estate to the Harringtons. Immediately before the Battle of Bosworth, being wary of Stanley, Richard took his son, Lord Strange, as hostage to discourage him from joining Henry.
Prelude.
Henry's crossing of the English Channel in 1485 was without incident. He sailed from Harfleur on 1 August and, with fair winds, landed in his native Wales, at Mill Bay (near Dale) on the north side of Milford Haven on 7 August, easily capturing nearby Dale Castle. Although hailed by contemporary Welsh bards as the native prince to bring their country back to glory, Henry received a muted response from the local population. No joyous welcome awaited him on shore, and few individual Welshmen joined his army as it marched inland. Historian Geoffrey Elton suggests only Henry's ardent supporters felt pride over his Welsh blood. It was a different story when Henry moved to Haverfordwest, the county town of Pembrokeshire. Richard's lieutenant in South Wales, Sir Walter Herbert, failed to move against Henry, and two of his officers, Richard Griffith and Evan Morgan, deserted to Henry with their men. Another prominent local figure, Rhys Fawr ap Maredudd, also joined Henry.
However, the most important defector to Henry in this early stage of the campaign was probably Rhys ap Thomas, who was the leading figure in West Wales. Richard had appointed Rhys Lieutenant in West Wales for his refusal to join Buckingham's rebellion, asking that he surrender his son Gruffydd ap Rhys ap Thomas as surety, although by some accounts Rhys had managed to evade this condition. However, Henry successfully courted Rhys, offering the lieutenancy of all Wales in exchange for his fealty. Henry marched via Aberystwyth while Rhys followed a more southerly route, recruiting 500 Welshmen "en route" to swell Henry's army when they reunited at Welshpool., thus ensuring that the majority of Henry's army in the ensuing battle would be Welsh. By 15 or 16 August, Henry and his men had crossed the English border, making for the town of Shrewsbury.
Since 22 June 1485 Richard had been aware of Henry's impending invasion, and had ordered his lords to maintain a high level of readiness. News of Henry's landing reached Richard on 11 August, but it took three to four days for his messengers to notify his lords of their king's mobilisation. On 16 August, the Yorkist army started to gather; Norfolk set off for Leicester, the assembly point, that night. The city of York, a traditional stronghold of Richard's family, asked the king for instructions, and receiving a reply three days later sent 80 men to join the king. Simultaneously Northumberland, whose northern territory was the most distant from the capital, had gathered his men and ridden to Leicester.
Although London was his goal, Henry did not move directly towards the city. After resting in Shrewsbury, his forces went eastwards and picked up Sir Gilbert Talbot and other English allies, including deserters from Richard's forces. Although its size had increased substantially since the landing, Henry's army was not yet large enough to contend with the numbers Richard could muster. Henry's pace through Staffordshire was slow, delaying the confrontation with Richard so that he could gather more recruits to his cause. Henry had been communicating on friendly terms with the Stanleys for some time before setting foot in England, and the Stanleys had mobilised their forces on hearing of Henry's landing. They ranged themselves ahead of Henry's march through the English countryside, meeting twice in secret with Henry as he moved through Staffordshire. At the second of these, at Atherstone in Warwickshire, they conferred "in what sort to arraign battle with King Richard, whom they heard to be not far off". On 21 August, the Stanleys were making camp on the slopes of a hill north of Dadlington, while Henry encamped his army at White Moors to the northwest of their camp.
On 20 August, Richard reached Leicester, joining Norfolk. Northumberland arrived the following day. The royal army proceeded westwards to intercept Henry's march on London. Passing Sutton Cheney, Richard moved his army towards Ambion Hill—which he thought would be of tactical value—and made camp on it. Richard's sleep was not peaceful and, according to the "Croyland Chronicle", in the morning his face was "more livid and ghastly than usual".
Engagement.
The Yorkist army, numbering about 10,000 men, deployed on the hilltop along the ridgeline from west to east. Norfolk's group (or "battle" in the parlance of the time) of spearmen stood on the right flank, protecting the cannon and about 1,200 archers. Richard's group, comprising 3,000 infantry, formed the centre. Northumberland's men guarded the left flank; he had approximately 4,000 men, many of them mounted. Standing on the hilltop, Richard had a wide, unobstructed view of the area. He could see the Stanleys and their 6,000 men holding their position at Dadlington Hill, while to the southwest was Henry's army.
Henry had very few Englishmen—fewer than a thousand—in his army. Between three and five hundred of them were exiles who had fled from Richard's rule, and the remainder were Talbot's men and recent deserters from Richard's army. Historian John Mackie believes that 1,800 French mercenaries, led by Philibert de Chandée, formed the core of Henry's army. John Mair, writing thirty-five years after the battle, claimed that this force contained a significant Scottish component, and this claim is accepted by some modern writers, but Mackie reasons that the French would not have released their elite Scottish knights and archers, and concludes that there were probably few Scottish troops in the army, although he accepts the presence of captains like Bernard Stewart, Lord of Aubigny. In total, Henry's army was around 5,000 strong, a substantial portion of which was made up by the recruits picked up in Wales. Rhys ap Thomas's Welsh force was described as being large enough to have "annihilated" the rest of Henry's force.
In their interpretations of the vague mentions of the battle in the old text, historians placed areas near the foot of Ambion Hill as likely regions where the two armies clashed, and thought up possible scenarios of the engagement. In their recreations of the battle, Henry started by moving his army towards Ambion Hill where Richard and his men stood. As Henry's army advanced past the marsh at the southwestern foot of the hill, Richard sent a message to Stanley, threatening to execute his son, Lord Strange, if Stanley did not join the attack on Henry immediately. Stanley replied that he had other sons. Incensed, Richard gave the order to behead Strange but his officers temporised, saying that battle was imminent, and it would be more convenient to carry out the execution afterwards. Henry had also sent messengers to Stanley asking him to declare his allegiance. The reply was evasive—the Stanleys would "naturally" come, after Henry had given orders to his army and arranged them for battle. Henry had no choice but to confront Richard's forces alone.
Well aware of his own military inexperience, Henry handed command of his army to Oxford and retired to the rear with his bodyguards. Oxford, seeing the vast line of Richard's army strung along the ridgeline, decided to keep his men together instead of splitting them into the traditional three battles: vanguard, centre, and rearguard. He ordered the troops to stray no further than 10 ft from their banners, fearing that they would become enveloped. Individual groups clumped together, forming a single large mass flanked by horsemen on the wings.
The Lancastrians were harassed by Richard's cannon as they manoeuvred around the marsh, seeking firmer ground. Once Oxford and his men were clear of the marsh, Norfolk's battle and several contingents of Richard's group started to advance. Hails of arrows showered both sides as they closed. Oxford's men proved the steadier in the ensuing hand-to-hand combat; they held their ground and several of Norfolk's men fled the field. Recognising that his force was at a disadvantage, Richard signalled for Northumberland to assist but Northumberland's group showed no signs of movement. Historians, such as Horrox and Pugh, believe Northumberland chose not to aid his king for personal reasons. Ross doubts the aspersions cast on Northumberland's loyalty, suggesting instead that Ambion Hill's narrow ridge hindered him from joining the battle. The earl would have had to either go through his allies or execute a wide flanking move—near impossible to perform given the standard of drill at the time—to engage Oxford's men.
At this juncture Henry rode off towards the Stanleys. Seeing this, Richard decided to end the fight quickly by killing the enemy commander. He led a charge of mounted men around the melee and tore into Henry's group; several accounts state that Richard's force numbered 800–1000 knights, but Ross says it was more likely that Richard was accompanied only by his household men and closest friends. Richard killed Henry's standard-bearer Sir William Brandon in the initial charge and unhorsed burly John Cheyne, Edward IV's former standard-bearer, with a blow to the head from his broken lance. Henry's bodyguards surrounded their master and succeeded in keeping him away from the Yorkist king. On seeing Richard embroiled with Henry's men and separated from his main force, William Stanley made his move. He led his men into the fight at Henry's side. Outnumbered, Richard's group was surrounded and gradually pressed back against the marsh. Richard's banner man—Sir Percival Thirwell—lost his legs but held the Yorkist banner aloft until he was hacked to death.
Polydore Vergil, Henry Tudor's official historian, recorded that "King Richard, alone, was killed fighting manfully in the thickest press of his enemies". Richard had come within a sword's length of Henry Tudor before being surrounded by Sir William Stanley's men and killed. The Burgundian chronicler Jean Molinet says that a Welshman struck the death-blow with a halberd while Richard's horse was stuck in the marshy ground. It was said that the blows were so violent that the king's helmet was driven into his skull. The contemporary Welsh poet Guto'r Glyn implies the leading Welsh Lancastrian Rhys ap Thomas, or one of his men, killed the king, writing that he "killed the boar, shaved his head". The identification in 2013 of King Richard's body shows that the skeleton had 10 wounds, eight of them to the head, clearly inflicted in battle and suggesting he had lost his helmet. The skull showed that a blade had hacked away part of the rear of the skull.
Richard's forces disintegrated as news of his death spread. Northumberland and his men fled north on seeing the king's fate, and Norfolk was killed.
Post-battle.
After the battle, Richard's circlet was found and brought to Henry, who was crowned king at the top of Crown Hill, near the village of Stoke Golding. According to Vergil, Henry's official historian, Lord Stanley found the circlet. Historian Stanley Chrimes and Professor Sydney Anglo dismiss the legend of the crown's finding in a hawthorn bush; none of the contemporary sources reported such an event. Ross, however, does not ignore the legend. He opines that the hawthorn bush would not be part of Henry's coat of arms if it did not have a strong relationship to his ascendance. In Vergil's chronicle, 100 of Henry's men, compared to 1,000 of Richard's, died in this battle—a ratio Chrimes believes to be an exaggeration. The bodies of the fallen were brought to St James Church at Dadlington for burial. However, Henry denied any immediate rest for Richard; instead the last Yorkist king's corpse was stripped naked and strapped across a horse. His body was brought to Leicester and openly exhibited in a church, possibly in the collegiate foundation of the Annunciation of Our Lady, to show his death. After two days, the corpse was interred in a plain unmarked tomb, within the church of the Greyfriars. The location of Richard's tomb was long uncertain, as the church was demolished following its dissolution in 1538.
On 12 September 2012 archaeologists announced the discovery of a battle-damaged skeleton suspected to be Richard's in the remains of his burial church in Leicester. On 4 February 2013, it was announced that DNA testing had conclusively identified ("beyond reasonable doubt") the remains as those of Richard. On Thursday 26th March 2015, these remains were ceremonially buried in Leicester Cathedral. On the following day the new royal tomb of Richard III was unveiled.
Henry dismissed the mercenaries in his force, retaining only a small core of local soldiers to form the "Yeomen of his Garde", and proceeded to establish his rule of England. Parliament reversed his attainder and recorded Richard's kingship as illegal, although the Yorkist king's reign remained officially in the annals of England history. The proclamation of Edward IV's children as illegitimate was also reversed, restoring Elizabeth's status to a royal princess. The marriage of Elizabeth, the heiress to the House of York, to Henry, the master of the House of Lancaster, marked the end of the feud between the two houses and the start of the Tudor dynasty. The royal matrimony, however, was delayed until Henry was crowned king and had established his claim on the throne firmly enough to preclude that of Elizabeth and her kin. Henry further convinced Parliament to backdate his reign to the day before the battle, retrospectively enabling those who fought against him at Bosworth Field to be declared traitors. Northumberland, who had remained inactive during the battle, was imprisoned but later released and reinstated to pacify the north in Henry's name. The purge of those who fought for Richard occupied Henry's first two years of rule, although later he proved prepared to accept those who submitted to him regardless of their former allegiances.
Of his supporters, Henry rewarded the Stanleys the most generously. Aside from making William his chamberlain, he bestowed the earldom of Derby upon Lord Stanley along with grants and offices in other estates. Henry rewarded Oxford by restoring to him the lands and titles confiscated by the Yorkists and appointing him as Constable of the Tower and admiral of England, Ireland, and Aquitaine. For his kin, Henry created Jasper Tudor the Duke of Bedford. He returned to his mother the lands and grants stripped from her by Richard, and proved to be a filial son, granting her a place of honour in the palace and faithfully attending to her throughout his reign. Parliament's declaration of Margaret as "femme sole" effectively empowered her; she no longer needed to manage her estates through Stanley. Elton points out that despite his initial largesse, Henry's supporters at Bosworth would only enjoy his special favour for the short term; in later years, he would instead promote those who best served his interests.
Like the kings before him, Henry faced dissenters. The first open revolt occurred two years after Bosworth Field; Lambert Simnel claimed to be Edward Plantagenet, 17th Earl of Warwick, who was Edward IV's nephew. The Earl of Lincoln backed him for the throne and led rebel forces in the name of the House of York. The rebel army fended off several attacks by Northumberland's forces, before engaging Henry's army at the Battle of Stoke Field on 16 June 1487. Oxford and Bedford led Henry's men, including several former supporters of Richard III. Henry won this battle easily, but other malcontents and conspiracies would follow. A rebellion in 1489 started with Northumberland's murder; military historian Michael C. C. Adams says that the author of a note, which was left next to Northumberland's body, blamed the earl for Richard's death.
Legacy and historical significance.
Contemporary accounts of the Battle of Bosworth can be found in four main sources, one of which is the English "Croyland Chronicle", written by a senior Yorkist chronicler who relied on second-hand information from nobles and soldiers. The other accounts were written by foreigners—Vergil, Jean Molinet, and Diego de Valera. Whereas Molinet was sympathetic to Richard, Vergil was in Henry's service and drew information from the king and his subjects to portray them in a good light. Diego de Valera, whose information Ross regards as unreliable, compiled his work from letters of Spanish merchants. However, other historians have used Valera's work to deduce possibly valuable insights not readily evident in other sources. Ross finds the poem, "The Ballad of Bosworth Field", a useful source to ascertain certain details of the battle. The multitude of different accounts, mostly based on second- or third-hand information, has proved an obstacle to historians as they try to reconstruct the battle. Their common complaint is that, except for its outcome, very few details of the battle are found in the chronicles. According to historian Michael Hicks, the Battle of Bosworth is one of the worst-recorded clashes of the Wars of the Roses.
Historical depictions and interpretations.
Henry tried to present his victory as a new beginning for the country; he hired chroniclers to portray his reign as a "modern age" with its dawn in 1485. Hicks states that the works of Vergil and the blind historian Bernard André, promoted by subsequent Tudor administrations, became the authoritative sources for writers for the next four hundred years. As such, Tudor literature paints a flattering picture of Henry's reign, depicting the Battle of Bosworth as the final clash of the civil war and downplaying the subsequent uprisings. For England the Middle Ages ended in 1485, and English Heritage claims that other than William the Conqueror's successful invasion of 1066, no other year holds more significance in English history. By portraying Richard as a hunchbacked tyrant who usurped the throne by killing his nephews, the Tudor historians attached a sense of myth to the battle: it became an epic clash between good and evil with a satisfying moral outcome. According to Reader Colin Burrow, André was so overwhelmed by the historic significance of the battle that he represented it with a blank page in his "Henry VII" (1502). For Professor Peter Saccio, the battle was indeed a unique clash in the annals of English history, because "the victory was determined, not by those who fought, but by those who delayed fighting until they were sure of being on the winning side."
Historians such as Adams and Horrox believe that Richard lost the battle not for any mythic reasons, but because of morale and loyalty problems in his army. Most of the common soldiers found it difficult to fight for a liege whom they distrusted, and some lords believed that their situation might improve if Richard was dethroned. According to Adams, against such duplicities Richard's desperate charge was the only knightly behaviour on the field. As fellow historian Michael Bennet puts it, the attack was "the swan-song of [mediaeval] English chivalry". Adams believes this view was shared at the time by the printer William Caxton, who enjoyed sponsorship from Edward IV and Richard III. Nine days after the battle, Caxton published Thomas Malory's story about chivalry and death by betrayal—"Le Morte d'Arthur"—seemingly as a response to the circumstances of Richard's death.
Elton does not believe Bosworth Field has any true significance, pointing out that the 20th-century English public largely ignored the battle until its quincentennial celebration. In his view, the dearth of specific information about the battle—no-one even knows exactly where it took place—demonstrates its insignificance to English society. Elton considers the battle as just one part of Henry's struggles to establish his reign, underscoring his point by noting that the young king had to spend ten more years pacifying factions and rebellions to secure his throne.
Mackie asserts that, in hindsight, Bosworth Field is notable as the decisive battle that established a dynasty which would rule unchallenged over England for more than a hundred years. Mackie notes that contemporary historians of that time, wary of the three royal successions during the long Wars of the Roses, considered Bosworth Field just another in a lengthy series of such battles. It was through the works and efforts of Francis Bacon and his successors that the public started to believe the battle had decided their futures by bringing about "the fall of a tyrant".
Shakespearian dramatisation.
William Shakespeare gives prominence to the Battle of Bosworth in his play, "Richard III". It is the "one big battle"; no other fighting scene distracts the audience from this action, represented by a one-on-one swordfight between Henry Tudor and Richard III. Shakespeare uses their duel to bring a climactic end to the play and the Wars of the Roses; he also uses it to champion morality, portraying the "unequivocal triumph of good over evil". Richard, the villainous lead character, has been built up in the battles of Shakespeare's earlier play, "Henry VI, Part 3", as a "formidable swordsman and a courageous military leader"—in contrast to the dastardly means by which he becomes king in "Richard III". Although the Battle of Bosworth has only five sentences to direct it, three scenes and more than four hundred lines precede the action, developing the background and motivations for the characters in anticipation of the battle.
Shakespeare's account of the battle was mostly based on chroniclers Edward Hall's and Raphael Holinshed's dramatic versions of history, which were sourced from Vergil's chronicle. However, Shakespeare's attitude towards Richard was shaped by scholar Thomas More, whose writings displayed extreme bias against the Yorkist king. The result of these influences is a script that vilifies the king, and Shakespeare had few qualms about departing from history to incite drama. Margaret of Anjou died in 1482, but Shakespeare had her speak to Richard's mother before the battle to foreshadow Richard's fate and fulfill the prophecy she had given in "Henry VI". Shakespeare exaggerated the cause of Richard's restless night before the battle, imagining it as a haunting by the ghosts of those whom the king had murdered, including Buckingham. Richard is portrayed as suffering a pang of conscience, but as he speaks he regains his confidence and asserts that he will be evil, if such needed to retain his crown.
The fight between the two armies is simulated by rowdy noises made off-stage ("alarums" or alarms) while actors walk on-stage, deliver their lines, and exit. To build anticipation for the duel, Shakespeare requests more "alarums" after Richard's councillor, William Catesby, announces that the king is "[enacting] more wonders than a man". Richard punctuates his entrance with the classic line, "A horse, a horse! My kingdom for a horse!" He refuses to withdraw, continuing to seek to slay Henry's doubles until he has killed his nemesis. There is no documentary evidence that Henry had five decoys at Bosworth Field; the idea was Shakespeare's invention. He drew inspiration from Henry IV's use of them at the Battle of Shrewsbury (1403) to amplify the perception of Richard's courage on the battlefield. Similarly, the single combat between Henry and Richard is Shakespeare's creation. "The True Tragedy of Richard III", a play earlier than Shakespeare's, has no signs of staging such an encounter: its stage directions give not a hint of visible combat.
Despite the dramatic licences taken, Shakespeare's version of the Battle of Bosworth was the model of the event for English textbooks for many years during the 18th and 19th centuries. This glamorised version of history, promulgated in books and paintings and played out on stages across the country, perturbed humorist Gilbert Abbott à Beckett. He voiced his criticism in the form of a poem, equating the romantic view of the battle to watching a "fifth-rate production of "Richard III"": shabbily costumed actors fight the Battle of Bosworth on-stage while those with lesser roles lounge at the back, showing no interest in the proceedings.
In Laurence Olivier's 1955 film adaptation of "Richard III", the Battle of Bosworth is represented not by a single duel but a general melee that became the film's most recognised scene and a regular screening at Bosworth Battlefield Heritage Centre. The film depicts the clash between the Yorkist and Lancastrian armies on an open field, focusing on individual characters amidst the savagery of hand-to-hand fighting, and received accolades for the realism portrayed. One reviewer for "The Manchester Guardian" newspaper, however, was not impressed, finding the number of combatants too sparse for the wide plains and a lack of subtlety in Richard's death scene. The means by which Richard is shown to prepare his army for the battle also earned acclaim. As Richard speaks to his men and draws his plans in the sand using his sword, his units appear on-screen, arraying themselves according to the lines that Richard had drawn. Intimately woven together, the combination of pictorial and narrative elements effectively turns Richard into a storyteller, who acts out the plot he has constructed. Shakespearian critic Herbert Coursen extends that imagery: Richard sets himself up as a creator of men, but dies amongst the savagery of his creations. Coursen finds the depiction a contrast to that of Henry V and his "band of brothers".
The adaptation of the setting for "Richard III" to a 1930s fascist England in Ian McKellen's 1995 film, however, did not sit well with historians. Adams posits that the original Shakespearian setting for Richard's fate at Bosworth teaches the moral of facing one's fate, no matter how unjust it is, "nobly and with dignity". By overshadowing the dramatic teaching with special effects, McKellen's film reduces its version of the battle to a pyrotechnic spectacle about the death of a one-dimensional villain. Coursen agrees that, in this version, the battle and Richard's end are trite and underwhelming.
Battlefield location.
Officially the site of the battle is deemed by Leicestershire County Council to be in the vicinity of the town of Market Bosworth. The council engaged historian Daniel Williams to research the battle, and in 1974 his findings were used to build the Bosworth Battlefield Heritage Centre and the presentation it houses. Williams's interpretation, however, has since been questioned. Sparked by the battle's quincentenary celebration in 1985, a dispute among historians has led many to suspect the accuracy of Williams's theory. In particular, geological surveys conducted from 2003 to 2009 by the Battlefields Trust, a charitable organisation that protects and studies old English battlefields, show that the southern and eastern flanks of Ambion Hill were solid ground in the 15th century, contrary to Williams's claim that it was a large area of marshland. Landscape archaeologist Glenn Foard, leader of the survey, said the collected soil samples and finds of medieval military equipment suggest that the battle took place two miles (3 km) southwest of Ambion Hill (52°34′41″N 1°26′02″W), contrary to the popular belief that it was fought near the foot of the hill.
Historians' theories.
The Historic Buildings and Monuments Commission for England (popularly referred to as "English Heritage") argues that the battle was named after Market Bosworth because the town was the nearest significant settlement to the battlefield in the 15th century. As explored by Professor Philip Morgan, a battle might initially not be named specifically at all. As time passes, writers of administrative and historical records find it necessary to identify a notable battle, ascribing it a name that is usually toponymical in nature and sourced from combatants or observers. This official name becomes accepted by society and future generations without question. Early records associated the Battle of Bosworth with "Brownehethe", "bellum Miravallenses", "Sandeford" and "Dadlyngton field". The earliest record, a municipal memorandum of 23 August 1485 from York, locates the battle "on the field of Redemore". This is corroborated by a 1485–86 letter that mentions "Redesmore" as its site. According to historian Peter Foss, records did not associate the battle with "Bosworth" until 1510.
Foss is named by English Heritage as the principal advocate for "Redemore" as the battle site. He suggests the name is derived from "Hreod Mor", an Anglo-Saxon phrase that means "reedy marshland". Basing his opinion on 13th- and 16th-century church records, he believes "Redemore" was an area of wetland that lay between Ambion Hill and the village of Dadlington, and was close to the Fenn Lanes, a Roman road running east to west across the region. Foard believes this road to be the most probable route that both armies took to reach the battlefield. Williams dismisses the notion of "Redmore" as a specific location, saying that the term refers to a large area of reddish soil; Foss argues that Williams's sources are local stories and flawed interpretations of records. Moreover, he proposes that Williams was influenced by William Hutton's 1788 "The Battle of Bosworth-Field", which Foss blames for introducing the notion that the battle was fought west of Ambion Hill on the north side of the River Sence. Hutton, as Foss suggests, misinterpreted a passage from his source, Raphael Holinshed's 1577 "Chronicle". Holinshed wrote, "King Richard pitched his field on a hill called Anne Beame, refreshed his soldiers and took his rest." Foss believes that Hutton mistook "field" to mean "field of battle", thus creating the idea that the fight took place on Anne Beame (Ambion) Hill. To "[pitch] his field", as Foss clarifies, was a period expression for setting up a camp.
Foss brings further evidence for his "Redemore" theory by quoting Edward Hall's 1550 "Chronicle". Hall stated that Richard's army stepped onto a plain after breaking camp the next day. Furthermore, historian William Burton, author of "Description of Leicestershire" (1622), wrote that the battle was "fought in a large, flat, plaine, and spacious ground, three miles [5 km] distant from [Bosworth], between the Towne of Shenton, Sutton [Cheney], Dadlington and Stoke [Golding]". In Foss's opinion both sources are describing an area of flat ground north of Dadlington.
Physical site.
English Heritage, responsible for managing England's historic sites, used both theories to designate the site for Bosworth Field. Without preference for either theory, they constructed a single continuous battlefield boundary that encompasses the locations proposed by both Williams and Foss. The region has experienced extensive changes over the years, starting after the battle. Holinshed stated in his chronicle that he found firm ground where he expected the marsh to be, and Burton confirmed that by the end of the 16th century, areas of the battlefield were enclosed and had been improved to make them agriculturally productive. Trees were planted on the south side of Ambion Hill, forming Ambion Wood. In the 18th and 19th centuries, the Ashby Canal carved through the land west and south-west of Ambion Hill. Winding alongside the canal at a distance, the Ashby and Nuneaton Joint Railway crossed the area on an embankment. The changes to the landscape were so extensive that when Hutton revisited the region in 1807 after an earlier 1788 visit, he could not readily find his way around.
Bosworth Battlefield Heritage Centre was built on Ambion Hill, near Richard's Well. According to legend, Richard III drank from one of the several springs in the region on the day of the battle. In 1788, a local pointed out one of the springs to Hutton as the one mentioned in the legend. A stone structure was later built over the location. The inscription on the well reads:
"Near this spot, on August 22nd 1485, at the age of 32, King Richard III fell fighting gallantly in defence of his realm & his crown against the usurper Henry Tudor.
The Cairn was erected by Dr. Samuel Parr in 1813 to mark the well from which the king is said to have drunk during the battle.
It is maintained by the Fellowship of the White Boar."
Northwest of Ambion Hill, just across the northern tributary of the Sence, a flag and memorial stone mark Richard's Field. Erected in 1973, the site was selected on the basis of Williams's theory. St James's Church at Dadlington is the only structure in the area that is reliably associated with the Battle of Bosworth; the bodies of those killed in the battle were buried there.
In 2010, a small silver badge depicting a boar was found in the area at a site one mile to the southwest. Experts believed this could indicate the actual site of the battle, since this badge depicts Richard III's personal emblem. Discussions were held about providing access to this site as well.
References.
Bibliography.
Books
</dl>
Periodicals
</dl>
Online sources
</dl>

</doc>
<doc id="3794" url="http://en.wikipedia.org/wiki?curid=3794" title="Brassicaceae">
Brassicaceae

Brassicaceae is a medium-sized and economically important family of flowering plants (Angiosperms), informally known as the mustards, mustard flowers, the crucifers or the cabbage family.
The name Brassicaceae is derived from the included genus "Brassica". Cruciferae, an older name, meaning "cross-bearing", describes the four petals of mustard flowers, which resemble a cross; it is one of eight plant family names without the suffix '-aceae' that are authorized alternative names (according to ICBN Art. 18.5 and 18.6 Vienna Code); thus both Cruciferae and Brassicaceae are used.
The family contains over 330 genera and about 3,700 species, according to the Royal Botanic Gardens, Kew. The largest genera are "Draba" (365 species), "Cardamine" (200 species, but its definition is controversial), "Erysimum" (225 species), "Lepidium" (230 species), and "Alyssum" (195 species).
The family contains well-known species such as "Brassica oleracea" (broccoli, cabbage, cauliflower, etc.), "Brassica rapa" (turnip, Chinese cabbage, etc.), "Brassica napus" (rapeseed, etc.), "Raphanus sativus" (common radish), "Armoracia rusticana" (horseradish), "Matthiola" (stock), "Arabidopsis thaliana" (thale cress) (model organism) and many others.
"Pieris rapae" and other butterflies of the Pieridae family are some of the most well known pests of the commercial cropping of Brassicaceae.
Taxonomy.
The family is included in Brassicales according to the APG system. Older systems (e.g., Arthur Cronquist's) placed them into the Capparales, a now-defunct order that had a similar definition.
This family comprises about 365 genera and 3200 species all over the world. 94 species of 38 genera are found in Nepal. The plants are mostly herbs. A close relationship has long been acknowledged between Brassicaceae and the caper family, Capparaceae, in part because members of both groups produce glucosinolate (mustard oil) compounds. Research published in 2002 suggested that Capparaceae as traditionally circumscribed were paraphyletic with respect to Brassicaceae, with "Cleome" and several related genera being more closely related to Brassicaceae than to other Capparaceae. The APG II system, therefore, has merged the two families under the name 'Brassicaceae'. Other classifications have continued to recognize Capparaceae but with a more restricted circumscription, either including "Cleome" and its relatives in Brassicaceae or recognizing them in the segregate family Cleomaceae. The APG III system has recently adopted this last solution, but this may change as a consensus arises on this point. This article deals with Brassicaceae "sensu stricto", i.e. treating Cleomaceae and Capparaceae as segregate families.
Description.
The family consists mostly of herbaceous plants with annual, biennial or perennial lifespans. However, around the Mediterranean they include also a dozen woody shrubs 1m - 3m tall, e.g. in northern Africa "(Zilla spinosa" and "Ptilotrichum spinosum)", in the Dalmatian islands "(Dendralyssum" and "Cramboxylon)", and chiefly in Canarias with some woody cruciferous genera: "Dendrosinapis, Descurainia, Parolinia, Stanleya, etc.".
The leaves are alternate (rarely opposite), sometimes organized in basal rosettes; in rare shrubby crucifers of Mediterranean their leaves are mostly in terminal rosettes, and may be coriaceous and evergreen. They are very often pinnately incised and do not have stipules.
The structure of the flowers is extremely uniform throughout the family. They have four free saccate sepals and four clawed free petals, staggered. They can be disymmetric or slightly zygomorphic, with a typical cross-like arrangement (hence the name 'Cruciferae'). They have six stamens, four of which are longer (as long as the petals, so relatively short in fact) and are arranged in a cross like the petals and the other two are shorter (tetradynamous flower). The pistil is made up of two fused carpels and the style is very short, with two lobes. Superior ovary. The flowers form ebracteate racemose inflorescences, often apically corymb-like.
Pollination occurs by entomogamy, nectar is produced at the base of the stamens and stored on the sepals.
The fruit is a peculiar kind of capsule named siliqua (plural siliquae, American English silique/siliques). It opens by two valves, which are the modified carpels, leaving the seeds attached to a framework made up of the placenta and tissue from the junction between the valves (replum). There is often an indehiscent beak at the top of the style and one or more seeds may be borne there. Where a siliqua is less than three times as long as it is broad, it is usually termed a silicula. The siliqua may break apart at constrictions occurring between the segments of the seeds, thus forming a sort of loment (e.g., "Raphanus"), it may eject the seeds explosively (e.g., "Cardamine") or may be evolved in a sort of samara (e.g., "Isatis"). The fruit is often the most important diagnostic character for plants in this family.
Brassicaceae do not form mycorrhizae, although rare exceptions do exist.
Most members share a suite of glucosinolate compounds that have a typical pungent odour usually associated with cole crops.
Uses.
The importance of this family for food crops has led to its selective breeding throughout history. Some examples of cruciferous food plants are the cabbage, broccoli, cauliflower, turnip, rapeseed, mustard, radish, horseradish, cress, wasabi, and watercress.
"Matthiola" (stock), "Cheiranthus", "Lobularia" and "Iberis" (candytufts) are appreciated for their flowers. "Lunaria" (honesty) is cultivated for the decorative value of the translucent replum of the round silicula that remains on the dried stems after dehiscence.
"Capsella bursa-pastoris", "Lepidium", and many "Cardamine" are common weeds.
"Isatis tinctoria" (woad) was used in the past to produce the colour indigo.
"Arabidopsis thaliana" is a very important model organism in the study of the flowering plants (Angiospermae).

</doc>
<doc id="3796" url="http://en.wikipedia.org/wiki?curid=3796" title="Books of the Bible">
Books of the Bible

Different religious groups include different books in their Biblical canons, in varying orders, and sometimes divide or combine books, or incorporate additional material into canonical books. Christian Bibles range from the sixty-six books of the Protestant canon to the eighty-one books of the Ethiopian Orthodox Tewahedo Church canon.
The Tanakh contains twenty-four books divided into three parts: the five books of the "Torah" ("teaching"); the "Nevi'im" ("prophets"); and the "Ketuvim" ("writings"). The first part of Christian Bibles is called the Old Testament, which contains, at minimum, the above twenty-four books but divided into thirty-nine books and ordered differently, sometimes also called the Hebrew Bible.
The Catholic Church and Eastern Christian churches also hold that certain deuterocanonical books and passages are part of the Old Testament canon. The second part is the New Testament, containing twenty-seven books; the four Canonical gospels, Acts of the Apostles, twenty-one Epistles or letters and the Book of Revelation.
The Eastern Orthodox, Oriental Orthodox and Eastern Catholic churches may have minor differences in their lists of accepted books. The list given here for these churches is the most inclusive: if at least one Eastern church accepts the book it is included here.
Hebrew Bible/Old Testament.
Hebrew Bible.
Rabbinic Judaism recognizes the 24 books of the Masoretic Text, commonly called the "Tanakh" or Hebrew Bible, as authoritative. Evidence suggests that the process of canonization occurred between 200 BCE and 200 CE. A popular former theory is that the Torah was canonized c. 400 BCE, the Prophets c. 200 BCE, and the Writings c. 100 CE, perhaps at a hypothetical Council of Jamnia, but this position is increasingly rejected by most modern scholars.
Christian Old Testament.
Protestants and Catholics use the Masoretic Text as the textual basis for their translations of the protocanonical books (those accepted as canonical by both Jews and all Christians), with various changes derived from a multiplicity of other ancient sources (such as the Septuagint, the Vulgate, the Dead Sea Scrolls, etc.), while generally using the Septuagint and Vulgate, now supplemented by the ancient Hebrew and Aramaic manuscripts, as the textual basis for the deuterocanonical books.
The Eastern Orthodox use the Septuagint as the textual basis for the entire Old Testament in both protocanonical and deuteroncanonical books—to use both in the Greek for liturgical purposes, and as the basis for translations into the vernacular. Most of the quotations (300 of 400) of the Old Testament in the New Testament, while differing more or less from the version presented by the Masoretic text, align with that of the Septuagint.
Intertestamental books.
The intertestamental books, largely written during the intertestamental period, are called the Biblical apocrypha ("hidden things") by Protestants, the deuterocanon ("second canon") by Catholics, and the deuterocanon or "anagignoskomena" ("worthy of reading") by Orthodox. These are works recognized by the Roman Catholic, Eastern Orthodox, and Oriental Orthodox Churches as being part of scripture (and thus deuterocanonical rather than apocryphal), but Protestants do not recognize them as divinely inspired. Orthodox differentiate scriptural books by omitting these (and others) from corporate worship and from use as a sole basis for doctrine.
Many other Christians recognize them as good, but not on the level of the other books of the Bible. Anglicanism considers the apocrypha "read for example of life" but not used "to establish any doctrine." Luther made a parallel statement in calling them: "not considered equal to the Holy Scriptures, but are useful and good to read."
The difference in canons derives from the difference in the Masoretic Text and the Septuagint. Books found in both the Hebrew and the Greek are accepted by all denominations, and by Jews, these are the protocanonical books. Catholics and Orthodox also accept those books present in manuscripts of the Septuagint, an ancient Greek translation of the Old Testament with great currency among the Jews of the ancient world, with the coda that Catholics consider 3 Esdras and 3 Maccabees apocryphal.
Most quotations of the Old Testament in the New Testament, differing by varying degrees from the Masoretic Text, are taken from the Septuagint. When the Jews closed the Old Testament canon, two criteria were used, that the book be written in Hebrew or Aramaic, and that it be no younger than the time of Ezra. This process led to the 24/39 books of the Tanakh and Old Testament. (However, Daniel was written several hundred years after the time of Ezra, and since that time several books of the Septuagint have been found in the original Hebrew, in the Dead Sea Scrolls, the Cairo Geniza, and at Masada, including a Hebrew text of Sirach (Qumran, Masada) and an Aramaic text of Tobit (Qumran); the additions to Esther and Daniel are also in their respective Semitic languages.)
The unanimous consensus of modern (and ancient) scholars consider several other books, including 1 Maccabees and Judith, to have been composed in Hebrew or Aramaic. Opinion is divided on the book of Baruch, while it is acknowledged that the Epistle of Jeremiah, the Wisdom of Solomon, and 2 Maccabees are originally Greek compositions.
Eastern Orthodox.
Additional books accepted by the Eastern Orthodox:
Syrian Orthodox.
Additional books accepted by the Syrian Orthodox (due to inclusion in the Peshitta):
Ethiopian Orthodox.
The Ethiopian Tewahedo church accepts all of the deuterocanonical books of Catholicism and anagignoskomena of Eastern Orthodoxy except for the four Books of Maccabees. It accepts the 24/39 books of the Masoretic Text along with the following books, called the "narrow canon". The enumeration of books in the Ethiopic Bible varies greatly between different authorities and printings.
Table.
The table uses the spellings and names present in modern editions of the Bible, such as the New American Bible Revised Edition, Revised Standard Version and English Standard Version. The spelling and names in both the 1609–1610 Douay Old Testament (and in the 1582 Rheims New Testament) and the 1749 revision by Bishop Challoner (the edition currently in print used by many Catholics, and the source of traditional Catholic spellings in English) and in the Septuagint differ from those spellings and names used in modern editions that derive from the Hebrew Masoretic text.
For the Orthodox canon, Septuagint titles are provided in parentheses when these differ from those editions. For the Catholic canon, the Douaic titles are provided in parentheses when these differ from those editions. Likewise, the King James Version references some of these books by the traditional spelling when referring to them in the New Testament, such as "Esaias" (for Isaiah).
In the spirit of ecumenism more recent Catholic translations (e.g., the New American Bible, Jerusalem Bible, and ecumenical translations used by Catholics, such as the Revised Standard Version Catholic Edition) use the same "standardized" (King James Version) spellings and names as Protestant Bibles (e.g., 1 Chronicles, as opposed to the Douaic 1 Paralipomenon, 1-2 Samuel and 1-2 Kings, instead of 1-4 Kings) in those books universally considered canonical—the protocanonicals.
The Talmud in Bava Batra 14b gives a different order for the books in "Nevi'im" and "Ketuvim". This order is also quoted in Mishneh Torah Hilchot Sefer Torah 7:15. The order of the books of the Torah are universal through all denominations of Judaism and Christianity.
The disputed books, included in one canon but not in others, are often called the Biblical apocrypha, a term that is sometimes used specifically to describe the books in the Catholic and Orthodox canons that are absent from the Jewish Masoretic Text and most modern Protestant Bibles. Catholics, following the Canon of Trent (1546), describe these books as deuterocanonical, while Greek Orthodox Christians, following the Synod of Jerusalem (1672), use the traditional name of "anagignoskomena", meaning "that which is to be read." They are present in a few historic Protestant versions; the German Luther Bible included such books, as did the English 1611 King James Version.
Empty table cells indicate that a book is absent from that canon.
Several of the books in the Eastern Orthodox canon are also found in the appendix to the Latin Vulgate, formerly the official Bible of the Roman Catholic Church.
New Testament.
In general, among Christian denominations, the New Testament canon is an agreed-upon list of 27 books, although book order can vary. The book order is the same in the Greek Orthodox, Roman Catholic, and Protestant tradition. The Slavonic, Armenian and Ethiopian traditions have different New Testament book orders.

</doc>
<doc id="3797" url="http://en.wikipedia.org/wiki?curid=3797" title="Baseball statistics">
Baseball statistics

Statistics play an important role in summarizing baseball performance and evaluating players in the sport.
Since the flow of a baseball game has natural breaks to it, and normally players act individually rather than performing in clusters, the sport lends itself to easy record-keeping and statistics. Statistics have been kept for professional baseball since the creation of the National League and American League, now part of Major League Baseball.
Many statistics are also available from outside of Major League Baseball, from leagues such as the National Association of Professional Base Ball Players and the Negro Leagues.
Development of statistics.
The practice of keeping records of player achievements was started in the 19th century by Henry Chadwick. Based on his experience with the sport called cricket, Chadwick devised the predecessors to modern day statistics including batting average, runs scored, and runs allowed.
Traditionally, statistics such as batting average (the number of hits divided by the number of at bats) and earned run average (the average number of earned runs allowed by a pitcher per nine innings) have dominated attention in the statistical world of baseball. However, the recent advent of sabermetrics has created statistics drawing from a greater breadth of player performance measures and playing field variables. Sabermetrics and comparative statistics attempt to provide an improved measure of a player's performance and contributions to his team from year to year, frequently against a statistical performance average.
Comprehensive, historical baseball statistics were difficult for the average fan to access until 1951, when researcher Hy Turkin published "The Complete Encyclopedia of Baseball". In 1969, Macmillan Publishing printed its first "Baseball Encyclopedia", using a computer to compile statistics for the first time. Known as "Big Mac", the encyclopedia became the standard baseball reference until 1988, when "Total Baseball" was released by Warner Books using more sophisticated technology. The publication of "Total Baseball" led to the discovery of several "phantom ballplayers", such as Lou Proctor, who did not belong in official record books and were removed.
Use of statistics.
Throughout modern baseball, a few core statistics have been traditionally referenced – batting average, RBI, and home runs. To this day, a player who leads the league in all of these three statistics earns the "Triple Crown." For pitchers, wins, ERA, and strikeouts are the most often-cited statistics, and a pitcher leading his league in these statistics may also be referred to as a "Triple Crown" winner. General managers and baseball scouts have long used the major statistics, among other factors and opinions, to understand player value. Managers, catchers and pitchers use the statistics of batters of opposing teams to develop pitching strategies and set defensive positioning on the field. Managers and batters study opposing pitcher performance and motions in attempting to improve hitting.
Some sabermetric statistics have entered the mainstream baseball world that measure a batter's overall performance including on-base plus slugging, commonly referred to as OPS. OPS adds the hitter's on-base percentage (number of times reached base by any means divided by total plate appearances) to his slugging percentage (total bases divided by at-bats). Some argue that the OPS formula is flawed and that more weight should be shifted towards OBP (on-base percentage). The statistic wOBA (weighted on-base average) attempts to correct for this.
OPS is also useful when determining a pitcher's level of success. "Opponent On-base Plus Slugging" (OOPS) is becoming a popular tool to evaluate a pitcher's actual performance. When analyzing a pitcher's statistics, some useful categories to consider include K/9IP (strikeouts per nine innings), K/BB (strikeouts per walk), HR/9 (Home runs per nine innings), WHIP (walks plus hits per inning pitched) and OOPS (opponent on-base plus slugging).
However, since 2001, more emphasis has been placed on Defense-Independent Pitching Statistics, including Defense-Independent ERA (dERA), in an attempt to evaluate a pitcher's performance regardless of the strength of the defensive players behind him.
All of the above statistics may be used in certain game situations. For example, a certain hitter's ability to hit left-handed pitchers might incline a manager to increase his opportunities to face left-handed pitchers. Other hitters may have a history of success against a given pitcher (or vice versa), and the manager may use this information to create a favorable
match-up. Broadcast commentators often refer to this as "playing the percentages".
Commonly used statistics.
Most of these terms also apply to softball. Commonly used statistics with their abbreviations are explained here. The explanations below are for quick reference and do not fully or completely define the statistic; for the strict definition, see the linked article for each statistic.
MLB statistical standards.
It is difficult to determine quantitatively what is considered to be a "good" value in a certain statistical category, and qualitative assessments may lead to arguments. Using full-season statistics available at the Official Site of Major League Baseball for the 2004 through 2011 seasons, the following tables show top ranges in various statistics, in alphabetical order. For each statistic, two values are given:

</doc>
<doc id="3800" url="http://en.wikipedia.org/wiki?curid=3800" title="At bat">
At bat

In baseball, an at bat (AB) or time at bat is a batter's turn batting against a pitcher. An at bat is different from a plate appearance. A batter is credited with a plate appearance regardless of what happens during his turn at bat. A batter is only credited with an at bat if that plate appearance does not have one of the results enumerated below. While at bats are used to calculate certain statistics, including batting average, on-base percentage, and slugging percentage, a player can only qualify for the season-ending rankings in these categories if he accumulates 502 plate appearances during the season.
A batter will not receive credit for an at bat if their plate appearance ends under the following circumstances:
Section 10.02.a.1 of the official rules of Major League Baseball defines an at bat as:
"Number of times batted, except that no time at bat shall be charged when a player: (1) hits a sacrifice bunt or sacrifice fly; (2) is awarded first base on four called balls; (3) is hit by a pitched ball; or (4) is awarded first base because of interference or obstruction..."
Examples.
An at bat is counted when:
At bat as a phrase.
"At bat", "up", "up at bat", and "at the plate" are all phrases describing a batter who is facing the pitcher. Note that just because a player is described as being "at bat" in this sense, he will not necessarily be given an at bat in his statistics; the phrase actually signifies a plate appearance (assuming it is eventually completed). This ambiguous terminology is usually clarified by context. To refer explicitly to the technical meaning of "at bat" described above, the term "official at bat" is sometimes used.

</doc>
<doc id="3801" url="http://en.wikipedia.org/wiki?curid=3801" title="Earned run">
Earned run

In baseball, an earned run is any run that was not necessarily enabled by a fielding error or a passed ball. In other words, an unearned run is one run that would not have been scored without the aid of an error or a passed ball committed by the defense, and an earned run is simply one that is not unearned. An error made by the pitcher in fielding at his position is counted the same as an error by any other player.
Regardless of the name, an unearned run is credited to the offensive team's score the same as any other run. It is only "unearned" in that it was, in a sense, "given away" by the defensive team.
Both total runs and earned runs are tabulated as part of a pitcher's statistics, but earned runs are specially denoted because of their use in calculating a pitcher's earned run average (ERA). One of the most prominent of baseball's traditional statistics for pitchers, the ERA is defined as the number of earned runs allowed per 9 innings pitched (i.e., averaged over a regulation game). Thus, in effect, the pitcher is held especially accountable for earned runs, while the responsibility for unearned runs is shared with the rest of the team. Nonetheless, the pitcher bears the burden of his teammates' defensive (and offensive) prowess in his win/loss record. For example, even though Nolan Ryan won the NL ERA title in 1987 (2.76), his record was just 8-16 due to subpar play by his teammates. This is why win/loss records can be a poor measure of a pitcher's performance.
To determine whether a run is earned, the official scorer must reconstruct the inning as it would have occurred without the errors (for purposes of this rule, the "errors" also include passed balls). This means, for example, that if an error extends an inning, and the offensive team scores additional runs beyond that which it would have scored without the aid of the error, all additional aforementioned runs are deemed unearned. Therefore, one error can mean countless runs scored. This also means that a pitcher can pitch a complete game, allow 0 earned runs (but any number of unearned runs), and still be credited with a loss. The benefit of the doubt is always given to the pitcher in determining which bases would have been reached by errorless play. This means that the official scorer does not assume that a runner would have advanced to bases beyond that which would have been obtained under conservative effort.
If no errors and no passed balls occur during the inning, all runs scored are automatically earned (assigned responsible to the pitcher). In a few cases, an error can be rendered harmless as the inning progresses. For example, a runner on first base advances to second on a passed ball and the next batter walks. Since the runner would now have been at second anyway, the passed ball no longer has any effect on the earned/unearned calculation. On the other hand, a batter/runner may make his entire circuit around the bases without the aid of an error, yet the run would be counted as unearned if an error prevented the third out from being made before he came up to bat.
Unearned run.
A run is counted as unearned when:
While the inning is still being played, the second and last scenario can cause a temporary situation where a run has already scored, but its earned/unearned status is not yet certain. Under the last circumstance, for example, with two outs, a runner on third base scores on a passed ball. For the time being, the run is unearned since the runner "should" still be at third. If the batter strikes out to end the inning, it will stay that way. If the batter gets a base hit, which would have scored the runner anyway, the run now becomes earned.
Under the second circumstance, if there are runners on base (but not on first base) and a batter hits a foul fly ball that is dropped, and then bats in the runners on base through a base hit (including a home run), the runs are unearned for the time being, as the runners should not have advanced. If the next batters either strike out or hit an infield fly that would not have advanced the runners, the runs remain unearned. However, if subsequent batters reach on clean plays which would have scored the runs anyway, the runs would count as earned, except for the runner that reached base through an at-bat extended by the dropped foul fly ball error.
A runner who reaches on catcher's interference and subsequently scores with two outs scores an unearned run, but baserunners who subsequently score after the runner who has reached on catcher's interference exclusively on clean plays score earned runs; the baserunner cannot be assumed to have been put out except for the error. (Rule 10.16(4)).
Neither the use of a pinch-runner to replace a baserunner who represents an unearned run nor the use of a pinch-hitter to continue the turn at bat of a batter who would be out except for an error transforms a run scored by such a person or his successors on base from an unearned run to an earned run.
When pitchers are changed in the middle of an inning, and one or more errors have already occurred, it is possible to have a run charged as earned against a specific pitcher, but unearned to the team. The simplest example is when the defensive team records two outs and makes an error on a play that would have been the third out. A new pitcher comes into the game, and the next batter hits a home run. The runner who reached on the error comes around to score, and his run is unearned to both the prior pitcher and the team. However, the run scored by the batter is counted as earned against the relief pitcher, but unearned to the team (since there should have already been three outs). Had the team not switched pitchers, neither run would be counted as an earned run because that pitcher should have already been out of that inning.
A pitcher is only charged with the number of runners that reached base while he was pitching, and this does not include baserunners who reach base as the result of a fielder's choice play that removes an existing runner; such a runner is charged to the pitcher whose baserunner has been removed by the fielder's choice play. When a pitching change occurs, the new pitcher is said to "inherit" any runners that are on base at the time, and if they later score, those runs are charged (earned or unearned) to the prior pitcher. Most box scores now list inherited runners, and the number that scored, as a statistic for the relief pitcher.
Historical differences.
In the early history of major league baseball, the difference between the number of earned runs given up by a pitcher and the total number of runs given up was much more significant than today. For instance, Jim Devlin in 1876 pitched 66 complete games (662 innings pitched) with a 1.56 ERA but managed to record only five shutouts. The seeming discrepancy comes from the difference in the number of allowed runs (309) versus earned runs (108).

</doc>
<doc id="3802" url="http://en.wikipedia.org/wiki?curid=3802" title="Base on balls">
Base on balls

A base on balls (BB), also known as a walk, occurs in baseball when a batter receives four pitches that the umpire calls "balls", and is then entitled to reach first base without the possibility of being put out. The base on balls is defined in Section 2.00 of baseball's Official Rules, and further detail is given in 6.08(a). It is considered a faux pas for a professional player to walk to first base; the batter-runner and any advancing runners normally jog on such a play, with Pete Rose earning his nickname "Charlie Hustle" due to him running towards first on a walk. 
The term "base on balls" distinguishes a walk from the other manners in which a batter can be awarded first base without liability to be put out (e.g., hit by pitch, catcher's interference). Though a base on balls, catcher interference, or a batter hit-by-a-pitched-ball (HPB) all result in the batter (and possibly runners on base) being awarded a base, the term "walk" usually refers only to a base on balls, and not the other methods of reaching base without the bat touching the ball. An important difference is that for a hit batter or catcher's interference, the ball is dead and no one may advance unless forced; the ball is live after a walk (see below for details).
A batter who draws a base on balls is commonly said to have been "walked" by the pitcher. When the batter is walked, runners advance one base without liability to be put out only if forced to vacate their base to allow the batter to take first base. If a batter draws a walk with the bases loaded, all preceding runners are forced to advance, including the runner on third base who is forced to home plate to score a run; when a run is forced on a walk, the batter is credited with an RBI per rule 10.04.
Receiving a base on balls does not count as a hit or an at bat for a batter but does count as a time on base and a plate appearance. Therefore, a base on balls does not affect a player's batting average, but it can increase his on-base percentage.
A hit by pitch is not counted statistically as a walk, though the effect is mostly the same, with the batter receiving a free pass to first base. One exception is that on a HBP (hit-by-pitch), the ball is dead. On a HBP, any runners attempting to steal on the play must return to their original base unless forced to the next base anyway. When a walk occurs, the ball is still live, any runner not forced to advance may nevertheless attempt to advance at his own risk, which might occur on a steal play, passed ball, or wild pitch. Also, because a ball is live when a base on balls occurs, runners on base forced to advance one base may attempt to advance beyond one base, at their own risk. The batter-runner himself may attempt to advance beyond first base, at his own risk. Rule 6.08 addresses this matter as well. An attempt to advance an additional base beyond the base awarded might occur when ball four is a passed ball or a wild pitch.
History.
In 1880, the National League changed the rules so that eight balls instead of nine were required for a walk. In 1884, the National League changed the rules so that six balls were required for a walk. In 1886, the American Association changed the rules so that six balls instead of seven were required for a walk; however, the National League changed the rules so that seven balls were required for a walk instead of six. In 1887, the National League and American Association agreed to abide by some uniform rule changes and decreased the number of balls required for a walk to five. In 1889, the National League and the American Association decreased the number of balls required for a walk to four.
Intentional base on balls.
A subset of the base on balls, an intentional base on balls (IBB) or intentional walk is when the pitcher deliberately pitches the ball away from the batter in order to issue a base on balls. As with any other walk, an intentional walk entitles the batter to first base without liability to be put out, and entitles any runners to advance if forced. Intentional walks are a strategic defensive maneuver, commonly done to bypass one hitter for one the defensive team believes is less likely to initiate a run-scoring play (e.g., a home run, sacrifice fly, or RBI base hit). Teams also commonly use intentional walks to set up a double play or force out situation for the next batter. 
Intentional walks do carry risks, however. They carry an obvious, inherent risk: they give the offensive team another runner on base, without any effort on their part, who could potentially score a run. They may carry additional risks. 
An intentional walk is signaled by the catcher standing and extending one arm to the side away from the batter. The pitcher then pitches the ball to that side several feet outside from home plate, usually outside the reach of the batter. A ball pitched in this manner is called an intentional ball and counts as a ball in the pitcher's pitch count. In order to count as an intentional ball, the ball must be legally pitched, i.e., the pitcher's foot must be on the pitcher's rubber, the catcher must be in the catcher's box, and the batter must be in the batter's box appearing ready to take a pitch at the time the ball is thrown. An intentional walk may be signaled at any time during the batter's turn at the plate; in these cases only enough additional intentional balls need to be thrown to bring the total to four. Only walks issued by the catcher signaling as described above are recorded as intentional walks (see below); walks issued without the catcher signaling – even if the pitches "are" intentionally thrown outside of the strike zone – are not recorded as intentional.
Because intentional walks involve throwing a ball, from the pitcher's usual spot on the pitcher's rubber, to a target that is significant distance from the plate and the batter, they involve some mental and physical adjustment, and/or re-adjustment to the subsequent batter. These could subtly affect or interrupt a pitcher's focus, mechanics, and/or rhythm; and, in turn, at least momentarily affect the pitcher's command, leading to an 'unintentional' walk to the next batter. The 'unintentional' walk following an intentional walk is not common, but is also not that rare.
Another risk taken by the defensive team in issuing a base on balls is that since intentional balls must be pitched in a legal manner, they can legally become wild pitches or passed balls. Likewise, a baserunner can attempt to steal a base, or the batter can choose to swing at an intentional ball; however, these rarely occur since taking these risks is rarely more beneficial to the offensive team than allowing the walk. In the Major Leagues, the most recent example of a swing at an intentional ball resulting in a hit occurred during a June 22, 2006 game between the Florida Marlins and the Baltimore Orioles. In the top of the 10th inning, with a runner on second base, Baltimore pitcher Todd Williams was signaled to intentionally walk the Marlins' Miguel Cabrera. Noticing that the intentional ball came in too close to the plate, Cabrera swung at the ball, resulting in a base hit, and a run scored for Florida.
Intentional walks also carry other nuanced risks. They might give the subsequent batter the strategic advantage of anticipating that the pitcher will avoid walking him. This subsequent batter might then more aggressively anticipate a pitch in the strike zone. If this batter guesses correctly, he could achieve more success than he otherwise would. Moreover, a subsequent batter might perceive an immediately preceding intentional walk as a slight to his abilities; if such a batter performs better when he feels underestimated by his opposition, an intentional walk could provide a spark. 
Though intentional walks are recorded as such in the records of the official scorer, they are combined with standard, non-intentional walks when calculating a player's on-base percentage, and have only received a separate column in a player's statistics since 1955.
Batters, on occasion, have been given intentional walks with the bases loaded (effectively giving the offensive team a risk-free run), although this occurs very infrequently.
A common nickname for the intentional walk is four-finger salute, since most managers call for an intentional walk by holding up four fingers. Outside the professional leagues, such as in high school or college baseball, the manager may simply request to the plate umpire to let the batter go to first instead of having the pitcher waste four outside pitches.
Barry Bonds is the all-time record holder with 688 intentional bases on balls. The next most is Hank Aaron with 293.

</doc>
<doc id="3805" url="http://en.wikipedia.org/wiki?curid=3805" title="List of Major League Baseball players with 4,000 total bases">
List of Major League Baseball players with 4,000 total bases

In baseball statistics, total bases (TBs) refers to the number of bases a player has gained with hits, i.e., the sum of his hits weighted by 1 for a single, 2 for a double, 3 for a triple and 4 for a home run.
Only bases attained from hits count toward this total. After a player collects a hit, whether it be a single, double, triple or home run, the total bases stat can be applied. Whether or not this player advances further during the inning, by stealing a base or advancing off another players hit, does not increase his/her total base number.
The total bases divided by the number of at bats is the player's slugging average.
List.
Through May 31st, 2015, these five active players are within 500 total bases of entering the 4,000 total bases list:
The record for the most Total Bases in a Single Game is 19, by Shawn Green of the Los Angeles Dodgers on May 23, 2002, playing against the Milwaukee Brewers. He scored 4 home runs (16), a double (2), and a single (1), to beat the previous record (held by Joe Adcock since 1954) by one base.

</doc>
<doc id="3806" url="http://en.wikipedia.org/wiki?curid=3806" title="Hit by pitch">
Hit by pitch

In baseball, hit by pitch (HBP) is a situation in which a batter or his clothing or equipment (other than his bat) is hit by a pitch from the pitcher; the batter is called a hit batsman (HB). A hit batsman is awarded first base, provided that (in the plate umpire's judgment) he made an honest effort to avoid the pitch, although failure to do so is rarely called by an umpire. Being hit by a pitch is often caused by a batter standing too close to, or "crowding", home plate.
Official rule.
Per baseball official rule 6.08(b), a batter becomes a baserunner and is awarded first base when he or his equipment (except for his bat):
If all these conditions are met, the ball is dead, and other baserunners advance if they are forced to vacate their base by the batter taking first. Rule 5.09(a) further clarifies that a hit by pitch is also called when a pitch touches a batter's clothing.
In the case where a batter swings and the pitch hits him anyway, the ball is dead and a strike is called. If the batter does not attempt to avoid the pitch, he is not awarded first base, and the pitch is ruled either a strike if in the strike zone or a ball if out of the strike zone. Umpires rarely make this call. A famous instance of a non-hit by pitch was on May 31, 1968, when Don Drysdale hit Dick Dietz with a pitch that would have forced in a run and ended Drysdale's scoreless innings streak at 44. Umpire Harry Wendelstedt ruled that Dietz made no effort to avoid the pitch; Dietz proceeded to fly out, and Drysdale's scoreless streak continued to a then-record 582⁄3 innings.
A hit by pitch can also be called on a pitch that has touched the ground. Such a bouncing pitch is like any other, and if a batter is hit by such a pitch, he will be awarded first unless he made no attempt to avoid it.
A batter hit by a pitch is not credited with a hit or at bat, but is credited with a time on base and a plate appearance; therefore, being hit by a pitch does not increase or decrease a player's batting average but does increase his on-base percentage. A batter hit by a pitch with the bases loaded is also credited with an RBI per MLB rule 10.04(a)(2). A pitch ruled a hit by pitch is recorded as a ball in the pitcher's pitch count, since by definition the ball must be outside the strike zone and not have been swung at.
The rule awarding first base to a batter hit by a pitch was instituted in 1887.
Tactical use.
Inside pitching is a common and legal tactic in baseball, and many players make use of brushback pitches, or pitches aimed underneath the chin, commonly referred to as "chin music", to keep players away from the plate. "Headhunter" is a common term for pitchers who have a reputation for throwing these kinds of pitches. However, throwing at a batter intentionally is illegal, and can be very dangerous. When an umpire suspects that a pitcher has thrown at a batter intentionally, but is not certain, a warning is issued to the pitcher and the managers of both teams. From that point on, any pitch thrown at a batter can cause the pitcher and the manager of the offending team to be ejected immediately from the game. Serious offenses such as a ball thrown at the head (called a beanball) can result in the immediate ejection of the pitcher, and the manager if he ordered the beanball, even without a warning. If the umpire is certain that the pitcher intentionally hit the batter with the pitch, the pitcher is ejected from the game with no warning.
Often, if a player is acting rude or unsportsmanlike, or having an extraordinarily good day, the pitcher may intentionally hit the batter, disguising it as a pitch that accidentally slipped his control. Managers may also order a pitcher to throw such a pitch (sometimes called a "plunking"). These pitches are often aimed at the lower back and slower than normal, designed to send a message more than anything else. The opposing team usually hits a batter in retaliation for this act. The plunkings generally end there because of umpire warnings, but in some cases things can get out of hand, and sometimes they lead to the batter charging the mound, bench-clearing brawls, and several ejections. Such plunking duels are more common in the American League than in the National League, because in the NL the pitchers must bat for themselves and open themselves up to direct retaliation (although hitting a fellow pitcher is a serious breach of baseball etiquette).
Records.
The all-time record for a player being hit by a pitch is held by Hughie Jennings, who was hit by 287 pitches between 1891 and 1903. The modern-day record is held by Craig Biggio of the Houston Astros, who had 285 as of the end of the 2007 season when he retired. Prior to Biggio, the modern-day record belonged to Don Baylor, who was hit 267 times.
The all-time single-season record also belongs to Jennings, who was hit 51 times during the 1896 season. Ron Hunt of the 1971 Montreal Expos was hit 50 times during that year, the modern-day record. The single-game record is three, held by numerous players.
The all-time record for pitchers is held by Gus Weyhing with 277 (1887-1901). The modern-day career pitching record for most hit batsmen is 205 by Hall-of-Famer Walter Johnson. The season record is 54 by Phil Knell in 1891, and the game record is six, held by Ed Knouff and John Grimes.
Brady Anderson was the first player to be hit by a pitch two times in the same inning in an American League game. On April 25, 2014, Brandon Moss became the second when he was hit twice in the top of the 9th inning by Houston Astros pitchers. Five players have been hit by a pitch twice in the same inning in the National League.
Dangers.
To date, one major league player has died as a result of being struck by a pitch: Ray Chapman of the Cleveland Indians was hit in the head by Carl Mays on August 16, 1920, and died the next morning.
It is possible to suffer serious injuries as a result of being hit by a pitch, even when wearing a helmet. On August 18, 1967, Red Sox batter Tony Conigliaro was hit almost directly in the left eye by a fastball thrown by Jack Hamilton of the California Angels. His cheekbone was shattered, he nearly lost the sight of the eye, was unable to play for over a year, and never regained his earlier batting ability. (Batting helmets at that time were not required to have an "ear flap"; indeed, it was not until 2002 that all major league batters were required to wear helmets with side protection.) On September 28, 1995, Kirby Puckett, of the Minnesota Twins, was struck in the cheek by a Dennis Martínez fastball, breaking his jaw and loosening two teeth. It would be his last game; during spring training the following year he developed glaucoma, which ended his career. Most recently, Mike Piazza, then of the New York Mets, was hit in the head by a pitch from Julián Tavárez of the St. Louis Cardinals on September 10, 2005. His helmet shattered, and he suffered a concussion. Other relatively minor injuries that are possible include broken fingers or hands, broken feet, broken ribs, injuries to the knee, or groin injuries.
Legal consequences.
Since inside pitching is a legitimate tactic in baseball, courts have recognized that being hit by a pitch is an inherent risk of the game, so that players cannot sue for any resulting injuries. On April 6, 2006, in a case arising from a game involving community college baseball teams, the Supreme Court of California ruled that baseball players in California assume the risk of being hit by baseballs "even if" the balls were intentionally thrown so as to cause injury. In the court's words: "For better or worse, being intentionally thrown at is a fundamental part and inherent risk of the sport of baseball. It is not the function of tort law to police such conduct."

</doc>
<doc id="3807" url="http://en.wikipedia.org/wiki?curid=3807" title="Hit (baseball)">
Hit (baseball)

In baseball statistics, a hit (denoted by H), also called a base hit, is credited to a batter when the batter safely reaches first base after hitting the ball into fair territory, without the benefit of an error or a fielder's choice.
Scoring a hit.
To achieve a hit, the batter must reach first base before any fielder can either tag him with the ball, throw to another player protecting the base before the batter reaches it, or tag first base while carrying the ball. The hit is scored the moment the batter reaches first base safely; if he is put out while attempting to stretch his hit to a double or triple or home run on the same play, he still gets credit for a hit (according to the last base he reached safely on the play).
If a batter reaches first base because of offensive interference by a preceding runner (including if a preceding runner is hit by a batted ball), he is also credited with a hit.
Types of hits.
A hit for one base is called a single, for two bases a double, and for three bases a triple. A home run is also scored as a hit. Doubles, triples, and home runs are also called extra base hits.
An "infield hit" is a hit where the ball does not leave the infield. Infield hits are uncommon by nature, and most often earned by speedy runners.
Pitching a no-hitter.
A no-hitter is a game in which one of the teams prevented the other from getting a hit. Throwing a no-hitter is rare and considered an extraordinary accomplishment for a pitcher or pitching staff. In most cases in the professional game, no-hitters are accomplished by a single pitcher who throws a complete game. A pitcher who throws a no-hitter could still allow runners to reach base safely, by way of walks, errors, hit batsmen, or batter reaching base due to interference or obstruction. If the pitcher allows no runners to reach base, the no-hitter is a perfect game.
History.
In 1887, Major League Baseball counted bases on balls (walks) as hits. The result was skyrocketing batting averages, including some near .500; Tip O'Neill of the St. Louis Browns batted .485 that season, which would still be a major league record if recognized. The experiment was abandoned the following season.
There is controversy regarding how the records of 1887 should be interpreted. The number of legitimate walks and at-bats are known for all players that year, so computing averages using the same method as in other years is straightforward. In 1968, Major League Baseball formed a Special Baseball Records Committee to resolve this (and other) issues. The Committee ruled that walks in 1887 should not be counted as hits. In 2000, Major League Baseball reversed its decision, ruling that the statistics which were recognized in each year's official records should stand, even in cases where they were later proven incorrect. Most current sources list O'Neill's 1887 average as .435, as calculated by omitting his walks. He would retain his American Association batting championship. However, the variance between methods results in differing recognition for the 1887 National League batting champion. Cap Anson would be recognized, with his .421 average, if walks are included, but Sam Thompson would be the champion at .372 if they are not.
Major League Baseball rules.
The official rulebook of Major League Baseball states in Rule 10.05:
Rule 10.05(a) Comment: In applying Rule 10.05(a), the official scorer shall always give the batter the benefit of the doubt. A safe course for the official scorer to follow is to score a hit when exceptionally good fielding of a ball fails to result in a putout.

</doc>
<doc id="3808" url="http://en.wikipedia.org/wiki?curid=3808" title="On-base percentage">
On-base percentage

In baseball statistics, on-base percentage (OBP; sometimes referred to as on-base average/OBA, as the statistic is rarely presented as a true percentage) is a measure of how often a batter reaches base for any reason other than a fielding error, fielder's choice, dropped/uncaught third strike, fielder's obstruction, or catcher's interference (the latter two are ignored as either times-on-base (TOB) or plate appearances in calculating OBP). OBP is added to slugging average to determine on-base plus slugging (OPS). It first became an official MLB statistic in 1984.
The on-base percentage of all batters faced by one pitcher or team is referred to as on-base against.
Overview.
Traditionally, players with the best on-base percentages bat as leadoff hitter, unless they are power hitters, who traditionally bat slightly lower in the batting order. The league average for on-base percentage in Major League Baseball has varied considerably over time; at its peak in the late 1990s, it was around .340, whereas it was typically .300 during the dead-ball era. On-base percentage can also vary quite considerably from player to player. The record for the highest career OBP by a hitter, based on over 3000 plate appearances, is .482 by Ted Williams. The lowest is by Bill Bergen, who had an OBP of .194.
For small numbers of at-bats, it is possible (though unlikely) for a player's on-base percentage to be lower than his batting average (H/AB). This happens when a player has almost no walks or times hit by pitch, with a higher number of sacrifice flies (e.g. if a player has 2 hits in 6 at-bats plus a sacrifice fly, his batting average would be .333, but his on-base percentage would be .286). The player who experienced this phenomenon with the most number of at-bats over a full season was Ernie Bowman. In 1963, with over 125 at-bats, Bowman had a batting average of .184 and an on-base percentage of .181.
On-base percentage is calculated using this formula:
where
NOTE: Sacrifice flies were not counted as an official statistic until 1954. Before that time, all sacrifices were counted as sacrifice hits (SH), which included both sacrifice flies and bunts. Sacrifice bunts (sacrifice hits since 1954), which would lower a batter's on-base percentage, are not included in the calculation for on-base percentage, as bunting is an offensive strategy – often dictated by the manager – the use of which does not necessarily reflect on the batter's ability and should not be used to penalize him. For calculations of OBP before 1954, or where sacrifice flies are not explicitly listed, the number of sacrifice flies should be assumed to be zero.
All-time leaders.
bold is active player

</doc>
<doc id="3809" url="http://en.wikipedia.org/wiki?curid=3809" title="Sacrifice fly">
Sacrifice fly

In baseball, a sacrifice fly is a batted ball that satisfies four criteria:
It is called a "sacrifice" fly because the batter presumably intends to cause a teammate to score a run, while sacrificing his own ability to do so. Sacrifice flies are traditionally recorded in box scores with the designation "SF".
Rules.
As addressed within Rule 10.08(d) of the Official Baseball Rules, a sacrifice fly is not counted as a turn at bat for the batter, though the batter is credited with a run batted in.
The purpose of not counting a sacrifice fly as an at-bat is to avoid penalizing hitters for a successful action. The sacrifice fly is one of two instances in baseball where a batter is not charged with a time at bat after putting a ball in play; the other is the sacrifice hit (also known as a sacrifice bunt). But, while a sacrifice fly doesn't affect a player's batting average, it counts as a plate appearance, thus lowering his on-base percentage, and a player on a hitting streak will have the hit streak end if he has no official at-bats but has a sacrifice fly. The reason for this is that the sacrifice fly, unlike the sacrifice bunt, is not considered a tactical maneuver (players don't try to hit a fly ball to advance a runner). Unlike a sacrifice bunt, which may be scored if a runner advances from any base to any base, a sacrifice fly is only credited if a runner scores on the play. Therefore, when a runner on first or second base tags on a fly ball and advances no further than third base, no sacrifice is given and the batter is charged with an at-bat. Note that, although rarely occurring, if a runner tags and advances from second base (or, theoretically, from first base) all the way to home and scores (without an intervening error) the batter is credited with a sacrifice fly.
The sacrifice fly is credited even if another runner is put out on appeal for failing to tag up, so long as a run scores prior to the third out. In the case of a fly ball dropped for an error, the sacrifice fly is only credited if the official scorer believes the run would have scored had the ball been caught.
On any fly ball, a runner can attempt to advance bases right as a fielder touches the ball. If the fielder bobbles the ball, the runner may still tag up, even before the fielder has full control of the ball (before the fielder is credited with a putout). The rule works this way because a smart fielder can theoretically purposely bobble the ball and run with it at the same time towards the infield and only completely catch it when he is within a good throwing distance to home plate. As addressed in the Official Baseball Rules, this was put into the baseball rulebook so it prevented these fielders from intentionally bobbling the ball.
Records.
The most sacrifice flies by a team in one game is five; the record was established by the Seattle Mariners in 1988, tied by the Colorado Rockies in 2006, and tied again by the Seattle Mariners in 2008. 
Five teams have collected three sacrifice flies in an inning: the Chicago White Sox (fifth inning, July 1, 1962 against the Cleveland Indians); the New York Yankees twice (fourth inning, June 29, 2000 against the Detroit Tigers and third inning, August 19, 2000 against the Anaheim Angels); the New York Mets (second inning, June 24, 2005 against the Yankees); and the Houston Astros (seventh inning, June 26, 2005 against the Texas Rangers). In these cases one or more of the flies did not result in a putout due to an error.
Since the rule was reinstated in its present form, Gil Hodges of the Dodgers holds the record for most sacrifice flies in one season with 19, in 1954; Eddie Murray holds the record for most sacrifice flies in a career with 128.
As of the end of the 2008 season, players who have hit 115 or more career sacrifice flies:
1. Eddie Murray (128)<BR>
2. Cal Ripken, Jr. (127)<BR>
3. Robin Yount (123)<BR>
4. Hank Aaron (121)<BR>
4. Frank Thomas (121)<BR>
6. George Brett (120)<BR>
6. Rubén Sierra (120)<BR>
8. Rafael Palmeiro (119)<BR>
8. Daniel "Rusty" Staub (119)<BR>
10. Andre Dawson (118)<BR>
11. Don Baylor (115)
History.
Batters have not been charged with a time at-bat for a sacrifice hit since 1893, but baseball has changed the sacrifice fly rule multiple times. The sacrifice fly as a statistical category was instituted in 1908, only to be discontinued in 1931. The rule was again adopted in 1939, only to be eliminated again in 1940, before being adopted for the last time in 1954.

</doc>
<doc id="3810" url="http://en.wikipedia.org/wiki?curid=3810" title="On-base plus slugging">
On-base plus slugging

On-base plus slugging (OPS) is a sabermetric baseball statistic calculated as the sum of a player's on-base percentage and slugging average. The ability of a player both to get on base and to hit for power, two important offensive skills, are represented. An OPS of .900 or higher in Major League Baseball puts the player in the upper echelon of hitters. Typically, the league leader in OPS will score near, and sometimes above, the 1.000 mark.
Equation.
The basic equation is
formula_1
where OBP is on-base percentage and SLG is slugging average. These averages are defined
formula_2
and
formula_3
where:
In one equation, OPS can be represented as:
formula_4
Interpretation of OPS.
OPS does not present a complete picture of a player's offensive contributions. Factors such as baserunning, basestealing, and the leverage/timeliness of performance are not considered.
More expansive sabermetric measurements do attempt to incorporate some or all of the abovementioned factors. Nonetheless, even though it does not include them, OPS correlates quite well with team run scoring.
Other sabermetric stats, such as runs created and Wins Above Replacement, attempt to express a player's contribution directly in terms of runs and/or wins. However, a player's OPS does not have a simple intrinsic meaning.
OPS is a convenient calculation, but presents several issues. OPS weighs on-base percentage and slugging average equally. However, on-base percentage correlates better with scoring runs. Statistics such as wOBA build on this distinction using linear weights. Additionally, the components of OPS are not typically equal (league-average slugging percentages are usually 75-100 points higher than league-average on-base percentages). As a point of reference, the OPS for all of Major League Baseball in 2008 was .749. Furthermore, despite being based on two already well-established stats and thus easy to calculate, the calculation does not follow mathematical rules of addition with fractions; i.e., the denominator for on-base percentage is plate appearances while the denominator for slugging percentage is at bats.
An OPS scale.
Bill James, in his essay titled "The 96 Families of Hitters" uses seven different categories for classification by OPS:
This effectively transforms OPS into a 7-point ordinal scale. Substituting quality labels such as Excellent (A), Very Good (B), Good (C), Average (D), Fair (E), Poor (F) and Very Poor (G) for the A-G categories creates a subjective reference for OPS values.
History.
On-base plus slugging was first popularized in 1984 by John Thorn and Pete Palmer's book, "The Hidden Game of Baseball". "The New York Times" then began carrying the leaders in this statistic in its weekly "By the Numbers" box, a feature that continued for four years. Baseball journalist Peter Gammons used and evangelized the statistic, and other writers and broadcasters picked it up. The popularity of OPS gradually spread, and by 2004 it began appearing on Topps baseball cards.
OPS was formerly sometimes known as "Production", for instance in early versions of Thorn's Total Baseball encyclopedia, and in the Strat-O-Matic computer baseball game. This term has fallen out of use.
Leaders.
The Top 10 Major League Baseball players in lifetime OPS, with at least 3,000 plate appearances through the end of the 2014 season are (active players in bold):
The top four were all left-handed batters. Jimmie Foxx has the highest career OPS for a right-handed batter.
Source: 
The Top 10 single-season performances in MLB are (all left-handed hitters):
The highest single-season mark for a right-handed hitter was 1.2449 by Rogers Hornsby in (), (13th on the all-time list). Since 1925, the highest single-season OPS for a right-hander is 1.2224 by Mark McGwire in (), which is good for 16th all-time.
Source: 
Adjusted OPS (OPS+).
OPS+, Adjusted OPS, is a closely related statistic. OPS+ is OPS adjusted for the park and the league in which the player played, but not for fielding position. An OPS+ of 100 is defined to be the league average. An OPS+ of 150 or more is excellent and 125 very good, while an OPS+ of 75 or below is poor.
The basic equation for OPS+ is
formula_5
where *lgOBP is the park adjusted OBP of the league (not counting pitchers hitting) and *lgSLG is the park adjusted SLG of the league.
A common misconception is that OPS+ closely matches the ratio of a player's OPS to that of the league. In fact, due to the additive nature of the two components in OPS+, a player with an OBP and SLG both 50% better than league average in those metrics will have an OPS+ of 200 (twice the league average OPS+) while still having an OPS that is only 50% better than the average OPS of the league. It would be a better (although not exact) approximation to say that a player with an OPS+ of 150 produces 50% more "runs", in a given set of plate appearances, as a player with an OPS+ of 100.
Leaders in OPS+.
Through the end of the 2014 season, the career leaders in OPS+ (minimum 3,000 plate appearances) were
  1. Babe Ruth, 206 <br>
  2. Ted Williams, 190 <br>
  3. Barry Bonds, 181 <br>
  4. Lou Gehrig, 178 <br>
  5. Rogers Hornsby, 175 <br>
  6. Mickey Mantle, 172 <br>
  7. Dan Brouthers, 170 <br>
  8. Joe Jackson, 169 <br>
  9. Ty Cobb, 168 <br>
 10. Pete Browning, Jimmie Foxx & Mark McGwire, 163
Source: .
The only purely right-handed batters to appear on this list are Hornsby, Foxx, and McGwire. Mantle is the only switch-hitter in the group.
The highest single-season performances were:
Source: 
If Dunlap's and Barnes' seasons were to be eliminated from the list, two other Ruth seasons (1926 and 1927) would be on the list. This would also eliminate the only right-handed batter in the list, Barnes.

</doc>
<doc id="3811" url="http://en.wikipedia.org/wiki?curid=3811" title="Stolen base">
Stolen base

In baseball, a stolen base occurs when a runner advances to a base to which he is not entitled and the official scorer rules that the advance should be credited to the action of the runner. The umpires determine whether the runner is safe or out at the next base, but the official scorer rules on the question of credit or blame for the advance under Rule 10.
A stolen base most often occurs when a baserunner successfully advances to the next base while the pitcher is pitching the ball to home plate.
Successful base-stealing requires not only that the runner be fast but also have good baserunning instincts and timing.
Background.
Ned Cuthbert, playing for the Philadelphia Keystones in either 1863 or 1865, was the first player to steal a base in a baseball game, although the term stolen base was not used until 1870. For a time in the 19th century, stolen bases were credited when a baserunner reached an extra base on a base hit from another player. For example, if a runner on first base reached third base on a single, it counted as a steal. In 1887, Hugh Nicol set a still-standing Major League record with 138 stolen bases, many of which would not have counted under modern rules. Modern steal rules were fully implemented in 1898.
Base stealing was popular in the game's early decades, with speedsters such as Ty Cobb and Clyde Milan stealing nearly 100 bases in a season. But the tactic fell into relative disuse after Babe Ruth introduced the era of the home run – in 1955, for example, no one in baseball stole more than 25 bases, and Dom DiMaggio won the AL stolen base title in 1950 with just 15. However, in the late 1950s and early 1960s, base-stealing was brought back to prominence primarily by Luis Aparicio and Maury Wills, who broke Cobb's modern single-season record by stealing 104 bases in 1962. Wills' record was broken in turn by Lou Brock in 1974, and Rickey Henderson in 1982. The stolen base remained a popular tactic through the 1980s, perhaps best exemplified by Vince Coleman and the St. Louis Cardinals, but began to decline again in the 1990s as the frequency of home runs reached unprecedented heights and the steal-friendly artificial turf ballparks began to disappear.
Base stealing is an important characteristic of the "small ball" managing style (or "manufacturing runs"). Such managers emphasize "doing the little things" (including risky running plays like base-stealing) to advance runners and score runs, often relying on pitching and defense to keep games close. The Los Angeles Dodgers of the 1960s, led by pitcher Sandy Koufax and speedy shortstop Maury Wills, were a successful example of this style. The antithesis of this is reliance on power hitting, exemplified by the Baltimore Orioles of the 1970s, which aspired to score most of its runs via home runs. Often the "small ball" model is associated with the National League, while power hitting is associated with the American League. However, some successful recent American League teams, including the 2002 Anaheim Angels, the 2001 Seattle Mariners and the 2005 Chicago White Sox have excelled at "small ball." The Kansas City Royals have embodied this style recently, leading the league in stolen bases but finishing last in home runs in 2013 and 2014. Successful teams often combine both styles, with a speedy runner or two complementing hitters with power, such as the 2005 White Sox, who despite playing "small ball", still hit 200 home runs.
Base-stealing technique.
Baseball's Rule 8 (The Pitcher) specifies the pitching procedure in detail. For example, in the Set Position, the pitcher must "com[e] to a complete stop"; thereafter, "any natural motion associated with his delivery of the ball to the batter commits him to the pitch without alteration or interruption." A runner intending to "steal on the pitcher" breaks for the next base the moment the pitcher commits to pitch to home plate. The pitcher cannot abort the pitch and try to put the runner out; this is a balk under Rule 8.
If the runner breaks too soon (before the pitcher is obliged to complete a pitch), the pitcher may throw to a base rather than pitch, and the runner is usually "picked off" by being tagged out between the bases. Past this moment, any delay in the runner's break makes it more likely that the catcher, after receiving the pitch, will be able to throw the runner out at the destination base.
Before the pitch, the runner takes a "lead-off," walking several steps away from the base as a head start toward the next base. Even a runner who does not intend to steal takes a "secondary lead" of a few more steps, once the pitcher has legally committed to complete the pitch.
The pitcher may, without limit, throw the ball to the runner's base. The runner must return to that base or risk being tagged out; but the underlying strategy is thereby to dissuade the runner from too big a lead-off; that is, to "hold the runner on" his original base.
The more adept base stealers are proficient at "reading the pickoff", meaning that they can detect certain "tells" (tell-tale signs) in a pitcher’s pre-pitch movements or mannerisms that indicate the pickoff attempt is or is not imminent. For example, one experienced base stealer noted that careless pitchers dig the toes on their back foot into the ground when they are about to pitch in order to get a better push off, but when they intend to turn and throw a pickoff, they do not.
If a batted ball is caught on the fly, the runner must return to his original base. In this case, a runner trying to steal is more likely to be caught off his original base, resulting in a doubleplay. This is a minor risk of a steal attempt. It is offset by the lower likelihood of a doubleplay on a ground ball.
Plays involving baserunning.
In the "hit-and-run play," coaches coordinate the actions of runner and batter. The runner tries to steal and the batter swings at almost any pitch, if only to distract the catcher. If the batter makes contact, the runner has a greater chance of reaching the next base; if the batter gets a base hit, the runner may be able to take an extra base. If the batter fails to hit the ball, the hit-and-run becomes a pure steal attempt.
In the "delayed steal," the runner does not take advantage of the pitcher's duty to complete a pitch, but relies on surprise and takes advantage of any complacency by the fielders. The runner gives the impression he is not trying to steal, and does not break for the next base until the ball crosses the plate. It is rare for Major League defenses to be fooled, but the play is used effectively at the college level. The first delayed steal on record was performed by Miller Huggins in 1903. The delayed steal was famously practiced by Eddie Stanky of the Brooklyn Dodgers.
Second base is the base most often stolen, because once a runner is on second base he is considered to be in "scoring position", meaning that he is expected to be able to run home and score on most routine singles hit into the outfield. Second base is also the easiest to steal, as it is farthest from home plate and thus a longer throw from the catcher is required to prevent it. Third base is a shorter throw for the catcher, but the runner is able to take a longer lead off second base. A steal of home plate is the riskiest, as the catcher only needs to tag out the runner after receiving the ball from the pitcher. It is difficult for the runner to cover the distance between the bases before the ball arrives home. Ty Cobb holds the records for most steals of home in a single season (8) as well as for a career (54). Steals of home are not officially recorded statistics, and must be researched through individual game accounts. Thus, Cobb's totals may be even greater than is recorded. Jackie Robinson famously stole home in Game 1 of the 1955 World Series. The most recent player credited with a "straight" steal of home (that is, with no other runners moving to distract the pitcher) was David Peralta of the Arizona Diamondbacks who stole home against the Colorado Rockies on August 8, 2014. 35 games have ended with a runner stealing home, but only 2 have occurred since 1980. In a variation on the steal of home, the batter is signaled to simultaneously execute a sacrifice bunt, which results in the "squeeze play." The "suicide squeeze" is a squeeze in which the runner on third begins to steal home without seeing the outcome of the bunt; it is so named because if the batter fails to bunt, the runner will surely be out. In contrast, when the runner on third does not commit until seeing that the ball is bunted advantageously, it is called a "safety squeeze."
In more recent years, most steals of home involve a "delayed double steal," in which a runner on first attempts to steal second, while the runner on third breaks for home as soon as the catcher throws to second base. If it is important to prevent the run from scoring, the catcher may hold on to the ball (conceding the steal of second) or may throw to the pitcher; this may deceive the runner at third and the pitcher may throw back to the catcher for the out.
Statistics.
In baseball statistics, stolen bases are denoted by SB. Attempts to steal that result in the baserunner being out are "caught stealing" (CS). The sum of these statistics is "steal attempts."
The rule on stolen bases states that:
Relative skill at stealing bases can be judged either by evaluating a player's total number of steals or the success rate (stolen bases as a percentage of steal attempts). Noted statistician Bill James has argued that unless a player can steal a high percentage of the time, then the stolen base may be detrimental to a team. A success rate of 67 to 70% or better is necessary to make stealing bases worthwhile.
Comparing skill against players from other eras is also problematic because the definition has not been constant. Caught stealing was not recorded regularly until the middle of the 20th century. Ty Cobb, for example, was known as a great base-stealer, with 892 steals and a success rate of over 83%. However the data on Cobb's caught stealing is missing from 12 seasons, strongly suggesting he was unsuccessful many more times than his stats indicate. Carlos Beltrán, with 286 steals, has the highest career success rate of all players with over 300 stolen base attempts, at 88.3%.
Evolution of rules and scoring.
The first mention of the stolen base as a statistic was in the 1877 scoring rules adopted by the National League, which noted credit toward a player's total bases when a base is stolen. It was not until 1886 that the stolen base appeared as something to be tracked, but was only to "appear in the summary of the game".
In 1887, the stolen base was given its own individual statistical column in the box score, and was defined for purposes of scoring: "...every base made after first base has been reached by a base runner, except for those made by reason of or with the aid of a battery error (wild pitch or passed ball), or by batting, balks or by being forced off. In short, shall include all bases made by a clean steal, or through a wild throw or muff of the ball by a fielder who is directly trying to put the base runner out while attempting to steal." The next year, it was clarified that any attempt to steal must be credited to the runner, and that fielders committing errors during this play must also be charged with an error. This rule also clarified that advancement of another base(s) beyond the one being stolen is not credited as a stolen base on the same play, and that an error is charged to the fielder who permitted the extra advancement. There was clarification that a runner is credited with a steal if the attempt began before a battery error. Finally, batters were credited with a stolen base if they were tagged out after over running the base.
In 1892, a rule credited runners with stolen bases if a base runner advanced on a fly out, or if they advanced more than one base on any safe hit or attempted out, providing an attempt was made by the defense to put the runner out. The rule was rescinded in 1897.
In 1898, stolen base scoring was narrowed to no longer include advancement in the event of a fielding error, or advancement caused by a hit batsman.
1904 saw an attempt to reduce the already wordy slew of rules governing stolen bases, with the stolen base now credited when "...the baserunner ["sic"] advances a base unaided by a base hit, a put out, (or) a fielding or batter error."
1910 saw the first addressing of the double and triple steal attempts. Under the new rule, when any runner is thrown out, and the other(s) are successful, the successful runners will not be credited with a stolen base.
Without using the term, 1920 saw the first rule that would be referred to today as defensive indifference, as stolen bases would not be credited, unless an effort was made to stop the runner by the defense. This is usually called if such is attempted in the ninth inning while that player's team is trailing, unless the runner represents the potential tying run.
1931 saw a further narrowing of the criteria for awarding a stolen base. Power was given to the official scorer, in the event of a muff by the catcher in throwing, that in the judgment of the scorer the runner would have been out, to credit the catcher with an error, and not credit the runner with a stolen base. Further, any successful steal on a play resulting in a wild pitch, passed ball, or balk would no longer be credited as a steal, even if the runner had started to steal before the play.
One of the largest rewrites to the rules in history came in 1950. The stolen base was specifically to be credited "to a runner whenever he advances one base unaided by a base hit, a putout, a forceout, a fielder's choice, a passed ball, a wild pitch, or a balk."
There were noted exceptions, such as denying a stolen base to an otherwise successful steal as a part of a double or triple steal, if one other runner was thrown out in the process. A stolen base would be awarded to runners who successfully stole second base as a part of a double steal with a man on third, if the other runner failed to steal home, but instead was able to return safely to third base. Runners who are tagged out oversliding the base after an otherwise successful steal would not be credited with a stolen base. Indifference was also credited as an exception. Runners would now be credited with stolen bases if they had begun the act of stealing, and the resulting pitch was wild, or a passed ball. Finally, for 1950 only, runners would be credited with a stolen base if they were "well advanced" toward the base they were attempting to steal", and the pitcher is charged with a balk, with the further exception of a player attempting to steal, who would otherwise have been forced to advance on the balk by a runner behind them. This rule was removed in 1951.
A clarification came in 1955 that awarded a stolen base to a runner even if he became involved in a rundown, provided he evaded the rundown and advanced to the base he intended to steal.
The criteria for "caught stealing" were fine-tuned in 1979, with a runner being charged with being caught if he is put out while trying to steal, overslides a base (otherwise successfully stolen), or is picked off a base and tries to advance to the next base. It is explicitly not caught stealing to be put out after a wild pitch or passed ball.
"Stealing first".
While not recorded as a stolen base, the same dynamic between batter/runner and defense is on display in the case of an uncaught third strike. The batter/runner can avoid an out and become a baserunner by reaching first base ahead of the throw. This case is a strikeout that is not an out; the batter/runner's acquisition of first base is scored as a passed ball or wild pitch.
In baseball's earlier decades, a runner on second base could "steal" first base, perhaps with the intention of drawing a throw that might allow a runner on third to score (a tactic famously employed by Germany Schaefer). However, such a tactic was not recorded as a stolen base. MLB rules now forbid running clockwise on the basepaths to "confuse the defense or make a travesty of the game". Further, after the pitcher assumes the pitching position, runners cannot return to any previous base.
In a game on April 19, 2013, Milwaukee Brewers shortstop Jean Segura stole second base in the bottom of the eighth inning. After the batter up, Ryan Braun, walked, Segura broke early for third base and the pitcher, Shawn Camp of the Chicago Cubs, threw ahead of him. As Segura was chased back to second base, Braun advanced to second as well and was tagged out. Segura, thinking "he" was out, began to return to the home dugout behind first base, but first base coach Garth Iorg directed him to stand at first. Segura had not intentionally run the bases backwards as a deception or mockery, but no fielder tried to tag him out. However, on the first pitch of the next at-bat, he attempted to re-steal second and was thrown out by catcher Welington Castillo.
The expression "You can't steal first base" is sometimes used in reference to a player who is fast but not very good at getting on base in the first place. Former Pittsburgh Pirates manager Lloyd McClendon is jokingly referred to as having "stolen first" in a June 26, 2001 game – after being ejected for disputing a call at first base, he yanked the base out of the ground and left the field with it, delaying the game.

</doc>
<doc id="3812" url="http://en.wikipedia.org/wiki?curid=3812" title="Plate appearance">
Plate appearance

In baseball statistics, a player is credited with a plate appearance (denoted by PA) each time he completes a turn batting. A player completes a turn batting when: he strikes out or is declared out before reaching first base; or he reaches first base safely or is awarded first base (by a base on balls, hit by pitch, or catcher's interference); or he hits a fair ball which causes a preceding runner to be put out for the third out before he himself is put out or reaches first base safely ("see also" left on base, fielder's choice, force play). In other words, a plate appearance ends when the batter is put out or becomes a runner. A very similar statistic, at bats, counts a subset of plate appearances that end under certain circumstances.
Uses.
While at bats are used to calculate such important player hitting statistics as batting averages, slugging percentages and on-base percentages, plate appearances have no such statistical value. However, at season's end. a player must have accumulated 502 plate appearances during a season to be ranked in any of these categories. For example, suppose Player A, with 510 plate appearances and 400 at bats, gets 100 hits during the season and finishes with a .250 batting average. And suppose Player B, with 490 plate appearances and 400 at bats, gets 110 hits during the season and finishes the season with a .275 batting average. Player B, even though he had the same amount of at bats as Player A and even though his batting average is higher, will not be eligible for season-ending rankings because he did not accumulate the required 502 plate appearances, while Player A did and therefore will be eligible. 
Rule 10.22(a).
Rule 10.22(a) of the Official Baseball Rules make a single allowance to the minimum requirement of 502 plate appearances. If a player has less than that minimum amount, but if that player would win the batting, slugging or on-base percentage title if he had that minimum amount, then the amount of at bats he is short may be added to his at bat (and hence plate appearance) total so that he may win that title (assuming that his batting average, which would have to be recalculated after these extra at bats were added, would still be the best in his respective league.) For example, in 2012, Melky Cabrera, then of the San Francisco Giants, finished the season with a league-high .346 batting average, but he had only 501 plate appearances, one short of the required 502. Per the rule, an extra at bat should have been added to his total because, after it is added and his batting average recalculated, he still would have won the batting title. Melky's case, however, turned out differently. The reason Melky finished the season with only 501 at bats was because he was suspended in mid-August when he tested positive for illegal performance enhancing drugs. Melky was still eligible for that extra plate appearance, but in a remarkable show of sportsmanship he requested that that extra plate appearance not be added to his total, and that he not be considered for the batting crown, because he admitted that his use of performance enhancing drugs had given him an unfair advantage over other players. As a result, Melky's name is nowhere to be found on the list of 2012 National League batting leaders.
Calculating.
A batter is not charged with a plate appearance if, while batting, a preceding runner is put out on the basepaths for the third out in a way other than by the batter putting the ball into play (i.e., picked off, caught stealing). In this case, the same batter continues his turn batting in the next inning with no balls or strikes against him.
A batter is not charged with a plate appearance if, while batting, the game ends as the winning run scores from third base on a balk, stolen base, wild pitch or passed ball.
A batter may or may not be charged with a plate appearance (and possibly at-bat) in the rare instance when he is replaced by a pinch hitter after having already started his turn at bat. In this case, the pinch hitter would receive the plate appearance (and potential of an at-bat) unless the original batter is replaced when having 2 strikes against him and the pinch hitter subsequently completes the strikeout. In this case the plate appearance and at-bat are charged to the first batter. (see )
Basically, "plate appearances" = at bats + some of the scenarios excluded from at bats such as base on balls, hit by pitch or sacrifice which positively affect the offensive team (although a sacrifice involves the batter being put out).
Other uses.
In common terminology, the term "at bat" is sometimes used to mean "plate appearance" (for example, "he fouled off the ball to keep the "at bat" alive"). The intent is usually clear from the context, although the term "official at bat" is sometimes used to explicitly refer to an "at bat" as distinguished from a "plate appearance". However, terms such as "turn at bat" or "time at bat" are synonymous with "plate appearance".
Scoring.
Section 10 of the official rules states that an at bat is not counted when the player:
The main use of the plate appearance statistic is in determining a player's eligibility for leadership in some offensive statistical categories, notably batting average; currently, a player must have 3.1 PAs per game scheduled to qualify for the batting title (for the 162-game schedule, that means 502 PAs). Also, it is often erroneously cited that total plate appearances is the divisor (i.e., denominator) used in calculating on-base percentage (OBP), an alternative measurement of a player's offensive performance; in reality, the OBP denominator does "not" include certain PAs, such as times reached via either catcher’s interference or fielder’s obstruction or Sacrifice Hits (Sacrifice Flies are included).
Plate appearances are also used by scorers for "proving" a box score. If the game has been scored correctly, the total number of plate appearances for a team should equal the total of that team's runs, men left on base, and men put out.
Major League Baseball leaders.
Career.
Totals are current through the end of the 2011 season. Active players in bold.
Season.
Single-season statistics are current through the end of the 2011 season. Active players in bold.

</doc>
<doc id="3814" url="http://en.wikipedia.org/wiki?curid=3814" title="Games played">
Games played

Games played (most often abbreviated as G or GP) is a statistic used in team sports to indicate the total number of games in which a player has participated (in any capacity); the statistic is generally applied irrespective of whatever portion of the game is contested.
Baseball.
In baseball, the statistic applies also to players who, prior to a game, are included on a starting lineup card or are announced as "ex ante" substitutes, whether they actually play or not, although, in Major League Baseball, the application of this statistic does not extend to consecutive games played streaks. A starting pitcher, then, may be credited with a game played even as he is not credited with a game started or an inning pitched.

</doc>
<doc id="3817" url="http://en.wikipedia.org/wiki?curid=3817" title="Boogie Down Productions">
Boogie Down Productions

Boogie Down Productions was a hip hop group that was originally composed of KRS-One, D-Nice, and DJ Scott La Rock. DJ Scott La Rock was murdered on August 27, 1987, months after the release of BDP's debut album, "Criminal Minded". The name of the group, Boogie Down, derives from a nickname for the South Bronx section of The Bronx, one of the five boroughs of New York City. The group pioneered the fusion of dancehall reggae and hip hop music and their debut LP "Criminal Minded" contained frank descriptions of life in the South Bronx of the late 1980s thus setting the stage for what would eventually become gangsta rap.
Members.
The membership of BDP changed continuously throughout its existence, the only constant being KRS-One. The group was originally founded by KRS-One and DJ Scott LaRock, with producer Lee Smith, who was essential in the production of the songs in the groups first album "Criminal Minded", being added as a member shortly after. From those beginnings, BDP members and collaborators included Lee Smith, Scott La Rock, D-Nice, Kenny Parker (younger brother of KRS-One), Mad Lion, DJ Premier,Just-Ice, ICU Channel Live, McBoo, Ms. Melodie, Heather B., Scottie Morris, Tony Rahsan, Willie D., RoboCop, Harmony, DJ Red Alert, Jay Kramer, D-Square, Rebekah Foster, Scott Whitehill, Scott King, Chris Tait and Sidney Mills. BDP as a group essentially ended because KRS-One began recording and performing under his own name rather than the group name. Original member Lee Smith, who has co-producer credit on the original 12” "South Bronx" single, was the last to be inexplicably jettisoned by KRS-One and the future new label after Scott’s death.
In the liner notes on BDP's 1992 album "Sex and Violence", KRS-One writes: "BDP in 1992 is KRS-One, Willie D, and Kenny Parker! BDP is not D-Nice, Jamal-ski, Harmony, Ms. Melodie, and Scottie Morris. They are not down with BDP so stop frontin'." Steve "Flash" Juon of RapReviews.com claimed that this initiated the ultimate breakup of the group. See Also: KRS-One, Scott La Rock, and D-Nice
Cultural Influences and Impact.
“The Bridge Wars”
A conflict arose in the late 80s about the origins of hip-hop, and BDP made conscious efforts in its early work to establish their interpretation of the issue. The origins of hip-hop to many, including BDP, are believed to be from the Bronx. A rival hip-hop collective known as the Juice Crew's lyrics were misunderstood to contain a claim in the song "The Bridge" that hip hop was directly a result of artists originating from Queensbridge. Boogie Down and KRS retorted angrily with songs such as “The Bridge is Over” and “South Bronx,” which started one of the first notable hip hop wars as MC Shan, Marley Marl, Roxanne Shanté and Blaq Poet all released songs featuring verses personally attacking KRS and Scott La Rock. The Bridge Wars, however, were only short-lived, and after the death of Scott La Rock prior to the group's second album, KRS began to concentrate on consciously focused music.
While "Criminal Minded" contained vivid descriptions of South Bronx street life, BDP changed after Scott's death. Producer Lee Smith was dropped and KRS-One adopted the Teacha moniker and made a deliberate attempt at creating politically and socially conscious Hip-Hop. BDP was hugely influential in provoking political and social consciousness in Hip-Hop however the group was sometimes overshadowed by the political hip hop group Public Enemy.
Jamaican Inspirations
The Jamaican influence present in "Criminal Minded" is well illustrated by the use of the "Mad Mad" or "Diseases" riddim started in 1981 with reggae star Yellowman's song "Zunguzung." BDP used this riff in the song "Remix for P is Free," and it was later resampled by artists such as Black Star and dead prez. As an album regarded by many as the start of the gangsta rap movement, "Criminal Minded" played an important role in reaffirming the social acceptance of having Jamaican roots. BDP referenced reggae in a way that helped to solidify Jamaica's place in modern hip-hop culture.
Political and Social Activism.
From its start, BDP was impactful in both the development of hip-hop and giving a sincere voice to the reality of life in the South Bronx, a section of New York City that is clouded with poverty and crime. With its debut album "Criminal Minded", this early hip-hop group combined the sounds of LaRock's harsh, spare, reggae-influenced beats and KRS-One's long-winded rhyme style on underground classics such as “9mm Goes Bang” and “South Bronx,” the album's gritty portrait of life on the streets (as well as the firearms that adorned its cover) influenced the gangsta rap movement that began in earnest two years later.
The influence of BDP in the creation and development of gansta rap highlights the cultural significance and impact of the type of music BDP and other early hip-hop artists like it created. This subgenre of hip-hop is most closely associated with hard-core hip-hop and is widely misinterpreted as promoting violence and gang activity. This misinterpretation or stigma is closely related to Boogie Down Productions and the general purpose behind their underlying themes of violence. For instance, the cover art of "Criminal Minded" displays the two artists in this group brandishing drawn guns and displaying other firearms. This is not an encouragement of the violence described in BDP’s music, but rather a portrayal or hinting at the violence present in the South Bronx as a means of expression, escape, and even condemnation. This album art is not meant to advocate for violence but to challenge the conception of a criminal, to assert that those who are really criminally minded are those who hold power. This conflicts with the general stigma surrounding gansta rap, which thrives off of displaying messages of violence in such a way that it doesn’t challenge these social ills, but rather supports them through the culture of the music.
The music of BDP became significantly more politically astute after the death of disc jockey Scott La Rock, KRS-One’s mentor and partner. La Rock’s death symbolized all of the injustices that BDP reacted to and lyrically described in their music, and thus inspired KRS-One, now the only original member left in the group, to become more passionate about the relevance of the message of BDP’s music. He went on to publish four more albums under the title of Boogie Down Productions, and each one was increasingly innovative and expanded from the thuggish imagery of "Criminal Minded" and began to explore themes like black-on-black crime, and black radicalism, using a riff on the words of Malcolm X, “by any means necessary”, which became the title of the second BDP album, and still remains as one of the most political hip-hop albums to date. It was in this album where KRS defined himself as the “teacha” or “teacher” symbolizing his emphasis on educating his audience members and fans about relevant social issues surrounding the African-American experience.
During his time in association with Boogie Down Productions, KRS-One joined with other rappers to create the Stop the Violence Movement, which addressed many of the issues brought about through BDP’s music and is the most conscious effort displayed by KRS-One and Boogie Down Productions of political Activism and engagement. The movement created the single “Self-Destruction” in 1989 through the collaboration of hip-hop artists Boogie Down Productions (KRS-One, D-Nice & Ms. Melodie), Stetsasonic (Delite, Daddy-O, Wise, and Frukwan), Kool Moe Dee, MC Lyte, Doug E. Fresh, Just-Ice, Heavy D, Biz Markie, Public Enemy (Chuck D & Flavor Flav) with the aim of spreading awareness about violence throughout African-American and hip-hop communities. All proceeds from this effort went to the National Urban League.

</doc>
<doc id="3821" url="http://en.wikipedia.org/wiki?curid=3821" title="Binary-coded decimal">
Binary-coded decimal

In computing and electronic systems, binary-coded decimal (BCD) is a class of binary encodings of decimal numbers where each decimal digit is represented by a fixed number of bits, usually four or eight. Special bit patterns are sometimes used for a sign or for other indications (e.g., error or overflow).
In byte-oriented systems (i.e. most modern computers), the term "uncompressed" BCD usually implies a full byte for each digit (often including a sign), whereas "packed" BCD typically encodes two decimal digits within a single byte by taking advantage of the fact that four bits are enough to represent the range 0 to 9. The precise 4-bit encoding may vary however, for technical reasons, see Excess-3 for instance. The ten states representing a BCD decimal digit are sometimes called "tetrades" (for the nibble typically needed to hold them also known as tetrade) with those don't care-states unused named "pseudo-tetrades").
BCD's main virtue is its more accurate representation and rounding of decimal quantities as well as an ease of conversion into human-readable representations, in comparison to binary positional systems. BCD's principal drawbacks are a small increase in the complexity of the circuits needed to implement basic arithmetics and a slightly less dense storage.
BCD was used in many early decimal computers, and is implemented in the instruction set of machines such as the IBM System/360 series and its descendants and Digital's VAX. Although BCD "per se" is not as widely used as in the past and is no longer implemented in computers' instruction sets, decimal fixed-point and floating-point formats are still important and continue to be used in financial, commercial, and industrial computing, where subtle conversion and fractional rounding errors that are inherent in floating point binary representations cannot be tolerated.
Basics.
BCD takes advantage of the fact that any one decimal numeral can be represented by a four bit pattern. The most obvious way of encoding digits is "natural BCD" (NBCD), where each decimal digit is represented by its corresponding four-bit binary value, as shown in the following table. This is also called "8421" encoding. 
Other encodings are also used, including so-called "4221" and "7421" — named after the weighting used for the bits — and "excess-3". For example the BCD digit 6, '0110'b in 8421 notation, is '1100'b in 4221 (two encodings are possible), '0110'b in 7421, and '1001'b (6+3=9) in excess-3.
As most computers deal with data in 8-bit bytes, it is possible to use one of the following methods to encode a BCD number:
As an example, encoding the decimal number 91 using uncompressed BCD results in the following binary pattern of two bytes:
 Decimal: 9 1
 Binary : 0000 1001 0000 0001
In packed BCD, the same number would fit into a single byte:
 Decimal: 9 1
 Binary : 1001 0001
Hence the numerical range for one uncompressed BCD byte is zero through nine inclusive, whereas the range for one packed BCD is zero through ninety-nine inclusive.
To represent numbers larger than the range of a single byte any number of contiguous bytes may be used.  For example, to represent the decimal number 12345 in packed BCD, using big-endian format, a program would encode as follows:
 Decimal: 1 2 3 4 5
 Binary : 0000 0001 0010 0011 0100 0101
Note that the most significant nibble of the most significant byte is zero, implying that the number is in actuality 012345.  Also note how packed BCD is more efficient in storage usage as compared to uncompressed BCD; encoding the same number (with the leading zero) in uncompressed format would consume twice the storage.
Shifting and masking operations are used to pack or unpack a packed BCD digit.  Other logical operations are used to convert a numeral to its equivalent bit pattern or reverse the process.
BCD in electronics.
BCD is very common in electronic systems where a numeric value is to be displayed, especially in systems consisting solely of digital logic, and not containing a microprocessor. By utilizing BCD, the manipulation of numerical data for display can be greatly simplified by treating each digit as a separate single sub-circuit. This matches much more closely the physical reality of display hardware—a designer might choose to use a series of separate identical seven-segment displays to build a metering circuit, for example. If the numeric quantity were stored and manipulated as pure binary, interfacing to such a display would require complex circuitry. Therefore, in cases where the calculations are relatively simple working throughout with BCD can lead to a simpler overall system than converting to binary.
The same argument applies when hardware of this type uses an embedded microcontroller or other small processor. Often, smaller code results when representing numbers internally in BCD format, since a conversion from or to binary representation can be expensive on such limited processors. For these applications, some small processors feature BCD arithmetic modes, which assist when writing routines that manipulate BCD quantities.
Packed BCD.
In Packed BCD (or simply packed decimal), each of the two nibbles of each byte represent a decimal digit. Packed BCD has been in use since at least the 1960s and is implemented in all IBM mainframe hardware since then. Most implementations are big endian, i.e. with the more significant digit in the upper half of each byte, and with the leftmost byte (residing at the lowest memory address) containing the most significant digits of the packed decimal value. The lower nibble of the rightmost byte is usually used as the sign flag, although some unsigned representations lack a sign flag. As an example, a 4-byte value consists of 8 nibbles, wherein the upper 7 nibbles store the digits of a 7-digit decimal value and the lowest nibble indicates the sign of the decimal integer value.
Standard sign values are 1100 (hex C) for positive (+) and 1101 (D) for negative (−). This convention was derived from abbreviations for accounting terms (Credit and Debit), as packed decimal coding was widely used in accounting systems. Other allowed signs are 1010 (A) and 1110 (E) for positive and 1011 (B) for negative. Most implementations also provide unsigned BCD values with a sign nibble of 1111 (F). ILE RPG uses 1111 (F) for positive and 1101 (D) for negative. In packed BCD, the number 127 is represented by 0001 0010 0111 1100 (127C) and −127 is represented by 0001 0010 0111 1101 (127D). Burroughs systems used 1101 (D) for negative, and any other value is considered a positive sign value (the processors will normalize a positive sign to 1100 (C)).
No matter how many bytes wide a word is, there are always an even number of nibbles because each byte has two of them. Therefore, a word of "n" bytes can contain up to (2"n")−1 decimal digits, which is always an odd number of digits. A decimal number with "d" digits requires ½("d"+1) bytes of storage space.
For example, a 4-byte (32-bit) word can hold seven decimal digits plus a sign, and can represent values ranging from ±9,999,999. Thus the number −1,234,567 is 7 digits wide and is encoded as:
 0001 0010 0011 0100 0101 0110 0111 1101
 "1 2 3 4 5 6 7 −"
In contrast, a 4-byte binary two's complement integer can represent values from −2,147,483,648 to +2,147,483,647.
While packed BCD does not make optimal use of storage (about 1/6 of the memory used is wasted), conversion to ASCII, EBCDIC, or the various encodings of Unicode is still trivial, as no arithmetic operations are required. The extra storage requirements are usually offset by the need for the accuracy and compatibility with calculator or hand calculation that fixed-point decimal arithmetic provides. Denser packings of BCD exist which avoid the storage penalty and also need no arithmetic operations for common conversions.
Packed BCD is supported in the COBOL programming language as the "COMPUTATIONAL-3" (an IBM extension adopted by many other compiler vendors) or "PACKED-DECIMAL" (part of the 1985 COBOL standard) data type. It is supported in PL/I as "FIXED DECIMAL". Besides the IBM System/360 and later compatible mainframes, packed BCD is implemented in the native instruction set of the original VAX processors from Digital Equipment Corporation and some models of the SDS Sigma series mainframes, and is the native format for the Burroughs Corporation Medium Systems line of mainframes (descended from the 1950s Electrodata 200 series).
Ten's complement representations for negative numbers offer an alternative approach to encoding the sign of packed (and other) BCD numbers. In this case, positive numbers always have a most significant digit between 0 and 4 (inclusive), while negative numbers are represented by the 10's complement of the corresponding positive number. As a result this system allows for, a 32-bit packed BCD numbers to range from -50,000,000 to 49,999,999, and -1 is represented as 99999999. (As with two's complement binary numbers, the range not symmetric about zero.)
Fixed-point packed decimal.
Fixed-point decimal numbers are supported by some programming languages (such as COBOL and PL/I). These languages allow the programmer to specify an implicit decimal point in front of one of the digits. For example, a packed decimal value encoded with the bytes 12 34 56 7C represents the fixed-point value +1,234.567 when the implied decimal point is located between the 4th and 5th digits:
 12 34 56 7C
 "12 34.56 7+"
The decimal point is not actually stored in memory, as the packed BCD storage format does not provide for it. Its location is simply known to the compiler and the generated code acts accordingly for the various arithmetic operations.
Higher-density encodings.
If a decimal digit requires four bits, then three decimal digits require 12 bits. However, since 210 (1,024) is greater than 103 (1,000), if three decimal digits are encoded together, only 10 bits are needed. Two such encodings are "Chen-Ho encoding" and "Densely Packed Decimal" (DPD). The latter has the advantage that subsets of the encoding encode two digits in the optimal seven bits and one digit in four bits, as in regular BCD.
Zoned decimal.
Some implementations, for example IBM mainframe systems, support zoned decimal numeric representations. Each decimal digit is stored in one byte, with the lower four bits encoding the digit in BCD form. The upper four bits, called the "zone" bits, are usually set to a fixed value so that the byte holds a character value corresponding to the digit. EBCDIC systems use a zone value of 1111 (hex F); this yields bytes in the range F0 to F9 (hex), which are the EBCDIC codes for the characters "0" through "9". Similarly, ASCII systems use a zone value of 0011 (hex 3), giving character codes 30 to 39 (hex).
For signed zoned decimal values, the rightmost (least significant) zone nibble holds the sign digit, which is the same set of values that are used for signed packed decimal numbers (see above). Thus a zoned decimal value encoded as the hex bytes F1 F2 D3 represents the signed decimal value −123:
 F1 F2 D3
 " 1 2 −3"
EBCDIC zoned decimal conversion table.
(*) "Note: These characters vary depending on the local character code page setting."
Fixed-point zoned decimal.
Some languages (such as COBOL and PL/I) directly support fixed-point zoned decimal values, assigning an implicit decimal point at some location between the decimal digits of a number. For example, given a six-byte signed zoned decimal value with an implied decimal point to the right of the fourth digit, the hex bytes F1 F2 F7 F9 F5 C0 represent the value +1,279.50:
 F1 F2 F7 F9 F5 C0
 " 1 2 7 9. 5 +0"
IBM and BCD.
IBM used the terms binary-coded decimal and BCD for 6-bit "alphanumeric" codes that represented numbers, upper-case letters and special characters. Some variation of BCD "alphamerics" is used in most early IBM computers, including the IBM 1620, IBM 1400 series, and non-Decimal Architecture members of the IBM 700/7000 series.
The IBM 1400 series are character-addressable machines, each location being six bits labeled "B, A, 8, 4, 2" and "1,"
plus an odd parity check bit ("C") and a word mark bit ("M").
For encoding digits "1" through "9", "B" and "A" are zero and the digit value represented by standard 4-bit BCD in bits "8" through "1".
For most other characters bits "B" and "A" are derived simply from the "12", "11", and "0" "zone punches" in the punched card character code, and bits "8" through "1" from the "1" through "9" punches.
A "12 zone" punch set both "B" and "A", an "11 zone" set "B", and a "0 zone" (a 0 punch combined with any others) set "A".
Thus the letter A, which is "(12,1)" in the punched card format, is encoded "(B,A,1)". The currency symbol $, "(11,8,3)" in the punched card, was encoded in memory as "(B,8,2,1)".
This allowed the circuitry to convert between the punched card format and the internal storage format to be very simple with only a few special cases.
One important special case is digit "0", represented by a lone "0" punch in the card, and "(8,2)" in core memory.
The memory of the IBM 1620 is organized into 6-bit addressable digits, the usual "8, 4, 2, 1" plus "F", used as a flag bit and "C", an odd parity check bit. BCD "alphamerics" are encoded using digit pairs, with the "zone" in the even-addressed digit and the "digit" in the odd-addressed digit, the "zone" being related to the "12", "11", and "0" "zone punches" as in the 1400 series. Input/Output translation hardware converted between the internal digit pairs and the external standard 6-bit BCD codes.
In the Decimal Architecture IBM 7070, IBM 7072, and IBM 7074 "alphamerics" are encoded using digit pairs (using two-out-of-five code in the digits, not BCD) of the 10-digit word, with the "zone" in the left digit and the "digit" in the right digit. Input/Output translation hardware converted between the internal digit pairs and the external standard 6-bit BCD codes.
With the introduction of System/360, IBM expanded 6-bit BCD "alphamerics" to 8-bit EBCDIC, allowing the addition of many more characters (e.g., lowercase letters). A variable length Packed BCD "numeric" data type is also implemented, providing machine instructions that perform arithmetic directly on packed decimal data.
On the IBM 1130 and 1800, packed BCD is supported in software by IBM's Commercial Subroutine Package.
Today, BCD data is still heavily used in IBM processors and databases, such as IBM DB2, mainframes, and Power6. In these products, the BCD is usually zoned BCD (as in EBCDIC or ASCII), Packed BCD (two decimal digits per byte), or "pure" BCD encoding (one decimal digit stored as BCD in the low four bits of each byte). All of these are used within hardware registers and processing units, and in software. To convert packed decimals in EBCDIC table unloads to readable numbers, you can use the OUTREC FIELDS mask of the JCL utility DFSORT.
Other computers and BCD.
The Digital Equipment Corporation VAX-11 series includes instructions that can perform arithmetic directly on packed BCD data and convert between packed BCD data and other integer representations. The VAX's packed BCD format is compatible with that on IBM System/360 and IBM's later compatible processors. The MicroVAX and later VAX implementations dropped this ability from the CPU but retained code compatibility with earlier machines by implementing the missing instructions in an operating system-supplied software library. This is invoked automatically via exception handling when the no longer implemented instructions are encountered, so that programs using them can execute without modification on the newer machines.
The Intel x86 architecture found on Intel systems supports a unique 18-digit (ten-byte) BCD format that can be loaded into and stored from the floating point registers, and computations can be performed there.
In more recent computers such capabilities are almost always implemented in software rather than the CPU's instruction set, but BCD numeric data is still extremely common in commercial and financial applications. There are tricks for implementing packed BCD and zoned decimal add or subtract operations using short but difficult to understand sequences of word-parallel logic and binary arithmetic operations. For example, the following code (written in C) computes an unsigned 8-digit packed BCD add using 32-bit binary operations:
Addition with BCD.
It is possible to perform addition in BCD by first adding in binary, and then converting to BCD afterwards. Conversion of the simple sum of two digits can be done by adding 6 (that is, 16 – 10) when the five-bit result of adding a pair of digits has a value greater than 9. For example:
 1001 + 1000 = 10001
 9 + 8 = 17
Note that 10001 is the binary, not decimal, representation of the desired result. 
In BCD as in decimal, there cannot exist a value greater than 9 (1001) per digit. 
To correct this, 6 (0110) is added to that sum and then the result is treated as two 
nibbles:
 10001 + 0110 = 00010111 => 0001 0111
 17 + 6 = 23 1 7
The two nibbles of the result, 0001 and 0111, correspond to the digits "1" and "7". This yields "17" in BCD, which is the correct result.
This technique can be extended to adding multiple digits by adding in groups from right to left, propagating the second digit as a carry, always comparing the 5-bit result of each digit-pair sum to 9. Some CPUs provide a half-carry flag to facilitate BCD arithmetic adjustments following binary addition and subtraction operations.
Subtraction with BCD.
Subtraction is done by adding the ten's complement of the subtrahend. To represent the sign of a number in BCD, the number 0000 is used to represent a positive number, and 1001 is used to represent a negative number. The remaining 14 combinations are invalid signs. To illustrate signed BCD subtraction, consider the following problem: 357 − 432.
In signed BCD, 357 is 0000 0011 0101 0111. The ten's complement of 432 can be obtained by taking the nine's complement of 432, and then adding one. So, 999 − 432 = 567, and 567 + 1 = 568. By preceding 568 in BCD by the negative sign code, the number −432 can be represented. So, −432 in signed BCD is 1001 0101 0110 1000.
Now that both numbers are represented in signed BCD, they can be added together:
 0000 0011 0101 0111
 0 3 5 7
 + 1001 0101 0110 1000
 9 5 6 8
 = 1001 1000 1011 1111
 9 8 11 15
Since BCD is a form of decimal representation, several of the digit sums above are invalid. In the event that an invalid entry (any BCD digit greater than 1001) exists, 6 is added to generate a carry bit and cause the sum to become a valid entry. The reason for adding 6 is that there are 16 possible 4-bit BCD values (since 24 = 16), but only 10 values are valid (0000 through 1001). So adding 6 to the invalid entries results in the following:
 1001 1000 1011 1111
 9 8 11 15
 + 0000 0000 0110 0110
 0 0 6 6
 = 1001 1001 0010 0101
 9 9 2 5
Thus the result of the subtraction is 1001 1001 0010 0101 (-925). To check the answer, note that the first bit is the sign bit, which is negative. This seems to be correct, since 357 − 432 should result in a negative number. To check the rest of the digits, represent them in decimal. 1001 0010 0101 is 925. The ten's complement of 925 is 1000 − 925 = 999 − 925 + 1 = 074 + 1 = 75, so the calculated answer is −75. To check, perform standard subtraction to verify that 357 − 432 is −75.
Note that in the event that there are a different number of nibbles being added together (such as 1053 − 122), the number with the fewest number of digits must first be padded with zeros before taking the ten's complement or subtracting. So, with 1053 − 122, 122 would have to first be represented as 0122, and the ten's complement of 0122 would have to be calculated.
Background.
The binary-coded decimal scheme described in this article is the most common encoding, but there are many others. The method here can be referred to as "Simple Binary-Coded Decimal" ("SBCD") or "BCD 8421".
In the headers to the table, the '8 4 2 1', "etc.", indicates the weight of each bit shown; note that in the fifth column two of the weights are negative. Both ASCII and EBCDIC character codes for the digits are examples of zoned BCD, and are also shown in the table.
The following table represents decimal digits from 0 to 9 in various BCD systems:
Legal history.
In the 1972 case Gottschalk v. Benson, the U.S. Supreme Court overturned a lower court decision which had allowed a patent for converting BCD encoded numbers to binary on a computer. This was an important case in determining the patentability of software and algorithms.
Application.
The BIOS in many personal computers stores the date and time in BCD because the MC6818 real-time clock chip used in the original IBM PC AT motherboard provided the time encoded in BCD. This form is easily converted into ASCII for display.
The Atari 8-bit family of computers used BCD to implement floating-point algorithms. The MOS 6502 processor used has a BCD mode that affects the addition and subtraction instructions.
Early models of the PlayStation 3 store the date and time in BCD. This led to a worldwide outage of the console on 1 March 2010. The last two digits of the year stored as BCD were misinterpreted as 16 causing an error in the unit's date, rendering most functions inoperable. This has been referred to as the Year 2010 Problem.
Representational variations.
Various BCD implementations exist that employ other representations for numbers. Programmable calculators manufactured by Texas Instruments, Hewlett-Packard, and others typically employ a floating-point BCD format, typically with two or three digits for the (decimal) exponent. The extra bits of the sign digit may be used to indicate special numeric values, such as infinity, underflow/overflow, and error (a blinking display).
Signed variations.
Signed decimal values may be represented in several ways. The COBOL programming language, for example, supports a total of five zoned decimal formats, each one encoding the numeric sign in a different way:
Telephony Binary Coded Decimal (TBCD).
3GPP developed TBCD, an expansion to BCD where the remaining (unused) bit combinations are used to add specific telephony characters, with digits similar to those found in telephone keypads original design. It is backward compatible to BCD.
Alternative encodings.
If errors in representation and computation are more important than the speed of conversion to and from display, a scaled binary representation may be used, which stores a decimal number as a binary-encoded integer and a binary-encoded signed decimal exponent. For example, 0.2 can be represented as 2×10-1.
This representation allows rapid multiplication and division, but may require shifting by a power of 10 during addition and subtraction to align the decimal points. It is appropriate for applications with a fixed number of decimal places that do not then require this adjustment— particularly financial applications where 2 or 4 digits after the decimal point are usually enough. Indeed this is almost a form of fixed point arithmetic since the position of the radix point is implied.
Chen-Ho encoding provides a boolean transformation for converting groups of three BCD-encoded digits to and from 10-bit values that can be efficiently encoded in hardware with only 2 or 3 gate delays. Densely Packed Decimal is a similar scheme that is used for most of the significand, except the lead digit, for one of the two alternative decimal encodings specified in the IEEE 754-2008 standard.

</doc>
<doc id="3822" url="http://en.wikipedia.org/wiki?curid=3822" title="BCD">
BCD

BCD may refer to:

</doc>
<doc id="3823" url="http://en.wikipedia.org/wiki?curid=3823" title="Binary">
Binary

Binary means "composed of two pieces or two parts" and may refer to:

</doc>
<doc id="3826" url="http://en.wikipedia.org/wiki?curid=3826" title="Bumin Qaghan">
Bumin Qaghan

Bumin Qaghan (Old Turkic: , Bumïn qaγan, a.k.a. Bumın Kagan) or Illig Qaghan (Chinese: 伊利可汗, Pinyin: yīlì kěhàn, Wade–Giles: i-li k'o-han, died 552 AD) was the founder of the Turkic Khaganate. He was the eldest son of Ashina Tuwu (吐務 / 吐务). He was the chieftain of the Turkic people under the sovereignty of Rouran Khaganate. He is also mentioned as "Tumen" (土門, 吐門, commander of ten thousand) of the Rouran Khaganate.
According to "History of Northern Dynasties" and "Zizhi Tongjian", in 545 Tumen's tribe started to rise and frequently invaded the western frontier of Wei. The chancellor of Western Wei Yuwen Tai sent An Nuopanto (Nanai-Banda, a Sogdian from Bukhara) to Kök Türks to greet its chieftain Tumen to try to establish commercial relationship. In 546, Tumen presented tribute to Western Wei.
And in the same year he put down a revolt of the Tiele tribes against their overlords the Rouran Khaganate. He took advantage of this success and requested a Rouran princess in marriage. But the qaghan of Rouran Anagui refused this request and sent to Bumin a mission and message: "You are my blacksmith slave. How dare you utter these words?". Bumin got angry and killed Anagui's mission and cut all relationship with Rouran Khaganate.
Anagui's "blacksmith" (鍛奴 / 锻奴, Pinyin: duàn nú, Wade–Giles: tuan-nu) insult was recorded in Chinese chronicles and historians accepted that the Kök Türks were indeed blacksmith servants for the Rouran elite, and that "blacksmith slavery" may indicate a kind of vassalage system prevailed in Rouran society. Nevertheless, after this incident Bumin emerged as the leader of the revolt against Rouran.
In 551, Bumin requested a Western Wei princess in marriage. Yuwen Tai permitted it and sent Princess Changle of Western Wei to Bumin.　In the same year when Emperor Wen of Western Wei died Bumin sent mission and gave two hundred horses.
The beginning of formal diplomatic relations with China propped up Bumin's authority among the Turks. He eventually united the local Turkic tribes and threw off the yoke of the Rouran domination.
In 552 Bumin's army defeated Anagui's forces at the north of Huaihuang and then Anagui committed suicide. With their defeat Bumin proclaimed himself "Illig Qaghan" and made his wife qaghatun. According to the Bilge Qaghan's memorial complex and the Kul Tigin's memorial complex, Bumin and Istemi ruled people by Turkic laws and they developed them.
Bumin died within several months after proclaiming himself Illig Qaghan. He was succeeded by his younger brother Istemi in the western part and by his son Issik Qaghan in the eastern part. In less than one century, his khaganate expanded to comprise most of Central Asia.

</doc>
<doc id="3827" url="http://en.wikipedia.org/wiki?curid=3827" title="Bilge Khagan">
Bilge Khagan

Bilge Khagan (Old Turkic: , Bilge qaγan) (683 or 684 – 734) was the khagan of the Second Turkic Khaganate. His accomplishments were described in the Orkhon inscriptions.
Names.
As was the custom, his personal name and the name after assuming the title Khagan were different. His personal name was : 阿史那默棘連, Ashǐnà mòjílián, a-shih-na mo-chi-lan) . His name after assuming the title was Bilge Khagan (or Bilge Qhagan). (Old Turkic: 𐰋𐰃𐰠𐰏𐰀 𐰴𐰍𐰣, Bilge qaγan, 毗伽可汗, Pinyin: píjiā kěhàn, Wade–Giles: p'i-chia k'o-han, official title: 𐱅𐰭𐰼𐰃𐱅𐰏 𐱅𐰭𐰼𐰃𐰓𐰀 𐰉𐰆𐰞𐰢𐱁 𐱅𐰇𐰼𐰜 𐰋𐰃𐰠𐰏𐰀 𐰴𐰍𐰣, Teŋіriteg Тeŋiride bolmuš Türük Bilge qaγan, His wife was Po Beg, Tonyukuk's daughter. "Bilge" meaning "wise" or "one who has wisdom" in Turkic languages. Khagan (Kağan) is a title for ruler (like king, earl, etc.). "Bilge", as is the case with many pure Turkic names, is a unisex name. Even in today's Turkis, Bilge continues to be adopted both as male and female name. Kağan or Kaan, however, is used only as male name in contemporary Turkish.
As a khagan.
In 716, Kapaghan Khagan the second khaghan of the khaganate was killed in his campaign against Toquz Oghuz and his severed head was sent to Chang'an. Although his son Inel Qaghan succeeded him, the legal claimant of the throne was his cousin Ashǐnà mòjílián. Mojilan's brother Kul Tigin and Tonyukuk carried out a coup d'état against Inel Qaghan. They killed Inel Qaghan and made Mojilan a khagan with the name Bilge Qaghan. His name literally means "wise king".
Bilge's khaganate spanned vast steppes from the Caspian Sea to Manchuria; he also invaded the western sections of the Chinese territories. After his death from poisoning, several stelae were erected in the capital area by the Orkhon River. These Orkhon inscriptions are the first known texts in the Old Turkic language.
He was poisoned by Buyruk Chor (梅錄啜/梅录啜, méilù chuò, mei-lu-ch'o). He didn't die immediately and he had time to punish the family of Buyruk Chor with death.
References.
Encyclopædia Britannica, Micropaedia, Vol. II, pp16–17

</doc>
<doc id="3832" url="http://en.wikipedia.org/wiki?curid=3832" title="Bauhaus">
Bauhaus

  , commonly known simply as Bauhaus, was an art school in Germany that combined crafts and the fine arts, and was famous for the approach to design that it publicised and taught. It operated from 1919 to 1933. At that time the German term "  "—literally "house of construction"—was understood as meaning "School of Building".
The Bauhaus was first founded by Walter Gropius in Weimar. In spite of its name, and the fact that its founder was an architect, the Bauhaus during the first years of its existence did not have an architecture department. Nonetheless, it was founded with the idea of creating a "total" work of art in which all arts, including architecture, would eventually be brought together. The Bauhaus style later became one of the most influential currents in modern design, Modernist architecture and art, design and architectural education. The Bauhaus had a profound influence upon subsequent developments in art, architecture, graphic design, interior design, industrial design, and typography.
The school existed in three German cities: Weimar from 1919 to 1925, Dessau from 1925 to 1932 and Berlin from 1932 to 1933, under three different architect-directors: Walter Gropius from 1919 to 1928, Hannes Meyer from 1928 to 1930 and Ludwig Mies van der Rohe from 1930 until 1933, when the school was closed by its own leadership under pressure from the Nazi-led government which had claimed that it was a centre of communist intellectualism. Though the school was closed, the staff continued to spread its idealistic precepts as they left Germany and emigrated all over the world.
The changes of venue and leadership resulted in a constant shifting of focus, technique, instructors, and politics. For instance: the pottery shop was discontinued when the school moved from Weimar to Dessau, even though it had been an important revenue source; when Mies van der Rohe took over the school in 1930, he transformed it into a private school, and would not allow any supporters of Hannes Meyer to attend it.
Bauhaus and German modernism.
Germany's defeat in World War I, the fall of the German monarchy and the abolition of censorship under the new, liberal Weimar Republic allowed an upsurge of radical experimentation in all the arts, previously suppressed by the old regime. Many Germans of left-wing views were influenced by the cultural experimentation that followed the Russian Revolution, such as constructivism. Such influences can be overstated: Gropius himself did not share these radical views, and said that Bauhaus was entirely apolitical. Just as important was the influence of the 19th century English designer William Morris, who had argued that art should meet the needs of society and that there should be no distinction between form and function. Thus the Bauhaus style, also known as the International Style, was marked by the absence of ornamentation and by harmony between the function of an object or a building and its design.
However, the most important influence on Bauhaus was modernism, a cultural movement whose origins lay as far back as the 1880s, and which had already made its presence felt in Germany before the World War, despite the prevailing conservatism. The design innovations commonly associated with Gropius and the Bauhaus—the radically simplified forms, the rationality and functionality, and the idea that mass-production was reconcilable with the individual artistic spirit—were already partly developed in Germany before the Bauhaus was founded. The German national designers' organization Deutscher Werkbund was formed in 1907 by Hermann Muthesius to harness the new potentials of mass production, with a mind towards preserving Germany's economic competitiveness with England. In its first seven years, the Werkbund came to be regarded as the authoritative body on questions of design in Germany, and was copied in other countries. Many fundamental questions of craftsmanship versus mass production, the relationship of usefulness and beauty, the practical purpose of formal beauty in a commonplace object, and whether or not a single proper form could exist, were argued out among its 1,870 members (by 1914).
The entire movement of German architectural modernism was known as Neues Bauen. Beginning in June 1907, Peter Behrens' pioneering industrial design work for the German electrical company AEG successfully integrated art and mass production on a large scale. He designed consumer products, standardized parts, created clean-lined designs for the company's graphics, developed a consistent corporate identity, built the modernist landmark AEG Turbine Factory, and made full use of newly developed materials such as poured concrete and exposed steel. Behrens was a founding member of the Werkbund, and both Walter Gropius and Adolf Meyer worked for him in this period.
The Bauhaus was founded at a time when the German zeitgeist had turned from emotional Expressionism to the matter-of-fact New Objectivity. An entire group of working architects, including Erich Mendelsohn, Bruno Taut and Hans Poelzig, turned away from fanciful experimentation, and turned toward rational, functional, sometimes standardized building. Beyond the Bauhaus, many other significant German-speaking architects in the 1920s responded to the same aesthetic issues and material possibilities as the school. They also responded to the promise of a "minimal dwelling" written into the new Weimar Constitution. Ernst May, Bruno Taut, and Martin Wagner, among others, built large housing blocks in Frankfurt and Berlin. The acceptance of modernist design into everyday life was the subject of publicity campaigns, well-attended public exhibitions like the Weissenhof Estate, films, and sometimes fierce public debate.
Bauhaus and Vkhutemas.
The Vkhutemas, the Russian state art and technical school founded in 1920 in Moscow, has been compared to Bauhaus. Founded a year after the Bauhaus school, Vkhutemas has close parallels to the German Bauhaus in its intent, organization and scope. The two schools were the first to train artist-designers in a modern manner. Both schools were state-sponsored initiatives to merge the craft tradition with modern technology, with a Basic Course in aesthetic principles, courses in color theory, industrial design, and architecture. Vkhutemas was a larger school than the Bauhaus, but it was less publicised outside the Soviet Union and consequently, is less familiar to the West.
With the internationalism of modern architecture and design, there were many exchanges between the Vkhutemas and the Bauhaus. The second Bauhaus director Hannes Meyer attempted to organise an exchange between the two schools, while Hinnerk Scheper of the Bauhaus collaborated with various Vkhutein members on the use of colour in architecture. In addition, El Lissitzky's book "Russia: an Architecture for World Revolution" published in German in 1930 featured several illustrations of Vkhutemas/Vkhutein projects there.
History of the Bauhaus.
Weimar.
The school was founded by Walter Gropius in Weimar in 1919 as a merger of the Grand Ducal School of Arts and Crafts and the Weimar Academy of Fine Art. Its roots lay in the arts and crafts school founded by the Grand Duke of Saxe-Weimar-Eisenach in 1906 and directed by Belgian Art Nouveau architect Henry van de Velde. When van de Velde was forced to resign in 1915 because he was Belgian, he suggested Gropius, Hermann Obrist and August Endell as possible successors. In 1919, after delays caused by the destruction of World War I and a lengthy debate over who should head the institution and the socio-economic meanings of a reconciliation of the fine arts and the applied arts (an issue which remained a defining one throughout the school's existence), Gropius was made the director of a new institution integrating the two called the Bauhaus. In the pamphlet for an April 1919 exhibition entitled "Exhibition of Unknown Architects", Gropius proclaimed his goal as being "to create a new guild of craftsmen, without the class distinctions which raise an arrogant barrier between craftsman and artist." Gropius' neologism "Bauhaus" references both building and the Bauhütte, a premodern guild of stonemasons. The early intention was for the Bauhaus to be a combined architecture school, crafts school, and academy of the arts. In 1919 Swiss painter Johannes Itten, German-American painter Lyonel Feininger, and German sculptor Gerhard Marcks, along with Gropius, comprised the faculty of the Bauhaus. By the following year their ranks had grown to include German painter, sculptor and designer Oskar Schlemmer who headed the theater workshop, and Swiss painter Paul Klee, joined in 1922 by Russian painter Wassily Kandinsky. A tumultuous year at the Bauhaus, 1922 also saw the move of Dutch painter Theo van Doesburg to Weimar to promote De Stijl ("The Style"), and a visit to the Bauhaus by Russian Constructivist artist and architect El Lissitzky.
From 1919 to 1922 the school was shaped by the pedagogical and aesthetic ideas of Johannes Itten, who taught the "Vorkurs" or "preliminary course" that was the introduction to the ideas of the Bauhaus. Itten was heavily influenced in his teaching by the ideas of Franz Cižek and Friedrich Wilhelm August Fröbel. He was also influenced in respect to aesthetics by the work of the Blaue Reiter group in Munich as well as the work of Austrian Expressionist Oskar Kokoschka. The influence of German Expressionism favoured by Itten was analogous in some ways to the fine arts side of the ongoing debate. This influence culminated with the addition of Der Blaue Reiter founding member Wassily Kandinsky to the faculty and ended when Itten resigned in late 1922. Itten was replaced by the Hungarian designer László Moholy-Nagy, who rewrote the "Vorkurs" with a leaning towards the New Objectivity favored by Gropius, which was analogous in some ways to the applied arts side of the debate. Although this shift was an important one, it did not represent a radical break from the past so much as a small step in a broader, more gradual socio-economic movement that had been going on at least since 1907 when van de Velde had argued for a craft basis for design while Hermann Muthesius had begun implementing industrial prototypes.
Gropius was not necessarily against Expressionism, and in fact himself in the same 1919 pamphlet proclaiming this "new guild of craftsmen, without the class snobbery," described "painting and sculpture rising to heaven out of the hands of a million craftsmen, the crystal symbol of the new faith of the future." By 1923 however, Gropius was no longer evoking images of soaring Romanesque cathedrals and the craft-driven aesthetic of the "Völkisch movement", instead declaring "we want an architecture adapted to our world of machines, radios and fast cars." Gropius argued that a new period of history had begun with the end of the war. He wanted to create a new architectural style to reflect this new era. His style in architecture and consumer goods was to be functional, cheap and consistent with mass production. To these ends, Gropius wanted to reunite art and craft to arrive at high-end functional products with artistic merit. The Bauhaus issued a magazine called "Bauhaus" and a series of books called "Bauhausbücher". Since the Weimar Republic lacked the quantity of raw materials available to the United States and Great Britain, it had to rely on the proficiency of a skilled labor force and an ability to export innovative and high quality goods. Therefore designers were needed and so was a new type of art education. The school's philosophy stated that the artist should be trained to work with the industry.
Weimar was in the German state of Thuringia, and the Bauhaus school received state support from the Social Democrat-controlled Thuringian state government. The school in Weimar experienced political pressure from conservative circles in Thuringian politics, increasingly so after 1923 as political tension rose. One condition placed on the Bauhaus in this new political environment was the exhibition of work undertaken at the school. This condition was met in 1923 with the Bauhaus' exhibition of the experimental Haus am Horn. In February 1924, the Social Democrats lost control of the state parliament to the Nationalists. The Ministry of Education placed the staff on six-month contracts and cut the school's funding in half. On 26 December 1924 the Bauhaus issued a press release and setting the closure of the school for the end of March 1925. At this point they had already been looking for alternative sources of funding. After the Bauhaus moved to Dessau, a school of industrial design with teachers and staff less antagonistic to the conservative political regime remained in Weimar. This school was eventually known as the Technical University of Architecture and Civil Engineering, and in 1996 changed its name to Bauhaus-University Weimar.
Dessau.
Gropius's design for the Dessau facilities was a return to the futuristic Gropius of 1914 that had more in common with the International style lines of the Fagus Factory than the stripped down Neo-classical of the Werkbund pavilion or the "Völkisch" Sommerfeld House. The Dessau years saw a remarkable change in direction for the school. According to Elaine Hoffman, Gropius had approached the Dutch architect Mart Stam to run the newly founded architecture program, and when Stam declined the position, Gropius turned to Stam's friend and colleague in the ABC group, Hannes Meyer.
Meyer became director when Gropius resigned in February 1928, and brought the Bauhaus its two most significant building commissions, both of which still exist: five apartment buildings in the city of Dessau, and the headquarters of the Federal School of the German Trade Unions (ADGB) in Bernau. Meyer favored measurements and calculations in his presentations to clients, along with the use of off-the-shelf architectural components to reduce costs, and this approach proved attractive to potential clients. The school turned its first profit under his leadership in 1929.
But Meyer also generated a great deal of conflict. As a radical functionalist, he had no patience with the aesthetic program, and forced the resignations of Herbert Bayer, Marcel Breuer, and other long-time instructors. Even though Meyer shifted the orientation of the school further to the left than it had been under Gropius, he didn't want the school to become a tool of left-wing party politics. He prevented the formation of a student Communist cell, and in the increasingly dangerous political atmosphere, this became a threat to the existence of the Dessau school. Dessau mayor Fritz Hesse fired him in the summer of 1930. The Dessau city council attempted to convince Gropius to return as head of the school, but Gropius instead suggested Ludwig Mies van der Rohe. Mies was appointed in 1930, and immediately interviewed each student, dismissing those that he deemed uncommitted. Mies halted the school's manufacture of goods so that the school could focus on teaching. Mies appointed no new faculty other than his close confidant Lilly Reich. By 1931, the National Socialist German Workers' Party was becoming more influential in German politics. When they gained control of the Dessau City Council they moved to close the school.
Berlin.
In late 1932, Mies rented a derelict factory in Berlin to use as the new Bauhaus with his own money. The students and faculty rehabilitated the building, painting the interior white. The school operated for ten months without further interference from the Nazi Party. In 1933, the Gestapo closed down the Berlin school. Mies protested the decision, eventually speaking to the head of the Gestapo, who agreed to allow the school to re-open. However, shortly after receiving a letter permitting the opening of the Bauhaus, Mies and the other faculty agreed to voluntarily shut down the school.
Although neither the Nazi Party nor Hitler himself had a cohesive architectural policy before they came to power in 1933, Nazi writers like Wilhelm Frick and Alfred Rosenberg had already labeled the Bauhaus "un-German" and criticized its modernist styles, deliberately generating public controversy over issues like flat roofs. Increasingly through the early 1930s, they characterized the Bauhaus as a front for communists and social liberals. Indeed, a number of communist students loyal to Meyer moved to the Soviet Union when he was fired in 1930.
Even before the Nazis came to power, political pressure on Bauhaus had increased. The Nazi movement, from nearly the start, denounced the Bauhaus for its "degenerate art", and the Nazi regime was determined to crack down on what it saw as the foreign, probably Jewish influences of "cosmopolitan modernism." Despite Gropius's protestations that as a war veteran and a patriot his work had no subversive political intent, the Berlin Bauhaus was pressured to close in April 1933. Emigrants did succeed, however, in spreading the concepts of the Bauhaus to other countries, including the “New Bauhaus” of Chicago: Mies decided to emigrate to the United States for the directorship of the School of Architecture at the Armour Institute (now Illinois Institute of Technology) in Chicago and to seek building commissions.[a] The simple engineering-oriented functionalism of stripped-down modernism, however, lead to some Bauhaus influences living on in Nazi Germany. When Hitler's chief engineer, Fritz Todt, began opening the new autobahn (highways) in 1935, many of the bridges and service stations were "bold examples of modernism"—among those submitting designs was Mies van der Rohe.
Architectural output.
The paradox of the early Bauhaus was that, although its manifesto proclaimed that the ultimate aim of all creative activity was building, the school did not offer classes in architecture until 1927. During the years under Gropius (1919–1927), he and his partner Adolf Meyer observed no real distinction between the output of his architectural office and the school. So the built output of Bauhaus architecture in these years is the output of Gropius: the Sommerfeld house in Berlin, the Otte house in Berlin, the Auerbach house in Jena, and the competition design for the Chicago Tribune Tower, which brought the school much attention. The definitive 1926 Bauhaus building in Dessau is also attributed to Gropius. Apart from contributions to the 1923 Haus am Horn, student architectural work amounted to un-built projects, interior finishes, and craft work like cabinets, chairs and pottery.
In the next two years under Meyer, the architectural focus shifted away from aesthetics and towards functionality. There were major commissions: one from the city of Dessau for five tightly designed "Laubenganghäuser" (apartment buildings with balcony access), which are still in use today, and another for the headquarters of the Federal School of the German Trade Unions (ADGB) in Bernau bei Berlin. Meyer's approach was to research users' needs and scientifically develop the design solution.
Mies van der Rohe repudiated Meyer's politics, his supporters, and his architectural approach. As opposed to Gropius's "study of essentials", and Meyer's research into user requirements, Mies advocated a "spatial implementation of intellectual decisions", which effectively meant an adoption of his own aesthetics. Neither van der Rohe nor his Bauhaus students saw any projects built during the 1930s.
The popular conception of the Bauhaus as the source of extensive Weimar-era working housing is not accurate. Two projects, the apartment building project in Dessau and the Törten row housing also in Dessau, fall in that category, but developing worker housing was not the first priority of Gropius nor Mies. It was the Bauhaus contemporaries Bruno Taut, Hans Poelzig and particularly Ernst May, as the city architects of Berlin, Dresden and Frankfurt respectively, who are rightfully credited with the thousands of socially progressive housing units built in Weimar Germany. The housing Taut built in south-west Berlin during the 1920s, close to the U-Bahn stop Onkel Toms Hütte, is still occupied.
Impact.
The Bauhaus had a major impact on art and architecture trends in Western Europe, the United States, Canada and Israel in the decades following its demise, as many of the artists involved fled, or were exiled by, the Nazi regime. Tel Aviv in 2004 was named to the list of world heritage sites by the UN due to its abundance of Bauhaus architecture; it had some 4,000 Bauhaus buildings erected from 1933 on.
In 1928, the Hungarian painter Alexander Bortnyik founded a school of design in Budapest called Miihely (also "Muhely" or "Mugely"), which means "the studio". Located on the seventh floor of a house on Nagymezo Street, it was meant to be the Hungarian equivalent to the Bauhaus. The literature sometimes refers to it—in an oversimplified manner—as "the Budapest Bauhaus". Bortnyik was a great admirer of Moholy-Nagy and had met Walter Gropius in Weimar between 1923 and 1925. Moholy-Nagy himself taught at the Miihely. Victor Vasarely, a pioneer of Op Art, studied at this school before establishing in Paris in 1930.
Walter Gropius, Marcel Breuer, and László Moholy-Nagy re-assembled in Britain during the mid 1930s to live and work in the Isokon project before the war caught up with them. Both Gropius and Breuer went to teach at the Harvard Graduate School of Design and worked together before their professional split. Their collaboration produced The Aluminum City Terrace in New Kensington, Pennsylvania and the Alan I W Frank House in Pittsburgh, among other projects. The Harvard School was enormously influential in America in the late 1920s and early 1930s, producing such students as Philip Johnson, I.M. Pei, Lawrence Halprin and Paul Rudolph, among many others.
In the late 1930s, Mies van der Rohe re-settled in Chicago, enjoyed the sponsorship of the influential Philip Johnson, and became one of the pre-eminent architects in the world. Moholy-Nagy also went to Chicago and founded the New Bauhaus school under the sponsorship of industrialist and philanthropist Walter Paepcke. This school became the Institute of Design, part of the Illinois Institute of Technology. Printmaker and painter Werner Drewes was also largely responsible for bringing the Bauhaus aesthetic to America and taught at both Columbia University and Washington University in St. Louis. Herbert Bayer, sponsored by Paepcke, moved to Aspen, Colorado in support of Paepcke's Aspen projects at the Aspen Institute. In 1953, Max Bill, together with Inge Aicher-Scholl and Otl Aicher, founded the Ulm School of Design (German: Hochschule für Gestaltung—HfG Ulm) in Ulm, Germany, a design school in the tradition of the Bauhaus. The school is notable for its inclusion of semiotics as a field of study. The school closed in 1968, but the "Ulm Model" concept continues to influence international design education.
The influence of the Bauhaus on design education was significant. One of the main objectives of the Bauhaus was to unify art, craft, and technology, and this approach was incorporated into the curriculum of the Bauhaus. The structure of the Bauhaus "Vorkurs" (preliminary course) reflected a pragmatic approach to integrating theory and application. In their first year, students learnt the basic elements and principles of design and colour theory, and experimented with a range of materials and processes. This approach to design education became a common feature of architectural and design school in many countries. For example, the Shillito Design School in Sydney stands as a unique link between Australia and the Bauhaus. The colour and design syllabus of the Shillito Design School was firmly underpinned by the theories and ideologies of the Bauhaus. Its first year foundational course mimicked the "Vorkurs" and focused on the elements and principles of design plus colour theory and application. The founder of the school, Phyllis Shillito, which opened in 1962 and closed in 1980, firmly believed that "A student who has mastered the basic principles of design, can design anything from a dress to a kitchen stove".
One of the most important contributions of the Bauhaus is in the field of modern furniture design. The ubiquitous Cantilever chair and the Wassily Chair designed by Marcel Breuer are two examples. (Breuer eventually lost a legal battle in Germany with Dutch architect/designer Mart Stam over the rights to the cantilever chair patent. Although Stam had worked on the design of the Bauhaus's 1923 exhibit in Weimar, and guest-lectured at the Bauhaus later in the 1920s, he was not formally associated with the school, and he and Breuer had worked independently on the cantilever concept, thus leading to the patent dispute.) The single most profitable tangible product of the Bauhaus was its wallpaper.
The physical plant at Dessau survived World War II and was operated as a design school with some architectural facilities by the German Democratic Republic. This included live stage productions in the Bauhaus theater under the name of "Bauhausbühne" ("Bauhaus Stage"). After German reunification, a reorganized school continued in the same building, with no essential continuity with the Bauhaus under Gropius in the early 1920s. In 1979 Bauhaus-Dessau College started to organize postgraduate programs with participants from all over the world. This effort has been supported by the Bauhaus-Dessau Foundation which was founded in 1974 as a public institution.
Later evaluation of the Bauhaus design credo was critical of its flawed recognition of the human element, an acknowledgement of "… the dated, unattractive aspects of the Bauhaus as a projection of utopia marked by mechanistic views of human nature…Home hygiene without home atmosphere."
The White City.
The White City of Tel Aviv (Hebrew: העיר הלבנה‎, "Ha-Ir HaLevana") refers to a collection of over 4,000 Bauhaus or International style buildings built in Tel Aviv from the 1930s by German Jewish architects who emigrated to the British Mandate of Palestine after the rise of the Nazis. Tel Aviv has the largest number of buildings in this style of any city in the world. In 2003, the United Nations Educational, Scientific and Cultural Organization (UNESCO) proclaimed Tel Aviv's "White City" a World Cultural Heritage site, as "an outstanding example of new town planning and architecture in the early 20th century."
Established in 2000, The Bauhaus Center in Tel Aviv is an organization dedicated to the ongoing documentation of the architectural heritage. In 2003, it hosted an exhibition on preservation of the architecture that showcased 25 buildings. To further the architectural culture in the city, a Bauhaus Museum opened in Tel Aviv in 2008, designed by Israeli architect Ron Arad.
Bauhaus artists.
The Bauhaus was not a formal group, but rather a school. Its three architect-directors (Walter Gropius, Hannes Meyer, and Ludwig Mies van der Rohe) are the names most closely associated with it.
Furthermore a large number of outstanding artists of their time were lecturers at the Bauhaus:

</doc>
<doc id="3833" url="http://en.wikipedia.org/wiki?curid=3833" title="Beowulf">
Beowulf

Beowulf (; in Old English ]) is an Old English epic poem consisting of 3182 alliterative long lines. It is possibly the oldest surviving long poem in Old English and is commonly cited as one of the most important works of Old English literature. It was written in England some time between the 8th and the early 11th century. The author was an anonymous Anglo-Saxon poet, referred to by scholars as the ""Beowulf" poet".
The poem is set in Scandinavia. Beowulf, a hero of the Geats, comes to the aid of Hroðgar, the king of the Danes, whose mead hall in Heorot has been under attack by a monster known as Grendel. After Beowulf slays him, Grendel's mother attacks the hall and is then also defeated. Victorious, Beowulf goes home to Geatland (Götaland in modern Sweden) and later becomes king of the Geats. After a period of fifty years has passed, Beowulf defeats a dragon, but is fatally wounded in the battle. After his death, his attendants bury him in a tumulus, a burial mound, in Geatland.
The full poem survives in the manuscript known as the Nowell Codex, located in the British Library. It has no title in the original manuscript, but has become known by the name of the story's protagonist. In 1731, the manuscript was badly damaged by a fire that swept through Ashburnham House in London that had a collection of medieval manuscripts assembled by Sir Robert Bruce Cotton. The poem was not studied until the end of the 18th century, and not published in its entirety until Johan Bülow funded the 1815 Latin translation, prepared by the Icelandic-Danish scholar Grímur Jónsson Thorkelin. After a heated debate with Thorkelin, Bülow offered to support a new translation into Danish by N. F. S. Grundtvig. The result, Bjovulfs Drape (1820), was the first modern language translation of "Beowulf".
Historical background.
The events described in the poem take place in the late 5th century, after the Angles and Saxons had begun their migration to England, and before the beginning of the 7th century, a time when the Anglo-Saxon people were either newly arrived or still in close contact with their Germanic kinsmen in Northern Germany and Scandinavia and possibly England. The poem may have been brought to England by people of Geatish origins. It has been suggested that "Beowulf" was first composed in the 7th century at Rendlesham in East Anglia, as the Sutton Hoo ship-burial also shows close connections with Scandinavia, and also that the East Anglian royal dynasty, the Wuffings, may have been descendants of Geatish Wulfings. Others have associated this poem with the court of King Alfred, or with the court of King Cnut.
The poem deals with legends, was composed for entertainment, and does not separate between fictional elements and real historic events, such as the raid by King Hygelac into Frisia. Scholars generally agree that many of the personalities of "Beowulf" also appear in Scandinavian sources (specific works designated in the following section). This does not only concern people (e.g., Healfdene, Hroðgar, Halga, Hroðulf, Eadgils and Ohthere), but also clans (e.g., Scyldings, Scylfings and Wulfings) and some of the events (e.g., the Battle on the Ice of Lake Vänern). The dating of the events in the poem has been confirmed by archaeological excavations of the barrows indicated by Snorri Sturluson and by Swedish tradition as the graves of Ohthere (dated to c. 530) and his son Eadgils (dated to c. 575) in Uppland, Sweden.
In Denmark, recent archaeological excavations at Lejre, where Scandinavian tradition located the seat of the Scyldings, i.e., Heorot, have revealed that a hall was built in the mid-6th century, exactly the time period of "Beowulf". Three halls, each about 50 metres (164 feet) long, were found during the excavation.
The majority view appears to be that people such as King Hroðgar and the Scyldings in "Beowulf" are based on real historical people from 6th-century Scandinavia. Like the "Finnesburg Fragment" and several shorter surviving poems, "Beowulf" has consequently been used as a source of information about Scandinavian personalities such as Eadgils and Hygelac, and about continental Germanic personalities such as Offa, king of the continental Angles.
19th-century archeological evidence may confirm elements of the "Beowulf" story. Eadgils was buried at Uppsala, according to Snorri Sturluson. When Eadgils' mound (to the left in the photo) was excavated in 1874, the finds supported "Beowulf" and the sagas. They showed that a powerful man was buried in a large barrow, c 575, on a bear skin with two dogs and rich grave offerings. These remains include a Frankish sword adorned with gold and garnets and a tafl game with Roman pawns of ivory. He was dressed in a costly suit made of Frankish cloth with golden threads, and he wore a belt with a costly buckle. There were four cameos from the Middle East which were probably part of a casket. This would have been a burial fitting a king who was famous for his wealth in Old Norse sources. Ongenþeow's barrow (to the right in the photo) has not been excavated.
Summary.
The main protagonist Beowulf, a hero of the Geats, comes to the aid of Hrothgar, the king of the Danes, whose great hall, Heorot, is plagued by the monster Grendel. Beowulf kills Grendel with his bare hands and Grendel's mother with a sword of a giant that he found in her lair.
Later in his life, Beowulf is himself king of the Geats, and finds his realm terrorised by a dragon whose treasure had been stolen from his hoard in a burial mound. He attacks the dragon with the help of his "thegns" or servants, but they do not succeed. Beowulf decides to follow the dragon into its lair, at Earnanæs, but only his young Swedish relative Wiglaf, whose name means "remnant of valor", dares join him. Beowulf finally slays the dragon, but is mortally wounded. He is buried in a tumulus or burial mound, by the sea.
"Beowulf" is considered an epic poem in that the main character is a hero who travels great distances to prove his strength at impossible odds against supernatural demons and beasts. The poem also begins in medias res ("into the middle of affairs") or simply, "in the middle of things", which is a characteristic of the epics of antiquity. Although the poem begins with Beowulf's arrival, Grendel's attacks have been an ongoing event. An elaborate history of characters and their lineages is spoken of, as well as their interactions with each other, debts owed and repaid, and deeds of valor. The warriors form a kind of brotherhood called a "comitatus", which seems to have formed an ethical basis for all words, deeds, and actions.
First battle: Grendel.
"Beowulf" begins with the story of King Hrothgar, who constructed the great hall Heorot for his people. In it he, his wife Wealhtheow, and his warriors spend their time singing and celebrating. Grendel, a troll-like monster descended from the biblical Cain, is pained by the noise, attacks the hall, and kills and devours many of Hrothgar's warriors while they sleep. Hrothgar and his people, helpless against Grendel, abandon Heorot.
Beowulf, a young warrior from Geatland, hears of Hrothgar's troubles and with his king's permission leaves his homeland to help Hroðgar.
Beowulf and his men spend the night in Heorot. Beowulf refuses to use any weapon because he holds himself to be the equal of Grendel. During the battle, Beowulf has been feigning sleep and leaps up to clench Grendel's hand. The two battle until it seems as though the hall might collapse. Beowulf's retainers draw their swords and rush to his aid, but their blades cannot pierce Grendel's skin. Finally, Beowulf tears Grendel's arm from his body at the shoulder and Grendel runs to his home in the marshes and slowly dies.
Second battle: Grendel's Mother.
The next night, after celebrating Grendel's defeat, Hrothgar and his men sleep in Heorot. Grendel's mother, angered by the punishment of her son, appears and attacks the hall. She kills Hrothgar's most trusted warrior, Æschere, in revenge for Grendel's defeat.
Hrothgar, Beowulf and their men track Grendel's mother to her lair under a lake. Beowulf prepares himself for battle. He is presented with a sword, Hrunting, by Unferth, a warrior who had doubted him and wishes to make amends. After stipulating a number of conditions to Hrothgar in case of his death (including the taking in of his kinsmen and the inheritance by Unferth of Beowulf's estate), Beowulf dives into the lake. He is swiftly detected and attacked by Grendel's mother. However, she is unable to harm Beowulf through his armor and drags him to the bottom of the lake. In a cavern containing Grendel's body and the remains of men that the two have killed, Grendel's mother and Beowulf engage in fierce combat.
At first, Grendel's mother appears to prevail. Beowulf, finding that Hrunting cannot harm his foe, discards it in fury. Beowulf is again saved from his opponent's attack by his armour. Beowulf grabs a magical sword from Grendel's mother's treasure and with it beheads her. Traveling further into the lair, Beowulf discovers Grendel's dying body and severs its head. The blade of the magic sword melts like ice when it touches Grendel's toxic blood, until only the hilt is left. Beowulf carries this hilt and the head of Grendel out of the cavern and presents them to Hrothgar upon his return to Heorot. Beowulf then returns to the surface and to his men at the "ninth hour" (about 3 pm). He returns to Heorot, where Hrothgar gives Beowulf many gifts, including (possibly) the sword Nægling, his family's heirloom. The hilt prompts a long reflection by the king, sometimes referred to as "Hrothgar's sermon", in which he urges Beowulf to be wary of pride and to reward his thanes.
Third battle: The Dragon.
Beowulf returns home and eventually becomes king of his own people. One day, fifty years after Beowulf's battle with Grendel's mother, a slave steals a golden cup from the lair of an unnamed dragon at Earnaness. When the dragon sees that the cup has been stolen, it leaves its cave in a rage, burning everything in sight. Beowulf and his warriors come to fight the dragon, but Beowulf tells his men that he will fight the dragon alone and that they should wait on the barrow. Beowulf descends to do battle with the dragon but finds himself outmatched. His men, upon seeing this display and fearing for their lives, creep back into the woods. One of his men, however, Siglaf, who finds great distress in seeing Beowulf's plight, comes to Beowulf's aid. The two slay the dragon, but Beowulf is mortally wounded. After Beowulf's death, he is ritually burned on a great pyre in Geatland while his people wail and mourn him. After, a barrow, visible from the sea, is built on his remains ("Beowulf" lines 2712–3182).
Authorship and date.
"Beowulf" was written in England, but is set in Scandinavia; its dating has attracted considerable scholarly attention. The poem has been dated to between the 8th and the early 11th centuries, with some recent scholarship offering what one reviewer called "a cohesive and compelling case for Beowulf’s early composition." Although its author is unknown, its themes and subject matter are rooted in the Old English poetic tradition.
Opinion differs as to whether the composition of the poem is contemporary with its transcription, or whether the poem was composed at an earlier time (possibly as one of the Bear's Son Tales) and orally transmitted for many years, and then transcribed at a later date. Lord felt strongly the manuscript represents the transcription of a performance, though likely taken at more than one sitting. Kiernan argues on the basis of evidence from paleography and codicology that the poem is contemporary with the manuscript. Kiernan's reasoning has in part to do with the political context of the poem: most scholars have held that the poem was composed in the 8th century, on the assumption that a poem eliciting sympathy for the Danes could not have been composed by Anglo-Saxons during the Viking Age of the 9th and 10th centuries. The poem begins with a tribute to the royal line of Danish kings, but is written in the dominant literary dialect of Anglo-Saxon England, which for some scholars points to the 11th century reign of Cnut (the Danish king whose empire included all of these areas, and whose primary place of residence was in England) as the most likely time of the poem's creation.
The view of J. R. R. Tolkien was that the poem retains too genuine a memory of Anglo-Saxon paganism to have been composed more than a few generations after the completion of the Christianisation of England around AD 700. Tolkien's conviction that the poem dates to the 8th century is defended by Tom Shippey.
The claim to an 11th-century date is due to scholars who argue that, rather than the transcription of a tale from the oral tradition by an earlier literate monk, "Beowulf" reflects an original interpretation of the story by the manuscript's two scribes. However, some scholars argue that linguistic, paleographical, and onomastic considerations align to support a date of composition in the first half of the eighth century; in particular, the poem's regular observation of etymological length distinctions (Kaluza's law) has been thought to suggest a date of composition in the first half of the eighth century. However, scholars disagree about whether the metrical phenomena described by Kaluza's law reflect an early date of composition or correspond to a longer prehistory of the Beowulf meter; B.R. Hutcheson, for instance, does not believe Kaluza's Law can be used to date the poem, while opining that "the weight of all the evidence Fulk presents in his book tells strongly in favor of an eighth-century date."
Manuscript.
"Beowulf" survives in a single manuscript dated on paleographical grounds to the late 10th or early 11th century. The manuscript measures 245 × 185 mm.
Provenance.
The earliest known owner of the "Beowulf" manuscript, the 16th-century scholar Laurence Nowell, lends his name to the manuscript (Nowell Codex), though its official designation is "British Library, Cotton Vitellius A.XV" because it was one of Sir Robert Bruce Cotton's holdings in the Cotton library in the middle of the 17th century. Many private antiquarians and book collectors, such as Sir Robert Cotton, used their own library classification systems. "Cotton Vitellius A.XV" translates as: the 15th book from the left on shelf A (the top shelf) of the bookcase with the bust of Roman Emperor Vitellius standing on top of it, in Cotton's collection. Kevin Kiernan argues that Nowell most likely acquired it through William Cecil, 1st Baron Burghley, in 1563, when Nowell entered Cecil's household as a tutor to his ward, Edward de Vere, 17th Earl of Oxford.
It suffered damage in the Cotton Library fire at Ashburnham House in 1731. Since then, parts of the manuscript have crumbled along with many of the letters. Rebinding efforts, though saving the manuscript from much degeneration, have nonetheless covered up other letters of the poem, causing further loss. Kevin Kiernan, in preparing his electronic edition of the manuscript, used fibre-optic backlighting and ultraviolet lighting to reveal letters in the manuscript lost from binding, erasure, or ink blotting.
The poem is known only from this single manuscript, which is estimated to date from close to AD 1000. Kiernan has argued from an examination of the manuscript that it was the author's own working copy. He dated the work to the reign of Cnut the Great (1016–35). The poem appears in what is today called the "Beowulf" manuscript or Nowell Codex (British Library MS Cotton Vitellius A.xv), along with other works. The earliest extant reference to the first foliation of the Nowell Codex was made sometime between 1628 and 1650 by Franciscus Junius (the younger). The ownership of the codex before Nowell remains a mystery.
The Reverend Thomas Smith (1638–1710) and Humfrey Wanley (1672–1726) both catalogued the Cotton library (in which the Nowell Codex was held). Smith's catalogue appeared in 1696, and Wanley's in 1705. The "Beowulf" manuscript itself is identified by name for the first time in an exchange of letters in 1700 between George Hickes, Wanley's assistant, and Wanley. In the letter to Wanley, Hickes responds to an apparent charge against Smith, made by Wanley, that Smith had failed to mention the "Beowulf" script when cataloguing Cotton MS. Vitellius A. XV. Hickes replies to Wanley "I can find nothing yet of Beowulph." Kiernan theorised that Smith failed to mention the "Beowulf" manuscript because of his reliance on previous catalogues or because either he had no idea how to describe it or because it was temporarily out of the codex.
Writing.
The "Beowulf" manuscript was transcribed from an original by two scribes, one of whom wrote the first 1939 lines and a second who wrote the remainder, with a difference in handwriting noticeable after line 1939. The script of the second scribe is archaic. While both scribes appear to proofread their work, there are nevertheless many errors. The second scribe slaved over the poem for many years "with great reverence and care to restoration". The work of the second scribe bears a striking resemblance to the work of the first scribe of the Blickling homilies, and so much so that it is believed they derive from the same scriptorium. From knowledge of books held in the library at Malmesbury Abbey and available as source works, and from the identification of certain words particular to the local dialect found in the text, the transcription may have been made there. However, for at least a century, some scholars have maintained that the description of Grendel's lake in "Beowulf" was borrowed from St. Paul's vision of Hell in Homily 16 of the Blickling homilies. Most intriguing in the many versions of the "Beowulf" MS is the transcription of alliterative verse. From the first scribe's edits, emenders such as Klaeber were forced to alter words for the sake of the poem.
Transcriptions.
Icelandic scholar Grímur Jónsson Thorkelin made the first transcriptions of the manuscript in 1786 and published the results in 1815, working as part of a Danish government historical research commission. He made one himself, and had another done by a professional copyist who knew no Anglo-Saxon. Since that time, however, the manuscript has crumbled further, making these transcripts a prized witness to the text. While the recovery of at least 2000 letters can be attributed to them, their accuracy has been called into question, and the extent to which the manuscript was actually more readable in Thorkelin's time is uncertain.
Translations.
In 1805, the historian Sharon Turner translated selected verses into modern English. This was followed in 1814 by John Josias Conybeare who published an edition "in English paraphrase and Latin verse translation." In 1815, Grímur Jónsson Thorkelin published the first complete edition in Latin. N. F. S. Grundtvig reviewed this edition in 1815 and created the first complete verse translation in Danish in 1820. In 1837, J. M. Kemble created an important literal translation in English. In 1895, William Morris & A. J. Wyatt published the ninth English translation. In 1909, Francis Barton Gummere's full translation in "English imitative meter" was published, and was used as the text of Gareth Hinds's graphic novel based on "Beowulf" in 2007.
During the early 20th century, Frederick Klaeber's "Beowulf and The Fight at Finnsburg" (which included the poem in Old English, an extensive glossary of Old English terms, and general background information) became the "central source used by graduate students for the study of the poem and by scholars and teachers as the basis of their translations."
A great number of translations are available, in poetry and prose. Andy Orchard, in "A Critical Companion to Beowulf", lists 33 "representative" translations in his bibliography, and it has been translated into at least 23 other languages.
Of particular importance is Seamus Heaney's 1999 translation of the poem (referred to by Howell Chickering and many others as "Heaneywulf") which is included in "The Norton Anthology of English Literature" since the seventh edition, ensuring "a dominant position of Beowulf in the college classroom". Translating "Beowulf" is one of the subjects of the 2012 publication "Beowulf at Kalamazoo", containing a section with 10 essays on translation, and a section with 22 reviews of Heaney's translation (some of which compare Heaney's with that by Anglo-Saxon scholar Roy Liuzza). R. D. Fulk, of Indiana University, published the first facing-page edition and translation of the entire manuscript in the Dumbarton Oaks Medieval Library series in 2010.
J. R. R. Tolkien's long-awaited translation (edited by his son Christopher) was published in 2014 ("").
Debate over oral tradition.
The question of whether "Beowulf" was passed down through oral tradition prior to its present manuscript form has been the subject of much debate, and involves more than the mere matter of how it was composed. Rather, given the implications of the theory of oral-formulaic composition and oral tradition, the question concerns how the poem is to be understood, and what sorts of interpretations are legitimate.
Scholarly discussion about "Beowulf" in the context of the oral tradition was extremely active throughout the 1960s and 1970s. The debate might be framed starkly as follows: on the one hand, we can hypothesise a poem put together from various tales concerning the hero (the Grendel episode, the Grendel's mother story, and the firedrake narrative). These fragments would be held for many years in tradition, and learned by apprenticeship from one generation of illiterate poets to the next. The poem is composed orally and extemporaneously, and the archive of tradition on which it draws is oral, pagan, Germanic, heroic, and tribal. On the other hand, one might posit a poem which is composed by a literate scribe, who acquired literacy by way of learning Latin (and absorbing Latinate culture and ways of thinking), probably a monk and therefore profoundly Christian in outlook. On this view, the pagan references would be a sort of decorative archaising. There is a third view that sees merit in both arguments above and attempts to bridge them, and so cannot be articulated as starkly as they can; it sees more than one Christianity and more than one attitude towards paganism at work in the poem, separated from each other by hundreds of years; it sees the poem as originally the product of a literate Christian author with one foot in the pagan world and one in the Christian, himself a convert perhaps or one whose forbears had been pagan, a poet who was conversant in both oral and literary milieus and was capable of a masterful "repurposing" of poetry from the oral tradition; this early Christian poet saw virtue manifest in a willingness to sacrifice oneself in a devotion to justice and in an attempt to aid and protect those in need of help and greater safety; good pagan men had trodden that noble path and so this poet presents pagan culture with equanimity and respect; yet overlaid upon this early Christian poet's composition are verses from a much later reformist "fire-and-brimstone" Christian poet who vilifies pagan practice as dark and sinful and who adds satanic aspects to its monsters.
However, scholars such as DK Crowne have proposed the idea that the poem was passed down from reciter to reciter under the theory of oral-formulaic composition, which hypothesises that epic poems were (at least to some extent) improvised by whoever was reciting them. In his landmark work, "The Singer of Tales", Albert Lord refers to the work of Francis P. Magoun and others, saying "the documentation is complete, thorough, and accurate. This exhaustive analysis is in itself sufficient to prove that Beowulf was composed orally."
Examination of "Beowulf" and other Anglo-Saxon poetry for evidence of oral-formulaic composition has met with mixed response. While "themes" (inherited narrative subunits for representing familiar classes of event, such as the "arming the hero", or the particularly well-studied "hero on the beach" theme) do exist across Anglo-Saxon and other Germanic works, some scholars conclude that Anglo-Saxon poetry is a mix of oral-formulaic and literate patterns, arguing that the poems both were composed on a word-by-word basis and followed larger formulae and patterns.
Larry Benson argued that the interpretation of "Beowulf" as an entirely formulaic work diminishes the ability of the reader to analyze the poem in a unified manner, and with due attention to the poet's creativity. Instead, he proposed that other pieces of Germanic literature contain "kernels of tradition" from which "Beowulf" borrows and expands upon. A few years later, Ann Watts published a book in which she argued against the imperfect application of traditional, Homeric, oral-formulaic theory to Anglo-Saxon poetry. She also argued that the two traditions are not comparable and should not be regarded as such. Thomas Gardner agreed with Watts, in a paper published four years later which argued that the "Beowulf" text is of too varied a nature to be completely constructed from formulae and themes.
John Miles Foley held, specifically with reference to the "Beowulf" debate, that while comparative work was both necessary and valid, it must be conducted with a view to the particularities of a given tradition; Foley argued with a view to developments of oral traditional theory that do not assume, or depend upon, finally unverifiable assumptions about composition, and that discard the oral/literate dichotomy focused on composition in favor of a more fluid continuum of traditionality and textuality.
Finally, in the view of Ursula Schaefer, the question of whether the poem was "oral" or "literate" becomes something of a red herring. In this model, the poem is created, and is interpretable, within both noetic horizons. Schaefer's concept of "vocality" offers neither a compromise nor a synthesis of the views which see the poem as on the one hand Germanic, pagan, and oral and on the other Latin-derived, Christian, and literate, but, as stated by Monika Otter: "... a 'tertium quid', a modality that participates in both oral and literate culture yet also has a logic and aesthetic of its own."
Sources and analogues.
Neither identified sources nor analogues for "Beowulf" can be definitively proven, but many conjectures have been made. These are important in helping historians understand the "Beowulf" manuscript, as possible source-texts or influences would suggest time-frames of composition, geographic boundaries within which it could be composed, or range (both spatial and temporal) of influence (i.e. when it was "popular" and where its "popularity" took it). There are five main categories in which potential sources and/or analogues are included: Scandinavian parallels, classical sources, Irish sources and analogues, ecclesiastical sources, and echoes in other Old English texts.
Early studies into Scandinavian sources and analogues proposed that "Beowulf" was a translation of an original Scandinavian work, but this idea has been discarded. In 1878, Guðbrandur Vigfússon made the connection between "Beowulf" and the "Grettis saga". This is currently one of the few Scandinavian analogues to receive a general consensus of potential connection. Tales concerning the Skjöldungs, possibly originating as early as the 6th century were later used as a narrative basis in such texts as "Gesta Danorum" by Saxo Grammaticus and "Hrólfs saga kraka". Some scholars see "Beowulf" as a product of these early tales along with "Gesta Danorum" and "Hrólfs saga kraka", and some early scholars of the poem proposed that the latter saga and "Beowulf" share a common legendary ancestry, "Beowulf"‍ '​s Hrothulf being identified with Hrólf Kraki ancestry. Paul Beekman Taylor argued that the "Ynglingasaga" was proof that the "Beowulf" poet was likewise working from Germanic tradition.
Friedrich Panzer attempted to contextualise "Beowulf" and other Scandinavian works, including "Grettis saga", under the international folktale type 301B, or "The Bear's Son" tale. However, although this folkloristic approach was seen as a step in the right direction, "The Bear's Son" tale was seen as too universal. In a term coined by Peter Jørgensen (the "two-troll tradition"), a more concise frame of reference was found. The "two-troll tradition" refers to "a Norse 'ecotype' in which a hero enters a cave and kills two giants, usually of different sexes." Both "Grettis saga" and "Beowulf" fit this folktale type.
Scholars who favored Irish parallels directly spoke out against pro-Scandinavian theories, citing them as unjustified. Wilhelm Grimm is noted to be the first person to link "Beowulf" with Irish folklore. Max Deutschbein, however, the first person to present the argument in academic form. He suggested the Irish "Feast of Bricriu" as a source for "Beowulf"—a theory that was soon denied by Oscar Olson. Swedish folklorist Carl Wilhelm Von Sydow argued against both Scandinavian translation and source material due to his theory that "Beowulf" is fundamentally Christian and written at a time when any Norse tale would have most likely been pagan.
In the late 1920s, Heinzer Dehmer suggested "Beowulf" as contextually based in the folktale type "The Hand and the Child," due to the motif of the "monstrous arm"—a motif that distances "Grettis saga" and "Beowulf" and further aligns "Beowulf" with Irish parallelism. James Carney and Martin Puhvel also agree with this "Hand and the Child" contextualisation. Carney also ties "Beowulf" to Irish literature through the "Táin Bó" "Fráech" story. Puhvel supported the "Hand and the Child" theory through such motifs as "the more powerful giant mother, the mysterious light in the cave, the melting of the sword in blood, the phenomenon of battle rage, swimming prowess, combat with water monsters, underwater adventures, and the bear-hug style of wrestling."
Attempts to find classical or Late Latin influence or analogue in "Beowulf" are almost exclusively linked with Homer's "Odyssey" or Virgil's "Aeneid". In 1926, Albert S. Cook suggested a Homeric connection due to equivalent formulas, metonymies, and analogous voyages. James A. Work's essay "Odyssean Influence on the "Beowulf"" also supported the Homeric influence. He stated that encounter between Beowulf and Unferth was parallel to the encounter between Odysseus and Euryalus in Books 7–8 of the "Odyssey" even to the point of them both giving the hero the same gift of a sword upon being proven wrong in their initial assessment of the hero's prowess. This theory of Homer's influence on "Beowulf" remained very prevalent in the 1920s, but started to die out in the following decade when a handful of critics stated that the two works were merely "comparative literature" although Greek was known in contemporary England. Bede states that Theodore, a Greek, was appointed Archbishop of Canterbury in 668, and he taught Greek. Several English scholars and churchmen are described by Bede as being fluent in Greek due to being taught by him; Bede claims to be fluent in Greek himself.
Friedrich Klaeber somewhat led the attempt to connect "Beowulf" and Virgil near the start of the 20th century, claiming that the very act of writing a secular epic in a Germanic world is contingent on Virgil. Virgil was seen as the pinnacle of Latin literature, and Latin was the dominant literary language of England at the time, therefore making Virgilian influence highly likely. Similarly, in 1971, Alistair Campbell stated that the apologue technique used in "Beowulf" is so infrequent in the epic tradition aside from when Virgil uses it that the poet who composed "Beowulf" could not have written the poem in such a manner without first coming across Virgil's writings.
Whether seen as a pagan work with "Christian colouring" added by scribes or as a "Christian historical novel, with selected bits of paganism deliberately laid on as 'local colour'," as Margaret E. Goldsmith did in 'The Christian Theme of "Beowulf",;" it cannot be denied that Biblical parallels occur in the text. "Beowulf" channels "Genesis", "Exodus", and "Daniel" in its inclusion of references to God's creation of the universe, the story of Cain, Noah and the flood, devils or the Devil, Hell, and the Last Judgment.
Dialect.
The poem mixes the West Saxon and Anglian dialects of Old English, though it predominantly uses West Saxon, as do other Old English poems copied at the time.
There is a wide array of linguistic forms in the "Beowulf" manuscript. It is this fact that leads some scholars to believe that "Beowulf" has endured a long and complicated transmission through all the main dialect areas. The poem retains a complicated mix of the following dialectical forms: Mercian, Northumbrian, Early West Saxon, Kentish and Late West Saxon. Kiernan argues that it is virtually impossible that there could have been a process of transmission which could have sustained the complicated mix of forms from dialect to dialect, from generation to generation, and from scribe to scribe.
Kiernan's argument against an early dating based on a mixture of forms is long and involved, but he concludes that the mixture of forms points to a comparatively straightforward history of the written text as:
...an 11th-century MS; an 11th-century Mercian poet using an archaic poetic dialect; and 11th-century standard literary dialect that contained early and late, cross-dialectical forms, and admitted spelling variations; and (perhaps) two 11th-century scribes following slightly different spelling practices.
According to this view, "Beowulf" can largely be seen to be the product of antiquarian interests and that it tells readers more about "an 11th-century Anglo-Saxon's notions about Denmark, and its pre-history, than it does about the age of Bede and a 7th- or 8th-century Anglo-Saxon's notions about his ancestors' homeland." There are in "Beowulf" rather more than thirty-one hundred distinct words, and almost thirteen hundred occur exclusively, or almost exclusively, in this poem and in the other poetical texts. Considerably more than one-third of the total vocabulary is alien from ordinary prose use. There are in round numbers three hundred and sixty uncompounded verbs in "Beowulf", and forty of them are poetical words in the sense that they are unrecorded or rare in the existing prose writings. One hundred and fifty more occur with the prefix "ge"- (reckoning a few found only in the past-participle), but of these one hundred occur also as simple verbs, and the prefix is employed to render a shade of meaning which was perfectly known and thoroughly familiar except in the latest Anglo-Saxon period. The nouns number sixteen hundred. Seven hundred of them, including those formed with prefixes, of which fifty (or considerably more than half) have "ge"-, are simple nouns. at the highest reckoning not more than one-fourth is absent in prose. That this is due in some degree to accident is clear from the character of the words, and from the fact that several reappear and are common after the Norman Conquest.
Form and metre.
An Old English poem such as "Beowulf" is very different from modern poetry. Anglo-Saxon poets typically used alliterative verse, a form of verse in which the first half of the line (the a-verse) is linked to the second half (the b-verse) through similarity in initial sound. In addition, the two halves are divided by a caesura: "Oft Scyld Scefing \\ sceaþena þreatum" (l. 4). This verse form maps stressed and unstressed syllables onto abstract entities known as metrical positions. There is no fixed number of beats per line: the first one cited has three (Oft SCYLD SCEFING, with ictus on the suffix -ING) whereas the second has two (SCEAþena ÞREATum).
The poet has a choice of epithets or formulae to use in order to fulfill the alliteration. When speaking or reading Old English poetry, it is important to remember for alliterative purposes that many of the letters are not pronounced the same way as they are in modern English. The letter "h", for example, is always pronounced (Hroðgar: HROTH-gar), and the digraph "cg" is pronounced like "dj", as in the word "edge". Both f and s vary in pronunciation depending on their phonetic environment. Between vowels or voiced consonants, they are voiced, sounding like modern v and z, respectively. Otherwise they are unvoiced, like modern f in "fat" and s in "sat". Some letters which are no longer found in modern English, such as thorn, þ, and eth, ð – representing both pronunciations of modern English "th", as in "thing" and "this" – are used extensively both in the original manuscript and in modern English editions. The voicing of these characters echoes that of f and s. Both are voiced (as in "this") between other voiced sounds: oðer, laþleas, suþern. Otherwise they are unvoiced (as in "thing"): þunor, suð, soþfæst.
Kennings are also a significant technique in "Beowulf". They are evocative poetic descriptions of everyday things, often created to fill the alliterative requirements of the metre. For example, a poet might call the sea the "swan-road" or the "whale-road"; a king might be called a "ring-giver." There are many kennings in "Beowulf", and the device is typical of much of classic poetry in Old English, which is heavily formulaic. The poem also makes extensive use of elided metaphors.
J. R. R. Tolkien argued that the poem is an elegy.
Interpretation and criticism.
The history of modern "Beowulf" criticism is often said to begin with J. R. R. Tolkien, author and Merton professor of Anglo-Saxon at Oxford University, who in his 1936 lecture to the British Academy criticised his contemporaries' excessive interest in its historical implications. He noted in "" that as a result the poem's literary value had been largely overlooked and argued that the poem "is in fact so interesting as poetry, in places poetry so powerful, that this quite overshadows the historical content..."
In historical terms, the poem's characters would have been Norse pagans (the historical events of the poem took place before the Christianisation of Scandinavia), yet the poem was recorded by Christian Anglo-Saxons who had largely converted from their native Anglo-Saxon paganism around the 7th century – both Anglo-Saxon paganism and Norse paganism share a common origin as both are forms of Germanic paganism. "Beowulf" thus depicts a Germanic warrior society, in which the relationship between the lord of the region and those who served under him was of paramount importance.
Stanley B. Greenfield has suggested that references to the human body throughout "Beowulf" emphasise the relative position of thanes to their lord. He argues that the term "shoulder-companion" could refer to both a physical arm as well as a thane (Aeschere) who was very valuable to his lord (Hrothgar). With Aeschere's death, Hrothgar turns to Beowulf as his new "arm." In addition, Greenfield argues the foot is used for the opposite effect, only appearing four times in the poem. It is used in conjunction with Unferth (a man described by Beowulf as weak, traitorous, and cowardly). Greenfield notes that Unferth is described as "at the king's feet" (line 499). Unferth is also a member of the foot troops, who, throughout the story, do nothing and "generally serve as backdrops for more heroic action."
At the same time, Richard North argues that the "Beowulf" poet interpreted "Danish myths in Christian form" (as the poem would have served as a form of entertainment for a Christian audience), and states: "As yet we are no closer to finding out why the first audience of "Beowulf" liked to hear stories about people routinely classified as damned. This question is pressing, given... that Anglo-Saxons saw the Danes as 'heathens' rather than as foreigners." Grendel's mother and Grendel are described as descendants of Cain, a fact which some scholars link to the Cain tradition.
Other scholars disagree, however, as to the meaning and nature of the poem: is it a Christian work set in a Germanic pagan context? The question suggests that the conversion from the Germanic pagan beliefs to Christian ones was a very slow and gradual process over several centuries, and it remains unclear the ultimate nature of the poem's message in respect to religious belief at the time it was written. Robert F. Yeager notes the facts that form the basis for these questions:
That the scribes of Cotton Vitellius A.XV were Christian is beyond doubt; and it is equally certain that Beowulf was composed in a Christianised England, since conversion took place in the sixth and seventh centuries. Yet the only Biblical references in Beowulf are to the Old Testament, and Christ is never mentioned. The poem is set in pagan times, and none of the characters is demonstrably Christian. In fact, when we are told what anyone in the poem believes, we learn that they are pagans. Beowulf's own beliefs are not expressed explicitly. He offers eloquent prayers to a higher power, addressing himself to the "Father Almighty" or the "Wielder of All." Were those the prayers of a pagan who used phrases the Christians subsequently appropriated? Or, did the poem's author intend to see Beowulf as a Christian Ur-hero, symbolically refulgent with Christian virtues?
 E. Talbot Donaldson claimed that it was probably composed more than twelve hundred years ago during the first half of the eighth century. Donaldson also believes the writer to be a native of what was then West Mercia, located in the Western Midlands of England. However, the late tenth-century manuscript "which alone preserves the poem" originated in the kingdom of the West Saxons – as it is more commonly known. Donaldson wrote that "the poet who put the materials into their present form was a Christian and ... poem reflects a Christian tradition".

</doc>
<doc id="3836" url="http://en.wikipedia.org/wiki?curid=3836" title="Barb Wire">
Barb Wire

Barb Wire was a superhero published by Comics Greatest World, an imprint of Dark Horse Comics. A regular series was published for 9 issues between 1994-1995, followed by a mini-series in 1996. In March 2015, Dark Horse announced they would be planning a new series starring the heroine.
Creators.
Regular Series:
Ace Of Spades (minseries):
1–4: Chris Warner, script and pencils/Tim Bradstreet, inks.
Character history.
In Steel Harbor, a bombed-out wreck of a town, thrill-junkie Barbara Kopetski - better known as "Barb Wire" - is a bar owner and part-time bounty hunter (in order to pay for her bar, The Hammerhead). She is skilled in many areas, but excels in combat-related abilities. While she has a brother and several allies, she is essentially a loner, although this is something which is uncomfortable for her to think about.
Film adaptation.
The movie adaptation was released in 1996 starring Pamela Anderson as Barb Wire. The film, panned by critics and fans alike, was nominated for the Razzie Award for Worst Picture, but lost to the Demi Moore film "Striptease".

</doc>
<doc id="3837" url="http://en.wikipedia.org/wiki?curid=3837" title="Blazing Saddles">
Blazing Saddles

 
Blazing Saddles is a 1974 satirical Western comedy film directed by Mel Brooks. Starring Cleavon Little and Gene Wilder, the film was written by Brooks, Andrew Bergman, Richard Pryor, Norman Steinberg, and Al Uger, and was based on Bergman's story and draft. The movie was nominated for three Academy Awards, and is ranked No. 6 on the American Film Institute's "100 Years...100 Laughs" list.
Brooks appears in two supporting roles, Governor William J. Le Petomane and a Yiddish-speaking Indian chief; he also dubs lines for one of Lili von Shtupp's backing troupe. The supporting cast includes Slim Pickens, Alex Karras, and David Huddleston, as well as Brooks regulars Dom DeLuise, Madeline Kahn, and Harvey Korman. Bandleader Count Basie has a cameo as himself.
The film satirizes the racism obscured by myth-making Hollywood accounts of the American West, with the hero being a black sheriff in an all-white town. The film is full of deliberate anachronisms, from the Count Basie Orchestra playing "April in Paris" in the Wild West, to Slim Pickens referring to the "Wide World of Sports", to the German army of World War II.
Plot.
In the American Old West of 1874, construction on a new railroad led by Lyle (Burton Gilliam) runs into quicksand. The route has to be changed, which will require it to go through Rock Ridge, a frontier town where everyone has the last name of "Johnson" (including a "Howard Johnson," a "Dr. Samuel Johnson," a "Van Johnson" and an "Olson N. Johnson"). The conniving State Attorney General Hedley Lamarr (Harvey Korman) wants to buy the land along the new railroad route cheaply by driving out the townspeople. He sends a gang of thugs, led by his flunky assistant Taggart (Slim Pickens), to scare them away, prompting the townsfolk to demand that Governor William J. Le Petomane (Mel Brooks) appoint a new sheriff. Lamarr persuades the dim-witted Le Petomane to select Bart (Cleavon Little), a black railroad worker who was about to be hanged, as he believes a black lawman will so offend the townspeople that they will either abandon Rock Ridge or lynch the new sheriff, with either result paving the way for him to take over the town.
With his quick wits and the assistance of drunken gunslinger Jim (Gene Wilder), also known as "The Waco Kid" ("I must have killed more men than Cecil B. DeMille"), Bart works to overcome the townsfolk's hostile reception. He defeats and befriends Mongo (Alex Karras), an immensely strong, slow-thinking (but surprisingly philosophical) henchman sent by Taggart and Lyle to kill Bart, and then beats German seductress-for-hire Lili von Shtupp (Madeline Kahn) at her own game. Lamarr is furious that his plans keep failing and decides to destroy Rock Ridge with a newly recruited and diverse army of thugs.
Bart gathers the town, along with the railroad workers, three miles east of Rock Ridge to build a fake town as a diversion. The workers labor all night to build a perfect replica, but with no people in it Bart realises it won't fool Lamarr's villains. Ordering the townspeople to make "exact replicas of themselves," Bart leaves with Jim and Mongo to construct a tollbooth. The booth delays the raiding party when they have to go back for "a shitload of dimes."
Once through the tollbooth, Lamarr's villains attack the fake town populated with dummies, which Bart boobytrapped with several dynamite bombs. Bart tries setting off the bombs but is unsuccessful as the detonator does not work. Jim is given the task of exploding the bombs, and fires his pistol at them. After the bombs explode, launching bad guys skyward, the Rock Ridgers attack the villains.
The resulting fight between the townsfolk and Lamarr's army of thugs breaks the fourth wall, literally. The fight spills out from the Warner Bros. film lot into a neighboring musical set being directed by Buddy Bizarre (Dom DeLuise), then into the studio commissary, and ultimately into the surrounding streets (specifically, Olive Avenue in Burbank). The citizens chase the villains back to Rock Ridge to destroy them, but Lamarr takes a taxi ". . . off this picture." He arrives at Grauman's Chinese Theatre to watch the "premiere" of "Blazing Saddles". Unfortunately, he sees on the movie screen that Bart has arrived outside the theatre. Bart ends up killing Lamarr by shooting him in the groin. Bart and Jim then go into the theatre to watch the end of the film, where Bart decides to leave Rock Ridge, much to the sadness of the townspeople and the railroad workers, for his work there is done. As he rides off, he finds Jim (who still has the popcorn that he bought at the theatre), and the two decide to go off to "nowhere special." They ride a short distance out of town, hand their horses off to the movie's wranglers and are driven away in a limousine into the sunset.
Cast.
Cast notes
Production.
The idea of "Blazing Saddles" came from an original story outline written by Andrew Bergman, which Brooks described as "hip talk — 1974 talk and expressions — happening in 1874 in the Old West". Brooks was immediately taken by the story, and despite having not worked with a writing team for some time, hired a group of writers, including Bergman, to expand on the script, reminding them "Please do not write a polite script". Brooks explained in the DVD commentary that the original title of the film, "Tex X" (as in the name of Black Muslim leader Malcolm X), was rejected, along with "Black Bart" and "Purple Sage". Finally, Brooks concocted the title "Blazing Saddles" while taking a shower.
"Blazing Saddles" was Brooks' first film shot in anamorphic format. To date, this film and "History of the World, Part I" are the only Brooks films in this format.
Brooks had repeated conflicts with studio executives over the cast and content. They objected to both the highly provocative script and the "irregular" activities of the writers (particularly Richard Pryor, who reportedly led all-night writing jams fueled by loud music and drugs). Brooks wanted Pryor to play the sheriff, but Warner executives expressed concern over Pryor's heavy drug use and alleged mental instability. Cleavon Little was cast in the role, and Pryor continued as co-screenwriter. In the midst of shooting, Gene Wilder — who had previously turned down the Hedley Lamarr role — was brought in to replace Gig Young, who collapsed during his first scene from what was later determined to be alcohol withdrawal syndrome.
After an in-studio screening, Warner Bros. executives objected to constant use of the word "nigger", the flatulent campfire scene, and Mongo appearing to punch a horse, and asked Brooks to modify those scenes. Brooks declined, as his contract gave him control of the final cut. He did remove the final line of Bart's response to Lili's attempt to seduce him in the dark: "I hate to disappoint you, ma'am, but you're sucking my arm." To an interviewer's query about his frequent use of "nigger" in the script, Brooks responded that if "Blazing Saddles" were to be remade today, the controversial word would have to be omitted ("and then, you've got no movie..."). He added that he had received consistent support for its use from writer Richard Pryor, and lead actor Cleavon Little.
Brooks wanted the movie's title song to reflect the western genre, and advertised in the trade papers for a "Frankie Laine-type" sound. Several days later, Laine himself visited Brooks' office to offer his services. Brooks had not told Laine that the movie was a comedy: "'Frankie sang his heart out... and we didn't have the heart to tell him it was a spoof — we just said, 'Oh, great!' He never heard the whip cracks; we put those in later. We got so lucky with his serious interpretation of the song."
In an interview included in the DVD release of "Blazing Saddles", Brooks claimed that Hedy Lamarr threatened legal action on grounds that the film's running "Hedley Lamarr" joke infringed on her right to privacy. This is lampooned in the film itself when Hedley corrects Governor Le Petomane's pronunciation of his name, and Le Petomane retorts, "What the hell are you worried about? This is 1874; you'll be able to sue "her!" " Brooks said that he and the actress settled out of court for a small sum. In the same interview, Brooks related that John Wayne was offered a cameo role. After reading the script Wayne declined, fearing the dialogue was "too dirty" for his family image, but told Brooks that he "would be first in line to see the film".
Reception.
While the film is widely considered a classic comedy today, critical reaction was mixed when the film was first released. Vincent Canby wrote:
"Blazing Saddles" has no dominant personality, and it looks as if it includes every gag thought up in every story conference. Whether good, bad or mild, nothing was thrown out. Mr. [Woody] Allen's comedy, though very much a product of our Age of Analysis, recalls the wonder and discipline of people like Keaton and Laurel and Hardy. Mr. Brooks's sights are lower. His brashness is rare, but his use of anachronism and anarchy recalls not the great film comedies of the past, but the middling ones like the Hope-Crosby "Road" pictures. With his talent he should do much better than that.
Roger Ebert gave the film four stars and called it a "crazed grabbag of a movie that does everything to keep us laughing except hit us over the head with a rubber chicken. Mostly, it succeeds. It's an audience picture; it doesn't have a lot of classy polish and its structure is a total mess. But of course! What does that matter while Alex Karras is knocking a horse cold with a right cross to the jaw?"
The film grossed $119.5 million in the box office, becoming only the tenth film in history up to that time to pass the $100 million mark.
On the film-critics aggregator Rotten Tomatoes, the film has 90% positive reviews based on 49 reviews.
Awards and honors.
In the scene where Lamarr addresses his band of bad guys, he says, "You men are only risking your lives, while I am risking an almost-certain Academy Award nomination for Best Supporting Actor!" Harvey Korman did not, in fact, get an Oscar nomination, but the film did receive three other Academy Awards nominations in 1974: Best Actress in a Supporting Role for Madeline Kahn, Best Film Editing, and Best Music, Original Song. The film also earned two BAFTA awards nominations, for Best Newcomer (Cleavon Little) and Best Screenplay.
The film won the Writers Guild of America Award for "Best Comedy Written Directly for the Screen" for writers Mel Brooks, Norman Steinberg, Andrew Bergman, Richard Pryor, and Alan Uger.
In 2006, "Blazing Saddles" was deemed "culturally, historically, or aesthetically significant" by the Library of Congress and was selected for preservation in the National Film Registry. The American film critic Dave Kehr queried if the historical significance of "Blazing Saddles" lay in the fact that it was the first film from a major studio to have a fart joke.
American Film Institute Lists
Legacy.
TV pilot.
A television pilot was produced for CBS based on Andrew Bergman's initial story, titled "Black Bart," which was the original title of the film. It featured Louis Gossett, Jr. as Bart and Steve Landesberg as his drunkard sidekick, a former Confederate officer named "Reb Jordan". Mel Brooks had little if anything to do with the pilot, as writer Andrew Bergman is listed as the sole creator. The pilot did not sell, but CBS aired it once on April 4, 1975. It was later included as a bonus feature on the "Blazing Saddles" 30th Anniversary DVD and the Blu-ray disc.
Musical adaptation.
With the production of musical adaptations of "The Producers" and "Young Frankenstein", rumors spread about a possible adaptation of "Blazing Saddles". Brooks joked about the concept in the final number in "Young Frankenstein", in which the full company sings, "next year, "Blazing Saddles"!" In 2010, Mel Brooks confirmed this, saying that the musical could be finished within a year. No creative team or plan has been announced.
Soundtrack.
The first studio-licensed release of the full music soundtrack to "Blazing Saddles" was on La-La Land Records on August 26, 2008. Remastered from original studio vault elements, the limited edition CD (a run of 3000) features the songs from the film as well as composer John Morris's score. Instrumental versions of all the songs are bonus tracks on the disc. The disc features exclusive liner notes featuring comments from Mel Brooks and John Morris.

</doc>
<doc id="3838" url="http://en.wikipedia.org/wiki?curid=3838" title="Bruce Sterling">
Bruce Sterling

Michael Bruce Sterling (born April 14, 1954) is an American science fiction author who is best known for his novels and his work on the "Mirrorshades" anthology. This work helped to define the cyberpunk genre.
Writings.
Sterling, along with William Gibson, Rudy Rucker, John Shirley, Lewis Shiner, and Pat Cadigan, is one of the founders of the cyberpunk movement in science fiction. In addition, he is one of the subgenre's chief ideological promulgators. This has earned him the nickname "Chairman Bruce". He was also one of the first organizers of the Turkey City Writer's Workshop, and is a frequent attendee at the Sycamore Hill Writer's Workshop. He won Hugo Awards for his novelettes "Bicycle Repairman" and "Taklamakan".
His first novel, "Involution Ocean", published in 1977, features the world Nullaqua where all the atmosphere is contained in a single, miles-deep crater. The story concerns a ship sailing on the ocean of dust at the bottom, which hunts creatures called dustwhales that live beneath the surface. It is partially a science-fictional pastiche of "Moby-Dick" by Herman Melville.
From the late 1970s onwards, Sterling wrote a series of stories set in the Shaper/Mechanist universe: the solar system is colonised, with two major warring factions. The Mechanists use a great deal of computer-based mechanical technologies; the Shapers do genetic engineering on a massive scale. The situation is complicated by the eventual contact with alien civilizations; humanity eventually splits into many subspecies, with the implication that many of these effectively vanish from the galaxy, reminiscent of The Singularity in the works of Vernor Vinge. The Shaper/Mechanist stories can be found in the collection "Crystal Express" and the collection "Schismatrix Plus", which contains the original novel "Schismatrix" and all of the stories set in the Shaper/Mechanist universe. Alastair Reynolds identified "Schismatrix" and the other Shaper/Mechanist stories as one of the greatest influences on his own work.
In the 1980s, Sterling edited the science fiction critical fanzine Cheap Truth, under the alias of Vincent Omniaveritas. He wrote a column called "Catscan", for the now-defunct science fiction critical magazine, "SF Eye".
He recently contributed a chapter to "Sound Unbound: Sampling Digital Music and Culture" (The MIT Press, 2008) edited by Paul D. Miller a.k.a. DJ Spooky. He also contributed, along with Lewis Shiner, to the short story "Mozart in Mirrorshades".
From April 2009 through May 2009, he was an editor at Cool Tools.
Since October 2003 Sterling has blogged at , which is hosted by "Wired".
His most recent novel (as of 2013) is "Love Is Strange" (December 2012), a Paranormal Romance (40k).
Projects.
He has been the instigator of three projects which can be found on the Web -
Neologisms.
Sterling has a habit of coining neologisms to describe things which he believes will be common in the future, especially items which already exist in limited numbers.
Personal.
In childhood, Sterling spent several years in India; today he has a notable fondness for Bollywood films. In 2003 he was appointed Professor at the European Graduate School where he is teaching summer intensive courses on media and design. In 2005, he became "visionary in residence" at Art Center College of Design in Pasadena, California. He lived in Belgrade with Serbian author and film-maker Jasmina Tešanović for several years. In September 2007 he moved to Turin, Italy. He also travels the world extensively giving speeches and attending conferences.

</doc>
<doc id="3840" url="http://en.wikipedia.org/wiki?curid=3840" title="Brain abscess">
Brain abscess

Brain abscess (or cerebral abscess) is an abscess caused by inflammation and collection of infected material, coming from local (ear infection, dental abscess, infection of paranasal sinuses, infection of the mastoid air cells of the temporal bone, epidural abscess) or remote (lung, heart, kidney etc.) infectious sources, within the brain tissue. The infection may also be introduced through a skull fracture following a head trauma or surgical procedures. Brain abscess is usually associated with congenital heart disease in young children. It may occur at any age but is most frequent in the third decade of life.
Signs and symptoms.
Fever, headache, and neurological problems, while classic, only occur in 20% of people.
The symptoms of brain abscess are caused by a combination of increased intracranial pressure due to a space-occupying lesion (headache, vomiting, confusion, coma), infection (fever, fatigue etc.) and focal neurologic brain tissue damage (hemiparesis, aphasia etc.). The most frequent presenting symptoms are headache, drowsiness, confusion, seizures, hemiparesis or speech difficulties together with fever with a rapidly progressive course. The symptoms and findings depend largely on the specific location of the abscess in the brain. An abscess in the cerebellum, for instance, may cause additional complaints as a result of brain stem compression and hydrocephalus. Neurological examination may reveal a stiff neck in occasional cases (erroneously suggesting meningitis). The famous triad of fever, headache and focal neurologic findings are highly suggestive of brain abscess.
Pathophysiology.
Bacterial.
Anaerobic and microaerophilic cocci and gram-negative and gram-positive anaerobic bacilli are the predominate bacterial isolates. Many brain abscesses are polymicrobical. The predominant organisms include: "Staphylococcus aureus", aerobic and anaerobic streptococci (especially "Streptococcus intermedius"), "Bacteroides", "Prevotella", and "Fusobacterium" species, Enterobacteriaceae, "Pseudomonas" species, and other anaerobes. Less common organisms include: "Haemophillus influenzae", "Streptococcus pneumoniae" and "Neisseria meningitides".
Bacterial abscesses rarely (if ever) arise "de novo" within the brain, although establishing a cause can be difficult in many cases. There is almost always a primary lesion elsewhere in the body that must be sought assiduously, because failure to treat the primary lesion will result in relapse. In cases of trauma, for example in compound skull fractures where fragments of bone are pushed into the substance of the brain, the cause of the abscess is obvious. Similarly, bullets and other foreign bodies may become sources of infection if left in place. The location of the primary lesion may be suggested by the location of the abscess: infections of the middle ear result in lesions in the middle and posterior cranial fossae; congenital heart disease with right-to-left shunts often result in abscesses in the distribution of the middle cerebral artery; and infection of the frontal and ethmoid sinuses usually results in collection in the subdural sinuses.
Other organisms.
Fungi and parasites may also cause the disease. Fungi and parasites are especially associated with immunocompromised patients. Other causes include: "Nocardia asteroides", "Mycobacterium", Fungi (e.g. "Aspergillus", "Candida", "Cryptococcus", "Mucorales", "Coccidioides", "Histoplasma capsulatum", "Blastomyces dermatitidis", "Bipolaris", "Exophiala dermatitidis", "Curvularia pallescens", "Ochroconis gallopava", "Ramichloridium mackenziei", "Pseudallescheria boydii"), Protozoa (e.g. "Toxoplasma gondii", "Entamoeba histolytica", "Trypanosoma cruzi", "Schistosoma", "Paragonimus"), and Helminths (e.g. "Taenia solium"). Organisms that are most frequently associated with brain abscess in patients with AIDS are poliovirus, "Toxoplasma gondii", and "Cryptococcus neoformans", though in infection with the latter organism, symptoms of meningitis generally predominate.
These organisms are associated with certain predisposing conditions: 
Diagnosis.
The diagnosis is established by a computed tomography (CT) (with contrast) examination. At the initial phase of the inflammation (which is referred to as cerebritis), the immature lesion does not have a capsule and it may be difficult to distinguish it from other space-occupying lesions or infarcts of the brain. Within 4–5 days the inflammation and the concomitant dead brain tissue are surrounded with a capsule, which gives the lesion the famous ring-enhancing lesion appearance on CT examination with contrast (since intravenously applied contrast material can not pass through the capsule, it is collected around the lesion and looks as a ring surrounding the relatively dark lesion). Lumbar puncture procedure, which is performed in many infectious disorders of the central nervous system is contraindicated in this condition (as it is in all space-occupying lesions of the brain) because removing a certain portion of the cerebrospinal fluid may alter the concrete intracranial pressure balances and causes the brain tissue to move across structures within the skull (brain herniation).
Ring enhancement may also be observed in cerebral hemorrhages (bleeding) and some brain tumors. However, in the presence of the rapidly progressive course with fever, focal neurologic findings (hemiparesis, aphasia etc.) and signs of increased intracranial pressure, the most likely diagnosis should be the brain abscess.
Treatment.
The treatment includes lowering the increased intracranial pressure and starting intravenous antibiotics (and meanwhile identifying the causative organism mainly by blood culture studies).
Hyperbaric Oxygen Therapy (HBO2 or HBOT) is indicated as a primary and adjunct treatment which provides four primary functions
Firstly HBOT reduces intracranial pressure. Secondly high partial pressures of oxygen act as a bactericide and thus inhibits the anaerobic and functionally anaerobic flora common in brain abscess. Third, HBOT optimizes the immune function thus enhancing the host defense mechanisms and fourth HBOT has been found to be of benefit when brain abscess is concomitant with cranial osteomyleitis. 
Secondary functions of HBOT include increased stem cell production and up-regulation of VEGF which aid in the healing and recovery process.
Surgical drainage of the abscess remains part of the standard management of bacterial brain abscesses. The location and treatment of the primary lesion also crucial, as is the removal of any foreign material (bone, dirt, bullets, and so forth).
There are few exceptions to this rule: Haemophilus influenzae meningitis is often associated with subdural effusions that are mistaken for subdural empyemas. These effusions resolve with antibiotics and require no surgical treatment. Tuberculosis can produce brain abscesses that look identical to conventional bacterial abscesses on CT imaging. Surgical drainage or aspiration is often necessary to identify "Mycobacterium tuberculosis", but once the diagnosis is made no further surgical intervention is necessary.
Prognosis.
Death occurs in about 10% and people do well about 70% of time. This is a large improvement from the 1960s due to improved ability to image the head, better neurosurgery and better antibiotics.

</doc>
<doc id="3841" url="http://en.wikipedia.org/wiki?curid=3841" title="Bud Spencer">
Bud Spencer

Bud Spencer (born Carlo Pedersoli; 31 October 1929) is an Italian actor, filmmaker and a former professional swimmer. He is known for past roles in action-comedy films together with his long-time film partner Terence Hill. Growing from a successful swimmer in his youth, he got a degree in law, and has registered several patents. Spencer also became a certified commercial airline and helicopter pilot, and supports and funds many children's charities, including the Spencer Scholarship Fund.
Bud Spencer and Terence Hill both appeared in, produced and directed over 20 films together and thus became and still remain best friends to this present day.
In a recent interview, Spencer commented: "Terence is like a brother to me, and like brothers we don't always agree, and he can be a pain in the neck sometimes." [laughs] "... our relationship I suppose is reflected in our movies, but he can always count on me as I can on him. We always enjoyed working together, probably why we made so many films together."
Biography.
Spencer was born in Santa Lucia, a historical rione of the city of Naples.
From 1947 to 1949 he worked in the Italian consulate in Recife (Brazil), where he learned fluent Portuguese. He changed his screen name in 1967, reportedly choosing to pay homage to Spencer Tracy as well as his favorite beer, Budweiser. Other sources report that he found it funny to call himself "bud" despite his weight and his imposing height at almost 6 ft.
Swimming and water polo career.
A professional swimmer in his youth, Spencer was the first Italian to swim the 100 m freestyle in less than one minute. He achieved this on 19 September 1950, when he swam the 100 m in 59.5 s.
In the 1951 Mediterranean Games, he won a silver medal in the same 100 m freestyle event.
Spencer participated in the 1952 Olympic Games in Helsinki, Finland, reaching the semi-finals in the 100 m freestyle (58.8 s heats, 58.9 s semi final). Four years later, in Melbourne, he also entered the semi-finals in the same category (58.5 s heat, 59.0 s semi final).
As a water polo player, he won the Italian Championship in 1954, with S.S. Lazio. His swimming career ended abruptly in 1957.
On 17 January 2005 he was awarded with the "Caimano d'oro" (Gold Caiman) by the Italian Swimming Federation.
On 24 January 2007, he received from the Italian Swimming Federation's president Paolo Barelli, swim and water polo coach diplomas.
Acting career.
Spencer's first movie role was in "Quel fantasma di mio marito", an Italian comedy shot in 1949 and released in 1950. Then he played a member of the Praetorian Guard in "Quo Vadis", a film shot in Italy, in 1951. During the 1950s and part of the 1960s, Spencer appeared in some Italian films but "his career was strictly minor league until the late 1960s."
Spencer met his future film partner, Terence Hill, on the set of "Hannibal" in 1959. They went on to work together on over 20 films, including (named using their most common US titles):
Films with Spencer alone include:
Many of these have alternative titles, depending upon the country and distributor. Some have longer Italian versions that were edited for release abroad. These films gathered popularity for both actors, especially in Europe.
The main dubber of Bud Spencer in Italy is Glauco Onorato who, with his characteristic voice, successfully enriched the character of Bud Spencer. Sergio Fiorentini dubbed Spencer in "Troublemakers", "To the Limit" (1997) and the series of Detective Extralarge, (1991–93) while in the movie "Everything Happens to Me" the dubber is Ferruccio Amendola.
Spencer also wrote the complete or partial screenplay for some of his movies. His feature film career slowed down after 1983, shifting more toward television. In the 1990s, he acted in the TV action-drama "Extralarge". His autobiography was published by Schwarzkopf & Schwarzkopf in 2011.
Personal life.
Spencer married Maria Amato in 1960, with whom he had three children: Giuseppe (1961), Christine (1962) and Diamante (1972). After appearing in "Più forte, ragazzi!", Spencer became a jet airplane and helicopter pilot. He established Mistral Air in 1984, an air-mail company that also transports pilgrims, but later sold it to Poste Italiane to buy a textile mill that produced clothes for children.
Career in politics.
In 2005, he entered politics, unsuccessfully standing as regional counselor in Lazio for the Forza Italia party. Spencer has stated: "In my life, I've done everything. There are only three things I haven't been – a ballet dancer, a jockey and a politician. Given that the first two jobs are out of the question, I'll throw myself into politics." The opposition criticised him for engaging in "politica spettacolo" ("showbiz politics").

</doc>
<doc id="3845" url="http://en.wikipedia.org/wiki?curid=3845" title="Brigitte Bardot">
Brigitte Bardot

Brigitte Anne-Marie Bardot (; ]; born 28 September 1934) is a French former actress, singer and fashion model, who later became an animal rights activist. She was one of the best known sex symbols of the 1950s and 1960s and was widely referred to by her initials. Starting in 1969, Bardot became the official face of Marianne (who had previously been anonymous) to represent the liberty of France.
Bardot was an aspiring ballerina in early life. She started her acting career in 1952 and after appearing in 16 routine comedy films, with limited international release, became world-famous in 1957, with the controversial film "And God Created Woman". She later starred in Jean-Luc Godard's 1963 film "Le Mépris". For her role in Louis Malle's 1965 film "Viva Maria!" Bardot was nominated for a BAFTA Award for Best Foreign Actress. Bardot caught the attention of French intellectuals. She was the subject of Simone de Beauvoir's 1959 essay, "The Lolita Syndrome", which described Bardot as a "locomotive of women's history" and built upon existentialist themes to declare her the first and most liberated woman of post-war France.
Bardot retired from the entertainment industry in 1973. During her career in show business, she starred in 47 films, performed in several musical shows and recorded over 60 songs. She was awarded the Legion of Honour in 1985 but refused to receive it. After her retirement, she established herself as an animal rights activist. During the 1990s, she generated controversy by criticizing immigration and Islam in France and has been fined five times for inciting racial hatred.
Early life.
Bardot was born in Paris, the daughter of Louis Bardot (1896–1975) and Anne-Marie "Toty" Bardot (née Mucel; 1912–1978). Louis had an engineering degree and worked with his father, Charles Bardot, in the family business. Louis and Anne-Marie married in 1933. Bardot grew up in a middle-class Roman Catholic observant home.
Brigitte's mother enrolled Brigitte and her younger sister, Marie-Jeanne (born 5 May 1938), in dance. Marie-Jeanne eventually gave up dancing lessons and did not tell her mother, whereas Brigitte concentrated on ballet. In 1947, Bardot was accepted to the Conservatoire de Paris. For three years she attended ballet classes by Russian choreographer Boris Knyazev. One of her classmates was Leslie Caron. The other ballerinas nicknamed Bardot "Bichette" ("Little Doe").
At the invitation of an acquaintance of her mother, she modelled in a fashion show in 1949. In the same year, she modelled for a fashion magazine "Jardin des Modes" managed by journalist Hélène Lazareff. Aged 15, she appeared on an 8 March 1950 cover of "Elle" and was noticed by a young film director, Roger Vadim, while babysitting. He showed an issue of the magazine to director and screenwriter Marc Allégret, who offered Bardot the opportunity to audition for "Les lauriers sont coupés". Although Bardot got the role, the film was cancelled but made her consider becoming an actress. Her acquaintance with Vadim, who attended the audition, influenced her further life and career.
Career.
Although the European film industry was then in its ascendancy, Bardot was one of the few European actresses to have the mass media's attention in the United States, an interest which she did not reciprocate. She debuted in a 1952 comedy film, "Le Trou Normand" (English title: "Crazy for Love"). From 1952 to 1956, she appeared in seventeen films; in 1953 she played a role in Jean Anouilh's stageplay "L'Invitation au Château" ("Invitation to the Castle"). She received media attention when she attended the Cannes Film Festival in April 1953.
Her films of the early and mid 1950s were generally lightweight romantic dramas, some historical, in which she was cast as ingénue or siren, often appearing nude or nearly so. She played bit parts in three English-language films, the British comedy "Doctor at Sea" (1955) with Dirk Bogarde, "Helen of Troy" (1954), in which she was understudy for the title role but appears only as Helen's handmaid and "Act of Love" (1954) with Kirk Douglas. Her French-language films were dubbed for international release. Roger Vadim (her husband) was not content with this light fare. The New Wave of French and Italian art directors and their stars were riding high internationally and he felt Bardot was being undersold. Looking for something more like an art film to push her as a serious actress, he showcased her in "And God Created Woman" (1956) opposite Jean-Louis Trintignant. The film, about an immoral teenager in a respectable small-town setting, was a huge success and turned Bardot into an international star.
During her early career, professional photographer Sam Lévin's photos contributed to her image of Bardot's sensuality. One showed Bardot from behind, dressed in a white corset. British photographer Cornel Lucas made images of Bardot in the 1950s and 1960s, that have become representative of her public persona. She divorced Vadim in 1957. In 1959, she married actor Jacques Charrier, with whom she starred in "Babette Goes to War". The press took great interest in her marriage, while she and her husband clashed over the direction of her career. Bardot's only child, Nicolas-Jacques Charrier, was a product of her marriage to Jacques Charrier.
Bardot was awarded a David di Donatello Award for Best Foreign actress for her role in "A Very Private Affair" ("Vie privée", 1962), directed by Louis Malle.
In May 1958, Bardot withdrew to the seclusion of Southern France, where she had bought the house "La Madrague" in Saint-Tropez. In 1963, she starred in Jean-Luc Godard's film "Le Mépris". Bardot was featured in many other films along with notable actors such as Alain Delon ("Famous Love Affairs"; "Spirits of the Dead"); Jean Gabin ("In Case of Adversity"); Sean Connery ("Shalako"); Jean Marais ("Royal Affairs in Versailles"; "School for Love"); Lino Ventura ("Rum Runners"); Annie Girardot ("The Novices"); Claudia Cardinale ("The Legend of Frenchie King"); Jeanne Moreau ("Viva Maria!"); Jane Birkin ("Don Juan, or If Don Juan Were a Woman"). Her career had traversed epochs where it was possible to say, "In the Sixties and early Seventies, there was no better known - or more scandalous - movie star on earth. — Not since the death of Valentino had a star aroused such insane devotion in their fans." In 1973, Bardot announced that she was retiring from acting as "a way to get out elegantly". 
She participated in several musical shows and recorded many popular songs in the 1960s and 1970s, mostly in collaboration with Serge Gainsbourg, Bob Zagury and Sacha Distel, including "Harley Davidson"; "Je Me Donne À Qui Me Plaît"; "Bubble gum"; "Contact"; "Je Reviendrai Toujours Vers Toi"; "L'Appareil À Sous"; "La Madrague"; "On Déménage"; "Sidonie"; "Tu Veux, Ou Tu Veux Pas?"; "Le Soleil De Ma Vie" (the cover of Stevie Wonder's "You Are the Sunshine of My Life"); and the notorious "Je t'aime... moi non-plus". Bardot pleaded with Gainsbourg not to release this duet and he complied with her wishes; the following year, he rerecorded a version with British-born model and actress Jane Birkin that became a massive hit all over Europe. The version with Bardot was issued in 1986 and became a popular download hit in 2006 when Universal Records made its back catalogue available to purchase online, with this version of the song ranking as the third most popular download.
Personal life.
On 21 December 1952, aged 18, Bardot married director Roger Vadim, seven years her senior. To receive permission from Bardot's parents to marry her, Vadim, originally a Russian Orthodox Christian, was urged to convert to Catholicism, although it is not clear if he ever did so. They divorced five years later, but remained friends and collaborated in later work. Bardot had an affair with her "And God Created Woman" co-star Jean-Louis Trintignant (married at the time to actress Stéphane Audran) before her divorce from Vadim. The two lived together for about two years. Their relationship was complicated by Trintignant's frequent absence due to military service and Bardot's affair with musician Gilbert Bécaud, and they eventually separated.
In early 1958, Bardot recovered, in Italy, from a reported nervous breakdown, according to newspaper reports. A suicide attempt with sleeping pills two days earlier was also noted, but was denied by her public relations manager.
On 18 June 1959, she married actor Jacques Charrier, by whom she had her only child, a son, Nicolas-Jacques Charrier (born 11 January 1960). After she and Charrier divorced in 1962, Nicolas was raised in the Charrier family and did not maintain close contact with Bardot until his adulthood.
Bardot's third marriage was to German millionaire playboy Gunter Sachs from 14 July 1966 to 1 October 1969. In the 1970s, Bardot lived with sculptor Miroslav Brozek and posed for some of his sculptures. In 1974, Bardot appeared in a nude photo shoot in "Playboy" magazine, which celebrated her 40th birthday.
Bardot's fourth and current husband is Bernard d'Ormale, former adviser of Jean-Marie Le Pen, former leader of the far right party Front National; they have been married since 16 August 1992.
Animal welfare activism.
In 1973, before her 39th birthday, Bardot announced her retirement. After appearing in more than forty motion pictures and recording several music albums, most notably with Serge Gainsbourg, she chose to use her fame to promote animal rights.
In 1986, she established the Brigitte Bardot Foundation for the Welfare and Protection of Animals. She became a vegetarian and raised three million francs to fund the foundation by auctioning off jewellery and personal belongings. Today she is a strong animal rights activist and a major opponent of the consumption of horse meat. In support of animal protection, she condemned seal hunting in Canada during a visit to that country with Paul Watson of the Sea Shepherd Conservation Society. On 25 May 2011 the Sea Shepherd Conservation Society renamed its fast interceptor vessel, MV "Gojira", as MV "Brigitte Bardot" in appreciation of her support.
She once had a neighbour's donkey castrated while looking after it, on the grounds of its "sexual harassment" of her own donkey and mare, for which she was taken to court by the donkey's owner in 1989. Bardot wrote a 1999 letter to Chinese President Jiang Zemin, published in French magazine VSD, in which she accused the Chinese of "torturing bears and killing the world's last tigers and rhinos to make aphrodisiacs".
She has donated more than $140,000 over two years for a mass sterilization and adoption program for Bucharest's stray dogs, estimated to number 300,000.
In August 2010, Bardot addressed a letter to the Queen of Denmark, Margrethe II of Denmark, appealing for the sovereign to halt the killing of dolphins in the Faroe Islands. In the letter, Bardot describes the activity as a "macabre spectacle" that "is a shame for Denmark and the Faroe Islands ... This is not a hunt but a mass slaughter ... an outmoded tradition that has no acceptable justification in today's world".
On 22 April 2011, French culture minister Frédéric Mitterrand officially included bullfighting in the country's cultural heritage. Bardot wrote him a highly critical letter of protest.
From year 2013 onwards Brigitte Bardot Foundation in collaboration with Kagyupa International Monlam Trust of India has operated annual Veterinary Care Camp. She has committed to the cause of Animal welfare in Bodhgaya year after year.
Politics and legal issues.
Bardot expressed support for President Charles de Gaulle in the 1960s. Her husband Bernard d'Ormale is a former adviser of the Front National, the main far right party in France, known for its nationalist and conservative beliefs.
In her 1999 book "Le Carré de Pluton" ("Pluto's Square"), Bardot criticizes the procedure used in the ritual slaughter of sheep during the Muslim festival of Eid al-Adha. Additionally, in a section in the book entitled, "Open Letter to My Lost France", Bardot writes that "my country, France, my homeland, my land is again invaded by an overpopulation of foreigners, especially Muslims". For this comment, a French court fined her 30,000 francs in June 2000. She had been fined in 1997 for the original publication of this open letter in "Le Figaro" and again in 1998 for making similar remarks.
In her 2003 book, "Un cri dans le silence" ("A Scream in the Silence"), she warned of an "Islamicization of France", and said of Muslim immigration:"Over the last twenty years, we have given in to a subterranean, dangerous, and uncontrolled infiltration, which not only resists adjusting to our laws and customs but which will, as the years pass, attempt to impose its own."
In the book, she also contrasted her close gay friends with today's homosexuals, who "jiggle their bottoms, put their little fingers in the air and with their little castrato voices moan about what those ghastly heteros put them through" and that some contemporary homosexuals behave like "fairground freaks". In her own defence, Bardot wrote in a letter to a French gay magazine: "Apart from my husband — who maybe will cross over one day as well — I am entirely surrounded by homos. For years, they have been my support, my friends, my adopted children, my confidants." In her book she wrote about issues such as racial mixing, immigration, the role of women in politics and Islam. The book also contained a section attacking what she called the mixing of genes and praised previous generations who, she said, had given their lives to push out invaders.
On 10 June 2004, Bardot was convicted for a fourth time by a French court for "inciting racial hatred" and fined €5,000. Bardot denied the racial hatred charge and apologized in court, saying: "I never knowingly wanted to hurt anybody. It is not in my character."
In 2008, Bardot was convicted of inciting racial/religious hatred in relation to a letter she wrote, a copy of which she sent to Nicolas Sarkozy when he was Interior Minister of France. The letter stated her objections to Muslims in France ritually slaughtering sheep by slitting their throats without anesthetizing them first. She also said, in reference to Muslims, that she was "fed up with being under the thumb of this population which is destroying us, destroying our country and imposing its habits". The trial concluded on 3 June 2008, with a conviction and fine of €15,000, the largest of her fines to date. The prosecutor stated that she was tired of charging Bardot with offences related to racial hatred.
During the 2008 United States presidential election, she branded the Republican Party vice-presidential candidate Sarah Palin as "stupid" and a "disgrace to women". She criticized the former governor of Alaska for her stance on global warming and gun control. She was also offended by Palin's support for Arctic oil exploration and for her lack of consideration in protecting polar bears.
On 13 August 2010, Bardot lashed out at director Kyle Newman regarding his plans to make a biographical film on her life. Her response was, "Wait until I'm dead before you make a movie about my life!" Bardot warned Newman that if the project progresses "sparks will fly".
Influence in pop culture.
In fashion, the Bardot neckline (a wide open neck that exposes both shoulders) is named after her. Bardot popularized this style which is especially used for knitted sweaters or jumpers although it is also used for other tops and dresses. Bardot popularized the bikini in her early films such as "Manina" (1952) (released in France as "Manina, la fille sans voiles"). The following year she was also photographed in a bikini on every beach in the south of France during the Cannes Film Festival. She gained additional attention when she filmed "...And God Created Woman" (1956) with Jean-Louis Trintignant (released in France as "Et Dieu Créa La Femme"). Bardot portrayed an immoral teenager cavorting in a bikini who seduces men in a respectable small-town setting. The film was an international success. The bikini was in the 1950s relatively well accepted in France but was still considered risqué in the United States. As late as 1959, Anne Cole, one of the United State's largest swimsuit designers, said, "It's nothing more than a G-string. It's at the razor's edge of decency."
She also brought into fashion the "choucroute" ("Sauerkraut") hairstyle (a sort of beehive hair style) and gingham clothes after wearing a checkered pink dress, designed by Jacques Esterel, at her wedding to Charrier. She was the subject for an Andy Warhol painting.
In addition to popularizing the bikini swimming suit, Bardot has also been credited with popularizing the city of St. Tropez and the town of Armação dos Búzios in Brazil, which she visited in 1964 with her boyfriend at the time, Brazilian musician Bob Zagury. The place where she stayed in Búzios is today a small hotel, Pousada do Sol, and also a French restaurant, Cigalon.
A statue by Christina Motta honours Brigitte Bardot in Armação dos Búzios.
Bardot was idolized by the young John Lennon and Paul McCartney. They made plans to shoot a film featuring The Beatles and Bardot, similar to "A Hard Day's Night", but the plans were never fulfilled. Lennon's first wife Cynthia Powell lightened her hair color to more closely resemble Bardot, while George Harrison made comparisons between Bardot and his first wife Pattie Boyd, as Cynthia wrote later in "A Twist of Lennon". Lennon and Bardot met in person once, in 1968 at the Mayfair Hotel, introduced by Beatles press agent Derek Taylor; a nervous Lennon took LSD before arriving, and neither star impressed the other. (Lennon recalled in a memoir, "I was on acid, and she was on her way out.") According to the liner notes of his first (self-titled) album, musician Bob Dylan dedicated the first song he ever wrote to Bardot. He also mentioned her by name in "I Shall Be Free", which appeared on his second album, "The Freewheelin' Bob Dylan". 
The first-ever official exhibition spotlighting Bardot's influence and legacy opened in Paris on 29 September 2009 – a day after her 75th birthday.
A type of Czechoslovak diesel-electric locomotives manufactured in the 1960s/70s has a nickname "Bardotka" – this comes from the fact that the locomotive has a distinctively shaped front, which resembles female breasts.
Discography.
Bardot released several albums and singles during the 1960s and 1970s
Books.
Bardot has also written five books:

</doc>
<doc id="3846" url="http://en.wikipedia.org/wiki?curid=3846" title="Banjo">
Banjo

The banjo is a four-, five- or (occasionally) six-stringed instrument with a thin membrane stretched over a frame or cavity as a resonator, called the head. The membrane, or head, is typically a piece of animal skin or plastic, and the frame is typically circular. Early forms of the instrument were fashioned by Africans in America, adapted from African instruments of similar design.
The banjo is frequently associated with country, folk, Irish traditional and bluegrass music. Historically, the banjo occupied a central place in African American traditional music, before becoming popular in the minstrel shows of the 19th century. The banjo, with the fiddle, is a mainstay of American old-time music.
History.
There are several theories concerning the origin of the name "banjo". It may derive from the Kimbundu term "mbanza". Some etymologists believe it comes from a dialectal pronunciation of the Portuguese "bandore" or from an early anglicisation of the Spanish word "bandurria", though other research suggests that it may come from a West African term for a bamboo stick formerly used for the instrument's neck.
Various instruments in Africa, chief among them the kora, feature a skin head and gourd (or similar shell) body. The African instruments differ from early African American banjos in that the necks do not possess a Western-style fingerboard and tuning pegs, instead having stick necks, with strings attached to the neck with loops for tuning. Banjos with fingerboards and tuning pegs are known from the Caribbean as early as the 17th century. 18th- and early 19th-century writers transcribed the name of these instruments variously as "bangie", "banza", "banjer", and "banjar". Instruments similar to the banjo (e.g., the Japanese "shamisen", Persian "tar", and Moroccan "sintir") have been played in many countries. Another likely banjo ancestor is the "akonting", a spike folk lute played by the Jola tribe of Senegambia, and the "ubaw-akwala" of the Igbo. Similar instruments include the "xalam" of Senegal and the "ngoni" of the Wassoulou region including parts of Mali, Guinea, and Côte d'Ivoire, as well as a larger variation of the "ngoni" developed in Morocco by sub-Saharan Africans known as the "gimbri".
Early, African-influenced banjos were built around a gourd body and a wooden stick neck. These instruments had varying numbers of strings, though often including some form of drone. The five-string banjo was popularized by Joel Walker Sweeney, an American minstrel performer from Appomattox Court House, Virginia.
In the 1830s Sweeney became the first caucasian to play the banjo on stage. His version of the instrument replaced the gourd with a drum-like sound box and included four full-length strings alongside a short fifth-string. This new banjo was at first tuned d'Gdf♯a, though by the 1890s this had been transposed up to g'cgbd'. Banjos were introduced in Britain by Sweeney's group, the American Virginia Minstrels, in the 1840s, and became very popular in music halls.
Technique.
Two techniques closely associated with the 5-string banjo are rolls and drones. Rolls are right hand accompanimental fingering pattern[s] that consist of eight (eighth) notes that subdivide each measure. Drone notes are quick little notes [typically eighth notes], usually played on the 5th (short) string to fill in around the melody notes [typically eighth notes]. These techniques are both idiomatic to the banjo in all styles, and their sound is characteristic of bluegrass.
Historically, the banjo was played in the "Clawhammer" style by the slaves who brought their version of the banjo with them. Several other styles of play were developed from this. Clawhammer consists of downward striking of one or more of the four main strings with the index, middle or both finger(s)while the drone or fifth string is played with a 'lifting' (as opposed to downward pluck) motion of the thumb. The notes typically sounded by the thumb in this fashion are, usually, on the off beat. Melodies can be quite intricate adding techniques such as double thumbing and drop thumb. In old time Appalachian Mountain music, there is also a style called two finger up-pick, and a three finger version that Earl Scruggs developed into the famous "Scruggs" style picking, nationally aired in 1945 on the Grand Ole Opry.
While 5-string banjos are traditionally played with either fingerpicks or the fingers themselves, tenor banjos and plectrum banjos are played with a pick, either to strum full chords or, most commonly in Irish Traditional Music, play single note melodies.
Modern banjo.
The modern banjo comes in a variety of forms, including four- and five-string versions. A six-string version, tuned and played similarly to a guitar, has gained popularity. In almost all of its forms, banjo playing is characterized by a fast arpeggiated plucking, though there are many different playing styles.
The body, or "pot", of a modern banjo typically consists of a circular rim (generally made of wood, though metal was also common on older banjos) and a tensioned head, similar to a drum head. Traditionally the head was made from animal skin, but today is often made of various synthetic materials. Most modern banjos also have a metal "tone ring" assembly that helps further clarify and project the sound, however many older banjos do not include a tone ring.
The banjo is usually tuned with friction tuning pegs or planetary gear tuners, rather than the worm gear machine head used on guitars. Frets have become standard since the late 19th century, though fretless banjos are still manufactured and played by those wishing to execute glissando, play quarter tones, or otherwise achieve the sound and feeling of early playing styles.
Modern banjos are typically strung with metal strings. Usually the fourth string is wound with either steel or bronze-phosphor alloy. Some players may string their banjos with nylon or gut strings to achieve a more mellow, old-time tone.
Open-back and resonator.
Some banjos have a separate resonator plate on the back of the pot, designed to project the sound forward and give the instrument more volume. This type of banjo is usually used in bluegrass music, though resonator banjos are played by players of all styles, and are also used in old-time, sometimes as a substitute for electric amplification when playing in large venues.
Open-back banjos generally have a mellower tone and weigh less than resonator banjos. They usually have a different setup than a resonator banjo, often with a higher string action. 
Five-string banjo.
The modern 5-string banjo is a variation on Sweeney's original design. The fifth string is usually the same gauge as the first, but starts from the fifth fret, three quarters the length of the other strings. (The long-necked Vega Pete Seeger model starts the fifth string from the eighth fret.) This lets the string be tuned to a higher open pitch than possible for the full-length strings. The short fifth string means that, unlike many string instruments, strings pitches on a five string banjo do not go in order from lowest to highest across the fingerboard. Instead, from low to high, they go fourth, third, second, first, and fifth. This is a form of reentrant tuning.
The short fifth string presents special problems for a capo. For small changes (going up or down one or two semitones, for example) it is possible simply to re-tune the fifth string. Otherwise, various devices called "fifth string capos" can effectively shorten the string. Many banjo players use model railroad spikes or titanium spikes (usually installed at the seventh fret and sometimes at others), that they hook the string under to press it down on the fret.
Many tunings are used for the five-string banjo. Probably the most common, particularly in bluegrass, is the Open-G tuning G4 D3 G3 B3 D4. In earlier times, the tuning G4 C3 G3 B3 D4 was commonly used instead, and this is still the preferred tuning for some types of folk music and for classic banjo. Other tunings found in old-time music include double C (G4 C3 G3 C4 D4), "sawmill" (G4 D3 G3 C4 D4) also called "mountain modal" and open D (F#4D3 F#3 A3 D4). These tunings are often taken up a tone, either by tuning up or using a capo. For example "old-time D" tuning (A4 D3 A3 D4 E4) – commonly reached by tuning up from double C – is often played to accompany fiddle tunes in the key of D and Open-A (A4 E3 A3 C#4 E4) is usually used for playing tunes in the key of A. There are dozens of other banjo tunings, used mostly in old-time music. These tunings are used to make it easier to play specific, usually, fiddle tunes, or groups of fiddle tunes.
While the size of the five string banjo is largely standardized, smaller and larger sizes are available including the long-neck or "Seeger neck" variation designed by Pete Seeger. Petite variations on the 5-string banjo have been available since the 1890s. S.S. Stewart introduced the banjeaurine, tuned one fourth above a standard five-string. Piccolo banjos are smaller, and tuned one octave above a standard banjo. Between these sizes and the standard there is the A-scale banjo, which is two frets shorter and usually tuned one full step above standard tunings. A "Stealth" brand banjo is a modern 5 string banjo with a 25.5" scale length, similar to a guitar.
American old-time music typically uses the five-string open back banjo. It is played in a number of different styles, the most common being clawhammer or frailing, characterized by the use of a downward rather than upward stroke when striking the strings with a fingernail. Frailing techniques use the thumb to catch the fifth string for a drone after most strums or after each stroke ("double thumbing"), or to pick out additional melody notes in what is known as "drop-thumb." Pete Seeger popularised a folk style by combining clawhammer with "up picking", usually without the use of fingerpicks. Another common style of old-time banjo playing is "Fingerpicking banjo" or "classic banjo". This style is based upon parlor-style guitar.
Bluegrass music, which uses the five-string resonator banjo almost exclusively, is played in several common styles. These include Scruggs style, named after Earl Scruggs; melodic, or Keith style, named for Bill Keith; and three-finger style with single string work, also called Reno style after Don Reno. In these styles the emphasis is on arpeggiated figures played in a continuous eighth-note rhythm, known as rolls. All of these styles are typically played with fingerpicks.
The first 5-string electric solid-body banjo was developed by Charles (Buck) Wilburn Trent, Harold "Shot" Jackson, and David Jackson in 1960.
Classical and modern.
The five-string banjo has been used in classical music since before the turn of the 20th century. Contemporary and modern works have been written or arranged for the instrument by Buck Trent, Béla Fleck, Tony Trischka, Ralph Stanley, Steve Martin, , George Crumb, Modest Mouse, Jo Kondo, Paul Elwood, Hans Werner Henze (notably in his Sixth Symphony), Daniel Mason of Hank Williams III's Damn Band, Beck, the Water Tower Bucket Boys, Todd Taylor, J.P. Pickens, Peggy Honeywell, Norfolk & Western, Putnam Smith, Iron & Wine, The Avett Brothers, Punch Brothers and Sufjan Stevens.
Frederick Delius wrote for a banjo in his opera Koanga.
Ernst Krenek includes two banjos in his Kleine Symphonie (Little Symphony).
Kurt Weill has a banjo in his opera The Rise and Fall of the City of Mahagonny.
John Bullard has released two albums – Classical Banjo and Bach On The Banjo.
Four-string banjos.
Four-string banjos, both plectrum and tenor, can be used strictly for chordal accompaniment (as in early jazz), strictly for single string melody playing (as in Irish traditional music), in "chord melody" style (a succession of chords are played in which the highest notes carry the melody), in tremolo style (both on chords and single strings) and a mixed technique called duo style, which combines single string tremolo and rhythm chords.
Plectrum banjo.
The plectrum banjo is a standard banjo without the short drone string. It usually has 22 frets on the neck and a scale length of 26 to 28 inches, and was originally tuned C3 G3 B3 D4. It can also be tuned like the top four strings of a guitar, which is known as "Chicago tuning." As the name suggests, it is usually played with a guitar-style pick (that is, a single one held between thumb and forefinger), unlike the five-string banjo, which is either played with a thumbpick and two fingerpicks, or with bare fingers. The plectrum banjo evolved out of the five-string banjo, to cater to styles of music involving strummed chords. The plectrum is also featured in many early jazz recordings and arrangements.
The four-string banjo is used from time to time in musical theater. Examples include: Hello, Dolly!, Mame, Chicago, Cabaret, Oklahoma!, Half a Sixpence, Annie, Barnum, The Threepenny Opera, Monty Python's Spamalot, and countless others. Joe Raposo had used it variably in the imaginative 7-piece orchestration for the long-running TV show Sesame Street, and has sometimes had it overdubbed with itself or an electric guitar. The banjo is still (albeit rarely) in use in the show's arrangement currently.
Tenor banjo.
The shorter-necked, tenor banjo, with 17 ("short scale") or 19 frets, is also typically played with a plectrum. It became a popular instrument after about 1910. Early models used for melodic picking typically had 17 frets on the neck and a scale length of 19½ to 21½ inches. By the mid-1920s, when the instrument was used primarily for strummed chordal accompaniment, 19-fret necks with a scale length of 21¾ to 23 inches became standard. The usual tuning is the all-fifths tuning C3 G3 D4 A4, in which there are exactly seven semitones (a perfect fifth) between the open notes of consecutive strings; all-fifths tuning is traditional for a viola or mandola, so banjola is the proper name for the tenor banjo. The tenor violin belongs a fourth below the viola or an octave below the violin. Early makers preferred the sound of the words tenor banjo. Other players (particularly in Irish traditional music) tune the banjo G2 D3 A3 E4 like an octave mandolin, which lets the banjoist duplicate fiddle and mandolin fingering. Fingerstyle opportunities of tenor banjo retuned to open G tuning dgd'g' or lower open D tuning Adad' (three finger picking, frailing) are explored by Mirek Patek. The popularisation of this tuning was usually attributed to the late Barney McKenna, banjoist with The Dubliners.
The tenor banjo was a common rhythm-instrument in early 20th-century dance-bands. Its volume and timbre suited early jazz (and jazz-influenced popular music styles) and could both compete with other instruments (such as brass instruments and saxophones) and be heard clearly on acoustic recordings. George Gershwin's Rhapsody in Blue, in Ferde Grofe's original jazz orchestra arrangement, includes tenor banjo, with widely spaced chords not easily playable on plectrum banjo in its conventional tuning(s). With development of the archtop and electric guitar, the tenor banjo largely disappeared from jazz and popular music, though keeping its place in traditional "Dixieland" jazz.
Some 1920s Irish banjo players picked out the melodies of jigs, reels and hornpipes on tenor banjos, decorating the tunes with snappy triplet ornaments. The most important Irish banjo player of this era was Mike Flanagan of the New York-based Flanagan Brothers, one of the most popular Irish-American groups of the day. Other pre-WW2 Irish banjo players included Neil Nolan, who recorded with Dan Sullivan's Shamrock Band in Boston, and Jimmy McDade, who recorded with the Four Provinces Orchestra in Philadelphia. Meanwhile in Ireland the rise of "ceili" bands provided a new market for a loud instrument like the tenor banjo. Use of the tenor banjo in Irish music has increased greatly since the folk revival of the 1960s.
The low banjos.
In the late 19th and early 20th centuries there was a vogue in plucked-string instrument ensembles—guitar orchestras, mandolin orchestras, banjo orchestras—in which the instrumentation was made to parallel that of the string section in symphony orchestras. Thus "violin, viola, 'cello, bass" became "mandolin, mandola, mandocello, mandobass", or in the case of banjos, "banjolin, banjola, banjo cello, bass banjo". Because the range of pluck stringed instrument generally isn't as great as that of comparably-size bowed string instruments, other instruments were often added to these plucked orchestras to extend the range of the ensemble upwards and downwards.
Cello banjo.
Rarer than either the tenor or plectrum banjo is the cello banjo (also "banjo cello"). It's normally tuned C2-G2-D3-A3, one octave below the tenor banjo like the cello and mandocello. It played a role in banjo orchestras in the late nineteenth and early twentieth centuries. 
A 5-string cello banjo, set up like a bluegrass banjo (with the short 5th string), but tuned one octave lower, has been produced by the Goldtone company.
Bass and contrabass banjo.
 Bass banjos have been produced in both upright bass formats and with standard, horizontally carried banjo bodies. Contrabass banjos with either three or four strings have also been made; some of these had headstocks similar to those of bass violins. Tuning varies on these large instruments, with four-string models sometimes being tuned in 4ths like a bass violin—E1-A1-D2-G2, and sometimes in 5ths, like a 4-string cello banjo, one octave lower—C1-G1-D2-A2. Other variants are also used.
Six-string banjos.
The 6-string banjo began as a British innovation by William Temlet, one of England's earliest banjo makers. He opened a shop in London in 1846, and sold banjos with closed backs and up to 7 strings. He marketed these as "zither" Banjos from his 1869 patent. American Alfred Davis Cammeyer (1862–1949), a young violinist-turned banjo concert player, devised the 5/6-string zither banjo around 1880. It had a wood resonator and metal "wire" strings (the 1st and 2nd melody strings and 5th "thumb" string. The 3rd melody string was gut and the 4th was silk covered) as well as frets and guitar-style tuning machines.
A zither banjo usually has a closed back and sides with the drum body (usually metal) and skin tensioning system suspended inside the wooden rim/back, the neck and string tailpiece was mounted on the wooden outer rim, the short string usually led through a tube in the neck so that the tuning peg could be mounted on the peg head. They were often made by builders who used guitar tuners that came in banks of three and so if 5 stringed had a redundant tuner. The banjos could also be somewhat easily converted over to a six-string banjo. British opera diva Adelina Patti advised Cammeyer that the zither-banjo might be popular with English audiences (which was certainly true, as it was invented there), and Cammeyer went to London in 1888. Due to his virtuoso playing he helped show that banjos could be used for more sophisticated music than was normally played by blackface minstrels, he was soon performing for London society, where he met Sir Arthur Sullivan, who recommended that Cammeyer progress from writing banjo arrangements of music to composing his own music. (Interesting to note that, supposedly unbeknownst to Cammeyer, William Temlett had patented a 7-string closed back banjo in 1869, and was already marketing it as a "zither-banjo.")
In the late 1890s Banjo maker F.C Wilkes developed a 6-string version of the banjo, with the 6th string "tunnelled" through the neck. It is arguable that Arthur O. Windsor influenced development and perfection of the zither banjo and created the open-back banjo along with other modifications to the banjo type instruments, such as the non-solid attached resonator of today. (Gibson claims credit for this modification on the American Continent.) Windsor claimed he created the hollow neck banjo with a truss rod, and buried the 5th string in the neck after the 5th fret so to put the tuning peg on the peg-head rather than in the neck. Gibson claims credit for perfecting the tone ring.
Modern six-string bluegrass banjos have been made. These add a bass string between the lowest string and the drone string on a 5-string banjo, and are usually tuned G4 G2 D3 G3 B3 D4. Sonny Osborne played one of these instruments for several years. It was modified by luthier Rual Yarbrough from a Vega 5-string model. A picture of Sonny with this banjo may be seen in Pete Wernick's "Bluegrass Banjo" method book.
Six-string banjos having a guitar neck and a banjo body have become quite popular since the mid-1990s. See under "Banjo Hybrids and variants", below.
Banjo hybrids and variants.
A number of hybrid instruments exist, crossing the banjo with other stringed instruments. Most of these use the body of a banjo, often with a resonator, and the neck of the other instrument. Examples include the banjo mandolin (first patented in 1882) and the banjo ukulele or banjolele, most famously played by the English comedian George Formby. These were especially popular in the early decades of the twentieth century, and were probably a result of a desire either to allow players of other instruments to jump on the banjo bandwagon at the height of its popularity, or to get the natural amplification benefits of the banjo resonator in an age before electric amplification.
The six-string banjo guitar basically consists of a six-string guitar neck attached to a bluegrass or plectrum banjo body. This was the instrument of the early jazz great Johnny St. Cyr, jazzmen Django Reinhardt, Danny Barker, Papa Charlie Jackson and Clancy Hayes, as well as the blues and gospel singer The Reverend Gary Davis. Nowadays, it appears under various names such as: guitanjo, guitjoe, ganjo, banjitar, or bantar. Today, musicians as diverse as Keith Urban, Rod Stewart, Taj Mahal, Joe Satriani, David Hidalgo, Larry Lalonde and Doc Watson play the 6-String guitar banjo.
Rhythm guitarist Dave Day of 1960s proto-punks The Monks replaced his guitar with a six-string, gut-strung guitar banjo on which he played guitar chords. This instrument sounds much more metallic, scratchy and wiry than a standard electric guitar, due to its amplification via a small microphone stuck inside the banjo's body.
Instruments using the five-string banjo neck on a wooden body (for example, that of a guitar, bouzouki, or dobro) have also been made, such as the banjola. A 20th-Century Turkish instrument very similar to the banjo is called the cümbüş, which has been made into eight different hybrid instruments, including guitar, mandolin, ukulele, and oud. At the end of the twentieth century, a development of the five-string banjo was the BanSitar. This features a bone bridge, giving the instrument a sitar-like resonance. A recent innovation is the patented Banjo-Tam, invented by Frank Abrams of Asheville North Carolina, U.S. patent No. 6,156,960. It combines a traditional five string banjo neck with a tambourine as a rim or "pot"

</doc>
<doc id="3850" url="http://en.wikipedia.org/wiki?curid=3850" title="Baseball">
Baseball

Baseball is a bat-and-ball game played between two teams of nine players each who take turns batting and fielding.
The offense attempts to score runs by hitting a ball that is thrown by the pitcher with a bat swung by the batter, then running counter-clockwise around a series of four bases: first, second, third, and home plate. A run is scored when a player advances around the bases and returns to home plate.
Players on the batting team take turns hitting against the pitcher of the fielding team, which tries to prevent runs by getting hitters out in any of several ways. A player on the batting team who reaches a base safely can later attempt to advance to subsequent bases during teammates' turns batting, such as on a hit or by other means. The teams switch between batting and fielding whenever the fielding team records three outs. One turn batting for both teams, beginning with the visiting team, constitutes an inning. A game comprises nine innings, and the team with the greater number of runs at the end of the game wins. Baseball is the only major team sport in America with no game clock, although almost all games end in the ninth inning.
Evolving from older bat-and-ball games, an early form of baseball was being played in England by the mid-18th century. This game was brought by immigrants to North America, where the modern version developed. By the late 19th century, baseball was widely recognized as the national sport of the United States. Baseball is now popular in North America and parts of Central and South America, the Caribbean, and East Asia.
In the United States and Canada, professional Major League Baseball (MLB) teams are divided into the National League (NL) and American League (AL), each with three divisions: East, West, and Central. The major league champion is determined by playoffs that culminate in the World Series. The top level of play is similarly split in Japan between the Central League and Pacific Leagues and in Cuba between the West League and East League.
History.
Origins of baseball.
The evolution of baseball from older bat-and-ball games is difficult to trace with precision. A French manuscript from 1344 contains an illustration of clerics playing a game, possibly "la soule", with similarities to baseball. Other old French games such as "thèque", "la balle au bâton", and "la balle empoisonnée" also appear to be related. Consensus once held that today's baseball is a North American development from the older game rounders, popular in Great Britain and Ireland. "Baseball Before We Knew It: A Search for the Roots of the Game" (2005), by David Block, suggests that the game originated in England; recently uncovered historical evidence supports this position. Block argues that rounders and early baseball were actually regional variants of each other, and that the game's most direct antecedents are the English games of stoolball and "tut-ball". It has long been believed that cricket also descended from such games, though evidence uncovered in early 2009 suggests that cricket may have been imported to England from Flanders.
The earliest known reference to baseball is in a 1744 British publication, "A Little Pretty Pocket-Book", by John Newbery. It contains a rhymed description of "base-ball" and a woodcut that shows a field set-up somewhat similar to the modern game—though in a triangular rather than diamond configuration, and with posts instead of ground-level bases. David Block discovered that the first recorded game of "Bass-Ball" took place in 1749 in Surrey, and featured the Prince of Wales as a player. William Bray, an English lawyer, recorded a game of baseball on Easter Monday 1755 in Guildford, Surrey. This early form of the game was apparently brought to North America by English immigrants. Rounders was also brought to the continent by both British and Irish immigrants. The first known American reference to baseball appears in a 1791 Pittsfield, Massachusetts, town bylaw prohibiting the playing of the game near the town's new meeting house. By 1796, a version of the game was well-known enough to earn a mention in a German scholar's book on popular pastimes. As described by Johann Gutsmuths, "englische Base-ball" involved a contest between two teams, in which "the batter has three attempts to hit the ball while at the home plate." Only one out was required to retire a side.
By the early 1830s, there were reports of a variety of uncodified bat-and-ball games recognizable as early forms of baseball being played around North America. These games were often referred to locally as "town ball", though other names such as "round-ball" and "base-ball" were also used. Among the earliest examples to receive a detailed description—albeit five decades after the fact, in a letter from an attendee to "Sporting Life" magazine—took place in Beachville, Ontario, in 1838. There were many similarities to modern baseball, and some crucial differences: five bases (or "byes"); first bye just 18 ft from the home bye; batter out if a hit ball was caught after the first bounce. The once widely accepted story that Abner Doubleday invented baseball in Cooperstown, New York, in 1839 has been conclusively debunked by sports historians.
In 1845, Alexander Cartwright, a member of New York City's Knickerbocker Club, led the codification of the so-called Knickerbocker Rules. The practice, common to bat-and-ball games of the day, of "soaking" or "plugging"—effecting a putout by hitting a runner with a thrown ball—was barred. The rules thus facilitated the use of a smaller, harder ball than had been common. Several other rules also brought the Knickerbockers' game close to the modern one, though a ball caught on the first bounce was, again, an out and only underhand pitching was allowed. While there are reports that the New York Knickerbockers played games in 1845, the contest now recognized as the first officially recorded baseball game in U.S. history took place on June 19, 1846, in Hoboken, New Jersey: the "New York Nine" defeated the Knickerbockers, 23–1, in four innings. With the Knickerbocker code as the basis, the rules of modern baseball continued to evolve over the next half-century.
History of baseball in the United States.
The game turns professional.
In the mid-1850s, a baseball craze hit the New York metropolitan area. By 1856, local journals were referring to baseball as the "national pastime" or "national game". A year later, sixteen area clubs formed the sport's first governing body, the National Association of Base Ball Players. In 1858 in Corona, Queens New York, at the Fashion Race Course, the first games of baseball to charge admission took place. The games, which took place between the all stars of Brooklyn, including players from the Brooklyn Atlantics, Excelsior of Brooklyn, Putnams and Eckford of Brooklyn, and the All Stars of New York (Manhattan), including players from the New York Knickerbockers, Gothams (predecessors of the San Francisco Giants), Eagles and Empire, are commonly believed to be the first all-star baseball games. In 1863, the organization disallowed putouts made by catching a fair ball on the first bounce. Four years later, it barred participation by African Americans. The game's commercial potential was developing: in 1869 the first fully professional baseball club, the Cincinnati Red Stockings, was formed and went undefeated against a schedule of semipro and amateur teams. The first professional league, the National Association of Professional Base Ball Players, lasted from 1871 to 1875; scholars dispute its status as a major league.
The more formally structured National League was founded in 1876. As the oldest surviving major league, the National League is sometimes referred to as the "senior circuit". Several other major leagues formed and failed. In 1884, African American Moses Walker (and, briefly, his brother Welday) played in one of these, the American Association. An injury ended Walker's major league career, and by the early 1890s, a gentlemen's agreement in the form of the baseball color line effectively barred black players from the white-owned professional leagues, major and minor. Professional Negro leagues formed, but quickly folded. Several independent African American teams succeeded as barnstormers. Also in 1884, overhand pitching was legalized. In 1887, softball, under the name of indoor baseball or indoor-outdoor, was invented as a winter version of the parent game. Virtually all of the modern baseball rules were in place by 1893; the last major change—counting foul balls as strikes—was instituted in 1901. The National League's first successful counterpart, the American League, which evolved from the minor Western League, was established that year. The two leagues, each with eight teams, were rivals that fought for the best players, often disregarding each other's contracts and engaging in bitter legal disputes.
A modicum of peace was eventually established, leading to the National Agreement of 1903. The pact formalized relations both between the two major leagues and between them and the National Association of Professional Base Ball Leagues, representing most of the country's minor professional leagues. The World Series, pitting the two major league champions against each other, was inaugurated that fall, albeit without express major league sanction: The Boston Americans of the American League defeated the Pittsburgh Pirates of the National League. The next year, the series was not held, as the National League champion New York Giants, under manager John McGraw, refused to recognize the major league status of the American League and its champion. In 1905, the Giants were National League champions again and team management relented, leading to the establishment of the World Series as the major leagues' annual championship event.
As professional baseball became increasingly profitable, players frequently raised grievances against owners over issues of control and equitable income distribution. During the major leagues' early decades, players on various teams occasionally attempted strikes, which routinely failed when their jobs were sufficiently threatened. In general, the strict rules of baseball contracts and the reserve clause, which bound players to their teams even when their contracts had ended, tended to keep the players in check. Motivated by dislike for particularly stingy owner Charles Comiskey and gamblers' payoffs, real and promised, members of the Chicago White Sox conspired to throw the 1919 World Series. The Black Sox Scandal led to the formation of a new National Commission of baseball that drew the two major leagues closer together. The first major league baseball commissioner, Kenesaw Mountain Landis, was elected in 1920. That year also saw the founding of the Negro National League; the first significant Negro league, it would operate until 1931. For part of the 1920s, it was joined by the Eastern Colored League.
Rise of Ruth and racial integration.
Compared with the present, professional baseball in the early 20th century was lower-scoring and pitchers, the likes of Walter Johnson and Christy Mathewson, were more dominant. The "inside game", which demanded that players "scratch for runs", was played much more aggressively than it is today: the brilliant and often violent Ty Cobb epitomized this style. The so-called dead-ball era ended in the early 1920s with several changes in rule and circumstance that were advantageous to hitters. Strict new regulations governing the ball's size, shape and composition along with a new rule officially banning the spitball, along with other pitches that depended on the ball being treated or roughed-up with foreign substances after the death of Ray Chapman who was hit by a pitch in August 1920, coupled with superior materials available after World War I, resulted in a ball that traveled farther when hit. The construction of additional seating to accommodate the rising popularity of the game often had the effect of bringing the outfield fences closer in, making home runs more common. The rise of the legendary player Babe Ruth, the first great power hitter of the new era, helped permanently alter the nature of the game. The club with which Ruth set most of his slugging records, the New York Yankees, built a reputation as the majors' premier team. In the late 1920s and early 1930s, St. Louis Cardinals general manager Branch Rickey invested in several minor league clubs and developed the first modern "farm system". A new Negro National League was organized in 1933; four years later, it was joined by the Negro American League. The first elections to the Baseball Hall of Fame took place in 1936. In 1939 Little League Baseball was founded in Pennsylvania. By the late 1940s, it was the organizing body for children's baseball leagues across the United States.
With America's entry into World War II, many professional players had left to serve in the armed forces. A large number of minor league teams disbanded as a result and the major league game seemed under threat as well. Chicago Cubs owner Philip K. Wrigley led the formation of a new professional league with women players to help keep the game in the public eye – the All-American Girls Professional Baseball League existed from 1943 to 1954. The inaugural College World Series was held in 1947, and the Babe Ruth League youth program was founded. This program soon became another important organizing body for children's baseball. The first crack in the unwritten agreement barring blacks from white-controlled professional ball occurred the previous year: Jackie Robinson was signed by the National League's Brooklyn Dodgers—where Branch Rickey had become general manager—and began playing for their minor league team in Montreal. In 1947, Robinson broke the major leagues' color barrier when he debuted with the Dodgers. Larry Doby debuted with the American League's Cleveland Indians the same year. Latin American players, largely overlooked before, also started entering the majors in greater numbers. In 1951, two Chicago White Sox, Venezuelan-born Chico Carrasquel and black Cuban-born Minnie Miñoso, became the first Hispanic All-Stars.
Facing competition as varied as television and football, baseball attendance at all levels declined. While the majors rebounded by the mid-1950s, the minor leagues were gutted and hundreds of semipro and amateur teams dissolved. Integration proceeded slowly: by 1953, only six of the 16 major league teams had a black player on the roster. That year, the Major League Baseball Players Association was founded. It was the first professional baseball union to survive more than briefly, but it remained largely ineffective for years. No major league team had been located west of St. Louis until 1958, when the Brooklyn Dodgers and New York Giants relocated to Los Angeles and San Francisco, respectively. The majors' final all-white bastion, the Boston Red Sox, added a black player in 1959. With the integration of the majors drying up the available pool of players, the last Negro league folded the following year. In 1961, the American League reached the West Coast with the Los Angeles Angels expansion team, and the major league season was extended from 154 games to 162. This coincidentally helped Roger Maris break Babe Ruth's long-standing single-season home run record, one of the most celebrated marks in baseball. Along with the Angels, three other new franchises were launched during 1961–62. With this, the first major league expansion in 60 years, each league now had ten teams.
Attendance records and the age of steroids.
The players' union became bolder under the leadership of former United Steelworkers chief economist and negotiator Marvin Miller, who was elected executive director in 1966. On the playing field, major league pitchers were becoming increasingly dominant again. After the 1968 season, in an effort to restore balance, the strike zone was reduced and the height of the pitcher's mound was lowered from 15 to 10 inches. In 1969, both the National and American leagues added two more expansion teams, the leagues were reorganized into two divisions each, and a post-season playoff system leading to the World Series was instituted. Also that same year, Curt Flood of the St. Louis Cardinals made the first serious legal challenge to the reserve clause. The major leagues' first general players' strike took place in 1972. In another effort to add more offense to the game, the American League adopted the designated hitter rule the following year. In 1975, the union's power—and players' salaries—began to increase greatly when the reserve clause was effectively struck down, leading to the free agency system. In 1977, two more expansion teams joined the American League. Significant work stoppages occurred again in 1981 and 1994, the latter forcing the cancellation of the World Series for the first time in 90 years. Attendance had been growing steadily since the mid-1970s and in 1994, before the stoppage, the majors were setting their all-time record for per-game attendance.
The addition of two more expansion teams after the 1993 season had facilitated another restructuring of the major leagues, this time into three divisions each. Offensive production—the number of home runs in particular—had surged that year, and again in the abbreviated 1994 season. After play resumed in 1995, this trend continued and non-division-winning wild card teams became a permanent fixture of the post-season. Regular-season interleague play was introduced in 1997 and the second-highest attendance mark for a full season was set. The next year, Mark McGwire and Sammy Sosa both surpassed Maris's decades-old single season home run record and two more expansion franchises were added. In 2000, the National and American leagues were dissolved as legal entities. While their identities were maintained for scheduling purposes (and the designated hitter distinction), the regulations and other functions—such as player discipline and umpire supervision—they had administered separately were consolidated under the rubric of Major League Baseball (MLB).
In 2001, Barry Bonds established the current record of 73 home runs in a single season. There had long been suspicions that the dramatic increase in power hitting was fueled in large part by the abuse of illegal steroids (as well as by the dilution of pitching talent due to expansion), but the issue only began attracting significant media attention in 2002 and there was no penalty for the use of performance-enhancing drugs before 2004. In 2007, Bonds became MLB's all-time home run leader, surpassing Hank Aaron, as total major league and minor league attendance both reached all-time highs. Even though McGwire, Sosa, and Bonds—as well as many other players, including storied pitcher Roger Clemens—have been implicated in the steroid abuse scandal, their feats and those of other sluggers had become the major leagues' defining attraction. In contrast to the professional game's resurgence in popularity after the 1994 interruption, Little League enrollment was in decline: after peaking in 1996, it dropped 1 percent a year over the following decade. With more rigorous testing and penalties for performance-enhancing drug use a possible factor, the balance between bat and ball swung markedly in 2010, which became known as the "Year of the Pitcher". Runs per game fell to their lowest level in 18 years, and the strikeout rate was higher than it had been in half a century.
Before the start of the 2012 season, MLB altered its rules to double the number of wild card teams admitted into the playoffs to two per league. The playoff expansion resulted in the addition of annual one-game playoffs between the wild card teams in each league.
Baseball around the world.
Baseball, widely known as America's pastime, is well established in several other countries as well. The history of baseball in Canada has remained closely linked with that of the sport in the United States. As early as 1877, a professional league, the International Association, featured teams from both countries. While baseball is widely played in Canada and many minor league teams have been based in the country, the American major leagues did not include a Canadian club until 1969, when the Montreal Expos joined the National League as an expansion team. In 1977, the expansion Toronto Blue Jays joined the American League. The Blue Jays won the World Series in 1992 and 1993, the first and still the only club from outside the United States to do so. After the 2004 season, Major League Baseball relocated the Expos to Washington, D.C., where the team is now known as the Nationals.
In 1847, American soldiers played what may have been the first baseball game in Mexico at Parque Los Berros in Xalapa, Veracruz. A few days after the Battle of Cerro Gordo, they used the "wooden leg captured (by the Fourth Illinois regiment) from General Santa Anna". The first formal baseball league outside of the United States and Canada was founded in 1878 in Cuba, which maintains a rich baseball tradition and whose national team has been one of the world's strongest since international play began in the late 1930s (all organized baseball in the country has officially been amateur since the Cuban Revolution). The Dominican Republic held its first islandwide championship tournament in 1912. Professional baseball tournaments and leagues began to form in other countries between the world wars, including the Netherlands (formed in 1922), Australia (1934), Japan (1936), Mexico (1937), and Puerto Rico (1938). The Japanese major leagues—the Central League and Pacific League—have long been considered the highest quality professional circuits outside of the United States. Japan has a professional minor league system as well, though it is much smaller than the American version—each team has only one farm club in contrast to MLB teams' four or five.
After World War II, professional leagues were founded in many Latin American nations, most prominently Venezuela (1946) and the Dominican Republic (1955). Since the early 1970s, the annual Caribbean Series has matched the championship clubs from the four leading Latin American winter leagues: the Dominican Professional Baseball League, Mexican Pacific League, Puerto Rican Professional Baseball League, and Venezuelan Professional Baseball League. In Asia, South Korea (1982), Taiwan (1990), and China (2003) all have professional leagues.
Many European countries have professional leagues as well, the most successful, other than the Dutch league, being the Italian league founded in 1948. Compared to those in Asia and Latin America, the various European leagues and the one in Australia historically have had no more than niche appeal. In 2004, Australia won a surprise silver medal at the Olympic Games. The Israel Baseball League, launched in 2007, folded after one season. The Confédération Européene de Baseball (European Baseball Confederation), founded in 1953, organizes a number of competitions between clubs from different countries, as well as national squads. Other competitions between national teams, such as the Baseball World Cup and the Olympic baseball tournament, were administered by the International Baseball Federation (IBAF) from its formation in 1938 until its 2013 merger with the International Softball Federation to create the current joint governing body for both sports, the World Baseball Softball Confederation (WBSC). By 2009, the IBAF had 117 member countries. Women's baseball is played on an organized amateur basis in many of the countries where it is a leading men's sport. Since 2004, the IBAF and now WBSC have sanctioned the Women's Baseball World Cup, featuring national teams.
After being admitted to the Olympics as a medal sport beginning with the 1992 Games, baseball was dropped from the 2012 Summer Olympic Games at the 2005 International Olympic Committee meeting. It remained part of the 2008 Games. The elimination of baseball, along with softball, from the 2012 Olympic program enabled the IOC to consider adding two different sports, but none received the votes required for inclusion. While the sport's lack of a following in much of the world was a factor, more important was Major League Baseball's reluctance to have a break during the Games to allow its players to participate, as the National Hockey League now does during the Winter Olympic Games. Such a break is more difficult for MLB to accommodate because it would force the playoffs deeper into cold weather. Seeking reinstatement for the 2016 Summer Olympics, the IBAF proposed an abbreviated competition designed to facilitate the participation of top players, but the effort failed. Major League Baseball initiated the World Baseball Classic, scheduled to precede the major league season, partly as a replacement, high-profile international tournament. The inaugural Classic, held in March 2006, was the first tournament involving national teams to feature a significant number of MLB participants. The Baseball World Cup was discontinued after its 2011 edition in favor of an expanded World Baseball Classic.
Rules and gameplay.
A game is played between two teams, each comprising nine players, that take turns playing offense (batting and baserunning) and defense (pitching and fielding). A pair of turns, one at bat and one in the field, by each team constitutes an inning. A game consists of nine innings (seven innings at the high school level and in doubleheaders in college and minor leagues). One team—customarily the visiting team—bats in the top, or first half, of every inning. The other team—customarily the home team—bats in the bottom, or second half, of every inning. The goal of the game is to score more points (runs) than the other team. The players on the team at bat attempt to score runs by circling or completing a tour of the four bases set at the corners of the square-shaped baseball diamond. A player bats at home plate and must proceed counterclockwise to first base, second base, third base, and back home in order to score a run. The team in the field attempts both to prevent runs from scoring and to record outs, which remove opposing players from offensive action until their turn in their team's batting order comes up again. When three outs are recorded, the teams switch roles for the next half-inning. If the score of the game is tied after nine innings, extra innings are played to resolve the contest. Many amateur games, particularly unorganized ones, involve different numbers of players and innings.
The game is played on a field whose primary boundaries, the foul lines, extend forward from home plate at 45-degree angles. The 90-degree area within the foul lines is referred to as fair territory; the 270-degree area outside them is foul territory. The part of the field enclosed by the bases and several yards beyond them is the infield; the area farther beyond the infield is the outfield. In the middle of the infield is a raised pitcher's mound, with a rectangular rubber plate (the rubber) at its center. The outer boundary of the outfield is typically demarcated by a raised fence, which may be of any material and height (many amateur games are played on unfenced fields). Fair territory between home plate and the outfield boundary is baseball's field of play, though significant events can take place in foul territory, as well.
There are three basic tools of baseball: the ball, the bat, and the glove or mitt:
Protective helmets are also standard equipment for all batters.
At the beginning of each half-inning, the nine players on the fielding team arrange themselves around the field. One of them, the pitcher, stands on the pitcher's mound. The pitcher begins the pitching delivery with one foot on the rubber, pushing off it to gain velocity when throwing toward home plate. Another player, the catcher, squats on the far side of home plate, facing the pitcher. The rest of the team faces home plate, typically arranged as four infielders—who set up along or within a few yards outside the imaginary lines between first, second, and third base—and three outfielders. In the standard arrangement, there is a first baseman positioned several steps to the left of first base, a second baseman to the right of second base, a shortstop to the left of second base, and a third baseman to the right of third base. The basic outfield positions are left fielder, center fielder, and right fielder. A neutral umpire sets up behind the catcher. Other umpires will be distributed around the field as well, though the number will vary depending on the level of play, amateur or children's games may only have an umpire behind the plate, while as many as six umpires can be used for important Major League Baseball games.
Play starts with a batter standing at home plate, holding a bat. The batter waits for the pitcher to throw a pitch (the ball) toward home plate, and attempts to hit the ball with the bat. The catcher catches pitches that the batter does not hit—as a result of either electing not to swing or failing to connect—and returns them to the pitcher. A batter who hits the ball into the field of play must drop the bat and begin running toward first base, at which point the player is referred to as a "runner" (or, until the play is over, a "batter-runner"). A batter-runner who reaches first base without being put out (see below) is said to be "safe" and is now on base. A batter-runner may choose to remain at first base or attempt to advance to second base or even beyond—however far the player believes can be reached safely. A player who reaches base despite proper play by the fielders has recorded a hit. A player who reaches first base safely on a hit is credited with a single. If a player makes it to second base safely as a direct result of a hit, it is a double; third base, a triple. If the ball is hit in the air within the foul lines over the entire outfield (and outfield fence, if there is one), it is a home run: the batter and any runners on base may all freely circle the bases, each scoring a run. This is the most desirable result for the batter. A player who reaches base due to a fielding mistake is not credited with a hit—instead, the responsible fielder is charged with an error.
Any runners already on base may attempt to advance on batted balls that land, or contact the ground, in fair territory, before or after the ball lands. A runner on first base "must" attempt to advance if a ball lands in play. If a ball hit into play rolls foul before passing through the infield, it becomes dead and any runners must return to the base they were at when the play began. If the ball is hit in the air and caught before it lands, the batter has flied out and any runners on base may attempt to advance only if they tag up or touch the base they were at when the play began, as or after the ball is caught. Runners may also attempt to advance to the next base while the pitcher is in the process of delivering the ball to home plate—a successful effort is a stolen base.
A pitch that is not hit into the field of play is called either a strike or a ball. A batter against whom three strikes are recorded strikes out. A batter against whom four balls are recorded is awarded a base on balls or walk, a free advance to first base. (A batter may also freely advance to first base if the batter's body or uniform is struck by a pitch outside the strike zone, provided the batter does not swing and attempts to avoid being hit.) Crucial to determining balls and strikes is the umpire's judgment as to whether a pitch has passed through the strike zone, a conceptual area above home plate extending from the midpoint between the batter's shoulders and belt down to the hollow of the knee.
A strike is called when one of the following happens:
A ball is called when the pitcher throws a pitch that is outside the strike zone, provided the batter has not swung at it.
While the team at bat is trying to score runs, the team in the field is attempting to record outs. Among the various ways a member of the batting team may be put out, five are most common:
It is possible to record two outs in the course of the same play—a double play. Even three—a triple play—is possible, though this is very rare. Players put out or retired must leave the field, returning to their team's dugout or bench. A runner may be stranded on base when a third out is recorded against another player on the team. Stranded runners do not benefit the team in its next turn at bat—every half-inning begins with the bases empty of runners.
An individual player's turn batting or plate appearance is complete when the player reaches base, hits a home run, makes an out, or hits a ball that results in the team's third out, even if it is recorded against a teammate. On rare occasions, a batter may be at the plate when, without the batter's hitting the ball, a third out is recorded against a teammate—for instance, a runner getting caught stealing (tagged out attempting to steal a base). A batter with this sort of incomplete plate appearance starts off the team's next turn batting; any balls or strikes recorded against the batter the previous inning are erased. A runner may circle the bases only once per plate appearance and thus can score at most a single run per batting turn. Once a player has completed a plate appearance, that player may not bat again until the eight other members of the player's team have all taken their turn at bat. The batting order is set before the game begins, and may not be altered except for substitutions. Once a player has been removed for a substitute, that player may not reenter the game. Children's games often have more liberal substitution rules.
If the designated hitter (DH) rule is in effect, each team has a tenth player whose sole responsibility is to bat (and run). The DH takes the place of another player—almost invariably the pitcher—in the batting order, but does not field. Thus, even with the DH, each team still has a batting order of nine players and a fielding arrangement of nine players.
Personnel.
Player rosters.
Roster, or squad, sizes differ between different leagues and different levels of organized play. Major League Baseball teams maintain 25-player active rosters. A typical 25-man roster in a league without the DH rule, such as MLB's National League, features:
Other personnel.
The manager, or head coach of a team, oversees the team's major strategic decisions, such as establishing the starting rotation, setting the lineup, or batting order, before each game, and making substitutions during games—in particular, bringing in relief pitchers. Managers are typically assisted by two or more coaches; they may have specialized responsibilities, such as working with players on hitting, fielding, pitching, or strength and conditioning. At most levels of organized play, two coaches are stationed on the field when the team is at bat: the first base coach and third base coach, occupying designated coaches' boxes just outside the foul lines, assist in the direction of baserunners when the ball is in play, and relay tactical signals from the manager to batters and runners during pauses in play. In contrast to many other team sports, baseball managers and coaches generally wear their team's uniforms; coaches must be in uniform in order to be allowed on the playing field during a game.
Any baseball game involves one or more umpires, who make rulings on the outcome of each play. At a minimum, one umpire will stand behind the catcher, to have a good view of the strike zone, and call balls and strikes. Additional umpires may be stationed near the other bases, thus making it easier to judge plays such as attempted force outs and tag outs. In Major League Baseball, four umpires are used for each game, one near each base. In the playoffs, six umpires are used: one at each base and two in the outfield along the foul lines.
Strategy and tactics.
Many of the pre-game and in-game strategic decisions in baseball revolve around a fundamental fact: in general, right-handed batters tend to be more successful against left-handed pitchers and, to an even greater degree, left-handed batters tend to be more successful against right-handed pitchers. A manager with several left-handed batters in the regular lineup who knows the team will be facing a left-handed starting pitcher may respond by starting one or more of the right-handed backups on the team's roster. During the late innings of a game, as relief pitchers and pinch hitters are brought in, the opposing managers will often go back and forth trying to create favorable matchups with their substitutions: the manager of the fielding team trying to arrange same-handed pitcher-batter matchups, the manager of the batting team trying to arrange opposite-handed matchups. With a team that has the lead in the late innings, a manager may remove a starting position player—especially one whose turn at bat is not likely to come up again—for a more skillful fielder.
Pitching and fielding tactics.
The tactical decision that precedes almost every play in a baseball game involves pitch selection. By gripping and then releasing the baseball in a certain manner, and by throwing it at a certain speed, pitchers can cause the baseball to break to either side, or downward, as it approaches the batter. Among the resulting wide variety of pitches that may be thrown, the four basic types are the fastball, the changeup (or off-speed pitch), and two breaking balls—the curveball and the slider. Pitchers have different repertoires of pitches they are skillful at throwing. Conventionally, before each pitch, the catcher signals the pitcher what type of pitch to throw, as well as its general vertical and/or horizontal location. If there is disagreement on the selection, the pitcher may shake off the sign and the catcher will call for a different pitch. With a runner on base and taking a lead, the pitcher may attempt a pickoff, a quick throw to a fielder covering the base to keep the runner's lead in check or, optimally, effect a tag out. Pickoff attempts, however, are subject to rules that severely restrict the pitcher's movements before and during the pickoff attempt. Violation of any one of these rules could result in the umpire calling a balk against the pitcher, with the result being runners on base, if any, advance one base with impunity. If an attempted stolen base is anticipated, the catcher may call for a pitchout, a ball thrown deliberately off the plate, allowing the catcher to catch it while standing and throw quickly to a base. Facing a batter with a strong tendency to hit to one side of the field, the fielding team may employ a shift, with most or all of the fielders moving to the left or right of their usual positions. With a runner on third base, the infielders may play in, moving closer to home plate to improve the odds of throwing out the runner on a ground ball, though a sharply hit grounder is more likely to carry through a drawn-in infield.
Batting and baserunning tactics.
Several basic offensive tactics come into play with a runner on first base, including the fundamental choice of whether to attempt a steal of second base. The hit and run is sometimes employed with a skillful contact hitter: the runner takes off with the pitch drawing the shortstop or second baseman over to second base, creating a gap in the infield for the batter to poke the ball through. The sacrifice bunt calls for the batter to focus on making contact with the ball so that it rolls a short distance into the infield, allowing the runner to advance into scoring position even at the expense of the batter being thrown out at first—a batter who succeeds is credited with a sacrifice. (A batter, particularly one who is a fast runner, may also attempt to bunt for a hit.) A sacrifice bunt employed with a runner on third base, aimed at bringing that runner home, is known as a squeeze play. With a runner on third and fewer than two outs, a batter may instead concentrate on hitting a fly ball that, even if it is caught, will be deep enough to allow the runner to tag up and score—a successful batter in this case gets credit for a sacrifice fly. The manager will sometimes signal a batter who is ahead in the count (i.e., has more balls than strikes) to take, or not swing at, the next pitch.
Distinctive elements.
Baseball has certain attributes that set it apart from the other popular team sports in the countries where it has a following, including but not limited to American and Canadian football, basketball, ice hockey, and soccer. All of these sports use a clock; in all of them, play is less individual and more collective; and in none of them is the variation between playing fields nearly as substantial or important. The comparison between cricket and baseball demonstrates that many of baseball's distinctive elements are shared in various ways with its cousin sports.
No clock to kill.
In clock-limited sports, games often end with a team that holds the lead killing the clock rather than competing aggressively against the opposing team. In contrast, baseball has no clock; a team cannot win without getting the last batter out and rallies are not constrained by time. At almost any turn in any baseball game, the most advantageous strategy is some form of aggressive strategy. In contrast, again, the clock comes into play even in the case of multi-day Test and first-class cricket: the possibility of a draw often encourages a team that is batting last and well behind to bat defensively, giving up any faint chance at a win to avoid a loss. Baseball offers no such reward for conservative batting.
While nine innings has been the standard since the beginning of professional baseball, the duration of the average major league game has increased steadily through the years. At the turn of the 20th century, games typically took an hour and a half to play. In the 1920s, they averaged just less than two hours, which eventually ballooned to 2:38 in 1960. By 1997, the average American League game lasted 2:57 (National League games were about 10 minutes shorter—pitchers at the plate making for quicker outs than designated hitters). In 2004, Major League Baseball declared that its goal was an average game of merely 2:45. By 2014, though, the average MLB game took over three hours to complete. The lengthening of games is attributed to longer breaks between half-innings for television commercials, increased offense, more pitching changes, and a slower pace of play with pitchers taking more time between each delivery, and batters stepping out of the box more frequently. Other leagues have experienced similar issues. In 2008, Nippon Professional Baseball took steps aimed at shortening games by 12 minutes from the preceding decade's average of 3:18.
Individual focus.
Although baseball is a team sport, individual players are often placed under scrutiny and pressure. In 1915, a baseball instructional manual pointed out that every single pitch, of which there are often more than two hundred in a game, involves an individual, one-on-one contest: "the pitcher and the batter in a battle of wits". Contrasting the game with both football and basketball, scholar Michael Mandelbaum argues that "baseball is the one closest in evolutionary descent to the older individual sports". Pitcher, batter, and fielder all act essentially independent of each other. While coaching staffs can signal pitcher or batter to pursue certain tactics, the execution of the play itself is a series of solitary acts. If the batter hits a line drive, the outfielder is solely responsible for deciding to try to catch it or play it on the bounce and for succeeding or failing. The statistical precision of baseball is both facilitated by this isolation and reinforces it. As described by Mandelbaum,
It is impossible to isolate and objectively assess the contribution each [football] team member makes to the outcome of the play ... [E]very basketball player is interacting with all of his teammates all the time. In baseball, by contrast, every player is more or less on his own ... Baseball is therefore a realm of complete transparency and total responsibility. A baseball player lives in a glass house, and in a stark moral universe ... Everything that every player does is accounted for and everything accounted for is either good or bad, right or wrong.
Cricket is more similar to baseball than many other team sports in this regard: while the individual focus in cricket is mitigated by the importance of the batting partnership and the practicalities of tandem running, it is enhanced by the fact that a batsman may occupy the wicket for an hour or much more. There is no statistical equivalent in cricket for the fielding error and thus less emphasis on personal responsibility in this area of play.
Uniqueness of each baseball park.
Unlike those of most sports, baseball playing fields can vary significantly in size and shape. While the dimensions of the infield are specifically regulated, the only constraint on outfield size and shape for professional teams following the rules of Major League and Minor League Baseball is that fields built or remodeled since June 1, 1958, must have a minimum distance of 325 ft from home plate to the fences in left and right field and 400 ft to center. Major league teams often skirt even this rule. For example, at Minute Maid Park, which became the home of the Houston Astros in 2000, the Crawford Boxes in left field are only 315 ft from home plate. There are no rules at all that address the height of fences or other structures at the edge of the outfield. The most famously idiosyncratic outfield boundary is the left-field wall at Boston's Fenway Park, in use since 1912: the Green Monster is 310 ft from home plate down the line and 37 ft tall.
Similarly, there are no regulations at all concerning the dimensions of foul territory. Thus a foul fly ball may be entirely out of play in a park with little space between the foul lines and the stands, but a foulout in a park with more expansive foul ground. A fence in foul territory that is close to the outfield line will tend to direct balls that strike it back toward the fielders, while one that is farther away may actually prompt more collisions, as outfielders run full speed to field balls deep in the corner. These variations can make the difference between a double and a triple or inside-the-park home run. The surface of the field is also unregulated. While the image to the left shows a traditional field surfacing arrangement (and the one used by virtually all MLB teams with naturally surfaced fields), teams are free to decide what areas will be grassed or bare. Some fields—including several in MLB—use an artificial surface, such as AstroTurf. Surface variations can have a significant effect on how ground balls behave and are fielded as well as on baserunning. Similarly, the presence of a roof (seven major league teams play in stadiums with permanent or retractable roofs) can greatly affect how fly balls are played. While football and soccer players deal with similar variations of field surface and stadium covering, the size and shape of their fields are much more standardized. The area out-of-bounds on a football or soccer field does not affect play the way foul territory in baseball does, so variations in that regard are largely insignificant.
These physical variations create a distinctive set of playing conditions at each ballpark. Other local factors, such as altitude and climate, can also significantly affect play. A given stadium may acquire a reputation as a pitcher's park or a hitter's park, if one or the other discipline notably benefits from its unique mix of elements. The most exceptional park in this regard is Coors Field, home of the Colorado Rockies. Its high altitude—5282 ft above sea level—is responsible for giving it the strongest hitter's park effect in the major leagues. Wrigley Field, home of the Chicago Cubs, is known for its fickle disposition: a hitter's park when the strong winds off Lake Michigan are blowing out, it becomes more of a pitcher's park when they are blowing in. The absence of a standardized field affects not only how particular games play out, but the nature of team rosters and players' statistical records. For example, hitting a fly ball 330 ft into right field might result in an easy catch on the warning track at one park, and a home run at another. A team that plays in a park with a relatively short right field, such as the New York Yankees, will tend to stock its roster with left-handed pull hitters, who can best exploit it. On the individual level, a player who spends most of his career with a team that plays in a hitter's park will gain an advantage in batting statistics over time—even more so if his talents are especially suited to the park.
Statistics.
Organized baseball lends itself to statistics to a greater degree than many other sports. Each play is discrete and has a relatively small number of possible outcomes. In the late 19th century, a former cricket player, English-born Henry Chadwick of Brooklyn, New York, was responsible for the "development of the box score, tabular standings, the annual baseball guide, the batting average, and most of the common statistics and tables used to describe baseball." The statistical record is so central to the game's "historical essence" that Chadwick came to be known as Father Baseball. In the 1920s, American newspapers began devoting more and more attention to baseball statistics, initiating what journalist and historian Alan Schwarz describes as a "tectonic shift in sports, as intrigue that once focused mostly on teams began to go to individual players and their statistics lines."
The Official Baseball Rules administered by Major League Baseball require the official scorer to categorize each baseball play unambiguously. The rules provide detailed criteria to promote consistency. The score report is the official basis for both the box score of the game and the relevant statistical records. General managers, managers, and baseball scouts use statistics to evaluate players and make strategic decisions.
Certain traditional statistics are familiar to most baseball fans. The basic batting statistics include:
The basic baserunning statistics include:
The basic pitching statistics include:
The basic fielding statistics include:
Among the many other statistics that are kept are those collectively known as "situational statistics". For example, statistics can indicate which specific pitchers a certain batter performs best against. If a given situation statistically favors a certain batter, the manager of the fielding team may be more likely to change pitchers or have the pitcher intentionally walk the batter in order to face one who is less likely to succeed.
Sabermetrics.
"Sabermetrics" refers to the field of baseball statistical study and the development of new statistics and analytical tools. The term is also used to refer directly to new statistics themselves. The term was coined around 1980 by one of the field's leading proponents, Bill James, and derives from the Society for American Baseball Research (SABR).
The growing popularity of sabermetrics since the early 1980s has brought more attention to two batting statistics that sabermetricians argue are much better gauges of a batter's skill than batting average:
Some of the new statistics devised by sabermetricians have gained wide use:
Popularity and cultural impact.
Writing in 1919, philosopher Morris Raphael Cohen described baseball as America's national religion. In the words of sports columnist Jayson Stark, baseball has long been "a unique paragon of American culture"—a status he sees as devastated by the steroid abuse scandal. Baseball has an important place in other national cultures as well: Scholar Peter Bjarkman describes "how deeply the sport is ingrained in the history and culture of a nation such as Cuba, [and] how thoroughly it was radically reshaped and nativized in Japan." Since the early 1980s, the Dominican Republic, in particular the city of San Pedro de Macorís, has been the major leagues' primary source of foreign talent. Hall-of-Famer Roberto Clemente remains one of the greatest national heroes in Puerto Rico's history. While baseball has long been the island's primary athletic pastime, its once well-attended professional winter league has declined in popularity since 1990, when young Puerto Rican players began to be included in the major leagues' annual first-year player draft. In the Western Hemisphere, baseball is also one of the leading sports in Canada, Colombia, Mexico, the Netherlands Antilles, Nicaragua, Panama, and Venezuela. In Asia, it is among the most popular sports in Japan, South Korea and Taiwan.
The major league game in the United States was originally targeted toward a middle-class, white-collar audience: relative to other spectator pastimes, the National League's set ticket price of 50 cents in 1876 was high, while the location of playing fields outside the inner city and the workweek daytime scheduling of games were also obstacles to a blue-collar audience. A century later, the situation was very different. With the rise in popularity of other team sports with much higher average ticket prices—football, basketball, and hockey—professional baseball had become among the most blue-collar-oriented of leading American spectator sports.
In the late 1900s and early 2000s, baseball's position compared to football in the United States moved in contradictory directions. In 2008, Major League Baseball set a revenue record of $6.5 billion, matching the NFL's revenue for the first time in decades. A new MLB revenue record of $6.6 billion was set in 2009. On the other hand, the percentage of American sports fans polled who named baseball as their favorite sport was 16%, compared to pro football at 31%. In 1985, the respective figures were pro football 24%, baseball 23%. Because there are so many more major league baseball games played, there is no comparison in overall attendance. In 2008, total attendance at major league games was the second-highest in history: 78.6 million, 0.7% off the record set the previous year. The following year, amid the U.S. recession, attendance fell by 6.6% to 73.4 million. Attendance at games held under the Minor League Baseball umbrella also set a record in 2007, with 42.8 million; this figure does not include attendance at games of the several independent minor leagues.
In Japan, where baseball is inarguably the leading spectator team sport, combined revenue for the twelve teams in Nippon Professional Baseball (NPB), the body that oversees both the Central and Pacific leagues, was estimated at $1 billion in 2007. Total NPB attendance for the year was approximately 20 million. While in the preceding two decades, MLB attendance grew by 50 percent and revenue nearly tripled, the comparable NPB figures were stagnant. There are concerns that MLB's growing interest in acquiring star Japanese players will hurt the game in their home country. In Cuba, where baseball is by every reckoning the national sport, the national team overshadows the city and provincial teams that play in the top-level domestic leagues. Revenue figures are not released for the country's amateur system. Similarly, according to one official pronouncement, the sport's governing authority "has never taken into account attendance ... because its greatest interest has always been the development of athletes".
As of 2007, Little League Baseball oversees more than 7,000 children's baseball leagues with more than 2.2 million participants—2.1 million in the United States and 123,000 in other countries. Babe Ruth League teams have over 1 million participants. According to the president of the International Baseball Federation, between 300,000 and 500,000 women and girls play baseball around the world, including Little League and the introductory game of Tee Ball.
A varsity baseball team is an established part of physical education departments at most high schools and colleges in the United States. In 2008, nearly half a million high schoolers and over 35,000 collegians played on their schools' baseball teams. The number of Americans participating in baseball has declined since the late 1980s, falling well behind the number of soccer participants. By early in the 20th century, intercollegiate baseball was Japan's leading sport. Today, high school baseball in particular is immensely popular there. The final rounds of the two annual tournaments—the National High School Baseball Invitational Tournament in the spring, and the even more important National High School Baseball Championship in the summer—are broadcast around the country. The tournaments are known, respectively, as Spring Koshien and Summer Koshien after the 55,000-capacity stadium where they are played. In Cuba, baseball is a mandatory part of the state system of physical education, which begins at age six. Talented children as young as seven are sent to special district schools for more intensive training—the first step on a ladder whose acme is the national baseball team.
Baseball in popular culture.
Baseball has had a broad impact on popular culture, both in the United States and elsewhere. Dozens of English-language idioms have been derived from baseball; in particular, the game is the source of a number of widely used sexual euphemisms. The first networked radio broadcasts in North America were of the 1922 World Series: famed sportswriter Grantland Rice announced play-by-play from New York City's Polo Grounds on WJZ–Newark, New Jersey, which was connected by wire to WGY–Schenectady, New York, and WBZ–Springfield, Massachusetts. The baseball cap has become a ubiquitous fashion item not only in the United States and Japan, but also in countries where the sport itself is not particularly popular, such as the United Kingdom.
Baseball has inspired many works of art and entertainment. One of the first major examples, Ernest Thayer's poem "Casey at the Bat", appeared in 1888. A wry description of the failure of a star player in what would now be called a "clutch situation", the poem became the source of vaudeville and other staged performances, audio recordings, film adaptations, and an opera, as well as a host of sequels and parodies in various media. There have been many baseball movies, including the Academy Award–winning "The Pride of the Yankees" (1942) and the Oscar nominees "The Natural" (1984) and "Field of Dreams" (1989). The American Film Institute's selection of the ten best sports movies includes "The Pride of the Yankees" at number 3 and "Bull Durham" (1988) at number 5. Baseball has provided thematic material for hits on both stage—the Adler–Ross musical "Damn Yankees"—and record—George J. Gaskin's "Slide, Kelly, Slide", Simon and Garfunkel's "Mrs. Robinson", and John Fogerty's "Centerfield". The baseball-founded comedic sketch "Who's on First", popularized by Abbott and Costello in 1938, quickly became famous. Six decades later, "Time" named it the best comedy routine of the 20th century. Baseball is also featured in various video games including ', "Wii Sports", ' and "Mario Baseball".
Literary works connected to the game include the short fiction of Ring Lardner and novels such as Bernard Malamud's "The Natural" (the source for the movie), Robert Coover's "The Universal Baseball Association, Inc., J. Henry Waugh, Prop.", and W. P. Kinsella's "Shoeless Joe" (the source for "Field of Dreams"). Baseball's literary canon also includes the beat reportage of Damon Runyon; the columns of Grantland Rice, Red Smith, Dick Young, and Peter Gammons; and the essays of Roger Angell. Among the celebrated nonfiction books in the field are Lawrence S. Ritter's "The Glory of Their Times", Roger Kahn's "The Boys of Summer", and Michael Lewis's "Moneyball". The 1970 publication of major league pitcher Jim Bouton's tell-all chronicle "Ball Four" is considered a turning point in the reporting of professional sports.
Baseball has also inspired the creation of new cultural forms. Baseball cards were introduced in the late 19th century as trade cards. A typical example would feature an image of a baseball player on one side and advertising for a business on the other. In the early 1900s they were produced widely as promotional items by tobacco and confectionery companies. The 1930s saw the popularization of the modern style of baseball card, with a player photograph accompanied on the rear by statistics and biographical data. Baseball cards—many of which are now prized collectibles—are the source of the much broader trading card industry, involving similar products for different sports and non-sports-related fields.
Modern fantasy sports began in 1980 with the invention of Rotisserie League Baseball by New York writer Daniel Okrent and several friends. Participants in a Rotisserie league draft notional teams from the list of active Major League Baseball players and play out an entire imaginary season with game outcomes based on the players' latest real-world statistics. Rotisserie-style play quickly became a phenomenon. Now known more generically as fantasy baseball, it has inspired similar games based on an array of different sports. The field boomed with increasing Internet access and new fantasy sports–related websites. By 2008, 29.9 million people in the United States and Canada were playing fantasy sports, spending $800 million on the hobby. The burgeoning popularity of fantasy baseball is also credited with the increasing attention paid to sabermetrics—first among fans, only later among baseball professionals.
Sources.
</dl>

</doc>
<doc id="3851" url="http://en.wikipedia.org/wiki?curid=3851" title="Baseball positions">
Baseball positions

There are 9 fielding positions in baseball. Each position conventionally has an associated number, which is used to score putouts: 1 (pitcher), 2 (catcher), 3 (first baseman), 4 (second baseman), 5 (third baseman), 6 (shortstop), 7 (left fielder), 8 (center fielder), and 9 (right fielder). For example:

</doc>
<doc id="3856" url="http://en.wikipedia.org/wiki?curid=3856" title="History of baseball in the United States">
History of baseball in the United States

The history of baseball in the United States can be traced to the 18th century, when amateurs played a baseball-like game by their own informal rules using improvised equipment. The popularity of the sport inspired the semipro national baseball clubs in the 1860s.
Early history.
The earliest known mention of baseball in the U.S was a 1791 Pittsfield, Massachusetts, ordinance banning the playing of the game within 80 yd of the town meeting house. In 1903, the British sportswriter Henry Chadwick published an article speculating that baseball derived from a British game called rounders, which Chadwick had played as a boy in England. But baseball executive Albert Spalding disagreed. Baseball, said Spalding, was fundamentally an American sport and began on American soil. To settle the matter, the two men appointed a commission, headed by Abraham Mills, the fourth president of the National League of Professional Baseball Clubs. The commission, which also included six other sports executives, labored for three years, after which it declared that Abner Doubleday invented the national pastime. This would have been a surprise to Doubleday. The late Civil War hero "never knew that he had invented baseball. [But] 15 years [after his death], he was anointed as the father of the game," writes baseball historian John Thorn. The myth about Doubleday inventing the game of baseball actually came from a Colorado mining engineer. Another early reference reports that "base ball" was regularly played on Saturdays in 1823 on the outskirts of New York City in an area that today is Greenwich Village.
In 1828, an article published in a Hagerstown, Maryland, newspaper briefly describes a young girl who's drawn away from her daily chores to play a familiar game with her friends. In "A Village Sketch," author Miss Mitford wrote: "Then comes a sun-burnt gipsy of six, beginning to grow tall and thin and to find the cares of the world gathering about her; with a pitcher in one hand, a mop in the other, an old straw bonnet of ambiguous shape, half hiding her tangled hair; a tattered stuff petticoat once green, hanging below an equally tattered cotton frock, once purple; her longing eyes fixed on a game of baseball at the corner of the green till she reaches the cottage door, flings down the mop and pitcher and darts off to her companions quite regardless of the storm of scolding with which the mother follows her runaway steps."
The first team to play baseball under modern rules were the New York Knickerbockers. The club was founded on September 23, 1845, as a social club for the upper middle classes of New York City, and was strictly amateur until it disbanded. The club members, which included its president Doc Adams and Alexander Cartwright, formulated the "Knickerbocker Rules", which in large part dealt with organizational matters but which also laid out rules for playing the game. Among the rules introduced then, and which characterize modern baseball are: nine-man teams; nine-inning games; bases 90 feet apart; elimination of the "bound rule". One of the significant rules prohibited "soaking" or "plugging" the runner; under older rules, a fielder could put a runner out by hitting the runner with the thrown ball, similarly to the common schoolyard game of kickball. The Knickerbocker Rules required fielders to tag or force the runner, as is done today, and avoided a lot of the arguments and fistfights that resulted from the earlier practice.
Writing the rules didn't help the Knickerbockers in the first known competitive game between two clubs under the new rules, played at Elysian Fields in Hoboken, New Jersey on June 19, 1846. The self-styled "New York Nine" humbled the Knickerbockers by a score of 23 to 1. Nevertheless, the Knickerbocker Rules were rapidly adopted by teams in the New York area and their version of baseball became known as the "New York Game" (as opposed to the "Massachusetts Game", played by clubs in the Boston area).
As late as 1855, the New York press was still devoting more space to coverage of cricket than to baseball.
In 1857, sixteen New York area clubs, including the Knickerbockers, formed the National Association of Base Ball Players (NABBP). The NABBP was the first organization to govern the sport and to establish a championship. Aided by the Civil War, membership grew to almost 100 clubs by 1865 and to over 400 by 1867, including clubs from as far away as California. During the Civil War, soldiers from different parts of the United States played baseball together, leading to a more unified national version of the sport. Beginning in 1869, the NABBP permitted professional play, addressing a growing practice that had not been permitted under its rules to that point. The first and most prominent professional clubs of the NABBP era were the Cincinnati Red Stockings in Ohio, which lasted only two years. Businessman Ivers Whitney Adams then courted manager Harry Wright and founded the "Boston Red Stockings" and the Boston Base Ball Club January 20, 1871.
In 1858 in Corona, Queens New York, at the Fashion Race Course, the first games of baseball to charge admission took place. The games, which took place between the all stars of Brooklyn, including players from the Brooklyn Atlantics, Excelsior of Brooklyn, Putnams and Eckford of Brooklyn, and the All Stars of New York (Manhattan), including players from the New York Knickerbockers, Gothams (predecessors of the San Francisco Giants), Eagles and Empire, are commonly believed to the first all star baseball games.
Growth.
Before the Civil War, baseball competed for public interest with cricket and regional variants of baseball, notably town ball played in Philadelphia and the Massachusetts Game played in New England. In the 1860s, aided by the War, "New York" style baseball expanded into a national game, as its first governing body, The National Association of Base Ball Players was formed. The NABBP soon expanded into a true national organization, although most of the strongest clubs remained those based in the northeastern part of the country. In its 12-year history as an amateur league, the Brooklyn Atlantics won seven championships, establishing themselves as the first true dynasty in the sport, although, the New York Mutuals were widely considered to be one of the best teams of the era as well. By the end of 1865, almost 100 clubs were members of the NABBP. By 1867, it ballooned to over 400 members, including some clubs from as far away as San Francisco and Louisiana. One of these clubs, the Chicago White Stockings, won the championship in 1870. Today known as the Chicago Cubs, they are the oldest team in American organized sports. Because of this growth, regional and state organizations began to assume a more prominent role in the governance of the sport.
Professionalism.
The NABBP of America was initially established upon principles of amateurism. However, even early in its history some star players, such as James Creighton of Excelsior, received compensation, either secretly or indirectly. In 1866, the NABBP investigated Athletic of Philadelphia for paying three players including Lip Pike, but ultimately took no action against either the club or the players. To address this growing practice, and to restore integrity to the game, at its December 1868 meeting the NABBP established a professional category for the 1869 season. Clubs desiring to pay players were now free to declare themselves professional.
The Cincinnati Red Stockings were the first to so declare themselves as openly professional, and were easily the most aggressive in recruiting the best available players. Twelve clubs, including most of the strongest clubs in the NABBP, ultimately declared themselves professional for the 1869 season.
The first attempt at forming a "major league" produced the National Association of Professional Base Ball Players, which lasted from 1871 to 1875. The now all professional Chicago White Stockings, financed by businessman William Hulbert, became a charter member of the league along with the Red Stockings, who had dissolved and moved to Boston. The White Stockings were close contenders all season, despite the fact that the Great Chicago Fire had destroyed the team's home field and most of their equipment. The White Stockings finished the season in second place, but ultimately were forced to drop out of the league during the city's recovery period, finally returning to National Association play in 1874. Over the next couple seasons, The Boston Red Stockings dominated the league and hoarded many of the game's best players, even those who were under contract with other teams. After Davy Force signed with Chicago, and then breached his contract to play in Boston, Hulbert became discouraged by the "contract jumping" as well as the overall disorganization of the N.A., and thus spearheaded the movement to form a stronger organization. The end result of his efforts was the formation a much more "ethical" league, which became known as the National Base Ball League. After a series of rival leagues were organized but failed, (most notably the American Base Ball Association, which spawned the clubs which would ultimately become the St. Louis Cardinals and Brooklyn Dodgers) the current American League, evolving from the minor Western League of 1893, was established in 1901.
Rise of the major leagues.
In 1870, a schism developed between professional and amateur ballplayers. The NABBP split into two groups. The National Association of "Professional" Base Ball Players operated from 1871 through 1875, and is considered by some to have been the first major league. Its amateur counterpart disappeared after only a few years.
William Hulbert's National League, which was formed after the National Association proved ineffective, put its emphasis on "clubs" rather than "players". Clubs now had the ability to enforce player contracts, preventing players from jumping to higher-paying clubs. Clubs in turn were required to play their full schedule of games, rather than forfeiting scheduled games once out of the running for the league championship, as happened frequently under the National Association. A concerted effort was made to reduce the amount of gambling on games which was leaving the validity of results in doubt.
At the same time, a "gentlemen's agreement" was struck between the clubs to exclude non-white players from professional baseball, a bar that remained until 1947. It is a common misconception that Jackie Robinson was the first African-American major-league ballplayer; he was actually only the first after a long gap (and the first in the modern era). Moses Fleetwood Walker and his brother Welday Walker were unceremoniously dropped from major and minor-league rosters in the 1880s, as were other African-Americans in baseball. An unknown number of African-Americans played in the major leagues by representing themselves as Indians, or South or Central Americans. And a still larger number played in the minor leagues and on amateur teams as well. In the majors, however, it was not until the signing of Robinson (in the National League) and Larry Doby (in the American League) that baseball began to remove its color bar.
The early years of the National League were tumultuous, with threats from rival leagues and a rebellion by players against the hated "reserve clause", which restricted the free movement of players between clubs. Competitive leagues formed regularly, and also disbanded regularly. The most successful was the American Association (1881–1891), sometimes called the "beer and whiskey league" for its tolerance of the sale of alcoholic beverages to spectators. For several years, the National League and American Association champions met in a postseason championship series—the first attempt at a World Series.
The Union Association survived for only one season (1884), as did the Players League (1890), an attempt to return to the National Association structure of a league controlled by the players themselves. Both leagues are considered major leagues by many baseball researchers because of the perceived high caliber of play (for a brief time anyway) and the number of star players featured. However, some researchers have disputed the major league status of the Union Association, pointing out that franchises came and went and contending that the St. Louis club, which was deliberately "stacked" by the league's president (who owned that club), was the only club that was anywhere close to major league caliber.
In fact, there were dozens of leagues, large and small, at this time. What made the National League "major" was its dominant position in the major cities, particularly New York City, the edgy, emotional nerve center of baseball. The large cities offered baseball teams national media distribution systems and fan bases that could generate revenues enabling teams to hire the best players in the country.
A number of other leagues, including the venerable Eastern League, threatened the dominance of the National League. The Western League, founded in 1893, became particularly aggressive. Its fiery leader Ban Johnson railed against the National League and promised to build a new league that would grab the best players and field the best teams. The Western League began play in April 1894 with teams in Detroit (the only league team that has not moved since), Grand Rapids, Indianapolis, Kansas City, Milwaukee, Minneapolis, Sioux City and Toledo. Prior to the 1900 season, the league changed its name to the American League and moved several franchises to larger, strategic locations. In 1901 the American League declared its intent to operate as a major league.
The resulting bidding war for players led to widespread contract-breaking and legal disputes. One of the most famous involved star second baseman Napoleon Lajoie, who in 1901 went across town in Philadelphia from the National League Phillies to the American League Athletics. Barred by a court injunction from playing baseball in the state of Pennsylvania the next year, Lajoie was traded to the Cleveland team, where he played and managed for many years.
The war between the American and National caused shock waves throughout the baseball world. At a meeting in 1901, the other baseball leagues negotiated a plan to maintain their independence. On September 5, 1901 Patrick T. Powers, president of the Eastern League announced the formation of the second National Association of Professional Baseball Leagues, the NABPL or "NA" for short.
These leagues did not consider themselves "minor" – a term that did not come into vogue until St. Louis Cardinals GM Branch Rickey pioneered the farm system in the 1930s. Nevertheless, these financially troubled leagues, by beginning the practice of selling players to the more affluent National and American leagues, embarked on a path that eventually led to the loss of their independent status.
Ban Johnson had other designs for the NA. While the NA continues to this day, he saw it as a tool to end threats from smaller rivals who might some day want to expand in other territories and threaten his league's dominance.
After 1902 both leagues and the NABPL signed a new National Agreement which achieved three things:
The new agreement tied independent contracts to the reserve-clause national league contracts. Baseball players were a commodity, like cars. $5,000 bought a player's skill set. It set up a rough classification system for independent leagues that regulated the dollar value of contracts, the forerunner of the system refined by Rickey and used today.
It also gave the NA great power. Many independents walked away from the 1901 meeting. The deal with the NA punished those other indies who had not joined the NA and submitted to the will of the 'majors.' The NA also agreed to the deal to prevent more pilfering of players with little or no compensation for the players' development. Several leagues, seeing the writing on the wall, eventually joined the NA, which grew in size over the next several years.
In the very early part of the 20th century, known as the "dead-ball era", baseball rules and equipment favored the "inside game" and the game was played more violently and aggressively than it is today. This period ended in the 1920s with several changes that gave advantages to hitters. In the largest parks, the outfield fences were brought closer to the infield. In addition, the strict enforcement of new rules governing the size, shape and construction of the ball caused it to travel farther.
The first professional black baseball club, the Cuban Giants, was organized in 1885. Subsequent professional black baseball clubs played each other independently, without an official league to organize the sport. Rube Foster, a former ballplayer, founded the Negro National League in 1920. A second league, the Eastern Colored League, was established in 1923. These became known as the Negro Leagues, though these leagues never had any formal overall structure comparable to the Major Leagues. The Negro National League did well until 1930, but folded during the Great Depression.
From 1942 to 1948, the Negro League World Series was revived. This was the golden era of Negro League baseball, a time when it produced some of its greatest stars. In 1947, Jackie Robinson signed a contract with the Brooklyn Dodgers, breaking the color barrier that had prevented talented African American players from entering the white-only major leagues. Although the transformation was not instantaneous, baseball has since become fully integrated. In 1948, the Negro Leagues faced financial difficulties that effectively ended their existence.
Pitchers dominated the game in the 1960s and early 1970s. In 1973, the designated hitter (DH) rule was adopted by the American League, while in the National League pitchers still bat for themselves to this day. The DH rule now constitutes the primary difference between the two leagues.
During the late 1960s, the Baseball Players Union became much stronger and conflicts between owners and the players' union led to major work stoppages in 1972, 1981, and 1994. The 1994 baseball strike led to the cancellation of the World Series, and was not settled until the spring of 1995. In the late 1990s, functions that had been administered separately by the two major leagues' administrations were united under the rubric of Major League Baseball.
The dead-ball era: 1900 to 1919.
At this time the games tended to be low scoring, dominated by such pitchers as Walter Johnson, Cy Young, Christy Mathewson, and Grover Cleveland Alexander to the extent that the period 1900–1919 is commonly called the "Dead-ball era". The term also accurately describes the condition of the baseball itself. Baseballs cost three dollars apiece, a hefty sum at the time, which in 1900 would be equal to $<br>{Inflation} - Amount must not have "" prefix: 3.   today; club owners were therefore reluctant to spend much money on new balls if not necessary. It was not unusual for a single baseball to last an entire game. By the end of the game, the ball would be dark with grass, mud, and tobacco juice, and it would be misshapen and lumpy from contact with the bat. Balls were only replaced if they were hit into the crowd and lost, and many clubs employed security guards expressly for the purpose of retrieving balls hit into the stands—a practice unthinkable today.
As a consequence, home runs were rare, and the "inside game" dominated—singles, bunts, stolen bases, the hit-and-run play, and other tactics dominated the strategies of the time.
Despite this, there were also several superstar hitters, the most famous being Honus Wagner, held to be one of the greatest shortstops to ever play the game, and Detroit's Ty Cobb, the "Georgia Peach." Cobb was a mean-spirited man, fiercely competitive and loathed by many of his fellow professionals, but his career batting average of .366 has yet to be bested.
The Merkle incident.
The 1908 pennant races in both the AL and NL were among the most exciting ever witnessed. The conclusion of the National League season, in particular, involved a bizarre chain of events, often referred to as the Merkle Boner. On September 23, 1908, the New York Giants and Chicago Cubs played a game in the Polo Grounds. Nineteen-year-old rookie first baseman Fred Merkle, later to become one of the best players at his position in the league, was on first base, with teammate Moose McCormick on third with two outs and the game tied. Giants shortstop Al Bridwell socked a single, scoring McCormick and apparently winning the game. However, Merkle, instead of advancing to second base, ran toward the clubhouse to avoid the spectators mobbing the field, which at that time was a common, acceptable practice. The Cubs' second baseman, Johnny Evers, noticed this. In the confusion that followed, Evers claimed to have retrieved the ball and touched second base, forcing Merkle out and nullifying the run scored. Evers brought this to the attention of the umpire that day, Hank O'Day, who after some deliberation called the runner out. Because of the state of the field O'Day thereby called the game. Despite the arguments by the Giants, the league upheld O'Day's decision and ordered the game replayed at the end of the season, if necessary. It turned out that the Cubs and Giants ended the season tied for first place, so the game was indeed replayed, and the Cubs won the game, the pennant, and subsequently the World Series (the last Cubs Series victory to date, as it turns out).
For his part, Merkle was doomed to endless criticism and vilification throughout his career for this lapse, which went down in history as "Merkle's Boner". In his defense, some baseball historians have suggested that it was not customary for game-ending hits to be fully "run out", it was only Evers's insistence on following the rules strictly that resulted in this unusual play. In fact, earlier in the 1908 season, the identical situation had been brought to the umpires' attention by Evers; the umpire that day was the same Hank O'Day. While the winning run was allowed to stand on that occasion, the dispute raised O'Day's awareness of the rule, and directly set up the Merkle controversy.
New places to play.
Turn of the century baseball attendances were modest by later standards. The average for the 1,110 games in the 1901 season was 3,247. However the first 20 years of the 20th century saw an unprecedented rise in the popularity of baseball. Large stadiums dedicated to the game were built for many of the larger clubs or existing grounds enlarged, including Tiger Stadium in Detroit, Shibe Park, home of the Philadelphia Athletics, Ebbets Field in Brooklyn, the Polo Grounds in Manhattan, Boston's Fenway Park along with Wrigley Field and Comiskey Park in Chicago. Likewise from the Eastern League to the small developing leagues in the West, and the rising Negro Leagues professional baseball was being played all across the country. Average major league attendances reached a pre World War I peak of 5,836 in 1909, before falling back during the war. Where there weren't professional teams, there were semi-pro teams, traveling teams barnstorming, company clubs and amateur men's leagues. In the days before television, if you wanted to see a game, you had to go to the ballpark.
The "Black Sox".
The fix of baseball games by gamblers and players working together had been suspected as early as the 1850s. Hal Chase was particularly notorious for throwing games, but played for a decade after gaining this reputation; he even managed to parlay these accusations into a promotion to manager. Even baseball stars such as Ty Cobb and Tris Speaker have been credibly alleged to have fixed game outcomes. When MLB's complacency during this "Golden Age" was eventually exposed after the 1919 World Series, it became known as the Black Sox scandal.
After an excellent regular season (88–52, .629 W%), the Chicago White Sox were heavy favorites to win the 1919 World Series. Arguably the best team in baseball, The White Sox had a deep lineup, a strong pitching staff, and a good defense. Even though the National League champion Cincinnati Reds had a superior regular season record (96–44, .689 W%,) no one, including gamblers and bookmakers, anticipated the Reds having a chance. When the Reds triumphed 5–3, many pundits cried foul.
At the time of the scandal, the White Sox were arguably the most successful franchise in baseball, with excellent gate receipts and record attendance. At the time, most baseball players were not paid especially well and had to work other jobs during the winter to survive. Some elite players on the big-city clubs made very good salaries, but Chicago was a notable exception.
For many years, the White Sox were owned and operated by Charles Comiskey, who paid the lowest player salaries, on average, in the American League. The White Sox players all intensely disliked Comiskey and his penurious ways, but were powerless to do anything, thanks to baseball’s so-called "reserve clause," that prevented players from switching teams without their team owner's consent.
By late 1919, Comiskey’s tyrannical reign over the Sox had sown deep bitterness among the players, and White Sox first baseman Arnold "Chick" Gandil decided to conspire to throw the 1919 World Series. He persuaded gambler Joseph "Sport" Sullivan, with whom he had had previous dealings, that the fix could be pulled off for $100,000 total (which would be equal to $ today), paid to the players involved. New York gangster Arnold Rothstein supplied the $100,000 that Gandil had requested through his lieutenant Abe Attell, a former featherweight boxing champion.
After the 1919 series, and through the beginning of the 1920 baseball season, rumors swirled that some of the players had conspired to purposefully lose. At last, in 1920, a grand jury was convened to investigate these and other allegations of fixed baseball games. Eight players (Charles "Swede" Risberg, Arnold "Chick" Gandil, "Shoeless" Joe Jackson, Oscar "Happy" Felsch, Eddie Cicotte, George "Buck" Weaver, Fred McMullin, and Claude "Lefty" Williams) were indicted and tried for conspiracy. The players were ultimately acquitted.
However, the damage to the reputation of the sport of baseball led the team owners to appoint Federal judge Kenesaw Mountain Landis to be the first Commissioner of Baseball. His first act as commissioner was to ban the "Black Sox" from professional baseball for life.
The Negro leagues.
Until July 5, 1947, baseball had two histories. One fills libraries, while baseball historians are only just beginning to chronicle the other fully. African Americans have played baseball as long as white Americans. Players of color, both African-American and Hispanic, played for white baseball clubs throughout the early days of the organizing amateur sport. Moses Fleetwood Walker is considered the first African-American to play at the major league level, in 1884.
The Negro Leagues were American professional baseball leagues comprising predominantly African-American teams. The term may be used broadly to include professional black teams outside the leagues and it may be used narrowly for the seven relatively successful leagues beginning 1920 that are sometimes termed "Negro Major Leagues".
The first professional team, established in 1885, achieved great and lasting success as the Cuban Giants, while the first league, the National Colored Base Ball League, failed in 1887 after only two weeks due to low attendance. The Negro American League of 1951 is considered the last major league season and the last professional club, the Indianapolis Clowns, operated amusingly rather than competitively from the mid-1960s to 1980s.
The first international leagues.
While many of the players that made up the black baseball teams were African-Americans, many more were Latin Americans from nations that deliver some of the greatest talents that make up the major league rosters of today. Black players moved freely through the rest of baseball, playing in Canadian Baseball, Mexican Baseball, Caribbean Baseball, and Central America and South America where more than a few found that level of fame that they were unable to attain in the country of their birth.
Babe Ruth and the end of the dead-ball era.
It was not the Black Sox scandal by which an end was put to the dead-ball era, but by a rule change and a single player.
Some of the increased offensive output can be explained by the 1920 rule change outlawing tampering with the ball, which pitchers had often done to produce "spitballs", "shine balls" and other trick pitches which had 'unnatural' flight through the air. Umpires were also required to put new balls into play whenever the current ball became scuffed or discolored. This rule change was enforced all the more stringently following the death of Ray Chapman, who was struck in the temple by a pitched ball from Carl Mays in a game on August 16, 1920 (he died the next day). Discolored balls, harder for batters to see and therefore harder for batters to dodge, have been rigorously removed from play ever since. There are two side effects. One, of course, is that if the batter can see the ball more easily, the batter can hit the ball more easily. The second is that without scuffs and other damage, pitchers are limited in their ability to control spin and so to cause altered trajectories.
At the end of the 1919 season Harry Frazee, then owner of the Boston Red Sox, sold a group of his star players to the New York Yankees. Amongst them was George Herman Ruth, known affectionately as "Babe". The story that Frazee did so in order to fund theatrical shows on Broadway for his actress lady friend is unfounded. No, No, Nanette was indeed first produced in 1925 by Harry Frazee, though the sale of baseball superstar Babe Ruth to the New York Yankees had occurred five years earlier. In the lore of the Curse of the Bambino, Frazee supposedly financed the production by selling Ruth, yet drawing a line five years apart from the sale's proceeds to the production costs of the musical are circumstantial at best.
Ruth's career mirrors the shift in dominance from pitching to hitting at this time. He started his career as a pitcher in 1914, and by 1916 was considered one of the dominant left-handed pitchers in the game. When Edward Barrow, managing the Red Sox, converted him to an outfielder, ballplayers and sportswriters were shocked. It was apparent, however, that Ruth's bat in the lineup every day was far more valuable than Ruth's arm on the mound every fourth day. Ruth swatted an unprecedented 29 home runs in his last season in Boston. The next year, as a Yankee, he would hit 54 and in 1921 he hit 59. His 1927 mark of 60 home runs would last until 1961, and, because of two record books (one for 154 game season and one for 162 game season), longer still.
Ruth's power hitting ability demonstrated a new way to play the game, and one that was extremely popular with the crowds. Accordingly, the ballparks were expanded, sometimes by building outfield seating which shrunk the size of the outfield and made home run hitting more practical. In addition to Ruth, hitters such as Rogers Hornsby also took advantage, with Hornsby compiling extraordinary figures for both power and average in the early 1920s. By the late 1920s and 1930s all the good teams had their home run hitting "sluggers": the Yankees' Lou Gehrig, Jimmie Foxx in Philadelphia, Hank Greenberg in Detroit and Chicago's Hack Wilson were the most storied. While the American League championship, and to a lesser extent the World Series, would be dominated by the Yankees, there were many other excellent teams in the inter-war years. Also, the National League's St. Louis Cardinals would win three titles themselves in nine years, the last with a group of players known as the "Gashouse Gang".
The first radio broadcast of a baseball game was on August 5, 1921 over Westinghouse station KDKA from Forbes Field in Pittsburgh. Harold Arlin announced the Pirates-Phillies game. Attendances in the 1920s were consistently better than they had been before the war. The interwar peak average attendance was 8,211 in 1930, but baseball was hit hard by the Great Depression and in 1933 the average fell below five thousand for the only time between the wars.
1933 also saw the introduction of the All-Star game, a mid-season break in which the greatest players in each league play against one another in a hard fought but officially meaningless demonstration game. In 1936 the Baseball Hall of Fame was instituted and five players elected: Ty Cobb, Walter Johnson, Christy Mathewson, Babe Ruth and Honus Wagner. The Hall formally opened in 1939.
The war years.
The beginning of US involvement in World War II necessitated depriving the game of many players who joined the armed forces, but the major leagues continued play throughout the duration. In 1941, a year which saw the premature death of Lou Gehrig, Boston's great left fielder Ted Williams had a batting average over .400 – the last time anyone has achieved that feat. During the same season Joe DiMaggio hit successfully in 56 consecutive games, an accomplishment both unprecedented and unequaled. Both Williams and DiMaggio would miss playing time in the services, with Williams also flying later in the Korean War. During this period Stan Musial led the St. Louis Cardinals to the 1942, 1944 and 1946 World Series titles. The war years also saw the founding of the All-American Girls Professional Baseball League.
Baseball boomed after World War II. 1945 saw a new attendance record and the following year average crowds leapt nearly 70% to 14,914. Further records followed in 1948 and 1949, when the average reached 16,913. While average attendances slipped to somewhat lower levels through the 1950s, 1960s and the first half of the 1970s, they remained well above pre-war levels, and total seasonal attendance regularly hit new highs from 1962 onwards as the number of major league games increased.
Racial integration in baseball.
The post-War years in baseball also witnessed the racial integration of the sport. Participation by African Americans in organized baseball had been precluded since the 1890s by formal and informal agreements, with only a few players surreptitiously being included in lineups on a sporadic basis.
American society as a whole moved toward integration in the post-War years, partially as a result of the distinguished service by African American military units such as the Tuskegee Airmen, 366th Infantry Regiment, and others. During the baseball winter meetings in 1943, noted African American athlete and actor Paul Robeson campaigned for integration of the sport. After World War II ended, several team managers considered recruiting members of the Negro Leagues for entry into organized baseball. In the early 1920s, New York Giants' manager John McGraw slipped a black player, Charlie Grant, into his lineup (reportedly by passing him off to the front office as an Indian), and McGraw's wife reported finding names of dozens of Negro players that McGraw fantasized about signing, after his death. Pittsburgh Pirates owner Bill Bensawanger reportedly signed Josh Gibson to a contract in 1943, and the Washington Senators were also said to be interested in his services. But those efforts (and others) were opposed by Kenesaw Mountain Landis, baseball's powerful commissioner and a staunch segregationist. Bill Veeck claimed that Landis blocked his purchase of the Philadelphia Phillies because he planned to integrate the team. While this is disputed, Landis was opposed to integration, and his death in 1944 (and subsequent replacement as Commissioner by Happy Chandler) removed a major obstacle for black players in the major leagues.
The general manager who would be eventually successful in breaking the color barrier was Branch Rickey of the Brooklyn Dodgers. Rickey himself had experienced the issue of segregation. While playing and coaching for his college team at Ohio Wesleyan University, Rickey had a black teammate named Charles Thomas. On one particular road trip through southern Ohio his fellow player was refused a room in a hotel. Although Rickey was able to get the player into his room for that night, he was taken aback when he reached his room to find Thomas upset and crying about this injustice. Rickey related this incident as an example of why he wanted a full de-segregation of the nation, not only in baseball.
In the mid-1940s, Rickey had compiled a list of Negro League ballplayers for a potential major league contract. Realizing that the first African American signee would be a magnet for prejudicial sentiment, however, Rickey was intent on finding a player with a distinguished personality and character that would allow him to tolerate the inevitable abuse. Rickey's sights eventually settled on Jackie Robinson, a shortstop with the Kansas City Monarchs. Although probably not the best player in the Negro Leagues at the time, Robinson was an exceptional talent, was college-educated, and had the marketable distinction of serving as an officer during World War II. More importantly, Robinson possessed the inner strength to handle the inevitable abuse to come. To prepare him for the task, Robinson first played in 1946 for the Dodgers' minor league team, the Montreal Royals, which proved an arduous emotional challenge, but he also enjoyed fervently enthusiastic support from the Montreal fans. On April 15, 1947, Robinson broke the color barrier, which had been tacitly recognized for over 50 years, with his appearance for the Brooklyn Dodgers at Ebbets Field.
Eleven weeks later, on July 5, 1947, the American League was integrated by the signing of Larry Doby to the Cleveland Indians. Over the next few years a handful of black baseball players made appearances in the majors, including Roy Campanella (teammate to Robinson in Brooklyn) and Satchel Paige (teammate to Doby in Cleveland). Paige, who had pitched more than 2400 innings in the Negro Leagues, sometimes two and three games a day, was still effective at 42, and still playing at 59. His ERA in the Major Leagues was 3.29.
However, the initial pace of integration was slow. By 1953, only six of the sixteen major league teams had a black player on the roster. The Boston Red Sox became the last major league team to integrate its roster with the addition of Pumpsie Green on July 21, 1959. While limited in numbers, the on-field performance of early black major league players was outstanding. In the fourteen years from 1947–1960, black players won one or more of the Rookie of the Year awards nine times.
While never prohibited in the same fashion as African Americans, Latin American players also benefitted greatly from the integration era. In 1951, two Chicago White Sox, Venezuelan-born Chico Carrasquel and Cuban-born (and black) Minnie Miñoso, became the first Hispanic All-Stars.
According to some baseball historians, Robinson and the other African American players helped reestablish the importance of baserunning and similar elements of play that were previously de-emphasized by the predominance of power hitting.
From 1947 to the 1970s, African American participation in baseball rose steadily. By 1974, 27% of baseball players were African American. As a result of this on-field experience, minorities began to experience long-delayed gains in managerial positions within baseball. In 1975, Frank Robinson (who had been the 1956 Rookie of the Year with the Cincinnati Reds) was named player-manager of the Cleveland Indians, making him the first African American manager in the major leagues.
Although these front-office gains continued, Major League Baseball saw a lengthy slow decline in the percentage of black players after the mid-1970s. By 2007, black players made up less than 9% of the major leagues. While this trend is largely attributed to an increased emphasis on the recruitment of players from Latin America (with the number of Hispanic players in the major leagues rising to 29% by 2007), other factors have been cited as well. Hall of Fame player Dave Winfield, for instance, has cited the fact that urban America places less emphasis and provides less resources for youth baseball than in the past. Despite this continued prevalence of Hispanic players, the percentage of black players rose again in 2008 to 10.2%.
Arturo Moreno became the first Hispanic owner of a MLB franchise when he purchased the Anaheim Angels in 2004.
In 2005, a Racial and Gender Report Card on Major League Baseball was issued, which generally found positive results on the inclusion of African Americans and Latinos in baseball, and gave Major League Baseball a grade of "A" or better for opportunities for players, managers and coaches as well as for MLB's central office. At that time, 37% of major league players were people of color: Latino (26 percent), African-American (9 percent) or Asian (2 percent). Also by 2004, 29% of the professional staff in MLB's central office were people of color, 11% of team vice presidents were people of color, and seven of the league's managers were of color (four African-Americans and three Latinos).
The Major Leagues move west.
Baseball had been in the West for almost as long as the National League and the American League had been around. It evolved into the Pacific Coast League, which included the Hollywood Stars, Los Angeles Angels, Oakland Oaks, Portland Beavers, Sacramento Solons, San Francisco Seals, San Diego Padres, Seattle Rainiers.
The PCL was huge in the West. A member of the National Association of Professional Baseball Leagues, it kept losing great players to the National and the American leagues for less than $8,000 a player.
The PCL was far more independent than the other "minor" leagues, and rebelled continuously against their Eastern masters. Clarence Pants Rowland, the President of the PCL, took on baseball commissioners Kenesaw Mountain Landis and Happy Chandler at first to get better equity from the major leagues, then to form a third major league. His efforts were rebuffed by both commissioners. Chandler and several of the owners, who saw the value of the markets in the West, started to plot the extermination of the PCL. They had one thing that Rowland did not: The financial power of the Eastern major league baseball establishment.
No one was going to back a PCL club building a major-league size stadium if the National or the American League was going to build one too, and potentially put the investment in the PCL ballpark into jeopardy.
1953–1955.
Until the 1950s, major league baseball franchises had been largely confined to the northeastern United States, with the teams and their locations having remained unchanged from 1903 to 1952. The first team to relocate in fifty years was the Boston Braves, who moved in 1953 to Milwaukee, where the club set attendance records. In 1954, the St. Louis Browns moved to Baltimore and were renamed the Baltimore Orioles. In 1955, the Philadelphia Athletics moved to Kansas City.
National League Baseball leaves New York.
In 1958 the New York market ripped apart. The Yankees were becoming the dominant draw, and the cities of the West offered generations of new fans in much more sheltered markets for the other venerable New York clubs, the Brooklyn Dodgers and the New York Giants. Placing these storied, powerhouse clubs in the two biggest cities in the West had the specific design of crushing any attempt by the PCL to form a third major league. Eager to bring these big names to the West, Los Angeles gave Walter O'Malley, owner of the Dodgers, a helicopter tour of the city and asked him to pick his spot. The Giants were given the lease to the PCL San Francisco Seals digs while Candlestick Park was built for them.
California.
The logical first candidates for major league "expansion" were the same metropolitan areas that had just attracted the Dodgers and Giants. It is said that the Dodgers and Giants—National League rivals in New York City—chose their new cities because Los Angeles (in southern California) and San Francisco (in northern California) already had a fierce rivalry (geographical, economic, cultural and political), dating back to the state's founding. The only California expansion team—and also the first in Major League Baseball in over 70 years—was the Los Angeles Angels (later the California Angels, the Anaheim Angels, and, as of 2005, the Los Angeles Angels of Anaheim), who brought the American League to southern California in 1961. Northern California, however, would later gain its own American League team, in 1968, when the Athletics would move again, settling in Oakland, across San Francisco Bay from the Giants.
1961–1962.
Along with the Angels, the other 1961 expansion team was the Washington Senators, who joined the American League and took over the nation's capital when the moved to Minnesota and became the Twins. 1961 is also noted as being the year in which Roger Maris surpassed Babe Ruth's single season home run record, hitting 61 for the New York Yankees, albeit in a slightly longer season than Ruth's. To keep pace with the American League—which now had ten teams—the National League likewise expanded to ten teams, in 1962, with the addition of the Houston Colt .45s and New York Mets.
1969.
In 1969, the American League expanded when the Kansas City Royals and Seattle Pilots, the latter in a longtime PCL stronghold, were admitted to the league. The Pilots stayed just one season in Seattle before moving to Milwaukee and becoming today's Milwaukee Brewers. The National League also added two teams that year, the Montreal Expos and San Diego Padres. The Padres were the last of the core PCL teams to be absorbed. The Coast League did not die, though. It reformed, and moved into other markets, and endures to this day as a Class AAA league.
1972–1998.
In 1972, the second Washington Senators moved to the Dallas-Fort Worth area and became the Texas Rangers.
In 1977, the American League expanded to fourteen teams, with the newly formed Seattle Mariners and Toronto Blue Jays. Sixteen years later, in 1993, the National League likewise expanded to fourteen teams, with the newly formed Colorado Rockies and Florida Marlins (now Miami Marlins).
Beginning with the 1994 season, both the AL and the NL were divided into three divisions (East, West, and Central), with the addition of a wild card team (the team with the best record among those finishing in second place) to enable four teams in each league to advance to the preliminary division series. However, due to the 1994 Major League Baseball strike (which canceled the 1994 World Series), the new rules did not go into effect until the 1995 World Series.
In 1998, the AL and the NL each added a fifteenth team, for a total of thirty teams in Major League Baseball. The Arizona Diamondbacks joined the National League, and the Tampa Bay Devil Rays—now called simply the Rays—joined the American League. In order to keep the number of teams in each league at an even number (14 – AL, 16 – NL), Milwaukee changed leagues and became a member of the National League.
Pitching dominance and rules changes.
By the late 1960s, the balance between pitching and hitting had swung in favor of the pitchers. In 1968 Carl Yastrzemski won the American League batting title with an average of just .301, the lowest in history. That same year, Detroit Tigers pitcher Denny McLain won 31 games – making him the last pitcher to win 30 games in a season. St. Louis Cardinals starting pitcher Bob Gibson achieved an equally remarkable feat by allowing an ERA of just 1.12.
In response to these events, major league baseball implemented certain rules changes in 1969 to benefit the batters. The pitcher's mound was lowered, and the strike zone was reduced.
In 1973 the American League, which had been suffering from much lower attendance than the National League, made a move to increase scoring even further by initiating the designated hitter rule.
Players assert themselves.
From the time of the formation of the Major Leagues to the 1960s, when it came to the control of the game of baseball the team owners held the whip hand. After the so-called "Brotherhood Strike" of 1890 and the failure of the Brotherhood of Professional Base Ball Players and its Players National League, the owners control of the game seemed absolute. It lasted over 70 years despite a number of short-lived players organizations. In 1966, however, the players enlisted the help of labor union activist Marvin Miller to form the Major League Baseball Players Association (MLBPA). The same year, Sandy Koufax and Don Drysdale – both Cy Young Award winners for the Los Angeles Dodgers – refused to re-sign their contracts, and the era of the reserve clause, which held players to one team, was coming toward an end.
The first legal challenge came in 1970. Backed by the MLBPA, St. Louis Cardinals outfielder Curt Flood took the leagues to court to negate a player trade, citing the 13th Amendment and antitrust legislation. In 1972 he finally lost his case in the United States Supreme Court by a vote of 5 to 3, but gained large-scale public sympathy, and the damage had been done. The reserve clause survived, but it had been irrevocably weakened. In 1975 Andy Messersmith of the Dodgers and Dave McNally of the Montreal Expos played without contracts, and then declared themselves free agents in response to an arbitrator's ruling. Handcuffed by concessions made in the Flood case, the owners had no choice but to accept the collective bargaining package offered by the MLBPA, and the reserve clause was effectively ended, to be replaced by the current system of free-agency and arbitration.
While the legal challenges were going on, the game continued. In 1969 the "Miracle Mets", just 7 years after their formation, recorded their first winning season, won the National League East and finally the World Series.
On the field, the 1970s saw some of the longest standing records fall and the rise of two powerhouse dynasties. In Oakland, the Swinging A's were overpowering, winning the Series in '72, '73 and '74, and five straight division titles. The strained relationships between teammates, who included Catfish Hunter, Vida Blue and Reggie Jackson, gave the lie to the need for "chemistry" between players. The National League, on the other hand, belonged to the Big Red Machine in Cincinnati, where Sparky Anderson's team, which included Pete Rose as well as Hall of Famers Tony Perez, Johnny Bench and Joe Morgan, succeeded the A's run in 1975.
The decade also contained great individual achievements as well. On April 8, 1974, Hank Aaron of the Atlanta Braves hit his 715th career home run, surpassing Babe Ruth's all-time record. He would retire in 1976 with 755 and that was just one of numerous records he achieved, many of which, including Total bases scored, still stand today. There was great pitching too: between 1973 and 1975, Nolan Ryan threw 4 "no-hit" games. He would add a record-breaking fifth in 1981 and two more before his retirement in 1993, by which time he had also accumulated 5,715 strikeouts, another record, in a 27-year career.
The marketing and hype era.
From the 1980s onward, the major league game has changed dramatically from a combination of effects brought about by free agency, improvements in the science of sports conditioning, changes in the marketing and television broadcasting of sporting events, and the push by brand-name products for greater visibility. These events lead to greater labor difficulties, fan disaffection, skyrocketing prices, changes in the way that the game is played, and problems with the use of performance enhancing substances like steroids tainting the race for records. Through this period crowds generally rose. Average attendances first broke 20,000 in 1979 and 30,000 in 1993. That year total attendance hit 70 million, but baseball was hit hard by a strike in 1994, and as of 2005 it has only marginally improved on those 1993 records.
The science of the sport changes the game.
During the 1980s, the science of conditioning and workouts greatly improved. Weight rooms and training equipment were improved. Trainers and doctors developed better diets and regimens to make athletes bigger, healthier, and stronger than they had ever been.
Another major change that had been occurring during this time was the adoption of the pitch count. Starting pitchers playing complete games had not been an unusual thing in baseball's history. Now pitching coaches watched to see how many pitches a player had thrown over the game. At anywhere from 100 to 125, pitchers increasingly would be pulled out to preserve their arms. Bullpens began to specialize more, with more pitchers being trained as middle relievers, and a few hurlers, usually possessing high velocity but not much durability, as closers.
Along with the expansion of teams, the addition of more pitchers needed to play a complete game stressed the total number of quality players available in a system that restricted its talent searches at that time to America, Canada, Latin America, and the Caribbean.
Television.
Baseball had been watched live since the mid 20th century. Television sports' arrival in the 1950s increased attention and revenue for all major league clubs at first. The television programming was extremely regional. It hurt the minor and independent leagues most. People stayed home to watch Maury Wills rather than watch unknowns at their local baseball park. Major League Baseball, as it always did, made sure that it controlled rights and fees charged for the broadcasts of all games, just as it did on radio. It brought additional revenues and attention both from the broadcast itself, and from the increases in attendance and merchandise sales that expanded audiences allowed.
The national networks began televising national games of the week, opening the door for a national audience to see particular clubs. While most teams were broadcast, emphasis was always on the league leaders and the major market franchises that could draw the largest audience.
The rise of cable.
In the 1970s the cable revolution began. The Atlanta Braves became a power contender with greater revenues generated by WTBS, Ted Turner's Atlanta-based Super-Station, that broadcast "America's Team" to cable households nationwide. The roll out of ESPN, then regional sports networks (now mostly under the umbrella of Fox Sports Net) changed sports news and particularly impacted baseball. Potboiled down to the thirty-second game highlight, and now under the microscope of news organizations that needed to fill 24 hours of time, the amount of attention paid to major league players magnified to staggering levels from where it had been just 20 years prior.
It brought with it increased attention for individual players, who reached super-star status nationwide on careers that often were not as compelling as those who had come before them in a less media intense time.
As player contract values soared, and the number of broadcasters, commentators, columnists, and sports writers also soared. The competition for a fresh angle on any story became fierce. Media pundits began questioning the high salaries that the players received. Coverage began to become intensely negative. Players personal lives, which had always been off-limits unless something extreme happened, became the fodder of editorials, insider stories on television, and features in magazines. When the use of performance-enhancing drugs became an issue, the gap between the sports media and the players whom they covered widened further.
With the development of satellite television and digital cable, Major League Baseball launched baseball channels with season subscription fees, making it possible for fans to watch virtually every game played as they played.
Team networks.
The next round became the single-team cable networks. YES Network & NESN, the New York Yankees & Boston Red Sox cable television networks, respectively, took in millions to broadcast games not only in New York and Boston but around the country. These networks generated as much revenue or more annually for large market teams like the Yankees and Red Sox as their entire baseball operations did. By making these separate companies, these owners were able to exclude the money from consideration of deals.
Sponsorships, endorsements, & merchandise.
Television and greater media coverage in magazines and newspapers trying to attract a new generation of non-readers also brought in the sponsors, and even more money, that would attract players to new financial opportunities and bring in other elements to the business of baseball that would impact the game.
Baseball memorabilia and souvenirs, including baseball cards, exploded in price as networks of adults became more sophisticated in their trading. This would explode yet again in the late 1990s, as the Internet, and the website eBay provided venues for collectors of all things baseball to trade with each other. Regionalized pricing was wiped away, and many objects, baseballs, bats, and the like began selling for high dollar values. This in turn brought in new businessmen whose sole means of making a living was acquiring autographs and memorabilia from the athletes. Memorabilia hounds fought with fans to get signatures worth $20, $60, or even $100 or more in their stores.
Beyond the staple billboards, large corporations like NIKE and Champion fought to make sure that their logos were seen on the clothing and shoes worn by athletes on the field. This kind of association branding became a new revenue stream. In the late 1990s and into the dawn of the 21st century, the dugout, the backstops behind home plate, and anywhere else that might be seen by a camera all became fair game for inserting advertising.
Player wealth and influence.
Players who had been dramatically underpaid for generations were now replaced by players who were paid extremely well for their services.
By the 1970s a new generation of sports agents were hawking the talents of players who knew baseball but didn't know how the business end of the game was played. The agents broke down what the teams were generating in revenue off of the players' performances. They calculated what their player might be worth to energize a television contract, or provide more merchandise revenue, or put more fans into seats.
The athletes signed shoe deals, baseball card sponsorships, and commercial endorsements for products of every size and shape.
Sky high salaries also changed many of the strategies of the game. Players rarely were "sent" down to the minors if they failed to perform. Who could justify paying a slumping player millions to sit in Toledo where the major league fans couldn't pay their way? Other players in the Triple-A level of the minor leagues, who used to rise on merit, became trapped under these overpaid "stars." Also, in order to make the media happy, trades, rather than call-ups, became the order of the day. It was much better to buy someone else's shortstop who was a known quantity to the national sports media than to take a chance on a player with no name value and no visibility if you were in a major market ballclub.
Tactics on the field changed too. Risky moves that could get players hurt, and sideline millions of dollars in payroll on the disabled list, became less common. Stealing home, a popular tactic of great stars of the day like Ty Cobb or Pete Rose, became infrequent occurrences.
The perception of players by the general public changed from larger-than-life heroes to a more cynical view of many of them as spoiled and overpaid. This was fed by the growing legions of television reporters, commentators, and print sports writers who also started asking questions about what justified the kind of money being paid to these players.
With players seeking greener pastures when their contracts came up, fewer players became career members of one ballclub. In the modern era, it is unusual to see a player stay with any one club for more than a few years if they are good enough to command a better salary.
Players with any ability increasingly gravitated towards the money. Large market clubs like the New York Yankees, the Boston Red Sox, and the Chicago Cubs given big revenues from their cable television operations signed more and more of the big name players away from mid-sized and smaller market baseball clubs that could not afford to compete with them for salaries.
Owners and players feud in the 1980s.
All was not well with the game. The many contractual disputes between players and owners came to a head in 1981. Previous players' strikes (in 1972, 1973 and 1980) had been held in preseason, with only the 1972 stoppage – over benefits – causing disruption to the regular season from April 1 to April 13. Also, in 1976 the owners had locked the players out of Spring training in a dispute over free agency.
The crux of the 1981 dispute was about compensation for the loss of players to free agency. After losing a top-rank player in such a way the owners wanted a mid-rank player in return, the so-called "sixteenth player" (each club was allowed to protect 15 players from this rule). Losing lower rated free agents would have correspondingly smaller compensation. The players, only recently freed from the bondage of the reserve clause, found this unacceptable, and withdrew their labor, striking on June 12. Immediately, the U.S. Government National Labor Relations Board ruled that the owners had not been negotiating in good faith, and installed a federal mediator to reach a solution. Seven weeks and 713 games were lost in the middle of the season, before the owners backed down on July 31, settling for much lower ranked players as compensation. By then much of the season had been lost, and the season was continued as distinct halves starting August 9, with the playoffs reorganized to reflect this.
Throughout the 1980s then, baseball seemed to prosper. The competitive balance between franchises saw fifteen different teams make the World Series, and nine different champions during the decade. Also, every season from 1978 through 1987 saw a different World Series winner, a streak unprecedented in baseball history. Turmoil was, however, just around the corner. In 1986 Pete Rose retired from playing for the Cincinnati Reds, having broken Ty Cobb's record by accumulating 4,256 hits during his career. He continued as Reds manager until, in 1989 it was revealed that he was being investigated for sports gambling, including the possibility that he had bet on teams with which he was involved. While Rose admitted a gambling problem, he denied having bet on baseball. Federal prosecutor John Dowd investigated and, on his recommendation, Rose was banned from organised baseball, a move which precluded his possible inclusion in the Hall of Fame. In a meeting with Commissioner Giamatti, Rose, having failed in a legal action to prevent it, accepted his punishment. It was, essentially, the same fate that had befallen the Black Sox seventy years previously. (Rose, however, would continue to deny that he bet on baseball until he finally confessed to it in his 2004 autobiography.)
Strike two (1994).
Labor relations were still strained. There had been a two-day strike in 1985 (over the division of television revenue money), and a 32-day spring training lockout in 1990 (again over salary structure and benefits). By far the worst action would come in 1994. The seeds were sown earlier: in 1992 the owners sought to renegotiate salary and free-agency terms, but little progress was made. The standoff continued until the beginning of 1994 when the existing agreement expired, with no agreement on what was to replace it. Adding to the problems was the perception that "small market" teams, such as the struggling Seattle Mariners could not compete with high spending teams such as those in New York or Los Angeles. Their plan was to institute TV revenue sharing to increase equity amongst the teams and impose a salary cap to keep expenditures down. Players felt that such a cap would reduce their potential earnings.
The players officially went on strike on August 12, 1994. In September 1994 Major League Baseball announced the cancellation of the World Series for the first time since 1904.
Home run mania and the second coming of baseball.
The cancellation of the 1994 World Series was a severe embarrassment for Major League Baseball. Americans were cursed, outraged, frightened, angered, frustrated, and plagued to their core as a result of the strike. Fans had declared the strike as an act of war. Although there were few signs of the predicted "outrage" on the part of the fans, attendance figures and broadcast ratings were lower in 1995 than before the strike. However, it would be a decade until baseball would recover from the strike.
On September 6, 1995, Baltimore Orioles shortstop Cal Ripken, Jr. played his 2,131st consecutive game, breaking Lou Gehrig's 56-year-old record. This was the first high-profile moment in baseball after the strike. Ripken continued his streak for another three years, voluntarily ending it at 2,632 consecutive games played on September 20, 1998.
In 1997, the Florida Marlins won the World Series in just their fifth season. This made them the youngest expansion team to win the Fall Classic (with the exception of the 1903 Boston Red Sox and later the 2001 Arizona Diamondbacks, who won in their fourth season). Virtually all the key players on the 1997 Marlins team were soon traded or let go to save payroll costs (although the 2003 Marlins did win a second world championship).
In 1998, St. Louis Cardinals first baseman Mark McGwire and Chicago Cubs outfielder Sammy Sosa engaged in a home run race for the ages. With both rapidly approaching Roger Maris's record of 61 home runs (set in 1961), seemingly the entire nation watched as the two power hitters raced to be the first to break the record. McGwire reached 62 first on September 8, 1998, with Sosa also eclipsing it later. Sosa finished with 66 home runs, just behind McGwire's unheard-of 70. However, recent steroid allegations have marred the season in the minds of many fans.
That same year, the New York Yankees won a record 125 games, including going 11–2 in the postseason, to win the World Series as what many consider to be one of the greatest teams of all time.
McGwire's record of 70 would last a mere three years following the meteoric rise of veteran San Francisco Giants left fielder Barry Bonds in 2001. In 2001 Bonds knocked out 73 home runs, breaking the record set by McGwire by hitting his 71st on October 5, 2001. In addition to the home run record, Bonds also set single-season marks for base on balls with 177 (breaking the previous record of 170, set by Babe Ruth in 1923) and slugging percentage with .863 (breaking the mark of .847 set by Ruth in 1920). Bonds continued his torrid home run hitting in the next few seasons, hitting his 660th career home run on April 12, 2004, tying him with his godfather Willie Mays for third place on the all-time career home run list. He hit his 661st home run the next day, April 13, to take sole possession of third place. Only three years later Bonds surpassed the great Hank Aaron to become baseball's most prolific home run hitter.
However, both Bonds' accomplishments in the 2000s have not been without controversy. During his run, journalists questioned McGwire about his use of the steroid-precursor androstenedione, and in March 2005 was unforthcoming when questioned as part of a Congressional inquiry into steroids. Bonds has also has been dogged by allegations of steroid use and his involvement in the BALCO drugs scandal, as his personal trainer Greg Anderson pled guilty to supplying steroids (without naming Bonds as a recipient). Neither Bonds nor McGwire has failed a drug test at any time since there was no steroid-testing until 2003 after the new August 7, 2002 agreement between owners and players was reached. McGwire retired after the 2001 season; in 2010, he admitted to having used steroids throughout his MLB career.
The 1990s also saw Major League Baseball expand into new markets as four new teams joined the league. In 1993, the Colorado Rockies and Florida Marlins began play, and in just their fifth year of existence, the Marlins became the first wild card team to win the championship. 
The year 1998 brought two more teams into the mix, the Tampa Bay Devil Rays and the Arizona Diamondbacks, the latter of which become the youngest expansion franchise to win the championship. 
For the most part, the late 1990s were dominated by the New York Yankees, who won four out of five World Series championships from 1996–2000.
The steroid era.
Drugs, baseball, and records.
The lure of big money pushed players harder and harder to perform at their peaks. There is only so much conditioning that one can do to obtain an edge without inducing injury. The wearying travel schedule and 162-game season meant that amphetamines, usually in the form of pep pills known as "greenies", had been widespread in baseball since at least the 1960s. Baseball's drug scene was no particular secret, having been discussed in Sports Illustrated and in Jim Bouton's groundbreaking book "Ball Four," but there was virtually no public backlash. Two decades later, however, some Major League players turned to newer performance enhancing drugs, including ephedra and improved steroids.
A memo circulated in 1991 by baseball commissioner Fay Vincent said, "The possession, sale or use of any illegal drug or controlled substance by Major League players and personnel is strictly prohibited ... [and those players involved] are subject to discipline by the Commissioner and risk permanent expulsion from the game... This prohibition applies to all illegal drugs and controlled substances, including steroids…" Some general managers of the time do not remember this memo, and it was not emphasized or enforced.
Ephedra, a Chinese herb used to cure cold symptoms, and also used in some allergy medications, sped up the heart and was considered by some to be a weight-loss short-cut. Overweight pitcher Steve Bechler, who wanted to stay on the Baltimore Orioles roster, took just such a shortcut. He collapsed on February 17, 2003 while pitching, and was soon pronounced dead. Bechler's death raised concerns over the use of performance enhancing drugs in baseball. Ephedra was banned, and soon the furor died down.
The 1998 home run race had generated nearly unbroken positive publicity, but Barry Bonds run for the all-time home run record provoked a backlash over steroids, which increase a person's testosterone level and subsequently enable that person to bodybuild with much more ease. Some athletes have said that the main advantage to steroids is not so much the additional power or endurance that they can provide, but that they can drastically shorten rehab time from injury.
Commissioner Bud Selig imposed a very strict anti-drug policy upon its minor league players, who are not part of the Major League Baseball Players Association (the PA). Random drug testing, education and treatment, and strict penalties for those caught were the rule of law. Anyone on a Major League team's forty man roster, including 15 minor leaguers that are on that list, were exempt from that program. Some called Selig's move a public relations stunt, or window dressing.
In a "Sports Illustrated" cover story in 2002, a year after his retirement, Ken Caminiti admitted that he had used steroids during his National League MVP-winning 1996 season, and for several seasons afterwards. Caminiti died unexpectedly of an apparent heart attack in The Bronx at the age of 41; he was pronounced dead on October 10, 2004 at New York's Lincoln Memorial Hospital. On November 1, the New York City Medical Examiners Office announced that Caminiti died from "acute intoxication due to the combined effects of cocaine and opiates," but coronary artery disease and cardiac hypertrophy (an enlarged heart) were also contributing factors.
In 2005, José Canseco published "" admitting steroid usage and claiming that it was prevalent throughout major league baseball. When the United States Congress decided to investigate the use of steroids in the sport, some of the games most prominent players came under scrutiny for possibly using steroids. These include Barry Bonds, Jason Giambi, and Mark McGwire. Other players, such as Canseco and Gary Sheffield, have admitted to have either knowingly (in Canseco's case) or not (Sheffield's) using steroids. In confidential testimony to the BALCO Grand Jury (that was later leaked to the "San Francisco Chronicle"), Giambi also admitted steroid use. He later held a press conference in which he appeared to affirm this admission, without actually saying the words. And after an appearance before Congress where he (unlike McGwire) emphatically denied using steroids, "period," slugger Rafael Palmeiro became the first major star to be suspended (10 days) on August 1, 2005 for violating Major League Baseball's newly strengthened ban on controlled substances, including steroids, adopted on August 7, 2002, starting in the 2003 season. Many lesser players (mostly from the minor leagues) have tested positive for use, as well.
In 2006, the Commissioner of Baseball tasked former United States Senator George J. Mitchell to lead an investigation into the use of performance-enhancing drugs in Major League Baseball (MLB) and on December 13, 2007, the 409-page Mitchell Report was released ('Report to the Commissioner of Baseball of an Independent Investigation into the Illegal Use of Steroids and Other Performance Enhancing Substances by Players in Major League Baseball'). The report described the use of anabolic steroids and human growth hormone (HGH) in MLB and assessed the effectiveness of the MLB Joint Drug Prevention and Treatment Program. Mitchell also advanced certain recommendations regarding the handling of past illegal drug use and future prevention practices. The report names 89 MLB players who are alleged to have used steroids or drugs.
Baseball has been taken to task for turning a blind eye to its drug problems. It benefited from these drugs in the ever-increasingly competitive fight for airtime and media attention. MLB and its Players Association finally announced tougher measures, but many felt that they did not go far enough. 
In December 2009, Sports Illustrated named Baseball's Steroid Scandal as the number one sports story of the decade of the 2000s. In 2013, no player from the first "steroid class" of players eligible for the Baseball Hall of Fame was elected. Bonds and Clemens received less than half the number of votes needed, and some voters stated that they would not vote for any first-time candidate who played during the steroid era—whether accused of using banned substances or not—because of the effect the substances had on baseball.
The BALCO steroids scandal.
In 2002, a major scandal arose when it was discovered that a company called BALCO (Bay Area Laboratory Co-operative), owned by Victor Conte, had been producing so-called "designer steroids", (specifically "the clear" and "the cream") which are steroids that could not be detected through drug tests at that time. In addition, the company had connections to several San Francisco Bay Area sports trainers and athletes, including the trainers of Jason Giambi and Barry Bonds. This revelation lead to a vast criminal investigation into BALCO's connections with athletes from baseball and many other sports. Among the many athletes who have been linked to BALCO are Olympic sprinters Tim Montgomery and Marion Jones, Olympic shot-putter C. J. Hunter, and Major League Baseball players Jason Giambi and Barry Bonds.
During grand jury testimony in December 2003 – which was illegally leaked to the San Francisco Chronicle and published in December 2004 – Giambi allegedly admitted to using many different steroids, including fertility drugs (which could account for his declining health in the past few years). The reports that came from the San Francisco Chronicle were done by Mark Fainaru-Wada and Lance Williams, who revealed that the Bay Area Laboratory Cooperative did not merely manufacture nutritional supplements, but also distributed exotic steroids. Williams and Fairanu-Wada also provided compelling evidence that Bonds, arguably the greatest player of his generation, was one of BALCO’s steroid clients. The paper reported that these substances were probably designer steroids. Bonds said that Greg Anderson gave him a rubbing balm and a liquid substance that at the time he did not believe them to be steroids and thought they were flaxseed oil and other health supplements. Based on the testimony from many of the athletes, Conte and Anderson accepted plea agreements from the government in 2005 on charges they distributed steroids and laundered money to avoid significant time in jail. Conte received a sentence of four months in jail, Anderson received a sentence of three months. Also that year, James Valente, the vice president of Balco, and Remi Korchemny, a track coach affiliated with BALCO, pleaded guilty to distributing banned substances and received probation.
Various baseball pundits, fans, and even players have taken this as confirmation that Bonds used illegal steroids. Bonds never tested positive in tests performed in 2003, 2004, and 2005, which may be attributable to successful obfuscation of continued use as documented in the 2006 book "Game of Shadows".
The Power Age.
While the introduction of steroids certainly increased the power production of greats there were other factors that drastically increased the power surge after 1994. The factors cited are: smaller sized ballparks than in the past, "juiced baseballs" implying that the balls are wound tighter thus travel further following contact with the bat, "watered down pitching" implying that lesser quality pitchers are up in the Major Leagues due to too many teams. Albeit that these factors did play a large role in increasing home run thus scoring totals during this time, others that directly impact ballplayers have an equally important role. As noted earlier one of those factors is anabolic steroids which have the capability of increasing muscle mass, which enables hitters to not only hit "mistake" pitches farther, but it also enables hitters to adjust to "good" pitches such as a well-placed fastball, slider, changeup, or curveball, and hit them for home runs. Another such factor is better nutrition, as well as training and training facilities/equipment which can work with (or without) steroids to produce a more potent ballplayer and further enhance his skills.
Routinely in today's baseball age we see players reach 40 and 50 home runs in a season, a feat that even in the 1980s was considered rare. Many modern baseball theorists believe that a new pitch will swing the balance of power back to the pitcher. A pitching revolution would not be unprecedented—several pitches have changed the game of baseball in the past, including the slider in the 1950s and 1960s and the split-fingered fastball in the 1970s to 1990s. Since the 1990s, the changeup has made a resurgence, being thrown masterfully by pitchers such as Tim Lincecum, Pedro Martinez, Trevor Hoffman, Greg Maddux, Matt Cain, Tom Glavine, Johan Santana, Justin Verlander and Cole Hamels.

</doc>
<doc id="3858" url="http://en.wikipedia.org/wiki?curid=3858" title="Major League Baseball Most Valuable Player Award">
Major League Baseball Most Valuable Player Award

The Major League Baseball Most Valuable Player Award (MVP) is an annual Major League Baseball (MLB) award, given to one outstanding player in the American League and one in the National League. Since 1931, it has been awarded by the Baseball Writers' Association of America (BBWAA). The winners receive the Kenesaw Mountain Landis Memorial Baseball Award, which became the official name of the award in 1944, in honor of the first MLB commissioner, who served from 1920 until his death on November 25, 1944.
MVP voting takes place before the postseason, but the results are not announced until after the World Series. The BBWAA began by polling three writers in each league city in 1938, reducing that number to two per league city in 1961. The BBWAA does not offer a clear-cut definition of what "most valuable" means, instead leaving the judgment to the individual voters.
First basemen, with 34 winners, have won the most MVPs among infielders, followed by second basemen (16), third basemen (15), and shortstops (15). Of the 24 pitchers who have won the award, 15 are right-handed while 9 are left-handed. Walter Johnson, Carl Hubbell, and Hal Newhouser are the only pitchers who have won multiple times, Newhouser winning consecutively in 1944 and 1945.
Hank Greenberg, Stan Musial, Alex Rodriguez, and Robin Yount have won at different positions, while Rodriguez is the only player who has won the award with two different teams at two different positions. Barry Bonds has won the most often (seven times) and the most consecutively (2001–04). Jimmie Foxx was the first player to win multiple times; 9 players have won three times, and 19 have won twice. Frank Robinson is the only player to win the award in both the American and National Leagues.
The award's only tie occurred in the National League in 1979, when Keith Hernandez and Willie Stargell received an equal number of points. There have been 17 unanimous winners, who received all the first-place votes. The New York Yankees have the most winning players with 22, followed by the St. Louis Cardinals with 17 winners. The award has never been presented to a member of the following five teams: Arizona Diamondbacks, Miami Marlins, New York Mets, Tampa Bay Rays, and Washington Nationals. The most recent recipients are Mike Trout in the American League and Clayton Kershaw in the National League.
In recent decades, pitchers have rarely won the award. When Justin Verlander won the AL award in 2011, he became the first pitcher in either league to be named the MVP since Dennis Eckersley in 1992. Verlander also became the first starting pitcher to win this award since Roger Clemens had accomplished the feat in 1986. The National League went even longer without an MVP award to a pitcher—after Bob Gibson won in 1968, no pitcher in that league was named MVP until Kershaw in 2014.
Chalmers Award (1911–1914).
Before the 1910 season, Hugh Chalmers of Chalmers Automobile announced he would present a Chalmers Model 30 automobile to the player with the highest batting average in Major League Baseball at the end of the season. The 1910 race for best average in the American League was between the Detroit Tigers' widely disliked Ty Cobb and Nap Lajoie of the Cleveland Indians. On the last day of the season, Lajoie overtook Cobb's batting average with seven bunt hits against the St. Louis Browns. American League President Ban Johnson said a recalculation showed that Cobb had won the race anyway, and Chalmers ended up awarding cars to both players.
The following season, Chalmers created the Chalmers Award. A committee of baseball writers were to convene after the season to determine the "most important and useful player to the club and to the league". Since the award was not as effective at advertising as Chalmers had hoped, it was discontinued after 1914.
League Awards (1922–1929).
In 1922 the American League created a new award to honor "the baseball player who is of the greatest all-around service to his club". Winners, voted on by a committee of eight baseball writers chaired by James Crusinberry, received a bronze medal and a cash prize. Voters were required to select one player from each team and player-coaches and prior award winners were ineligible. These flaws resulted in the awards being dropped after 1928. The National League award, without these restrictions, lasted from 1924 to 1929.
Baseball Writers' Association of America's Most Valuable Player (1931–present).
The BBWAA first awarded the modern MVP after the 1931 season, adopting the format the National League used to distribute its league award. One writer in each city with a team filled out a ten-place ballot, with ten points for the recipient of a first-place vote, nine for a second-place vote, and so on. In 1938, the BBWAA raised the number of voters to three per city and gave 14 points for a first-place vote. The only significant change since then occurred in 1961, when the number of voters was reduced to two per league city.

</doc>
<doc id="3859" url="http://en.wikipedia.org/wiki?curid=3859" title="Major League Baseball Rookie of the Year Award">
Major League Baseball Rookie of the Year Award

In Major League Baseball, the Rookie of the Year Award is annually given to one player from each league as voted on by the Baseball Writers Association of America (BBWAA). The award was established in 1940 by the Chicago chapter of the BBWAA, which selected an annual winner from 1940 through 1946. The award became national in 1947; Jackie Robinson, the Brooklyn Dodgers' second baseman, won the inaugural award. One award was presented for both leagues in 1947 and 1948; since 1949, the honor has been given to one player each in the National and American League. Originally, the award was known as the J. Louis Comiskey Memorial Award, named after the Chicago White Sox owner of the 1930s. The award was renamed the Jackie Robinson Award in July 1987, 40 years after Jackie Robinson broke the baseball color line.
Of the 128 players named Rookie of the Year, 14 have been elected to the National Baseball Hall of Fame—Jackie Robinson, five American League players, and eight others from the National League. The award has been shared twice: once by Butch Metzger and Pat Zachry of the National League in 1976; and once by John Castino and Alfredo Griffin of the American League in 1979. Members of the Brooklyn and Los Angeles Dodgers have won the most awards of any franchise (with 16), twice the total of the New York Yankees (eight), who have produced the most in the American League. Fred Lynn and Ichiro Suzuki are the only two players who have been named Rookie of the Year and Most Valuable Player in the same year, and Fernando Valenzuela is the only player to have won Rookie of the Year and the Cy Young Award in the same year. Sam Jethroe is the oldest player to have won the award, at age 32, 33 days older than 2000 winner Kazuhiro Sasaki (also 32). José Abreu of the Chicago White Sox and Jacob deGrom of the New York Mets are the most recent winners.
Qualifications and voting.
From 1947 through 1956, each BBWAA voter used discretion as to who qualified as a rookie. In 1957, the term was first defined as someone with fewer than 75 at bats or 45 innings pitched in any previous Major League season. This guideline was later amended to 90 at bats, 45 innings pitched, or 45 days on a Major League roster before September 1 of the previous year. The current standard of 130 at bats, 50 innings pitched or 45 days on the active roster of a Major League club (excluding time in military service or on the disabled list) before September 1 was adopted in 1971.
Since 1980, each voter names three rookies: a first-place choice is given five points, a second-place choice three points, and a third-place choice one point. The award goes to the player who receives the most overall points. Edinson Volquez received three second-place votes in 2008 balloting despite no longer being a rookie under the award's definition.
The award has drawn criticism in recent years because several players with experience in Nippon Professional Baseball (NPB) have won the award, such as Hideo Nomo in 1995, Kazuhiro Sasaki in 2000, and Ichiro Suzuki in 2001. The current definition of rookie status for the award is based only on Major League experience, but some feel that past NPB players are not true rookies because of their past professional experience. Others, however, believe it should make no difference since the first recipient and the award's namesake played for the Negro Leagues prior to his MLB career and thus could also not be considered a "true rookie". This issue arose in 2003 when Hideki Matsui narrowly lost the AL award to Ángel Berroa. Jim Souhan of the "Minneapolis Star Tribune" said he did not see Matsui as a rookie in 2003 because "it would be an insult to the Japanese league to pretend that experience didn't count." "The Japan Times" ran a story in 2007 on the labeling of Daisuke Matsuzaka, Kei Igawa, and Hideki Okajima as rookies, saying "[t]hese guys aren't rookies." Past winners such as Jackie Robinson, Don Newcombe, and Sam Jethroe had professional experience in the Negro Leagues.
References.
</dl>

</doc>
<doc id="3860" url="http://en.wikipedia.org/wiki?curid=3860" title="National League Championship Series">
National League Championship Series

The National League Championship Series (NLCS) is a best-of-seven series played in October in the Major League Baseball postseason that determines the winner of the National League pennant. The winner of the series advances to play the winner of the American League Championship Series in the World Series, Major League Baseball's championship series. The reigning National League Champions are the San Francisco Giants.
History.
Prior to 1969, the National League champion (the "pennant winner") was determined by the best win-loss record at the end of the regular season. There were four "ad hoc" three-game playoff series due to ties under this formulation (in 1946, 1951, 1959, and 1962). (The American League had to resolve a tie in 1948, but used a single-game playoff for that.)
A structured postseason series began in 1969, when both the National and American Leagues were reorganized into two divisions each, East and West. The two division winners within each league played each other in a best-of-five series to determine who would advance to the World Series. In 1985, the format changed to best-of-seven.
The NLCS and ALCS, since the expansion to best-of-seven, are always played in a 2–3–2 format: games 1, 2, 6, and 7 are played in the stadium of the team that has home field advantage, and Games 3, 4, and 5 are played in the stadium of the team that does not. Home field advantage is given to the team that has the better record, with the exception that the team that made the postseason as the Wild Card team cannot get home field advantage. From 1969 to 1993, home field advantage was alternated between divisions each year regardless of regular season record and from 1995 to 1997 home field advantage was predetermined before the season.
In 1981, a divisional series was held due to a split season caused by a players' strike.
In 1994, the league was restructured into three divisions, with the three division winners and a wild-card team advancing to a best-of-five postseason round, the National League Division Series (NLDS). The winners of that round advance to the best-of-seven NLCS. The NLDS was first played in 1995 due to the cancellation of the 1994 postseason during another players' strike.
Every current National League franchise except the Washington Nationals has appeared in the NLCS at least once, although the Nationals appeared as the Montreal Expos in 1981. The Houston Astros made four NLCS appearances before moving to the American League in 2013.
As of the 2014 season, the Milwaukee Brewers are the only franchise to play in both the NLCS (in 2011) and the ALCS (in 1982). No franchise has yet to win both the National and American League Championship Series.
Championship Trophy.
The Warren C. Giles Trophy, named for the president of the NL from 1951 to 1969, is awarded to the NLCS winner.
Most Valuable Player Award.
A Most Valuable Player (MVP) award is given to the outstanding player in each series, though voters can consider performances made during the divisional series. The MVP award has been given to a player on the losing team twice, in 1986 to Mike Scott of the Houston Astros and in 1987 to Jeff Leonard of the San Francisco Giants.
Although the National League began its LCS MVP award in 1977, the American League did not begin its LCS MVP award until 1980.

</doc>
<doc id="3861" url="http://en.wikipedia.org/wiki?curid=3861" title="American League Championship Series">
American League Championship Series

The American League Championship Series (ALCS) is a best-of-seven series played in October in the Major League Baseball postseason that determines the winner of the American League pennant. The winner of the series advances to play the winner of the National League Championship Series in the World Series, Major League Baseball's championship series.
History.
It started in 1969, when the American League was reorganized into two divisions, East and West. The winners of each division played each other in a best-of-five series to determine who would advance to the World Series. In 1985, the format changed to best-of-seven. In 1994, the league was restructured into three divisions, with the three division winners and a wild-card team advancing to a best-of-five postseason round, known as the American League Division Series (ALDS). The winners of that round then advanced to the best-of-seven ALCS. In 2012, the playoffs were expanded again so that two wild card teams face off in a one-game wild card round to determine which team advances to the division series, with the playoffs then continuing as it had before 2012 (though with the possibility of a fifth seed being in the playoffs and a fourth seed being out) after the end of the wild card round. This is the system currently in use.
The ALCS and NLCS, since the expansion to best-of-seven, are always played in a 2–3–2 format: Games 1, 2, 6, and 7 are played in the stadium of the team that has home field advantage, and Games 3, 4, and 5 are played in the stadium of the team that does not. The series concludes when one team records its fourth win. Since 1998, home field advantage has been given to the team that has the better regular season record, unless that team happens to be the Wild Card team. In that case, the other team gets home field advantage, because by rule the Wild Card team is never allowed home field advantage in a Division Series or LCS. In the event that both teams have identical records in the regular season, home field advantage goes to the team that has the winning head-to-head record. From 1969 to 1993, home field advantage alternated between the two divisions, and from 1995 to 1997 home field advantage was determined before the season.
Every current American League team except for the Houston Astros has appeared in the ALCS at least once, as did the Milwaukee Brewers, before that club was moved to the National League in 1998.
The 2004 ALCS is noted as the only instance where a team has come back from a 0-3 deficit to win 4-3. The Boston Red Sox achieved this against their bitter rivals, The New York Yankees, and went on to win their first World Series in 86 years.
Championship Trophy.
The William Harridge Trophy is awarded to the ALCS champion. The trophy's namesake comes from the American League president from 1931 to 1959.
Most Valuable Player Award.
The Lee MacPhail Most Valuable Player (MVP) award is given to the outstanding player in the ALCS. No MVP award is given for Division Series play.
Although the National League began its LCS MVP award in 1977, the American League did not begin its LCS MVP award till 1980. The winners are listed (1) below in the section on "ALCS results (1969–present)", in the "Series MVP" column, (2) at League Championship Series Most Valuable Player Award, and (3) on the MLB website.
ALCS results (1969–present).
Click the link on the far left for detailed information on that series.

</doc>
<doc id="3862" url="http://en.wikipedia.org/wiki?curid=3862" title="American League Division Series">
American League Division Series

In Major League Baseball, the American League Division Series (ALDS) determines which two teams from the American League will advance to the American League Championship Series. The Division Series consists of two concurrent best-of-five series, featuring the three division winners and the winner of the wild-card game.
History.
The Division Series was implemented in 1981 as a result of a midseason strike, with the first place teams before the strike taking on the teams in first place after the strike. After 1993, it was implemented for good when Major League Baseball restructured each league into three divisions, but their next playing was in 1995 due to the cancellation of the 1994 playoffs. In 1981, a split-season format forced the first ever divisional playoff series, in which the New York Yankees won the Eastern Division series over the Milwaukee Brewers (who were in the American League until 1998) in five games while in the Western Division, the Oakland Athletics swept the Kansas City Royals (the only team with an overall losing record to ever make the postseason). The Yankees have currently played in the most division series in history, with fifteen appearances. The Houston Astros and the Toronto Blue Jays are the only teams to have never played in the ALDS, Houston not having made the postseason since 2005 when they won their only pennant to date while still a member of the National League, and Toronto being the last team to win the World Series under the old 4 division format in 1993. Before switching leagues in 2013, Houston had competed in seven National League Division Series.
Determining the matchups.
From 1998 to 2011, the wild-card team was assigned to play in the division winner with the best winning percentage (outside of their own division) in one series, and the other two division winners met in the other series. However, if the wild-card team and the division winner with the best record were from the same division, the wild-card team played the division winner with the second-best record, and the remaining two division leaders played each other. Beginning with the 2012 season, the wild card team that advances to the Division Series was to face the number 1 seed, regardless of whether or not they are in the same division. The two series winners move on to the best-of-seven ALCS. Home field advantage goes to the team with the better regular season record, except for the wild card team, which never receives the home field advantage.
Beginning in 2007, MLB has implemented a new rule to give the team from the league that wins the All-Star Game with the best regular season record a slightly greater advantage. In order to spread out the Division Series games for broadcast purposes, the two ALDS series follow one of two off-day schedules. Starting in 2007, after consulting the MLBPA, MLB has decided to allow the team with the best record in the league that wins the All-Star Game to choose whether to use the seven-day schedule (1-2-off-3-4-off-5) or the eight-day schedule (1-off-2-off-3-4-off-5). The team only gets to choose the schedule; the opponent is still determined by win-loss records.
Initially, the best-of-5 series played in a 2-3 format, with the first two games set at home for the lower seed team and the last three for the higher seed. Since 1998, the series has followed a 2-2-1 format, where the higher seed team plays at home in Games 1 and 2, the lower seed plays at home in Game 3 and Game 4 (if necessary), and if a Game 5 is needed, the teams return to the higher seed's field. When MLB added a second wild card team in 2012, the Division Series re-adopted the 2-3 format due to scheduling conflicts. It reverted to the 2-2-1 format in 2013.

</doc>
<doc id="3863" url="http://en.wikipedia.org/wiki?curid=3863" title="National League Division Series">
National League Division Series

In Major League Baseball, the National League Division Series (NLDS) determines which two teams from the National League will advance to the National League Championship Series. The Division Series consists of two best-of-five series, featuring the three division winners and a wild-card team.
History.
The Division Series was implemented in 1981 as a result of a midseason strike with first place teams before the strike taking on the first place teams after. After 1993, it was implemented for good when Major League Baseball restructured each league into three divisions, but their next playing was in 1995 due to the cancellation of the 1994 playoffs. Previously, because of a players' strike in 1981, a split-season format forced a divisional playoff series, in which the Montreal Expos won the Eastern Division series over the Philadelphia Phillies three games to two while the Los Angeles Dodgers beat the Houston Astros three games to two in the Western Division. The team with the best overall record in the major leagues, the Cincinnati Reds, failed to win their division in either half of that season and were controversially excluded, as were the St. Louis Cardinals, who finished with the NL's second-best record. The Atlanta Braves have currently played in the most NL division series with thirteen appearances. The St. Louis Cardinals have currently won the most NL division series, winning ten of the twelve series in which they have played. The Pittsburgh Pirates (whose finished with a losing record from 1993-2012) were the last team to make their first appearance in the NL division series, making their debut in 2013 after winning the 2013 National League Wild Card Game. In 2011, the Milwaukee Brewers became the first team to play in division series in both leagues when they won the National League Central Division title, their first postseason berth since winning the American League East Division title in 1982 before switching leagues in 1998. Milwaukee had competed in an American League Division Series in the strike-shortened 1981 season.
Format.
From 1998 to 2011, the wild-card team was assigned to play the division winner with the best winning percentage (outside of their own division) in one series, and the other two division winners met in the other series. However, if the wild-card team and the division winner with the best record were from the same division, the wild-card team played the division winner with the second-best record, and the remaining two division leaders played each other. Beginning with the 2012 season, the wild card team that advances to the Division Series was to face the number 1 seed, regardless of whether or not they are in the same division. The two series winners move on to the best-of-seven NLCS. The winner of the wild card has won the first round 7 out of the 11 years since the re-alignment and creation of the NLDS. According to Nate Silver, the advent of this playoff series, and especially of the wild card, has caused teams to focus more on "getting to the playoffs" rather than "winning the pennant" as the primary goal of the regular season.
Initially, the best-of-5 series played in a 2-3 format, with the first two games set at home for the lower seed team and the last three for the higher seed. Since 1998, the series has followed a 2-2-1 format, where the higher seed team plays at home in Games 1 and 2, the lower seed plays at home in Game 3 and Game 4 (if necessary), and if a Game 5 is needed, the teams return to the higher seed's field. When MLB added a second wild card team in 2012, the Division Series re-adopted the 2-3 format due to scheduling conflicts. It will revert to the 2-2-1 format from 2013 onwards.

</doc>
<doc id="3864" url="http://en.wikipedia.org/wiki?curid=3864" title="2001 World Series">
2001 World Series

The 2001 World Series, the 97th edition of Major League Baseball's championship series, took place between the Arizona Diamondbacks of the National League and the New York Yankees of the American League. The Diamondbacks won the best-of-seven series 4 games to 3. This is considered one of the greatest World Series of all time; memorable aspects included 2 extra-inning games and 3 late-inning comebacks. It ended on a Game 7 walk-off hit in the form of a bases-loaded blooper single off the bat of Luis Gonzalez. This was the 3rd World Series to end in this way, following 1997 and 1991.
This was the first World Series ever played in the state of Arizona and the Mountain Time Zone. With the All-Star Game format change in 2003, the World Series would not open in the city of the National League champion again until 2010. This was the last World Series not to feature a wild card team until 2008. This was also the first World Series to end in November, due to the delay in the regular season after the September 11 attacks.
With the win by the Diamondbacks, the franchise became the first World Series champions from a Far West state other than California.
Randy Johnson and Curt Schilling were the co-MVPs of the 2001 World Series, combining for a 4–0 record and a 1.40 ERA and striking out 45 Yankees in 38 2⁄3 innings.
Background.
The Arizona Diamondbacks reached the Series in just their fourth season, breaking a record previously held by the Florida Marlins, and took on the three-time defending champion New York Yankees, who had won the World Series in four of the last five years and tried to become the first team to win four straight titles since the Yankees' five consecutive titles from 1949 to 1953. Arizona captured the best of seven games Series, four games to three, thereby dethroning the defending World Champions and earning their first title.
Arizona won the first two games at home handily, but New York won the next three in close contests at the Yankee Stadium, including two dramatic ninth-inning comebacks against Arizona closer Byung-Hyun Kim. Arizona won the sixth game handily with Randy Johnson pitching a masterful game. Johnson also pitched in relief of Curt Schilling in Game 7. The Diamondbacks won that game by the score of 3–2, ending when Jay Bell scored the winning run on a bloop single by Luis Gonzalez, in the bottom of the ninth inning off the Yankees' ace closer, Mariano Rivera. Johnson, credited with the Game 7 win, became the first pitcher to win three games in the same World Series since Detroit Tigers' Mickey Lolich in 1968.
The home team won every game in the Series. This had only happened twice before, in 1987 and 1991 also in domed ballparks; in the two earlier championships, the Minnesota Twins won the Series. This Series was the subject of an HBO documentary "Nine Innings from Ground Zero" in 2004.
Though the series was played to the maximum seven games, the Diamondbacks outscored the Yankees 37–14 as a result of large margins of victory achieved by Arizona in Bank One Ballpark relative to the one run margins the Yankees achieved at Yankee Stadium. Arizona held powerhouse New York to an .183 batting average, the lowest ever in a seven-game World Series. The previous record was .185 by the St. Louis Cardinals in the 1985 World Series when they lost to the Kansas City Royals.
September 11 and the month of November.
Due to the postponement of MLB games as a result of the September 11 attacks, the World Series began Saturday, October 27, 2001, the latest start date ever for a World Series until the 2009 World Series, which started on October 28. The last three games were the first major-league games (other than exhibitions) played in the month of November. This was just the fourth time that no World Series champion was decided within the traditional month of October. The previous three occurrences were in 1904 (no series), 1918 (series held in September due to World War I), and 1994 (no series due to work stoppage). Additionally, the Series took place in New York City only seven weeks after the attacks, representing a remarkable boost in morale for the fatigued city.
Summary.
 NL Arizona Diamondbacks (4) vs. AL New York Yankees (3)
Matchups.
Game 1.
Saturday, October 27, 2001 at Bank One Ballpark in Phoenix, Arizona
The Yankees struck first in Game 1 when Derek Jeter was hit by a pitch with one out in the first and scored on Bernie Williams's double two batters later. However, Arizona's Curt Schilling and two relievers held the Yankees scoreless afterward. They managed to get only two walks and two hits for the rest of the game, Scott Brosius's double in the second and Jorge Posada's single in the fourth, both with two outs.
Meanwhile, the Diamondbacks tied the game on Craig Counsell's one-out home run in the first off of Mike Mussina. After a scoreless second, Mussina led off the third by hitting Tony Womack with a pitch. He moved to second on Counsell's sacrifice bunt before Luis Gonzalez's home run put the Diamondbacks up 3-1. A single and right fielder David Justice's error put runners on second and third before Matt Williams's sacrifice fly put Arizona up 4-1. After Mark Grace is intentionally walked, Damian Miller's RBI double gave Arizona a 5-1 lead.
Next inning, Gonzalez hit a two-out double off of Randy Choate. Reggie Sanders is intentionally walked before Gonzalez scored on Steve Finley's single. An error by third baseman Brosius's scored Sanders, put Finley at third, and Williams at second. Both men scored on Mark Grace's double, putting Arizona up 9-1. Though the Diamondbacks got just one more hit for the rest of the game off of Sterling Hitchcock and Mike Stanton (Williams's leadoff single in the seventh), they went up 1-0 in the series.
Game 2.
Sunday, October 28, 2001 at Bank One Ballpark in Phoenix, Arizona
Arizona continued to take control of the Series with the strong pitching performance of Randy Johnson. The Big Unit pitched a complete game shutout, allowing only four baserunners and three hits while striking out eleven Yankees. Andy Pettitte meanwhile nearly matched him, retiring Arizona in order in five of the seven innings he pitched. In the second, he allowed a leadoff single to Reggie Sanders, who scored on Danny Bautista's double. Bautista was the only Arizona runner stranded for the entire game. In the seventh, Pettitte hit Luis Gonzalez with a pitch before Sanders grounded into a forceout. After Bautista singled, Matt Williams's three-run home run put Arizona up 4-0. They won the game with that score and led the series two games to none as it moved to New York City.
Game 3.
Tuesday, October 30, 2001 at Yankee Stadium (I) in Bronx, New York
The game was opened in New York by United States President George W. Bush, who threw the ceremonial first pitch, a strike to Yankees backup catcher Todd Greene. Bush became the first sitting US President to throw a World Series first pitch since Dwight D. Eisenhower in 1956. He also threw the baseball from the mound where the pitcher would be set (unlike most ceremonial first pitches which are from in front of the mound) and threw it for a strike. Chants of "U-S-A, U-S-A" rang throughout Yankee Stadium. Yankees starter Roger Clemens allowed only three hits and struck out nine in seven innings of work. Yankees closer Mariano Rivera pitched two innings for the save.
Jorge Posada's leadoff home run off of Brian Anderson in the second put the Yankees up 1-0. The Diamondbacks loaded the bases in the fourth on two walks and one hit before Matt Williams's sacrifice fly tied the game. Bernie Williams hits a leadoff single in the sixth and move to second on a wild pitch one out later before Jorge Posada walked. Mike Morgan relieved Anderson and struck out David Justice before Scott Brosius broke the tie with an RBI single. The Yankees cut Arizona's series lead to 2-1 with the win.
Wednesday, October 31, 2001 at Yankee Stadium (I) in Bronx, New York
Game 4 saw the Yankees send Orlando Hernandez to the mound while the Diamondbacks elected to bring back Curt Schilling on three days' rest. Both pitchers gave up solo home runs, with Schilling doing so to Shane Spencer in the third inning and Hernandez doing so to Mark Grace in the fourth. Hernandez pitched 6 1⁄3 solid innings, giving up four hits while Schilling went seven innings and gave up three.
With the game still tied entering the eighth, Arizona struck. After Mike Stanton recorded the first out of the inning, Luis Gonzalez singled and Erubiel Durazo hit a double to bring him in. to score. Matt Williams followed by grounding into a fielder's choice off of Ramiro Mendoza, which scored pinch runner Midre Cummings and gave the team a 3-1 lead.
With his team on the verge of taking a commanding 3-1 series lead, Diamondbacks manager Bob Brenly elected to bring in closer Byung-Hyun Kim in the bottom of the eighth for a two-inning save. Kim, at 22, became the first Korean-born player ever to play in the MLB World Series. Kim struck out the side in the eighth, but ran into trouble in the ninth.
Derek Jeter led off by trying to bunt for a hit but was thrown out by Williams. Paul O'Neill then lined a single in front of Gonzalez. After Bernie Williams struck out, Kim seemed to be out of trouble with Tino Martinez coming to the plate. However, Martinez drove the first pitch he saw from Kim into the right-center field bleachers, tying the score at 3-3. The Yankees were not done, as Jorge Posada walked and David Justice moved him into scoring position with a single. Kim struck Spencer out to end the threat.
When the scoreboard clock in Yankee Stadium passed midnight, World Series play in November began, with the message on the scoreboard "Welcome to November Baseball". 
Mariano Rivera took the hill for the Yankees in the tenth and retired the Diamondbacks in order.
Kim went out for a third inning of work and retired Scott Brosius and Alfonso Soriano, but Jeter hit an opposite field walk-off home run on a 3–2 pitch count from Kim. This walk-off home run gave the Yankees a 4–3 victory and tied the Series at two games apiece, making Jeter the first player to hit a November home run and earning him the tongue-in-cheek nickname of "Mr. November". 
Game 5.
Thursday, November 1, 2001 at Yankee Stadium (I) in Bronx, New York
Game 5 saw the Yankees return to Mike Mussina for the start while the Diamondbacks sent Miguel Batista, who had not pitched in twelve days, to the mound. Batista pitched a strong 7 2⁄3 scoreless innings, striking out six. Mussina bounced back from his poor Game 1 start, recording ten strikeouts, but allowed solo home runs to Steve Finley and Rod Barajas in the fifth. 
With the Diamondbacks leading 2–0 in the ninth, Byung-Hyun Kim was called upon for the save despite having thrown three innings the night before. Jorge Posada doubled to open the inning, but Kim got Shane Spencer to ground out and then struck out Chuck Knoblauch. Unfortunately, as had happened the previous night, Kim could not hold the lead as Scott Brosius hit a 1–0 pitch over the left field wall, the second straight game tying home run in the bottom of the ninth for the Yankees. Kim was pulled from the game in favor of Mike Morgan who recorded the final out. 
Morgan retired the Yankees in order in the tenth and eleventh innings, while the Diamondbacks got to Mariano Rivera in the eleventh. Danny Bautista and Erubiel Durazo opened the inning with hits and Matt Williams advanced them into scoring position with a sacrifice bunt. Rivera then intentionally walked Steve Finley to load the bases, then got Reggie Sanders to line out and Mark Grace grounded out to end the inning.
Arizona went to Albie Lopez in the twelfth, and in his first at bat he gave up a single to Knoblauch. Brosius moved him over with a bunt, and then Alfonso Soriano ended the game with an RBI single to give the Yankees a 3-2 victory and a 3-2 lead in the series.
Game 6.
Saturday, November 3, 2001 at Bank One Ballpark in Phoenix, Arizona
With Arizona in a must-win situation, Johnson pitched seven innings and struck out seven, giving up just two runs. The Diamondbacks rocked Yankees starter Andy Pettitte for six runs after two innings and nine more runs against reliever Jay Witasick in one and a third innings before Randy Choate and Mike Stanton kept them scoreless for the rest of the game. The Diamondbacks hit six doubles and Danny Bautista batted 3-for-4 with five RBIs. The team set a World Series record with 22 hits and defeated the New York Yankees in its most lopsided postseason loss in 293 postseason games. The 15–2 win evened the series at three games apiece and set up a Game 7 for the ages between Roger Clemens and Curt Schilling, again pitching on three days' rest.
Game 7.
Sunday, November 4, 2001 at Bank One Ballpark in Phoenix, Arizona
It was a matchup of two twenty-game winners in the Series finale that would crown a new champion. Clemens at 39 years old became the oldest Game 7 starter ever. Schilling had already started two games of the Series and pitched his 300th inning of the season on just three days' rest. The two aces matched each other inning by inning and after seven full innings, the game was tied at 1–1. The Diamondbacks scored first in the sixth inning with a Steve Finley single and a Danny Bautista double (Bautista would be called out at third base). The Yankees responded with an RBI single from Tino Martinez, which drove in Derek Jeter. Brenly stayed with Schilling into the eighth, and the move backfired as Alfonso Soriano hit a solo home run on an 0–2 pitch. After Schilling got one out, he gave up a single to David Justice, and he left the game trailing 2–1. Brenly brought in Miguel Batista to get out Derek Jeter and then in an unconventional move, brought in the previous night's starter Randy Johnson, who had thrown 104 pitches, in relief to keep it a one-run game. It proved to be a smart move, as Johnson retired all four Yankees he faced.
With the Yankees ahead 2–1 in the bottom of the eighth, manager Joe Torre turned the game over to his ace closer Mariano Rivera for a two-inning save. Rivera struck out the side in the eighth, including Arizona's Luis Gonzalez, Matt Williams, and Danny Bautista, which lowered his ERA in the postseason to a major league-best of 0.70 . Although he was effective in the eighth, this game would end in the third ninth-inning comeback of the Series.
Mark Grace led off the inning with a single to center on a 1–0 pitch. Rivera's errant throw to second base on a bunt attempt by Damian Miller on an 0–1 pitch put runners on first and second. Derek Jeter tried to reach for the ball, but got tangled in the legs of pinch-runner David Dellucci, who was sliding in an attempt to break up the double play. Rivera appeared to regain control when he fielded Jay Bell's bunt and threw out Dellucci at third base, but third baseman Scott Brosius decided to hold onto the baseball instead of throwing to first to complete the double play. Midre Cummings was sent in to pinch-run for Damien Miller. With Cummings at second and Bell at first, the next batter, Tony Womack, hit a double down the right-field line on a 2–2 pitch that tied the game and earned Rivera a blown save. Bell advanced to third and the Yankees pulled the infield and outfield in as the potential winning run stood at third with fewer than two outs. After Rivera hit Craig Counsell with an 0–1 pitch, the bases were loaded. On an 0–1 pitch, Luis Gonzalez lofted a soft single over the drawn-in Derek Jeter that barely reached the outfield grass, plating Jay Bell with the winning run. This ended New York's bid for a fourth consecutive title and brought Arizona its first championship within its fourth year of existence, making the Diamondbacks the fastest expansion team to win a World Series, as well as the first major professional sports championship for the state of Arizona.
In 2009, Game 7 was chosen by "Sports Illustrated" as the Best Postseason Game of the Decade (2000–2009).
Composite box.
2001 World Series (4–3): Arizona Diamondbacks (N.L.) over New York Yankees (A.L.)
Media coverage.
For the second consecutive year, Fox carried the World Series over its network with its top broadcast team, Joe Buck and Tim McCarver (himself a Yankees broadcaster). This was the first year of Fox's exclusive rights to the World Series (in the previous contract, Fox only broadcast the World Series in even numbered years while NBC broadcast it in odd numbered years), which it has held ever since (this particular contract also had given Fox exclusive rights to the entire baseball postseason, which aired over its family of networks; the contract was modified following Disney's purchase of Fox Family Channel shortly after the World Series ended, as ESPN regained their postseason rights following a year of postseason games on ABC Family, Fox Family's successor). ESPN Radio provided national radio coverage for the fourth consecutive year, with Jon Miller and Joe Morgan calling the action.
Locally, the Series was broadcast by KTAR-AM in Phoenix with Thom Brennaman, Greg Schulte, Rod Allen and Jim Traber, and by WABC-AM in New York City with John Sterling and Michael Kay. This would be Sterling and Kay's last World Series working together, and Game 7 would be the last Yankee broadcast on WABC. Kay moved to television and the new YES Network the following season and WCBS picked up radio rights to the Yankees. It was Kay who announced Derek Jeter's game-winning home run in Game 4 of the series and subsequently anointed him as "Mr. November".
Aftermath.
After the Yankees lost the World Series, several players moved onto other teams or retired, the most notable changes being the signing of Jason Giambi to replace Martinez, and the retirements of Brosius and O'Neil. Martinez would later finish his career with the Yankees in 2005 after spending the previous three years in St. Louis and Tampa Bay. The Yankees would lose the 2003 World Series to the Florida Marlins and wouldn't win another World Series until 2009, when they defeated the defending champions, the Philadelphia Phillies, in six games.
After winning the NL West again in 2002 the Diamondbacks were swept 3–0 by St. Louis in the NLDS. From here they declined, losing 111 games in 2004 as Bob Brenly was fired during that season. Arizona would not win another NL West title until 2007. Schilling was traded to the Boston Red Sox after the 2003 season and in 2004 helped lead them to their first world championship since 1918. He helped them win another championship in 2007 and retired after four years with Boston, missing the entire 2008 season with a shoulder injury. Johnson was traded to the Yankees after the 2004 season, a season that saw him throw a perfect game against the Atlanta Braves, though he would be traded back to the Diamondbacks two years later and finish his career with the San Francisco Giants in 2009. One Diamondbacks player from 2001 is still active, Lyle Overbay (rejoined the Diamondbacks in August 2011 and now on the Milwaukee Brewers). With the retirements of both Derek Jeter and Alfonso Soriano after the 2014 season, Randy Choate is now the last remaining member of the 2001 AL Champion Yankees still active in the majors. Choate is currently on the roster of the St. Louis Cardinals.
From 2004 through 2007, the Yankees' misfortune in the postseason continued, with the team losing the ALCS to the Boston Red Sox in 2004, the ALDS to Anaheim in 2005, the ALDS to Detroit in 2006, and the ALDS to Cleveland in 2007. Joe Torre's contract was allowed to expire and he was replaced by Joe Girardi in 2008, a season in which the Yankees would miss the playoffs for the first time since 1993.
Buster Olney, who covered the Yankees for the "New York Times" before joining ESPN, would write a book titled "The Last Night of the Yankee Dynasty". The book is a play by play account of Game 7 in addition to stories about key players, executives, and moments from the 1996–2001 dynasty. In a 2005 reprinting, Olney included a new epilogue covering the aftermath of the 2001 World Series up to the Boston Red Sox epic comeback from down 3–0 in the 2004 ALCS.
s of 2014[ [update]], this is the state of Arizona's only world championship among the four major professional sports. The NFL's Arizona Cardinals came close, but lost to the Pittsburgh Steelers in Super Bowl XLIII.
DVD.
On October 11, 2005 A&E Home Video released the New York Yankees Fall Classic Collectors Edition (1996–2001) DVD set. Game 4 of the 2001 World Series is included in the set. On April 29, 2008 The Arizona Diamondbacks 2001 World Series DVD set was released. All seven games are included.

</doc>
<doc id="3865" url="http://en.wikipedia.org/wiki?curid=3865" title="1903 World Series">
1903 World Series

The 1903 World Series was the first modern World Series to be played in Major League Baseball. It matched the Boston Americans of the American League against the Pittsburgh Pirates of the National League in a best-of-nine series, with Boston prevailing five games to three, winning the last four.
Pittsburgh pitcher Sam Leever injured his shoulder while trap-shooting, so his teammate Deacon Phillippe pitched five complete games for Pittsburgh. Phillippe won three of his games, but it was not enough to overcome the club from the new American League. Boston pitchers Bill Dinneen and Cy Young led Boston to victory. In Game 1, Phillippe set a World Series record by striking out ten Boston batters. That record lasted barely one day, as Dinneen struck out eleven Pittsburgh batters in Game 2.
Honus Wagner, bothered by injuries, batted only 6 for 27 (.222) in the Series and committed six errors. The shortstop was deeply distraught by his performance. The following spring, Wagner (who in 1903 led the league in batting average) refused to send his portrait to a "Hall of Fame" for batting champions. "I was too bum last year," he wrote. "I was a joke in that Boston-Pittsburgh Series. What does it profit a man to hammer along and make a few hits when they are not needed only to fall down when it comes to a pinch? I would be ashamed to have my picture up now."(p138)
Due to overflow crowds at the Exposition Park games in Pittsburgh, if a batted ball rolled under a rope in the outfield that held spectators back, a "ground-rule triple" would be scored. Seventeen ground-rule triples were hit in the four games played at the stadium.
In the series, Boston came back from a three games to one deficit, winning the final four games to capture the title. Such a large comeback would not happen again until the Pirates came back to defeat the Washington Senators in the 1925 World Series, and has happened only ten times in baseball history. (The Pirates themselves repeated this feat in 1979 against the Baltimore Orioles.) Much was made of the influence of Boston's "Royal Rooters", who traveled to Pittsburgh and sang their theme song "Tessie" to distract the opposing players (especially Wagner). Boston wound up winning three out of four games in Pittsburgh.
Pirates owner Barney Dreyfuss added his share of the gate receipts to the players' share, so the losing team's players actually finished with a larger individual share than the winning team's.
The Series brought the new American League prestige and proved its best could beat the best of the National League, thus strengthening the demand for future World Series competitions.
Background.
A new league.
In 1901, Ban Johnson, president of the Western League, a minor league organization, formed the American League to take advantage of the National League's 1900 contraction from twelve teams to eight. Johnson and fellow owners raided the National League and signed away many star players, including Cy Young and Jimmy Collins. Johnson had a list of 46 National Leaguers he targeted for the American League; by 1902, all but one had made the jump.(p99) The constant raiding, however, scotched the idea of a championship between the two leagues. Pirates owner Barney Dreyfuss, whose team ran away with the 1902 National League pennant, was open to a post-season contest and even said he would allow the American League champion to stock its roster with all-stars.(p105) However, Johnson had spoken of putting a team in Pittsburgh and even attempted to raid the Pirates' roster in August 1902, which soured Dreyfuss. At the end of the season, however, the Pirates played a group of American League All-Stars in a four-game exhibition series, winning two games to one, with one tie.(p102)
The leagues finally called a truce in the winter of 1902–03 and formed the National Commission to preside over organized baseball. The following season, the Boston Americans and Pittsburgh Pirates had secured their respective championship pennants by September. That August, Dreyfuss challenged the American League to an eleven-game championship series. Encouraged by Johnson and National League President Harry Pulliam, Americans owner Henry J. Killilea met with Dreyfuss in Pittsburgh in September and instead agreed to a best-of-nine championship, with the first three games played in Boston, the next four in Pittsburgh, and the remaining two (if necessary) in Boston.(p122)
One significant point about this agreement was that it was an arrangement primarily between the two clubs rather than a formal arrangement between the leagues. In short, it was a voluntary event, a fact which would result in no Series at all for 1904, and eventually to the formal establishment of the Series as a compulsory event starting in 1905.
The teams.
The Pirates won their third straight pennant in 1903 thanks to a powerful line-up that included legendary shortstop Honus Wagner, who hit .355 and drove in 101 runs, player-manager Fred Clarke, who hit .351, and Ginger Beaumont, who hit .341 and led the league in hits and runs. The Pirates' pitching was weaker than it had been in previous years but boasted 24-game winner Deacon Phillippe and 25-game winner Sam Leever.(pp119, 123)
The Americans had a strong pitching staff, led by Cy Young, who went 28–9 in 1903 and became the all-time wins leader that year. Bill Dinneen and Long Tom Hughes, right-handers like Young, had won 21 games and 20 games each. The Boston outfield, featuring Chick Stahl (.274), Buck Freeman (.287, 104 RBIs) and Patsy Dougherty (.331, 101 runs scored) was considered excellent.(p124)
Although the Pirates had dominated their league for the previous three years, they went into the series riddled with injuries and plagued by bizarre misfortunes. Otto Krueger, the team's only utility player, was beaned on September 19 and never fully played in the series. 16-game winner Ed Doheny left the team three days later, exhibiting signs of paranoia; he was committed to an insane asylum the following month.(p122) Leever had been battling an injury to his pitching arm (which he made worse by entering a trapshooting competition). Worst of all, Wagner, who had a sore thumb throughout the season, injured his right leg in September and was never 100 percent for the post-season.(pp122–123)
Some sources say Boston were heavy underdogs. Boston bookies actually gave even odds to the teams (and only because Dreyfuss and other "sports" were alleged to have bet on Pittsburgh to bring down the odds).(p124) The teams were generally thought to be evenly matched, with the Americans credited with stronger pitching and the Pirates with superior offense and fielding. The outcome, many believed, hinged on Wagner's health. "If Wagner does not play, bet your money at two to one on Boston," said the Sporting News, "but if he does play, place your money at two to one on Pittsburgh."(quoted in p. 124)
Summary.
AL Boston Americans (5) vs. NL Pittsburgh Pirates (3)
Matchups.
Game 1.
Thursday, October 1, 1903 at Huntington Avenue Baseball Grounds in Boston, Massachusetts
The Pirates started Game 1 strong, scoring six runs in the first four innings. They extended their lead to 7–0 on a solo home run by Jimmy Sebring in the seventh, the first home run in World Series history. Boston tried to mount a comeback in the last three innings, but it was too little too late, and they ended up losing by a score of 7–3 in the first ever World Series game. Both Phillippe and Young threw complete games, with Phillippe striking out ten and Young fanning five, but Young also gave up twice as many hits and allowed three earned runs to Phillippe's two.
Game 2.
Friday, October 2, 1903 at Huntington Avenue Baseball Grounds in Boston, Massachusetts
After starting out strong in Game 1, the Pirates simply shut down offensively, eking out a mere three hits, all singles. Pittsburgh starter Sam Leever went 1 inning and gave up three hits and two runs, before his ailing arm forced him to leave in favor of Bucky Veil, who finished the game. Bill Dinneen struck out eleven and pitched a complete game for the Americans, while Patsy Dougherty hit home runs in the first and sixth innings for two of the Boston's three runs.
Game 3.
Saturday, October 3, 1903 at Huntington Avenue Baseball Grounds in Boston, Massachusetts
Phillippe, pitching after only a single day of rest, started Game 3 for the Pirates and didn't let them down, hurling his second complete game victory of the Series to put Pittsburgh up two games to one.
Game 4.
Tuesday, October 6, 1903 at Exposition Park (III) in Allegheny, Pennsylvania
After two days of rest, Phillippe was ready to pitch a second straight game. He threw his third complete game victory of the series against Bill Dinneen, who was making his second start of the series. But Phillippe's second straight win was almost not to be, as the Americans, down 5–1 in the top of the ninth, rallied to narrow the deficit to one run. The comeback attempt failed, as Phillippe managed to put an end to it and give the Pirates a commanding 3–1 Series lead.
Game 5.
Wednesday, October 7, 1903 at Exposition Park (III) in Allegheny, Pennsylvania
Game 5 was a pitcher's duel for the first five innings, with Boston's Cy Young and Pittsburgh's Brickyard Kennedy giving up no runs. That changed in the top of the sixth, however, when the Americans scored a then-record six runs before being retired. Young, on the other hand, managed to keep his shutout intact before finally giving up a pair of runs in the bottom of the eighth. He went the distance and struck out four for his first World Series win.
Game 6.
Thursday, October 8, 1903 at Exposition Park (III) in Allegheny, Pennsylvania
Game 6 was a rematch between the starters of Game 2, Boston's Dinneen and Pittsburgh's Leever. Leever pitched a complete game this time but so did Dinneen, who outmatched him to earn his second complete game victory of the series. After losing three of the first four games of the World Series, the underdog Americans had tied the series at three games apiece.
Game 7.
Saturday, October 10, 1903 at Exposition Park (III) in Allegheny, Pennsylvania
The fourth and final game in Pittsburgh saw Phillippe start his fourth game of the Series for the Pirates. This time, however, he wouldn't fare as well as he did in his first three starts. Cy Young, in his third start of the Series, held the Pirates to three runs and put the Americans ahead for the first time as the Series moved back to Boston.
Game 8.
Tuesday, October 13, 1903 at Huntington Avenue Baseball Grounds in Boston, Massachusetts
The final game of this inaugural World Series started out as an intense pitcher's duel, scoreless until the bottom of the fourth when Hobe Ferris hit a two-run single. Phillippe started his fifth and final game of the series and Dinneen his fourth. As he did in Game 2, Dinneen threw a complete game shutout, striking out seven and leading his Americans to victory, while Phillippe pitched respectably but just couldn't match Dinneen because his arm had been worn out with five starts in the eight games, giving up three runs to give the first 20th-century World Championship to the Boston Americans, Honus Wagner striking out to end the Series.
Composite line score.
1903 World Series (5–3): Boston Americans (A.L.) over Pittsburgh Pirates (N.L.)
Series statistics.
Boston Americans.
Batting.
"Note: GP=Games played; AB=At Bats; H=Hits; Avg.=Batting Average; HR=Home Runs; RBI=Runs Batted In"

</doc>
<doc id="3866" url="http://en.wikipedia.org/wiki?curid=3866" title="Bluetongue disease">
Bluetongue disease

Bluetongue disease is a non-contagious, insect-borne, viral disease of ruminants, mainly sheep and less frequently cattle, goats, buffalo, deer, dromedaries and antelope. It is caused by the Bluetongue virus (BTV). The virus is transmitted by the midge "Culicoides imicola", "Culicoides variipennis" and other culicoids.
Signs and symptoms.
In sheep, BTV causes an acute disease with high morbidity and mortality. BTV also infects goats, cattle and other domestic animals as well as wild ruminants (for example, blesbuck, white-tailed deer, elk, and pronghorn antelope).
Major signs are high fever, excessive salivation, swelling of the face and tongue and cyanosis of the tongue. Swelling of the lips and tongue gives the tongue its typical blue appearance, though this sign is confined to a minority of the animals. Nasal symptoms may be prominent, with nasal discharge and stertorous respiration. 
Some animals also develop foot lesions, beginning with coronitis, with consequent lameness. In sheep, this can lead to knee-walking. In cattle, constant changing of position of the feet gives bluetongue the nickname The Dancing Disease. Torsion of the neck (opisthotonos or torticollis) is observed in severely affected animals. 
Not all animals develop symptoms, but all those that do lose condition rapidly, and the sickest die within a week. For affected animals which do not die, recovery is very slow, lasting several months.
The incubation period is 5–20 days, and all symptoms usually develop within a month. The mortality rate is normally low, but it is high in susceptible breeds of sheep. In Africa, local breeds of sheep may have no mortality, but in imported breeds it may be up to 90 percent.
In cattle, goats and wild ruminants infection is usually asymptomatic despite high virus levels in blood. Red deer are an exception, and in them the disease may be as acute as in sheep.
Microbiology.
Bluetongue is caused by the pathogenic virus, Bluetongue virus (BTV), of the genus "Orbivirus", of the Reoviridae family. Twenty-six serotypes are now recognised for this virus.
The virus particle consists of ten strands of double-stranded RNA surrounded by two protein shells. Unlike other arboviruses, BTV lacks a lipid envelope. The particle has a diameter of 86 nm. The structure of the 70 nm core was determined in 1998 and was at the time the largest atomic structure to be solved.
The two outer capsid proteins, VP2 and VP5, mediate attachment and penetration of BTV into the target cell. The virus makes initial contact with the cell with VP2, triggering receptor-mediated endocytosis of the virus. The low pH within the endosome then triggers BTV's membrane penetration protein VP5 to undergo a conformational change that disrupts the endosomal membrane. Uncoating yields a transcriptionally active 470S core particle which is composed of two major proteins VP7 and VP3, and the three minor proteins VP1, VP4 and VP6 in addition to the dsRNA genome. There is no evidence that any trace of the outer capsid remains associated with these cores, as has been described for reovirus. The cores may be further uncoated to form 390S subcore particles that lack VP7, also in contrast to reovirus. Subviral particles are probably akin to cores derived "in vitro" from virions by physical or proteolytic treatments that remove the outer capsid and causes activation of the BTV transcriptase. In addition to the seven structural proteins, three non-structural (NS) proteins, NS1, NS2, NS3 (and a related NS3A) are synthesised in BTV-infected cells. Of these, NS3/NS3A is involved in the egress of the progeny virus. The two remaining non-structural proteins, NS1 and NS2, are produced at high levels in the cytoplasm and are believed to be involved in virus replication, assembly and morphogenesis.
Epidemiology.
Bluetongue has been observed in Australia, the USA, Africa, the Middle East, Asia and Europe. 
Its occurrence is seasonal in the affected Mediterranean countries, subsiding when temperatures drop and hard frosts kill the adult midge vectors. 
Viral survival and vector longevity is seen during milder winters. 
A significant contribution to the northward spread of Bluetongue disease has been the ability of "Culicoides obsoletus" and "C.pulicaris" to acquire and transmit the disease, both of which are spread widely throughout Europe. This is in contrast to the original "C.imicola" vector which is limited to North Africa and the Mediterranean. The relatively recent novel vector has facilitated a far more rapid spread than the simple expansion of habitats North through global warming. 
In August 2006, cases of bluetongue were found in the Netherlands, then Belgium, Germany, and Luxembourg. 
In 2007, the first case of bluetongue in the Czech Republic was detected in one bull near Cheb at the Czech-German border. 
In September 2007, the UK reported its first ever suspected case of the disease, in a Highland cow on a rare breeds farm near Ipswich, Suffolk. 
Since then the virus has spread from cattle to sheep in Britain. 
By October 2007 bluetongue had become a serious threat in Scandinavia and Switzerland 
and the first outbreak in Denmark was reported. In autumn 2008, several cases were reported in the southern Swedish provinces of Småland, Halland, and Skåne,
as well as in areas of the Netherlands bordering Germany, prompting veterinary authorities in Germany to intensify controls.
Norway saw its first finding in February 2009, when cows at two farms in Vest-Agder in the south of Norway showed an immune response to bluetongue. Norway have since been declared free of the disease in 2011.
Although the disease is not a threat to humans the most vulnerable common domestic ruminants in the UK are cattle, goats and, especially, sheep.
Overwintering.
A puzzling aspect of BTV is its survival between midge seasons in temperate regions. Adult "Culicoides" are killed by cold winter temperatures, and BTV infections typically do not last for more than 60 days, which is not long enough for BTV to last until the next spring. It is believed that the virus somehow survives in overwintering midges or animals. Multiple mechanisms have been proposed. A few adult "Culicoides" midges infected with BTV may survive the mild winters of the temperate zone. Some midges may even move indoors to avoid the cold temperature of the winter. Additionally, BTV could cause a chronic or latent infection in some animals, providing another means for BTV to survive the winter. BTV can also be transmitted from mother to fetus. The outcome is abortion or stillbirth if fetal infection occurs early in gestation and survival if infection occurs late. However infection at an intermediate stage, before the fetal immune system is fully developed, may result in a chronic infection that lingers until the first months after birth of the lamb. Midges will then spread the disease from the calves to other animals, starting a new season of infection.
Treatment and prevention.
There is no efficient treatment. Prevention is effected via quarantine, inoculation with live modified virus vaccine and control of the midge vector, including inspection of aircraft.
Livestock management and insect control.
However, simple husbandry changes and practical midge control measures may help break the livestock infection cycle. Housing livestock during times of maximum midge activity (from dusk to dawn) may lead to significantly reduced biting rates. Similarly, protecting livestock shelters with fine mesh netting or coarser material impregnated with insecticide will reduce contact with the midges. The "Culicoides" midges that carry the virus usually breed on animal dung and moist soils, either bare or covered in short grass. Identifying breeding grounds and breaking the breeding cycle will significantly reduce the local midge population. Turning off taps, mending leaks and filling in or draining damp areas will also help dry up breeding sites. Control by trapping midges and removing their breeding grounds may reduce vector numbers. Dung heaps or slurry pits should be covered or removed, and their perimeters (where most larvae are found) regularly scraped.
Vaccines.
Outbreaks in southern Europe have been caused by serotypes 2 and 4, and vaccines are available against these serotypes (ATCvet codes: QI04 for sheep, QI02 for cattle). However, the disease found in northern Europe (including the UK) in 2006 and 2007 has been caused by serotype 8. Vaccine companies Fort Dodge Animal Health (Wyeth), Merial and Intervet were developing vaccines against serotype 8 (Fort Dodge Animal Health has serotype 4 for sheep, serotype 1 for sheep and cattle and serotype 8 for sheep and cattle) and the associated production facilities. A vaccine for this is now available in the UK, produced by Intervet. Fort Dodge Animal Health has their vaccines available for multiple European Countries (vaccination will start in 2008 in Germany, Belgium, Switzerland, Spain and Italy). However, immunization with any of the available vaccines preclude later serological monitoring of affected cattle populations, a problem which could be resolved using next-generation subunit vaccines currently in development. 
In January 2015, Indian researchers launched its vaccine. Named 'Raksha Blu', it will protect the animals against five strains of the ‘bluetongue’ virus prevalent in the country.
History.
Although bluetongue disease was already recognized in South Africa in the early 19th century, a comprehensive description of the disease was not published until the first decade of the 20th century. In 1906 Arnold Theiler showed that bluetongue was caused by a filterable agent. He also created the first bluetongue vaccine, which was developed from an attenuated BTV strain. For many decades bluetongue was thought to be confined to Africa. The first confirmed outbreak outside of Africa occurred in Cyprus in 1943.
Related diseases.
African horse sickness is related to Bluetongue and is spread by the same midges ("Culicoides" species). It can kill the horses it infects and mortality may go as high as 90% of the infected horses during an epidemic.
The Epizootic Hemorrhagic Disease (EHD) virus is closely related and crossreacts with Bluetongue virus on many blood tests.

</doc>
<doc id="3869" url="http://en.wikipedia.org/wiki?curid=3869" title="Bruce Perens">
Bruce Perens

Bruce Perens (born around 1958) is an American computer programmer and advocate in the free software movement. He created The Open Source Definition and published the first formal announcement and manifesto of open source. He co-founded the Open Source Initiative (OSI) with Eric S. Raymond.
In 2005, Perens represented Open Source at the United Nations World Summit on the Information Society, at the invitation of the United Nations Development Programme. He has appeared before national legislatures and is often quoted in the press, advocating for open source and the reform of national and international technology policy.
Perens is also an amateur radio operator, with call sign K6BP. He promotes open radio communications standards.
Early life.
Perens grew up in Long Island, New York. He was born with cerebral palsy, which caused him to have slurred speech and difficulty reading as a child, a condition that led to a misdiagnosis of him as developmentally disabled in school. He developed an interest in technology at an early age: besides his interest in amateur radio, he ran a pirate radio station in the town of Lido Beach, and briefly engaged in phone phreaking.
Career.
Computer graphics.
Perens worked for seven years at the New York Institute of Technology Computer Graphics Lab. After that, he worked at Pixar for 12 years, from 1987 to 1999. He is credited as a studio tools engineer on the Pixar films "A Bug's Life" (1998) and "Toy Story 2" (1999).
BusyBox.
In 1995, Perens created BusyBox, a package of UNIX-style utilities for operating systems including Linux and FreeBSD. He stopped working on it in 1996, after which it was taken over by other developers.
Starting in 2007, several lawsuits were filed for infringement of BusyBox copyright and licensing. These lawsuits were filed by the Software Freedom Law Center (SFLC), and some of the later managing developers of BusyBox.
In 2009, Bruce Perens released a statement about the lawsuits and those filing them. In it, he claims that he maintains a significant or even majority ownership of the software in the litigation, but was not contacted nor represented by the plaintiffs; and that some of the plaintiffs had themselves modified BusyBox and its distribution package in such a way as to violate applicable licensing terms and copyright owned by Perens and additional BusyBox developers. Perens supports enforcement of the GPL license used on Busybox. Because he was denied participation in the Busybox cases on the side of the prosecution, Perens started a consulting business to assist the defendants in coming into compliance with the GPL and arriving at an amicable settlement with the Software Freedom Law Center.
Debian Project Leader.
From April 1996 to December 1997, while still working at Pixar, Perens served as Debian Project Leader, the person who coordinates development of the Debian open source operating system. He replaced Ian Murdock, the creator of Debian, who had been the first project leader.
Software in the Public Interest.
In 1997, Perens was a co-founder of Software in the Public Interest, a nonprofit organization intended to serve as an umbrella organization to aid open-source software and hardware projects. It was originally created to allow the Debian Project to accept donations.
Debian Social Contract.
In 1997, Debian developer Ean Schuessler proposed to create a "social contract" for Debian, guaranteeing to its users that it was committed to the principles of open source software and organizational transparency. The end result of this was the Debian Social Contract, the writing of which was headed by Perens. (It was based in part on The Free Software Definition, written by Richard Stallman in 1986.) Perens proposed a draft of the Debian Social Contract to the Debian developers on the debian-private mailing list early in June 1997. Debian developers contributed discussion and changes for the rest of the month while Perens edited, and the completed document was then announced as Debian project policy. Part of the Debian Social Contract was the Debian Free Software Guidelines, a set of 10 guidelines for determining whether a set of software can be described as "free software", and thus whether it could be included in Debian.
Open Source Definition and The Open Source Initiative.
On February 3, 1998, a group of people (not including Perens) met at VA Linux Systems to discuss the promotion of Free Software to business in pragmatic terms, rather than the moral terms preferred by Richard Stallman. Christine Petersen of the nanotechnology organization Foresight Institute, who was present because Foresight took an early interest in Free Software, suggested the term "Open Source". The next day, Eric S. Raymond recruited Perens to work with him on the formation of Open Source. Perens modified the Debian Free Software Guidelines into the Open Source Definition by removing Debian references and replacing them with "Open Source".
The original announcement of The Open Source Definition was made on February 9, 1998 on Slashdot and elsewhere; the definition was given in Linux Gazette on February 10, 1998.
Concurrently, Perens and Raymond established the Open Source Initiative, an organization intended to promote open source software.
Perens left OSI in 1999, a year after co-founding it. In an email to the Debian developers mailing list explaining his decision, he stated that, though "most hackers know that Free Software and Open Source are just two words for the same thing", the success of "open source" as a marketing term had "de-emphasized the importance of the freedoms involved in Free Software"; he added, "It's time for us to fix that." He also stated his regret that OSI co-founder Eric Raymond "seems to be losing his free software focus."
Linux Capital Group.
In 1999, Perens left Pixar and became the president of Linux Capital Group, a business incubator and venture capital firm focusing on Linux-based businesses. Their major investment was in Progeny Linux Systems, a company headed by Debian founder Ian Murdock. In 2000, as a result of the economic downturn, Perens shut down Linux Capital Group. (Progeny Linux Systems would end operations in 2007.)
Hewlett-Packard.
From December 2000 to September 2002, Perens served as "Senior Global Strategist for Linux and Open Source" at Hewlett-Packard, internally evangelizing for the use of Linux and other open-source software. He was fired as a result of his anti-Microsoft statements, which especially became an issue after HP acquired Compaq, a major manufacturer of Microsoft Windows-based PCs, in 2002.
Linux Standard Base.
In 2001, Perens founded, and became the first project leader, of the Linux Standard Base project, a joint project by several Linux distributions under the organizational structure of the Linux Foundation to standardize the Linux software system structure.
UserLinux.
In 2003 Perens created UserLinux, a Debian-based distribution whose stated goal was, "Provide businesses with freely available, high quality Linux operating systems accompanied by certifications, service, and support options designed to encourage productivity and security while reducing overall costs." UserLinux was eventually overtaken in popularity by Ubuntu, another Debian-based distribution, which was started in 2004, and UserLinux became unmaintained in 2006.
SourceLabs.
Perens was an employee of SourceLabs, a Seattle-based open source software and services company, from June 2005 until December 2007. He produced a video commercial, "Impending Security Breach", for SourceLabs in 2007. (SourceLabs went out of business in 2009.)
Other activities.
In 2002, Perens worked remotely as Senior Scientist for Open Source with the Cyber Security Policy Research Institute of George Washington University. In 2006, he received a three-year grant from the Competence Fund of Southern Norway. With this funding, he spent part of the summer as a visiting lecturer and researcher at University of Agder in 2006 and 2007. During this time he consulted the Norwegian Government and other entities on government policy issues related to computers and software.
In 2007, some of his government advisory roles included a meeting with the President of the Chamber of Deputies (the lower house of parliament) in Italy and testimony to the Culture Committee of the Chamber of Deputies; a keynote speech at the foundation of Norway's Open Source Center, following Norway's Minister of Governmental Reform (Perens is on the advisory board of the center); he provided input on the revision of the European Interoperability Framework; and he was keynote speaker at a European Commission conference on "Digital Business Ecosystems at the Centre Borschette, Brussels, on November 7".
In 2009, Perens acted as an expert witness on open source in the Jacobsen v. Katzer U.S. Federal lawsuit. His report, which was made publicly available by Jacobsen, presented the culture and impact of open-source software development to the federal courts.
Perens delivered one of the keynote addresses at the 2012 linux.conf.au conference in Ballarat, Australia. He discussed the need for open source software to market itself better to non-technical users. He also discussed some of the latest developments in open source hardware, such as Papilio and Bus Pirate.
Views.
Perens poses Open Source as a means of marketing the free software philosophy of Richard Stallman to business people who are more concerned with profit than politics, and states that open source and free software are only two ways of talking about the same phenomenon. This differs from Stallman and Raymond. Perens postulates an economic theory for business use of Open Source in his paper "The Emerging Economic Paradigm of Open Source" and his speech "Innovation Goes Public". This differs from Raymond's theory in "The Cathedral and the Bazaar", which having been written before there was much business involvement in open source, explains open source as a consequence of programmer motivation and leisure.
In February 2008, for the 10th anniversary of the phrase "open source", Perens published a message to the community called "State of Open Source Message: A New Decade For Open Source". Around the same time the ezine RegDeveloper published an interview with Perens where he spoke of the successes of open source, but also warned of dangers, including a proliferation of OSI-approved licenses which had not undergone legal scrutiny. He advocated the use of the GPLv3 license, especially noting Linus Torvalds' refusal to switch away from GPLv2 for the Linux kernel.
Amateur radio and other activities.
Perens is an avid amateur radio enthusiast (call sign K6BP) and maintained technocrat.net, which he closed in late 2008, because its revenues did not cover its costs.
Media appearances.
Perens is featured in the 2001 documentary film "Revolution OS" and the 2006 BBC television documentary "The Code-Breakers".
From 2002 to 2006, Prentice Hall PTR published the Bruce Perens' Open Source Series, a set of 24 books covering various open source software tools, for which Perens served as the series editor. It was the first book series to be published under an open license.
Personal life.
Perens lives in Berkeley, California with his wife, Valerie, and son, Stanley, born in 2000.

</doc>
<doc id="3870" url="http://en.wikipedia.org/wiki?curid=3870" title="Bundle theory">
Bundle theory

Bundle theory, originated by the 18th century Scottish philosopher David Hume, is the ontological theory about objecthood in which an object consists only of a collection ("bundle") of properties, relations or tropes.
According to bundle theory, an object consists of its properties and nothing more: thus neither can there be an object without properties nor can one even "conceive" of such an object; for example, bundle theory claims that thinking of an apple compels one also to think of its color, its shape, the fact that it is a kind of fruit, its cells, its taste, or at least one other of its properties. Thus, the theory asserts that the apple is no more than the collection of its properties. In particular, there is no "substance" in which the properties are "inherent".
Arguments for the bundle theory.
The difficulty in conceiving of or describing an object without also conceiving of or describing its properties is a common justification for bundle theory, especially among current philosophers in the Anglo-American tradition.
The inability to comprehend any aspect of the thing other than its properties implies, this argument maintains, that one cannot conceive of a "bare particular" (a "substance" without properties), an implication that directly opposes substance theory. The conceptual difficulty of "bare particulars" was illustrated by John Locke when he described a "substance" by itself, apart from its properties, as "something, I know not what."
Whether a "relation" of an object is one of its properties may complicate such an argument. However, the argument concludes that the conceptual challenge of "bare particulars" leaves a bundle of properties and nothing more as the only possible conception of an object, thus justifying bundle theory.
Objections to the bundle theory.
Objections to bundle theory concern the nature of the "bundle of properties", the properties' "compresence" relation (the "togetherness" relation between those constituent properties), and the impact of language on understanding reality.
"Compresence" objection.
Bundle theory maintains that properties are "bundled" together in a collection without describing how they are tied together. For example, bundle theory regards an apple as red, four inches (100 mm) wide, and juicy but lacking an underlying "substance". The apple is said to be a "bundle of properties" including redness, being four inches (100 mm) wide, and juiciness.
Critics question how bundle theory accounts for the properties' "compresence" (the "togetherness" relation between those properties) without an underlying "substance". Critics also question how any two given properties are determined to be properties of the same object if there is no "substance" in which they both "inhere".
Traditional bundle theory explains the "compresence" of properties by defining an object as a collection of properties "bound" together. Thus, different combinations of properties and relations produce different objects. Redness and juiciness, for example, may be found together on top of the table because they are part of a bundle of properties located on the table, one of which is the "looks like an apple" property.
By contrast, substance theory explains the "compresence" of properties by asserting that the properties are found together because it is the "substance" that has those properties. In substance theory, a "substance" is the thing in which properties "inhere". For example, redness and juiciness are found on top of the table because redness and juiciness "inhere" in an apple, making the apple red and juicy.
The "bundle theory of substance" explains "compresence". Specifically, it maintains that properties' compresence itself engenders a "substance". Thus, it determines "substancehood" empirically by the "togetherness" of properties rather than by a "bare particular" or by any other non-empirical underlying strata. The "bundle theory of substance" thus rejects the substance theories of Aristotle, Descartes, and more recently, J.P. Moreland, Jia Hou, Joseph Bridgman, Quentin Smith, and others.
"Language-reality" objection.
The "language-reality" objection to bundle theory relates to the impact language has on understanding reality. The objection maintains that language causes confusion that supports bundle theory.
Per the objection, properties are synthetic constructions of language and thinking alone provides reality to the properties of any object. An apple, it claims, does not have the properties "red" or "juicy", but rather observers who already believe in a concept called "Red" use that concept to experience an apple as red. Further, the objection maintains that "Red" cannot be distilled from an apple because "Red" is an abstraction from other experiences and not an innate property an apple might contain. Per the objection, expressions such as, "An apple is red and juicy," includes at least six concepts and would best be left as dead-end logical propositions. Since the objection regards the words "Red" and "Juicy" as simply abstractions of previous experiences, it contends that they contain only a personal summary concept of one individual. Thus, the experience of an apple is as close to the "Apple" concept that one can get. The objection regards any additional analytic work of the mind as a synthesis of other experiences that is incapable of logically revealing any true essence of "Apple".
The "language-reality" objection asserts that language encourages the belief that "synthetic exercises" distill experiences, yet it rejects the results of such exercises by maintaining that observers actually combine experiences to create each concept of any particular property. It holds that language is a complicated belief system whose only connection to reality is an abstraction of experience. The "language-reality" objection may even suggest that "reality/non-reality" or "objective/subjective" distinctions themselves are merely artifacts of language and therefore are also solely abstractions of experience.
Bundle theory and Buddhism.
The Buddhist Madhyamaka philosopher, Chandrakirti, used the aggregate nature of objects to demonstrate the lack of essence in what is known as the sevenfold reasoning. In his work, "Guide to the Middle Way" (Sanskrit: "Madhyamakāvatāra"), he says:
[The self] is like a cart, which is not other than its parts, not non-other, and does not possess them. It is not within its parts, and its parts are not within it. It is not the mere collection, and it is not the shape.
He goes on to explain what is meant by each of these seven assertions, but briefly in a subsequent commentary he explains that the conventions of the world do not exist essentially when closely analyzed, but exist only through being taken for granted, without being subject to scrutiny that searches for an essence within them.
Another view of the Buddhist theory of the self, especially in early Buddhism, is that the Buddhist theory is essentially an eliminativist theory. According to this understanding, the self can not be reduced to a bundle because there is nothing that answers to the concept of a self. Consequently, the idea of a self must be eliminated.

</doc>
<doc id="3873" url="http://en.wikipedia.org/wiki?curid=3873" title="Bernard Montgomery, 1st Viscount Montgomery of Alamein">
Bernard Montgomery, 1st Viscount Montgomery of Alamein

Field Marshal Bernard Law Montgomery, 1st Viscount Montgomery of Alamein, KG, GCB, DSO, PC (; 17 November 1887 – 24 March 1976), nicknamed "Monty" and the "Spartan General", was a British Army officer.
He saw action in the First World War as a junior officer in the Royal Warwickshire Regiment. At Méteren, near the Belgian border at Bailleul, he was shot through the right lung by a sniper. He returned to the Western Front as a general staff officer and took part in the Battle of Arras in April/May 1917. He also took part in the Battle of Passchendaele in Autumn 1917 before finishing the war as chief of staff of the 47th (2nd London) Division.
In the inter-war years he commanded the 17th Battalion, Royal Fusiliers and, later, the 1st Battalion, Royal Warwickshire Regiment before becoming commander of 9th Infantry Brigade and then General Officer Commanding 8th Infantry Division.
During the Second World War he commanded the British Eighth Army from August 1942 in the Western Desert until the final Allied victory in Tunisia. This command included the Battle of El Alamein, a turning point in the Western Desert Campaign. He subsequently commanded the British Eighth Army during the Allied invasion of Sicily and then during the Allied invasion of Italy.
He was in command of all Allied ground forces during Operation "Overlord" from the initial landings until after the Battle of Normandy. He then continued in command of the 21st Army Group for the rest of the campaign in North West Europe. As such he was the principal field commander for the failed airborne attempt to bridge the Rhine at Arnhem and the Allied Rhine crossing. On 4 May 1945 he took the German surrender at Lüneburg Heath in northern Germany. After the war he became Commander-in-Chief of the British Army of the Rhine (BAOR) in Germany and then Chief of the Imperial General Staff.
Early life.
Montgomery was born in Kennington, London, in 1887, the fourth child of nine, to an Anglo-Irish Church of Ireland minister, the Reverend Henry Montgomery, and his wife, Maud (née Farrar). The Montgomerys, an 'Ascendancy' gentry family, were the County Donegal branch of the Clan Montgomery. Henry Montgomery, Vicar of St Mark's Church, Kennington, at that time, was the second son of General Sir Robert Montgomery, a native of Inishowen in County Donegal, the noted soldier and proconsul in British India, who died a month after his grandson's birth. He was probably a descendant of Colonel Alexander Montgomery (1686–1729). Bernard's mother, Maud, was the daughter of the preacher Frederic William Farrar and was eighteen years younger than her husband. After the death of Sir Robert Montgomery, Henry inherited the Montgomery ancestral estate of New Park in Moville, County Donegal. However, there was still £13,000 to pay on a mortgage, a large debt in the 1880s, and Henry was at the time still only an Anglican vicar. Despite selling off all the farms that were at Ballynally, "there was barely enough to keep up New Park and pay for the blasted summer holiday" (i.e., at New Park).
It was a financial relief of some magnitude when, in 1889, Henry was made Lord Bishop of Tasmania, then still a British colony and Bernard spent his formative years there. Bishop Montgomery considered it his duty to spend as much time as possible in the rural areas of Tasmania and was away for up to six months at a time. While he was away, his wife, still in her mid-twenties, gave her children "constant" beatings, then ignored them most of the time as she performed the public duties of the bishop's wife. Of Bernard's siblings, Sibyl died prematurely in Tasmania, and Harold, Donald and Una all emigrated. Maud Montgomery took little active interest in the education of her young children other than to have them taught by tutors brought from England. The loveless environment made Bernard something of a bully, as he himself recalled, "I was a dreadful little boy. I don't suppose anybody would put up with my sort of behaviour these days." Later in life Montgomery refused to allow his son David to have anything to do with his grandmother, and refused to attend her funeral in 1949.
The family returned to England once for a Lambeth Conference in 1897, and Bernard and his brother Harold were educated for a term at the King's School, Canterbury. In 1901, Bishop Montgomery became secretary of the Society for the Propagation of the Gospel, and the family returned to London. Montgomery attended St Paul's School and then the Royal Military College, Sandhurst, from which he was almost expelled for rowdiness and violence. On graduation in September 1908 he was commissioned into the 1st Battalion the Royal Warwickshire Regiment as a second lieutenant, and first saw overseas service later that year in India. He was promoted to lieutenant in 1910, and in 1912 became adjutant of the 1st Battalion of his regiment at Shorncliffe Army Camp.
First World War.
The First World War began in August 1914 and Montgomery moved to France with his regiment that month. He saw action at the Battle of Le Cateau that month and during the retreat from Mons. At Méteren, near the Belgian border at Bailleul on 13 October 1914, during an Allied counter-offensive, he was shot through the right lung by a sniper. Montgomery was hit once more though, in the knee. He was awarded the Distinguished Service Order for gallant leadership: the citation for this award, published in the "London Gazette" in December 1914 reads:Conspicuous gallant leading on 13th October, when he turned the enemy out of their trenches with the bayonet. He was severely wounded.
After recovering in early 1915, he was appointed to be brigade major first of 112th Brigade and then with 104th Brigade under training in Lancashire. He returned to the Western Front in early 1916 as a general staff officer in the 33rd Division and took part in the Battle of Arras in April/May 1917. He became a general staff officer with IX Corps, part of General Sir Herbert Plumer's Second Army, in July 1917.
Montgomery served at the Battle of Passchendaele in Autumn 1917 before finishing the war as General Staff Officer 1 and effectively chief of staff of the 47th (2nd London) Division, with the temporary rank of lieutenant-colonel. A photograph from October 1918, reproduced in many biographies, shows the then unknown Lt.-Col. Montgomery standing in front of Winston Churchill (Minister of Munitions) at the parade following the liberation of Lille.
Between the world wars.
After the First World War Montgomery commanded the 17th Battalion the Royal Fusiliers, a battalion in the British Army of the Rhine, before reverting to his substantive rank of captain (brevet major) in November 1919. He had not at first been selected for Staff College (his only hope of ever achieving high command). But at a tennis party in Cologne, he was able to persuade the Commander-in-Chief of the British Army of Occupation, Sir William Robertson, to add his name to the list.
After graduating from Staff College, he was appointed brigade major in the 17th Infantry Brigade in January 1921. The brigade was stationed in County Cork carrying out counter-insurgency operations during the final stages of the Irish War of Independence.
Montgomery came to the conclusion that the conflict could not be won without harsh measures, and that self-government was the only feasible solution; in 1923, after the establishment of the Irish Free State and during the Irish Civil War, Montgomery wrote to Colonel Arthur Percival of the Essex Regiment: Personally, my whole attention was given to defeating the rebels but it never bothered me a bit how many houses were burnt. I think I regarded all civilians as 'Shinners' and I never had any dealings with any of them. My own view is that to win a war of this sort, you must be ruthless. Oliver Cromwell, or the Germans, would have settled it in a very short time. Nowadays public opinion precludes such methods, the nation would never allow it, and the politicians would lose their jobs if they sanctioned it. That being so, I consider that Lloyd George was right in what he did, if we had gone on we could probably have squashed the rebellion as a temporary measure, but it would have broken out again like an ulcer the moment we removed the troops. I think the rebels would probably [have] refused battles, and hidden their arms etc. until we had gone.
In May 1923, Montgomery was posted to the Territorial 49th Division. He returned to the 1st Royal Warwickshire Regiment in 1925 as a company commander. In January 1926, having been promoted to major in July 1925, he was appointed Deputy Assistant Adjutant General at the Staff College, Camberley in the temporary rank of lieutenant-colonel, a position he held until January 1929 by which time he had been made a (brevet lieutenant-colonel).
In 1927, he met and married Elizabeth Carver, née Hobart, widow of Oswald Carver, Olympic rowing medallist who was killed in the First World War. Their son, David, was born in August 1928. Elizabeth Carver was the sister of the Second World War commander Percy Hobart.
He returned to 1st Royal Warwickshire Regiment again, as Commander of Headquarters Company in January 1929 and went to the War Office to help write the Infantry Training Manual in Summer 1929. In 1931 Montgomery was promoted to lieutenant-colonel commanding the 1st Battalion of The Royal Warwickshire Regiment and saw service in Palestine and India. He was promoted to colonel in June 1934 (seniority from January 1932). He attended and was then recommended to become an instructor at the Indian Army Staff College (now the Pakistan Army Staff College) in Quetta, British India.
On completion of his tour of duty in India, Montgomery returned to Britain in June 1937 where he became commanding officer of the 9th Infantry Brigade with the temporary rank of brigadier, but that year saw personal tragedy when his wife died. While on holiday in Burnham-on-Sea, she had suffered an insect bite which became infected, and she died in his arms from septicaemia following an amputation. The loss devastated Montgomery, but he insisted on throwing himself back into his work immediately after the funeral."
In 1938, he organised an amphibious combined operations landing exercise that impressed the new commander-in-chief, Southern Command, General Wavell. He was promoted to major-general in October 1938 and took command of the 8th Infantry Division in Palestine. There he quashed an Arab revolt before returning in July 1939 to Britain, suffering a serious illness on the way, to command the 3rd (Iron) Infantry Division. On hearing of the rebel defeat in April 1939, Montgomery said, "I shall be sorry to leave Palestine in many ways, as I have enjoyed the war out here".
Second World War.
British Expeditionary Force.
Retreat to Dunkirk and evacuation.
Britain declared war on Germany on 3 September 1939. The 3rd Division was deployed to Belgium as part of the British Expeditionary Force (BEF). During this time, Montgomery faced serious trouble from his military superiors and the clergy for his frank attitude regarding the sexual health of his soldiers, but was defended from dismissal by his superior Alan Brooke, commander of II Corps. Montgomery's training paid off when the Germans began their invasion of the Low Countries on 10 May 1940 and the 3rd Division advanced to the River Dijle and then withdrew to Dunkirk with great professionalism, entering the Dunkirk perimeter in a famous night-time march which placed his forces on the left flank which had been left exposed by the Belgian surrender. The 3rd Division returned to Britain intact with minimal casualties. During Operation "Dynamo" — the evacuation of 330,000 BEF and French troops to Britain — Montgomery assumed command of the II Corps.
On his return Montgomery antagonised the War Office with trenchant criticisms of the command of the BEF and was briefly relegated back to divisional command of 3rd Division. He was however made a Companion of the Order of the Bath. 3rd Division was at that time the only fully equipped division in England.
Montgomery was ordered to make ready his 3rd division to invade the neutral Portuguese Azores. Models of the islands were prepared and detailed plans worked out for the invasion. The invasion plans did not go ahead and plans switched to invading Cape Verde island also belonging to neutral Portugal. These invasion plans also did not go ahead. Montgomery was then ordered to prepare plans for the invasion of neutral Ireland and to seize Cork, Cobh and Cork harbor. These invasion plans like those of the Portuguese islands also did not go ahead and in July 1940, Montgomery was appointed acting lieutenant-general, placed in command of V Corps, responsible for the defence of Hampshire and Dorset, and started a long-running feud with the new commander-in-chief, Southern Command, Claude Auchinleck.
In April 1941, he became commander of XII Corps responsible for the defence of Kent. During this period he instituted a regime of continuous training and insisted on high levels of physical fitness for both officers and other ranks. He was ruthless in sacking officers he considered would be unfit for command in action. Promoted to temporary lieutenant-general in July, in December Montgomery was given command of South-Eastern Command overseeing the defence of Kent, Sussex and Surrey.
He renamed his command the South-Eastern Army to promote offensive spirit. During this time he further developed and rehearsed his ideas and trained his soldiers, culminating in Exercise Tiger in May 1942, a combined forces exercise involving 100,000 troops.
North Africa and Italy.
Montgomery's early command.
In 1942, a new field commander was required in the Middle East, where Auchinleck was fulfilling both the role of commander-in-chief Middle East Command and commander Eighth Army. He had stabilised the Allied position at the First Battle of El Alamein, but after a visit in August 1942, the Prime Minister, Winston Churchill, replaced him as C-in-C with Alexander and William Gott as commander of the Eighth Army in the Western Desert. After Gott was killed flying back to Cairo Churchill was persuaded by Brooke, who by this time was Chief of the Imperial General Staff, to appoint Montgomery, who had only just been nominated to replace Alexander as commander of the British ground forces for Operation "Torch".
A story, probably apocryphal but popular at the time, is that the appointment caused Montgomery to remark that "After having an easy war, things have now got much more difficult." A colleague is supposed to have told him to cheer up – at which point Montgomery said "I'm not talking about me, I'm talking about Rommel!"
Montgomery's assumption of command transformed the fighting spirit and abilities of the Eighth Army. Taking command on 13 August 1942, he immediately became a whirlwind of activity. He ordered the creation of the X Corps, which contained all armoured divisions to fight alongside his XXX Corps which was all infantry divisions. This was in no way similar to a German Panzer Corps. One of Rommel's Panzer Corps combined infantry, armour and artillery units under one corps commander. The only common commander for Montgomery's all infantry and all armour corps was the Eighth Army Commander himself. Correlli Barnett commented that Montgomery's solution "... was in every way opposite to Auchinleck's and in every way wrong, for it carried the existing dangerous separatism still further." Montgomery reinforced the 30 mi long front line at El Alamein, something that would take two months to accomplish. He asked Alexander to send him two new British divisions (51st Highland and 44th) that were then arriving in Egypt and were scheduled to be deployed in defence of the Nile Delta. He moved his field HQ to Burg al Arab, close to the Air Force command post in order better to coordinate combined operations.
Montgomery was determined that the Army, Navy and Air Forces should fight their battles in a unified, focused manner according to a detailed plan. He ordered immediate reinforcement of the vital heights of Alam Halfa, just behind his own lines, expecting the German commander, Erwin Rommel, to attack with the heights as his objective, something that Rommel soon did. Montgomery ordered all contingency plans for retreat to be destroyed. "I have cancelled the plan for withdrawal. If we are attacked, then there will be no retreat. If we cannot stay here alive, then we will stay here dead", he told his officers at the first meeting he held with them in the desert, though, in fact, Auchinleck had no plans to withdraw from the strong defensive position he had chosen and established at El Alamein.
Montgomery made a great effort to appear before troops as often as possible, frequently visiting various units and making himself known to the men, often arranging for cigarettes to be distributed. Although he still wore a standard British officer's cap on arrival in the desert, he briefly wore an Australian broad-brimmed hat before switching to wearing the black beret (with the badge of the Royal Tank Regiment next to the British General Officer's badge) for which he became notable. The black beret was offered to him by Jim Fraser while the latter was driving him on an inspection tour. Both Brooke and Alexander were astonished by the transformation in atmosphere when they visited on 19 August, less than a week after Montgomery had taken command.
First battles with Rommel.
Rommel attempted to turn the left flank of the Eighth Army at the Battle of Alam Halfa from 31 August 1942. The German/Italian armoured Corps infantry attack was stopped in very heavy fighting. Rommel's forces had to withdraw urgently lest their retreat through the British minefields be cut off. Montgomery was criticised for not counter-attacking the retreating forces immediately, but he felt strongly that his methodical build-up of British forces was not yet ready. A hasty counter-attack risked ruining his strategy for an offensive on his own terms in late October, planning for which had begun soon after he took command. He was confirmed in the permanent rank of lieutenant-general in mid October.
The conquest of Libya was essential for airfields to support Malta and to threaten the rear of Axis forces opposing Operation "Torch". Montgomery prepared meticulously for the new offensive after convincing Churchill that the time was not being wasted. (Churchill sent a telegram to Alexander on 23 September 1942 which began, "We are in your hands and of course a victorious battle makes amends for much delay.") He was determined not to fight until he thought there had been sufficient preparation for a decisive victory, and put into action his beliefs with the gathering of resources, detailed planning, the training of troops—especially in clearing minefields and fighting at night—and in the use of 252 of the latest American-built Sherman tanks, 90 M7 Priest self-propelled howitzers, and making a personal visit to every unit involved in the offensive. By the time the offensive was ready in late October, Eighth Army had 231,000 men on its ration strength.
El Alamein.
The Second Battle of El Alamein began on 23 October 1942, and ended 12 days later with the first large-scale, decisive Allied land victory of the war. Montgomery correctly predicted both the length of the battle and the number of casualties (13,500). However, soon after Allied armoured units and infantry broke through the German and Italian lines and were pursuing the enemy forces at speed along the coast road, a violent rainstorm burst over the region, bogging down the tanks and support trucks in the desert mud. Montgomery, standing before his officers at headquarters and close to tears, announced that he was forced to call off the pursuit. Corelli Barnett has pointed out that the rain also fell on the Germans, and that the weather is therefore an inadequate explanation for the failure to exploit the breakthrough, but nevertheless the Battle of El Alamein had been a great success. Over 30,000 prisoners were taken, including the German second in command, General von Thoma, as well as eight other general officers. Rommel, having been in a hospital in Germany at the start of the battle, was forced to return on 25 October 1942 after General Stumme – his replacement as German commander – died of a heart attack in the early hours of the battle.
Tunisia.
Montgomery was advanced to KCB and promoted to full general. He kept the initiative, applying superior strength when it suited him, forcing Rommel out of each successive defensive position. On 6 March 1943, Rommel's attack on the over-extended Eighth Army at Medenine (Operation "Capri") with the largest concentration of German armour in North Africa was successfully repulsed. At the Mareth Line, 20 to 27 March, when Montgomery encountered fiercer frontal opposition than he had anticipated, he switched his major effort into an outflanking inland pincer, backed by low-flying RAF fighter-bomber support. For his role in North Africa he was awarded the Legion of Merit by the United States government in the rank of Chief Commander.
Sicily.
The next major Allied attack was the Allied invasion of Sicily (Operation "Husky"). Montgomery considered the initial plans for the Allied invasion, which had been agreed in principle by Eisenhower and Alexander, to be unworkable because of the dispersion of effort. He managed to have the plans recast to concentrate the Allied forces, having Patton's Seventh US Army land in the Gulf of Gela (on the left flank of Eighth Army, which landed around Syracuse in the south-east of Sicily) rather than near Palermo in the west and north of Sicily. Inter-Allied tensions grew as the American commanders Patton and Bradley (then commanding II US Corps under Patton), took umbrage at what they saw as Montgomery's attitudes and boastfulness.
Italian campaign.
During the autumn of 1943, Montgomery continued to command the Eighth Army during the landings on the mainland of Italy itself. In conjunction with the Anglo-American landings at Salerno (near Naples) by Mark Clark's Fifth Army and seaborne landings by British paratroops in the heel of Italy (including the key port of Taranto, where they disembarked without resistance directly into the port), Montgomery led the Eighth Army up the toe of Italy. Montgomery abhorred the lack of coordination, the dispersion of effort, and the strategic muddle and opportunism he saw in the Allied effort in Italy and was glad to leave the "dog's breakfast" on 23 December 1943.
Normandy.
Montgomery returned to Britain in January 1944. He was assigned to command the 21st Army Group which consisted of all Allied ground forces that would take part in Operation "Overlord", the invasion of Normandy under overall direction of the Supreme Commander, Allied Expeditionary Forces, American General Dwight D. Eisenhower. At St Paul's School on 7 April and 15 May he presented his strategy for the invasion. He envisaged a ninety-day battle, ending when all the forces reached the Seine, pivoting on an Allied-held Caen, with British and Canadian armies forming a shoulder to attract and defeat the main German counter-attacks, while the US armies took the Cherbourg peninsula and Brittany, wheeling south and then east on the right.
During the hard fought two and a half month Battle of Normandy that followed, the impact of a series of unfavourable autumnal weather conditions disrupted the Normandy landing areas. Montgomery's initial plan was to break out immediately towards Caen. Depending on the historical interpretation he was unable or unwilling to do so. As the campaign progressed Montgomery altered his initial plan for the invasion and switched to a strategy of attracting and holding German counter-attacks in the area north of Caen, which was designed to allow the United States Army in the west to take Cherbourg. Hampered by stormy weather and the bocage terrain, Montgomery had to ensure Rommel focused on the British in the east rather than the Americans in the west, who had to take the Cotentin Peninsula and Brittany before the Germans could be trapped by a general swing east. By the middle of July Caen had not been taken, as Rommel continued to prioritise prevention of the break-out by British forces rather than the western territories being taken by the Americans. This was broadly as Montgomery had planned, albeit not with the same speed as he outlined at St Paul's. An American break-out was achieved with Operation "Cobra" and the encirclement of German forces in the Falaise pocket at the cost of British sacrifice with the diversionary Operation "Goodwood".
Advance to the Rhine.
General Eisenhower took over Ground Forces Command on 1 September, while continuing as Supreme Commander, with Montgomery continuing to command the 21st Army Group, now consisting mainly of British and Canadian units. Montgomery bitterly resented this change, although it had been agreed before the D-Day invasion.
Winston Churchill had Montgomery promoted to field marshal by way of compensation. Montgomery was able to persuade Eisenhower to adopt his strategy of a single thrust to the Ruhr with Operation "Market Garden" in September 1944. It was uncharacteristic of Montgomery's battles: the offensive was strategically bold but poorly planned. Montgomery either did not receive or ignored ULTRA intelligence which warned of the presence of German armoured units near the site of the attack.
When the surprise attack on the Ardennes took place on 16 December 1944, starting the Battle of the Bulge, the front of the U.S. 12th Army Group was split, with the bulk of the U.S. First Army being on the northern shoulder of the German 'bulge'. The Army Group commander, General Omar Bradley, was located south of the penetration at Luxembourg and command of the U.S. First Army became problematic. Montgomery was the nearest commander on the ground and on 20 December, Eisenhower (who was in Versailles) temporarily transferred Courtney Hodges' U.S. First Army and William Simpson's U.S. Ninth Army to his 21st Army Group until the "bulge" could be reduced and a simpler line of communications restored, despite Bradley's vehement objections on national grounds. Montgomery grasped the situation quickly, visiting all divisional, corps, and army field commanders himself and instituting his 'Phantom' network of liaison officers. He grouped the British XXX Corps as a strategic reserve behind the Meuse and reorganised the US defence of the northern shoulder, shortening and strengthening the line and ordering the evacuation of St Vith. The German commander of the 5th Panzer Army, Hasso von Manteuffel said:
The operations of the American 1st Army had developed into a series of individual holding actions. Montgomery's contribution to restoring the situation was that he turned a series of isolated actions into a coherent battle fought according to a clear and definite plan. It was his refusal to engage in premature and piecemeal counter-attacks which enabled the Americans to gather their reserves and frustrate the German attempts to extend their breakthrough.
Montgomery's 21st Army Group advanced to the Rhine with operations "Veritable" and "Grenade" in February 1945. A meticulously planned Rhine crossing occurred on 24 March. While successful it was two weeks after the Americans had unexpectedly captured the Ludendorff Bridge and crossed the river on 7 March with less than a battalion. Montgomery's river crossing was followed by the encirclement of the German Army Group B in the Ruhr. Initially Montgomery's role was to guard the flank of the American advance. This was altered, however, to forestall any chance of a Red Army advance into Denmark, and the 21st Army Group occupied Hamburg and Rostock and sealed off the Danish peninsula. On 4 May 1945, on Lüneburg Heath, Montgomery accepted the Surrender of German forces in north-west Germany, Denmark and the Netherlands.
Montgomery's lack of diplomacy.
Montgomery was notorious for his lack of tact and diplomacy. Even his "patron" the Chief of the Imperial General Staff Lord Alanbrooke frequently mentions it in his war diaries: "he is liable to commit untold errors in lack of tact" and "I had to haul him over the coals for his usual lack of tact and egotistical outlook which prevented him from appreciating other people's feelings". One incident that illustrated this occurred during the North African campaign when Montgomery bet Walter Bedell Smith that he could capture Sfax by the middle of April 1943. Smith jokingly replied that if Montgomery could do it he would give him a Flying Fortress complete with crew. Smith promptly forgot all about it, but Montgomery did not, and when Sfax was taken on 10 April he sent a message to Smith "claiming his winnings". Smith tried to laugh it off, but Montgomery was having none of it and insisted on his aircraft. It got as high as Eisenhower who was said to be absolutely furious, but with his renowned skill in diplomacy he ensured Montgomery did in fact get his Flying Fortress, though at a great cost in ill feeling. Even Alanbrooke thought it "crass stupidity".
In August 1945, whilst Alanbrooke, Andrew Cunningham and Charles Portal were discussing their possible successors as "Chiefs Of Staff" they concluded that Montgomery would be very efficient as CIGS from the Army's point of view but that he was also very unpopular with a large proportion of the Army. Despite this Cunningham and Portal were strongly in favour of Montgomery succeeding Alanbrooke after his retirement.
Later life.
After the war Montgomery became the C-in-C of the British Army of the Rhine (BAOR), the name given to the British Occupation Forces, and was the British member of the Allied Control Council. He was created 1st Viscount Montgomery of Alamein in 1946. He was Chief of the Imperial General Staff from 1946 to 1948, succeeding Alanbrooke, but was largely a failure as the role required strategic and political skills he did not possess. He was barely on speaking terms with his fellow chiefs, sending his VCIGS to attend their meetings and he clashed particularly with Arthur Tedder, who as Deputy Supreme Commander had intrigued for Montgomery's dismissal during the Battle of Normandy, and who was by now Chief of the Air Staff. When Montgomery's term of office expired, Prime Minister Clement Attlee appointed Sir William Slim from retirement with the rank of Field Marshal as his successor; when Montgomery protested that he had told his protégé John Crocker, a former corps commander from the 1944–45 campaign, that the job was to be his, Attlee is said to have given the memorable retort "Untell him".
He was then appointed Chairman of the Western European Union's commanders-in-chief committee. Volume 3 of Nigel Hamilton's "Life of Montgomery of Alamein" gives an account of the bickering between Montgomery and his land forces chief, a French general, which created splits through the Union headquarters. He was thus pleased to become Eisenhower's deputy in creating the North Atlantic Treaty Organisation's European forces in 1951. He continued to serve under Eisenhower's successors, Matthew Ridgway and Al Gruenther, until his retirement, aged nearly 71, in 1958. His mother Maud, Lady Montgomery, died at New Park in Moville in Inishowen in 1949; she was buried alongside her husband in the 'kirkyard' behind St. Columb's Church, the small Church of Ireland church beside New Park, overlooking Lough Foyle. Lord Montgomery did not attend the funeral, claiming he was "too busy".
He was chairman of the governing body of St. John's School in Leatherhead, Surrey, from 1951 to 1966, and a generous supporter. Lord Montgomery was an Honorary Member of the Winkle Club, a noted charity in Hastings, East Sussex, and introduced Sir Winston Churchill to the club in 1955.
In 1953, the Hamilton Board of Education in Hamilton, Ontario, Canada, wrote to Montgomery and asked permission to name a new school in the city's east end after him. Viscount Montgomery Elementary was billed as "the most modern school in North America" and the largest single-storey school in Hamilton, when the sod was turned on 14 March 1951. The school officially opened on 18 April 1953, with Montgomery in attendance among almost 10,000 well-wishers. At the opening, he gave the motto "Gardez Bien" from his own family's coat of arms. Montgomery referred to the school as his "beloved school" and visited on five separate occasions, the last being in 1960. On his last visit, he said to "his" students: Let's make Viscount Montgomery School the best in Hamilton, the best in Ontario, the best in Canada. I don't associate myself with anything that is not good. It is up to you to see that everything about this school is good. It is up to the students to not only be their best in school but in their behaviour outside of Viscount. Education is not just something that will help you pass your exams and get you a job, it is to develop your brain to teach you to marshal facts and do things.
Montgomery's memoirs (1958) criticised many of his wartime comrades in harsh terms, including Eisenhower, whom he accused, among other things, of prolonging the war by a year through poor leadership — allegations which ended their friendship, not least as Eisenhower was still US President at the time. He was threatened with legal action by Field-Marshal Auchinleck for suggesting that Auchinleck had intended to retreat from the Alamein position if attacked again, and had to give a radio broadcast (20 November 1958) expressing his gratitude to Auchinleck for having stabilised the front at the First Battle of Alamein. The 1960 paperback edition of his memoirs contains a publisher's note drawing attention to that broadcast, and stating that in the publisher's view the reader might reasonably assume from Montgomery's text that Auchinleck had been planning to retreat "into the Nile Delta or beyond" and pointing out that it had been Auchinleck's intention to launch an offensive as soon as the Eighth Army was "rested and regrouped". Montgomery was stripped of his honorary citizenship of Montgomery, Alabama, and was challenged to a duel by an Italian officer.
In retirement he publicly supported apartheid after a visit to South Africa in 1962, outraging much British liberal opinion, and after a visit to China declared himself impressed by the Chinese leadership. He spoke out against the legalisation of homosexuality in the United Kingdom, arguing that the "Sexual Offences Act 1967" was a "charter for buggery" and that "this sort of thing may be tolerated by the French, but we're British – thank God." Biographer Nigel Hamilton has suggested Montgomery may have been a repressed homosexual; in the late 1940s Montgomery maintained an affectionate friendship with a 12-year-old Swiss boy. One biographer called the friendship "bizarre", although not "improper", and a sign of "pitiful loneliness."
He twice met with Israeli general Moshe Dayan. After an initial meeting in the early 1950s, Montgomery met Dayan again in the 1960s to discuss the Vietnam War, which Dayan was studying. Montgomery was harshly critical of US strategy in Vietnam, which involved deploying large numbers of combat troops, aggressive bombing attacks, and uprooting entire village populations and forcing them into strategic hamlets. Montgomery said that the Americans' most important problem was that they had no clear-cut objective, and allowed local commanders to set military policy. At the end of their meeting, Montgomery asked Dayan to tell the Americans, in his name, that they were "insane".
Death.
Montgomery died from unspecified causes in 1976 at his home Isington Mill in Isington, near Alton in Hampshire, aged 88. After his funeral at St George's Chapel, Windsor, Montgomery was interred in Holy Cross churchyard, Binsted.
Legacy.
His portrait (by Frank O. Salisbury, 1945) hangs in the National Portrait Gallery.
A statue of Montgomery is outside the Ministry of Defence in Whitehall, alongside those of Field Marshal Lord Slim and Field Marshal Lord Alanbrooke.
Montgomery gave his name to the French commune Colleville-Montgomery, Normandy.
The Imperial War Museum holds a variety of material relating to Montgomery in its collections. These include Montgomery's Grant command tank (on display in the atrium at the Museum's London branch), his command caravans as used in North West Europe (on display at IWM Duxford), and his papers are held by the Museum's Department of Documents. The Museum maintains a permanent exhibition about Montgomery, entitled "Monty: Master of the Battlefield".
The World Champion Field Marshal Montgomery Pipe Band from Northern Ireland is named after him.
His Rolls-Royce staff car is on display at the Royal Logistic Corps Museum, Deepcut, Surrey.
The Montgomery cocktail is a martini mixed at a ratio of 15:1, facetiously named that because Montgomery supposedly refused to go into battle unless his numerical advantage was at least that high. Ironically, following severe internal injuries received in the First World War, Montgomery himself could neither smoke nor drink.
In the 1998 documentary "Live At Aspen" during the US Comedy Arts Festival, the British comedy troupe "Monty Python" explained how they came up with their name, saying that the name Monty "... made us laugh because Monty to us means Lord Montgomery, our great general of the Second World War".
Honours and awards.
Viscount Montgomery's ribbons as they would appear today, not including campaign or other awards.

</doc>
<doc id="3874" url="http://en.wikipedia.org/wiki?curid=3874" title="Herman Boerhaave">
Herman Boerhaave

Herman Boerhaave (], 31 December 1668 – 23 September 1738) was a Dutch botanist, Christian humanist and physician of European fame. He is regarded as the founder of clinical teaching and of the modern academic hospital and is sometimes referred to as "the father of physiology," along with his pupil Albrecht von Haller.
He is best known for demonstrating the relation of symptoms to lesions and, in addition, he was the first to isolate the chemical urea from urine. His motto was "Simplex sigillum veri"; "Simplicity is the sign of truth".
Biography.
Boerhaave was born at Voorhout near Leiden. The son of a Protestant pastor, in his youth Boeerhave studied for a divinity degree and wanted to become a preacher. After the death of his father, however, he was offered a scholarship and he entered the University of Leiden, where he took his degree in philosophy in 1689, with a dissertation "De distinctione mentis a corpore" (on the difference of the mind from the body). There he attacked the doctrines of Epicurus, Thomas Hobbes and Spinoza. He then turned to the study of medicine, in which he graduated in 1693 at Harderwijk in present-day Gelderland.
In 1701 he was appointed lecturer on the institutes of medicine at Leiden; in his inaugural discourse, "De commendando Hippocratis studio", he recommended to his pupils that great physician as their model. In 1709 he became professor of botany and medicine, and in that capacity he did good service, not only to his own university, but also to botanical science, by his improvements and additions to the botanic garden of Leiden, and by the publication of numerous works descriptive of new species of plants.
On 14 September 1710, Boerhaave married Maria Drolenvaux, the daughter of the rich merchant, Alderman Abraham Drolenvaux. They had four children, of whom one daughter, Maria Joanna, lived to adulthood. In 1722, he began to suffer from an extreme case of gout, recovering the next year.
In 1714, when he was appointed rector of the university, he succeeded Govert Bidloo in the chair of practical medicine, and in this capacity he introduced the modern system of clinical instruction. Four years later he was appointed to the chair of chemistry as well. In 1728 he was elected into the French Academy of Sciences, and two years later into the Royal Society of London. In 1729 declining health obliged him to resign the chairs of chemistry and botany; and he died, after a lingering and painful illness, at Leiden.
Legacy.
His reputation so increased the fame of the University of Leiden, especially as a school of medicine, that it became popular with visitors from every part of Europe. All the princes of Europe sent him pupils, who found in this skillful professor not only an indefatigable teacher, but an affectionate guardian. When Peter the Great went to Holland in 1716 (he was in Holland before in 1697 to instruct himself in maritime affairs), he also took lessons from Boerhaave. Voltaire traveled to see him, as did Carl Linnaeus, who became a close friend. His reputation was not confined to Europe; a Chinese mandarin sent him a letter addressed to "the illustrious Boerhaave, physician in Europe," and it reached him in due course.
The operating theatre of the University of Leiden in which he once worked as an anatomist is now at the center of a museum named after him; the Boerhaave Museum. Asteroid 8175 Boerhaave is named after Boerhaave. From 1955 to 1961 Boerhaave's image was printed on Dutch 20-guilder banknotes. The Leiden University Medical Centre organises medical trainings called "Boerhaave-courses".
Boerhaave first described Boerhaave syndrome, which involves tearing of the esophagus, usually a consequence of vigorous vomiting. He notoriously described in 1724 the case of Baron Jan von Wassenaer, a Dutch admiral who died of this condition following a gluttonous feast and subsequent regurgitation. This condition was uniformly fatal prior to modern surgical techniques allowing repair of the esophagus.
Boerhaave was critical of his Dutch contemporary, Baruch Spinoza, attacking him in his dissertation in 1689. At the same time, he admired Isaac Newton and was a devout Christian who often wrote about God in his works. A collection of his religious thoughts on medicine, translated from Latin to English, has been compiled by the "Sir Thomas Browne Instituut Leiden" under the name "Boerhaaveìs Orations" (meaning "Boherhaavian Prayers"). Among other things, he considered nature as God's Creation and he used to say that the poor were his best patients because God was their paymaster.
Publications.
The standard author abbreviation Boerh. is used to indicate this individual as the author when citing a botanical name.

</doc>
