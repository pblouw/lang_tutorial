<doc id="6654" url="http://en.wikipedia.org/wiki?curid=6654" title="Chicago Cubs">
Chicago Cubs

The Chicago Cubs are an American professional baseball franchise located on the north side of Chicago, Illinois. They are members of the Central Division of Major League Baseball's National League.
The club played its first games in 1870 as the Chicago White Stockings, before officially becoming the Chicago Cubs for the 1907 season. The Cubs are the oldest currently active U.S. professional sports club, continuously existing in the same city for their entire history. They are one of the two remaining charter members of the National League (the other being the Atlanta Braves). Since Chicago did not have a fully operating White Stockings team for due to the Great Chicago Fire, differences continue to be voiced when considering the elder status of this ball club: Although the Braves have played for more consecutive seasons, the Cubs hold the distinction of having been founded a full season earlier (Cubs in 1870 and Braves in 1871).
The Cubs are also one of two active major league clubs based in Chicago, the other being the Chicago White Sox of the American League. The team is currently owned by Thomas S. Ricketts, son of TD Ameritrade founder Joe Ricketts.
In 1906, the franchise recorded a Major League Baseball record 116 wins (tied by the 2001 Seattle Mariners) and posted a modern-era record winning percentage of .763, still held today. They appeared in their first World Series the same year, falling to their crosstown rivals, the White Sox, four games to two. The Cubs won back-to-back World Series championships in 1907 and 1908, becoming the first Major League club to play in three consecutive Fall Classics, and the first to win it twice. The club has appeared in seven World Series following their 1908 title, most recently in 1945. The Cubs have not won the World Series in 106 years, the longest championship drought of any major North American professional sports team,
and are often referred to as the "Lovable Losers" because of this distinction. They are also known as "The North Siders" because Wrigley Field, their home park since 1916, is located in Chicago's north side Lake View community at 1060 West Addison Street.
Team history.
Early club history.
1870–1875: The beginning.
Although 1876 is generally recognized as the birth-year of the Cubs franchise, the Chicago club was initially founded in 1870 as the Chicago White Stockings and played a single season in a pro-am league called the National Association of Base Ball Players. The White Stockings won that league's championship and followed that effort playing the next five seasons (with the exception of 1872 and 1873, when the club temporarily ceased operations following the Great Chicago Fire) in the National Association of Professional Base Ball Players along with the Boston Red Stockings (now the Atlanta Braves). Both of these early leagues were hampered by a variety of ethical issues, such as the "throwing" of games and the league's inability (or unwillingness) to enforce rules and player contracts. Because of these challenges, William Hulbert, the president of the Chicago club, spearheaded the development of the National League, whose inaugural season was 1876.
1876–1902: A National League.
 Hulbert signed multiple star players, such as pitcher Albert Spalding and infielders Ross Barnes, Deacon White, and Adrian "Cap" Anson, to join the team prior to the N.L.'s first season. The White Stockings played their home games at West Side Grounds, and quickly established themselves as one of the new league's top teams. Spalding won forty-seven games and Barnes led the league in hitting at .429 as Chicago won the first ever National League pennant, which at the time was the game's top prize.
After back-to-back pennants in 1880 and 1881, Hulbert died, and Spalding, who had retired to start Spalding sporting goods, assumed ownership of the club. The White Stockings, with Anson acting as player/manager, captured their third consecutive pennant in 1882, and Anson established himself as the game's first true superstar. In 1885 and '86, after winning N.L. pennants, the White Stockings met the short-lived American Association champion in that era's version of a World Series. Both seasons resulted in match ups with the St. Louis Brown Stockings, with the clubs tying in 1885 and with St. Louis winning in 1886. This was the genesis of what would eventually become one of the greatest rivalries in sports. In all, the Anson-led Chicago Base Ball Club won six National League pennants between 1876 and 1886. As a result, Chicago's club nickname transitioned, and by 1890 they had become known as the Chicago Colts, or sometimes "Anson's Colts," referring to Cap's influence within the club. Anson was the first player in history credited with collecting 3,000 career hits. After a disappointing record of 59-73 and a 9th-place finish in 1897, Anson was released by the Cubs as both a player and manager. Due to Anson's absence from the club after twenty two years, local newspaper reporters started to refer to the Cubs as the "Orphans".
After the 1900 season, the American Base-Ball League formed as a rival professional league, and incidentally the club's old White Stockings nickname would be adopted by a new American League neighbor to the south.
1902–1920: A Cub dynasty.
 In 1902, Spalding, who by this time had revamped the roster to boast what would soon be one of the best teams of the early century, sold the club to Jim Hart. The franchise was nicknamed the Cubs by the "Chicago Daily News" in 1902, although not officially becoming the Chicago Cubs until the 1907 season. During this period, which has become known as baseball's dead-ball era, Cub infielders Joe Tinker, Johnny Evers, and Frank Chance were made famous as a double-play combination by Franklin P. Adams' poem "Baseball's Sad Lexicon". The poem first appeared in the July 18, 1910 edition of the "New York Evening Mail". Mordecai "Three-Finger" Brown, Jack Taylor, Ed Reulbach, Jack Pfiester, and Orval Overall were several key pitchers for the Cubs during this time period. With Chance acting as player-manager from 1905 to 1912, the Cubs won four pennants and two World Series titles over a five-year span. Although they fell to the "Hitless Wonders" White Sox in the 1906 World Series, the Cubs recorded a record 116 victories and the best winning percentage (.763) in Major League history. With mostly the same roster, Chicago won back-to-back World Series championships in 1907 and 1908, becoming the first Major League club to play three times in the Fall Classic and the first to win it twice. However, the Cubs have not won a World Series since; this remains the longest championship drought in North American professional sports.
The next season, veteran catcher Johnny Kling left the team to become a professional pocket billiards player. Some historians think Kling's absence was significant enough to prevent the Cubs from also winning a third straight title in 1909, as they finished 6 games out of first place. When Kling returned the next year, the Cubs won the pennant again, but lost to the Philadelphia Athletics in the 1910 World Series.
In 1914, advertising executive Albert Lasker obtained a large block of the club's shares and before the 1916 season assumed majority ownership of the franchise. Lasker brought in a wealthy partner, Charles Weeghman, the proprietor of a popular chain of lunch counters who had previously owned the Chicago Whales of the short-lived Federal League. As principal owners, the pair moved the club from the West Side Grounds to the much newer Weeghman Park, which had been constructed for the Whales only two years earlier. The Cubs responded by winning a pennant in the war-shortened season of 1918, where they played a part in another team's curse: the Boston Red Sox defeated Grover Cleveland Alexander's Cubs four games to two in the 1918 World Series, Boston's last Series championship until 2004.
Beginning in 1916, Bill Wrigley of chewing-gum fame acquired an increasing quantity of stock in the Cubs. By 1921 he was the majority owner, maintaining that status into the 1930s.
Meanwhile, the year 1919 saw the start of the tenure of Bill Veeck, Sr. as team president. Veeck would hold that post throughout the 1920s and into the 30s. The management team of Wrigley and Veeck came to be known as the "double-Bills."
The Wrigley years (1921–1981).
1929–1938: Every three years.
Near the end of the first decade of the double-Bills' guidance, the Cubs won the NL pennant in 1929 and then achieved the unusual feat of winning a pennant every three years, following up the 1929 flag with league titles in 1932, 1935, and 1938. Unfortunately, their success did not extend to the Fall Classic, as they fell to their AL rivals each time. The '32 series against the Yankees featured Babe Ruth's "called shot" at Wrigley Field in Game 3. There were some historic moments for the Cubs as well; In 1930, Hack Wilson, one of the top home run hitters in the game, had one of the most impressive seasons in MLB history, hitting 56 home runs and establishing the current runs-batted-in record of 191. That 1930 club, which boasted six eventual Hall of Famers (Wilson, Gabby Hartnett, Rogers Hornsby, George "High Pockets" Kelly, Kiki Cuyler and manager Joe McCarthy) established the current team batting average record of .309. In 1935 the Cubs claimed the pennant in thrilling fashion, winning a record 21 games in a row in September. The '38 club saw Dizzy Dean lead the team's pitching staff and provided a historic moment when they won a crucial late-season game at Wrigley Field over the Pittsburgh Pirates with a walk-off home run by Gabby Hartnett, which became known in baseball lore as "The Homer in the Gloamin'".
After the "double-Bills" (Wrigley and Veeck) died in 1932 and 1933 respectively, P.K. Wrigley, son of Bill Wrigley, took over as majority owner. He was unable to extend his father's baseball success beyond 1938, and the Cubs slipped into years of mediocrity, although the Wrigley family would retain control of the team until 1981.
1945: The Curse.
The Cubs enjoyed one more pennant at the close of World War II, finishing 98–56. Due to the wartime travel restrictions, the first three games of the 1945 World Series were played in Detroit, where the Cubs won two games, including a one-hitter by Claude Passeau, and the final four were played at Wrigley. In Game 4 of the Series, the Curse of the Billy Goat was allegedly laid upon the Cubs when P.K. Wrigley ejected Billy Sianis, who had come to Game 4 with two box seat tickets, one for him and one for his goat. They paraded around for a few innings, but Wrigley demanded the goat leave the park due to its unpleasant odor. Upon his ejection, Mr. Sianis uttered, "The Cubs, they ain't gonna win no more." The Cubs lost Game 4, lost the Series, and have not been back since. It has also been said by many that Sianis put a "curse" on the Cubs, apparently preventing the team from playing in the World Series. After losing the 1945 World Series to the Detroit Tigers, the Cubs finished with winning seasons the next two years, but those teams did not enter post-season play.
In the following two decades after Sianis' ill will, the Cubs played mostly forgettable baseball, finishing among the worst teams in the National League on an almost annual basis. Longtime infielder/manager Phil Cavarretta, who had been a key player during the '45 season, was fired during spring training in 1954 after admitting the team was unlikely to finish above fifth place. Although shortstop Ernie Banks would become one of the star players in the league during the next decade, finding help for him proved a difficult task, as quality players such as Hank Sauer were few and far between. This, combined with poor ownership decisions such as the College of Coaches, and the ill-fated trade of future Hall of Famer Lou Brock to the Cardinals for pitcher Ernie Broglio (who won only 7 games over the next three seasons), hampered on-field performance.
1969: The Fall of '69.
The late-1960s brought hope of a renaissance, with third baseman Ron Santo, pitcher Ferguson Jenkins, and outfielder Billy Williams joining Banks. After losing a dismal 103 games in 1966, the Cubs brought home consecutive winning records in '67 and '68, marking the first time a Cub team had accomplished that feat in over two decades.
In the Cubs, managed by Leo Durocher, built a substantial lead in the newly created National League Eastern Division by mid-August. Ken Holtzman pitched a no-hitter on August 19, and the division lead grew to 8 1⁄2 games over the St. Louis Cardinals and by 9 1⁄2 games over the New York Mets. After the game of September 2, the Cubs record was 84-52 with the Mets in second place at 77-55. But then a losing streak began just as a Mets winning streak was beginning. The Cubs lost the final game of a series at Cincinnati, then came home to play the resurgent Pittsburgh Pirates (who would finish in third place). After losing the first two games by scores of 9-2 and 13-4, the Cubs led going into the ninth inning. A win would be a positive springboard since the Cubs were to play a crucial series with the Mets the very next day. But Willie Stargell drilled a 2-out, 2-strike pitch from the Cubs' ace reliever, Phil Regan, onto Sheffield Avenue to tie the score in the top of the ninth. The Cubs would lose 7-5 in extra innings.[6]
Burdened by a four-game losing streak, the Cubs traveled to Shea Stadium for a short two-game set. The Mets won both games, and the Cubs left New York with a record of 84-58 just 1⁄2 game in front. Disaster followed in Philadelphia, as a 99 loss Phillies team nonetheless defeated the Cubs twice, to extend Chicago's losing streak to eight games. In a key play in the second game, on September 11, Cubs starter Dick Selma threw a surprise pickoff attempt to third baseman Ron Santo, who was nowhere near the bag or the ball. Selma's throwing error opened the gates to a Phillies rally.
After that second Philly loss, the Cubs were 84-60 and the Mets had pulled ahead at 85-57. The Mets would not look back. The Cubs' eight-game losing streak finally ended the next day in St. Louis, but the Mets were in the midst of a ten-game winning streak, and the Cubs, wilting from team fatigue, generally deteriorated in all phases of the game.[1] The Mets (who had lost a record 120 games 7 years earlier), would go on to win the World Series. The Cubs, despite a respectable 92-70 record, would be remembered for having lost a remarkable 17 1⁄2 games in the standings to the Mets in the last quarter of the season.
1977–1979: The June Swoon.
Following the '69 season, the club posted winning records for the next few seasons, but no playoff action. After the core players of those teams started to move on, the 70s got worse for the team, and they became known as "The Loveable Losers." In , the team found some life, but ultimately experienced one of its The Cubs hit a high-water mark on June 28 at 47–22, boasting an 8 1⁄2 game NL East lead, as they were led by Bobby Murcer (27 Hr/89 RBI), and Rick Reuschel (20–10). However, the Philadelphia Phillies cut the lead to two by the All-star break, as the Cubs sat 19 games over .500, but they swooned late in the season, going 20–40 after July 31. The Cubs finished in 4th place at 81–81, while Philadelphia surged, finishing with 101 wins. The following two seasons also saw the Cubs get off to a fast start, as the team rallied to over 10 games above .500 well into both seasons, only to again wear down and play poorly later on, and ultimately settling back to mediocrity. This trait became known as the ""June Swoon"." Again, the Cubs' unusually high number of day games is often pointed to as one reason for the team's inconsistent late season play.
P.K. Wrigley died in 1977. The Wrigley family sold the team to the Chicago Tribune in 1981, ending a 65-year family relationship with the Cubs.
Tribune Company years (1981–2008).
1984: Heartbreak.
After over a dozen more subpar seasons, in 1981 the Cubs hired GM Dallas Green from Philadelphia to turn around the franchise. Green had managed the 1980 Phillies to the World Series title. One of his early GM moves brought in a young Phillies minor-league 3rd baseman named Ryne Sandberg, along with Larry Bowa for Ivan DeJesus. The 1983 Cubs had finished 71–91 under Lee Elia, who was fired before the season ended by Green. Green continued the culture of change and overhauled the Cubs roster, front-office and coaching staff prior to 1984. Jim Frey was hired to manage the 1984 Cubs, with Don Zimmer coaching 3rd base and Billy Connors serving as pitching coach.
Green shored up the 1984 roster with a series of transactions. In December, 1983 Scott Sanderson was acquired from Montreal in a three-team deal with San Diego for Carmelo Martinez. Pinch hitter Richie Hebner (.333 BA in 1984) was signed as a free-agent. In spring training, moves continued: LF Gary Matthews and CF Bobby Dernier came from Philadelphia on March 26, for Bill Campbell and a minor leaguer. Reliever Tim Stoddard (10–6 3.82, 7 saves) was acquired the same day for a minor leaguer; veteran pitcher Ferguson Jenkins was released.
The team's commitment to contend was complete when Green made a midseason deal on June 15 to shore up the starting rotation due to injuries to Rick Reuschel (5–5) and Sanderson. The deal brought 1979 NL Rookie of the Year pitcher Rick Sutcliffe from the Cleveland Indians. Iowa Cub Joe Carter and CF Mel Hall were sent to Cleveland for Sutcliffe and back-up C Ron Hassey (.333 with Cubs in 1984). Sutcliffe (5–5 with the Indians) immediately joined Sanderson (8–5 3.14), Eckersley (10–8 3.03), Steve Trout (13–7 3.41) and Dick Ruthven (6–10 5.04) in the starting rotation. Sutcliffe proceeded to go 16–1 for Cubs and capture the Cy Young Award.
The Cubs 1984 starting lineup was very strong. It consisted of LF Matthews (.291 14–82 101 runs 17 SB), C Jody Davis (.256 19–94), RF Keith Moreland (.279 16–80), SS Larry Bowa (.223 10 SB), 1B Leon "Bull" Durham (.279 23–96 16SB), CF Dernier (.278 45 SB), 3B Ron Cey (.240 25–97), Closer Lee Smith(9–7 3.65 33 saves) and 1984 NL MVP Ryne Sandberg (.314 19–84 114 runs, 19 triples,32 SB).
Reserve players Hebner, Thad Bosley, Henry Cotto, Hassey and Dave Owen produced exciting moments. The bullpen depth of Rich Bordi, George Frazier, Warren Brusstar and Dickie Noles did their job in getting the game to Smith or Stoddard.
At the top of the order, Dernier and Sandberg were exciting, aptly coined "the Daily Double" by Harry Caray. With strong defense – Dernier CF and Sandberg 2B, won the NL Gold Glove- solid pitching and clutch hitting, the Cubs were a well balanced team. Following the "Daily Double," Matthews, Durham, Cey, Moreland and Davis gave the Cubs an order with no gaps to pitch around. Sutcliffe anchored a strong top to bottom rotation and Smith was one of the top closers in the game.
The shift in the Cubs' fortunes was characterized June 23 on the "NBC Saturday Game of the Week" contest against the St. Louis Cardinals. it has since been dubbed simply "The Sandberg Game." With the nation watching and Wrigley Field packed, Sandberg emerged as a superstar with not one, but two game-tying home runs against Cardinals closer Bruce Sutter. With his shots in the 9th and 10th innings Wrigley Field erupted and Sandberg set the stage for a comeback win that cemented the Cubs as the team to beat in the East. No one would catch them, except the Padres in the playoffs.
In early August the Cubs swept the Mets in a 4-game home series that further distanced them from the pack. An infamous Keith Moreland-Ed Lynch fight erupted after Lynch hit Moreland with a pitch, perhaps forgetting Moreland was once a linebacker at the University of Texas. It was the second game of a double header and the Cubs had won the first game in part due to a three run home run by Moreland. After the bench-clearing fight the Cubs won the second game, and the sweep put the Cubs at 68–45.
In 1984, the two leagues, American and National, each had two divisions, East and West. The divisional winners met in a best-of-5 series to advance to the World Series, in a "2–3" format, first two games were played at the home of the team who did not have home field advantage. Then the last three games were played at the home of the team, with home field advantage. Thus the first two games were played at Wrigley Field and the next three at the home of their opponents, San Diego. A common and unfounded myth is that since Wrigley Field did not have lights at that time the National League decided to give the home field advantage to the winner of the NL West. In fact, home field advantage had rotated between the winners of the East and West since 1969 when the league expanded. In even numbered years, the NL West had home field advantage. In odd numbered years, the NL East had home field advantage. Since the NL East winners had had home field advantage in 1983, the NL West winners were entitled to it.
The confusion may stem from the fact that Major League Baseball did decide that, should the Cubs make it to the World Series, the American League winner would have home field advantage unless the Cubs hosted home games at an alternate site since the Cubs home field of Wrigley Field did not yet have lights. Rumor was the Cubs could hold home games across town at Comiskey Park, home of the American League's Chicago White Sox. Rather than hold any games in the cross town rival Sox Park, the Cubs made arrangements with the August A. Busch, owner of the St. Louis Cardinals, to use Busch Stadium in St. Louis as the Cubs "home field" for the World Series. This was approved by Major League Baseball and would have enabled the Cubs to host games 1 and 2, along with games 6 and 7 if necessary. At the time home field advantage was rotated between each league. Odd numbered years the AL had home field advantage. Even numbered years the NL had home field advantage. In the 1982 World Series the St. Louis Cardinals of the NL had home field advantage. In the 1983 World Series the Baltimore Orioles of the AL had home field advantage.
In the NLCS, the Cubs easily won the first two games at Wrigley Field against the San Diego Padres. The Padres were the winners of the Western Division with Steve Garvey, Tony Gwynn, Eric Show, Goose Gossage and Alan Wiggins. With wins of 13–0 and 4–2, the Cubs needed to win only one game of the next three in San Diego to make it to the World Series. After being beaten in Game 3 7–1, the Cubs lost Game 4 when Smith, with the game tied 5–5, allowed a game-winning home run to Garvey in the bottom of the ninth inning. In Game 5 the Cubs took a 3–0 lead into the 6th inning, and a 3–2 lead into the seventh with Sutcliffe (who won the Cy Young Award that year) still on the mound. Then, Leon Durham had a sharp grounder go under his glove. This critical error helped the Padres win the game 6–3, with a 4-run 7th inning and keep Chicago out of the 1984 World Series against the Detroit Tigers. The loss ended a spectacular season for the Cubs, one that brought alive a slumbering franchise and made the Cubs relevant for a whole new generation of Cubs fans.
The Padres would be defeated in 5 games by Sparky Anderson's Tigers in the World Series. Baseball experts felt the Cubs would have better represented the National League and would have won at least two World Series games.
The 1985 season brought high hopes. The club started out well, going 35–19 through mid-June, but injuries to Sutcliffe and others in the pitching staff contributed to a 13-game losing streak that pushed the Cubs out of contention.
1989: NL East champions.
In 1989, the first full season with night baseball at Wrigley Field, Don Zimmer's Cubs were led by a core group of veterans in Ryne Sandberg, Rick Sutcliffe and Andre Dawson, who were boosted by a crop of youngsters such as Mark Grace, Shawon Dunston, Greg Maddux, Rookie of the Year Jerome Walton, and Rookie of the Year Runner-Up Dwight Smith. The Cubs won the NL East once again that season winning 93 games. This time the Cubs met the San Francisco Giants in the NLCS. After splitting the first two games at home, the Cubs headed to the Bay Area, where despite holding a lead at some point in each of the next three games, bullpen meltdowns and managerial blunders ultimately led to three straight losses. The Cubs couldn't overcome the efforts of Will Clark, whose home run off Maddux, just after a managerial visit to the mound, led Maddux to think Clark knew what pitch was coming. Afterward, Maddux would speak into his glove during any mound conversation, beginning what is a norm today. Mark Grace was 11–17 in the series with 8 RBI. Eventually, the Giants lost to the "Bash Brothers" and the Oakland A's in the famous ""Earthquake Series"."
1998: Wild card race and home run chase.
The '98 season would begin on a somber note with the death of legendary broadcaster Harry Caray. After the retirement of Sandberg and the trade of Dunston, the Cubs had holes to fill and the signing of Henry Rodriguez, known affectionately as "H-Rod" to bat cleanup provided protection for Sammy Sosa in the lineup, as Rodriguez slugged 31 round-trippers in his first season in Chicago. Kevin Tapani led the club with a career high 19 wins, Rod Beck anchored a strong bullpen and Mark Grace turned in one of his best seasons. The Cubs were swamped by media attention in 1998, and the team's two biggest headliners were Sosa and rookie flamethrower Kerry Wood. Wood's signature performance was one-hitting the Houston Astros, a game in which he tied the major league record of 20 strikeouts in nine innings. His torrid strikeout numbers earned Wood the nickname "Kid K," and ultimately earned him the 1998 NL Rookie of the Year award. Sosa caught fire in June, hitting a major league record 20 home runs in the month, and his home run race with Cardinals slugger Mark McGwire transformed the pair into international superstars in a matter of weeks. McGwire finished the season with a new major league record of 70 home runs, but Sosa's .308 average and 66 homers earned him the National League MVP Award. After a down-to-the-wire Wild Card chase with the San Francisco Giants, Chicago and San Francisco ended the regular season tied, and thus squared off in a one-game playoff at Wrigley Field in which third baseman Gary Gaetti hit the eventual game winning homer. The win propelled the Cubs into the postseason once again with a 90–73 regular season tally. Unfortunately, the went cold in October, as manager Jim Riggleman's club batted .183 and scored only four runs en route to being swept by Atlanta. On a positive note, the home run chase between Sosa, McGwire and Ken Griffey, Jr. helped professional baseball to bring in a new crop of fans as well as bringing back some fans who had been disillusioned by the 1994 strike. The Cubs retained many players who experienced career years in '98, and after a fast start in 1999, they collapsed again (starting with being swept at the hands of the cross-town White Sox in mid-June) and finished in the bottom of the division for the next two seasons.
2001: Playoff push.
Despite losing fan favorite Grace to free agency, and the lack of production from newcomer Todd Hundley, skipper Don Baylor's Cubs put together a good season in 2001. The season started with Mack Newton being brought in to preach "positive thinking." One of the biggest stories of the season transpired as the club made a midseason deal for Fred McGriff, which was drawn out for nearly a month as McGriff debated waiving his no-trade clause, as the Cubs led the wild card race by 2.5 games in early September. That run died when Preston Wilson hit a three run walk off homer off of closer Tom "Flash" Gordon, which halted the team's momentum. The team was unable to make another serious charge, and finished at 88–74, five games behind both Houston and St. Louis, who tied for first. Sosa had perhaps his finest season and Jon Lieber led the staff with a 20 win season.
2003: Five more outs.
The Cubs had high expectations in 2002, but the squad played poorly. On July 5, 2002 the Cubs promoted assistant general manager and player personnel director Jim Hendry to the General Manager position. The club responded by hiring Dusty Baker and by making some major moves in '03. Most notably, they traded with the Pittsburgh Pirates for outfielder Kenny Lofton and third baseman Aramis Ramirez, and rode dominant pitching, led by Kerry Wood and Mark Prior, as the Cubs led the division down the stretch.
Chicago halted St. Louis' run to the playoffs by taking 4 of 5 games from the Cardinals at Wrigley Field in early September, after which the hapless Cubs finally won their first division title in 14 years. They then went on to defeat the Atlanta Braves in a dramatic five-game Division Series, the franchise's first postseason series win since beating the Detroit Tigers in the 1908 World Series.
After losing an extra-inning game in Game 1, the Cubs rallied and took a 3 games to 1 lead over the Wild Card Florida Marlins in the NLCS. Florida shut the Cubs out in Game 5, but young pitcher Mark Prior led the Cubs in Game 6 as they took a 3–0 lead into the 8th inning and it was at this point when a now-infamous incident took place. Several spectators attempted to catch a foul ball off the bat of Luis Castillo. A Chicago Cubs fan by the name of Steve Bartman, of Northbrook, IL, reached for the ball and deflected it away from the glove of Moisés Alou for the second out of the 8th inning. Alou reacted angrily toward the stands, and after the game stated that he would have caught the ball. Alou at one point recanted, saying he would not have been able to make the play, but later said this was just an attempt to make Bartman feel better and believing the whole incident should be forgotten. Interference was not called on the play, as the ball was ruled to be on the spectator side of the wall. Castillo was eventually walked by Prior. Two batters later, and to the chagrin of the packed stadium, Cubs shortstop Alex Gonzalez misplayed an inning ending double play, loading the bases and leading to eight Florida runs and a Marlin victory. Despite sending Kerry Wood to the mound and holding a lead twice, the Cubs ultimately dropped Game 7, and failed to reach the World Series.
2004–2006.
In 2004, the Cubs were a consensus pick by most media outlets to win the World Series. The offseason acquisition of Derek Lee (who was acquired in a trade with Florida for Hee-seop Choi) and the return of Greg Maddux only bolstered these expectation. Despite a mid-season deal for Nomar Garciaparra, misfortune struck the Cubs again. They led the Wild Card by 1.5 games over San Francisco and Houston on September 25, and both of those teams lost that day, giving the Cubs a chance at increasing the lead to a commanding 2.5 games with only eight games remaining in the season, but reliever LaTroy Hawkins blew a save to the Mets, and the Cubs lost the game in extra innings, a defeat that seemingly deflated the team, as they proceeded to drop 6 of their last 8 games as the Astros won the Wild Card. 
Despite the fact that the Cubs had won 89 games, this fallout was decidedly unlovable, as the Cubs traded superstar Sammy Sosa after he had left the season's final game early and then lied about it publicly. Already a controversial figure in the clubhouse after his corked-bat incident, Sammy's actions alienated much of his once strong fan base as well as the few teammates still on good terms with him, (many teammates grew tired of Sosa playing loud salsa music in the locker room) and possibly tarnished his place in Cubs' lore for years to come. The disappointing season also saw fans start to become frustrated with the constant injuries to ace pitchers Mark Prior and Kerry Wood. Additionally, the '04 season led to the departure of popular commentator Steve Stone, who had become increasingly critical of management during broadcasts and was verbally attacked by reliever Kent Mercker. Things were no better in 2005, despite a career year from first baseman Derrek Lee and the emergence of closer Ryan Dempster. The club struggled and suffered more key injuries, only managing to win 79 games after being picked by many to be a serious contender for the N.L. pennant. In 2006, bottom fell out as the Cubs finished 66–96, last in the NL Central.
2007–2008: Back to back division titles.
After finishing last in the NL Central with 66 wins in 2006, the Cubs re-tooled and went from "worst to first" in 2007. In the offseason they signed Alfonso Soriano to a contract at 8 years for $136 million, and replaced manager Dusty Baker with fiery veteran manager Lou Piniella. After a rough start, which included a brawl between Michael Barrett and Carlos Zambrano, the Cubs overcame the Milwaukee Brewers, who had led the division for most of the season, with winning streaks in June and July, coupled with a pair of dramatic, late-inning wins against the Reds, and ultimately clinched the NL Central with a record of 85–77. The Cubs traded Barrett to the Padres, and later acquired Jason Kendall from Oakland. Kendall was highly successful with his management of the pitching rotation and helped at the plate as well. By September, Geovany Soto became the full-time starter behind the plate, replacing the veteran Kendall. They met Arizona in the NLDS, but controversy followed as Piniella, in a move that has since come under scrutiny, pulled Carlos Zambrano after the sixth inning of a pitcher's duel with D-Backs ace Brandon Webb, to "...save Zambrano for (a potential) Game 4." The Cubs, however, were unable to come through, losing the first game and eventually stranding over 30 baserunners in a 3-game Arizona sweep.
The Tribune company, in financial distress, was acquired by real-estate mogul Sam Zell in December 2007. This acquisition included the Cubs. However, Zell did not take an active part in running the baseball franchise, instead concentrating on putting together a deal to sell it.
The Cubs successfully defended their National League Central title in 2008, going to the postseason in consecutive years for the first time since 1906–08. The offseason was dominated by three months of unsuccessful trade talks with the Orioles involving 2B Brian Roberts, as well as the signing of Chunichi Dragons star Kosuke Fukudome. The team recorded their 10,000th win in April, while establishing an early division lead. Reed Johnson and Jim Edmonds were added early on and Rich Harden was acquired from the Oakland Athletics in early July. The Cubs headed into the All-Star break with the N.L.'s best record, and tied the league record with eight representatives to the All-Star game, including catcher Geovany Soto, who was named Rookie of the Year. The Cubs took control of the division by sweeping a four-game series in Milwaukee. On September 14, in a game moved to Miller Park due to Hurricane Ike, Zambrano pitched a no-hitter against the Astros, and six days later the team clinched by beating St. Louis at Wrigley. The club ended the season with a 97–64 record and met Los Angeles in the NLDS. The heavily favored Cubs took an early lead in Game 1, but James Loney's grand slam off Ryan Dempster changed the series' momentum. Chicago committed numerous critical errors and were outscored 20–6 in a Dodger sweep, which provided yet another sudden ending.
The Ricketts era (2009–present).
The Ricketts family acquired a majority interest in the Cubs in 2009, ending the Tribune years. Apparently handcuffed by the Tribune's bankruptcy and the sale of the club to the Ricketts family, the Cubs' quest for a NL Central 3-peat started with notice that there would be less invested into contracts than in previous years. Chicago engaged St. Louis in a see-saw battle for first place into August 2009, but the Cardinals played to a torrid 20–6 pace that month, designating their rivals to battle in the Wild Card race, from which they were eliminated in the season's final week. The Cubs were plagued by injuries in 2009, and were only able to field their Opening Day starting lineup three times the entire season. Third baseman Aramis Ramírez injured his throwing shoulder in an early May game against the Milwaukee Brewers, sidelining him until early July and forcing journeyman players like Mike Fontenot and Aaron Miles into more prominent roles. Additionally, key players like Derrek Lee (who still managed to hit .306 with 35 HR and 111 RBI that season), Alfonso Soriano and Geovany Soto also nursed nagging injuries. The Cubs posted a winning record (83–78) for the third consecutive season, the first time the club had done so since 1972, and a new era of ownership under the Ricketts' family was approved by MLB owners in early October.
The decline and rebuild.
Rookie Starlin Castro debuted in early May as the starting shortstop. However, the club played poorly in the early season, finding themselves 10 games under .500 at the end of June. In addition, long-time ace Carlos Zambrano was pulled from a game against the White Sox on June 25 after a tirade and shoving match with Derrek Lee, and was suspended indefinitely by Jim Hendry, who called the conduct "unacceptable." On August 22, Lou Piniella, who had already announced his retirement at the end of the season, announced that he would leave the Cubs prematurely to take care of his sick mother. Mike Quade took over as the interim manager for the final 37 games of the year. Despite being well out of playoff contention the Cubs went 24–13 under Quade, the best record in baseball during that 37 game stretch, earning Quade to have the interim tag removed on October 19.
On December 3, Cubs broadcaster and former third baseman, Ron Santo, died due to complications from bladder cancer and diabetes. He spent 13 seasons as a player with the Cubs, and at the time of his death was regarded as one of the greatest players not in the Hall of Fame. He has since been elected to the Major League Baseball Hall of Fame.
Despite trading for pitcher Matt Garza and signing free-agent slugger Carlos Pena, the Cubs finished the 2011 season 20 games under .500 with a record of 71-91. Weeks after the season came to an end, the club was rejuvenated in the form of a new philosophy, as owner Tom Ricketts signed Theo Epstein away from the Boston Red Sox, naming him club President and giving him a five-year contract worth over $18M, and subsequently discharged the GM Jim Hendry manager Mike Quade. Epstein, a proponent of sabremetrics and one of the architects of two world series titles in Boston brought along Jed Hoyer to fill the role of GM and hired Dale Sveum as manager. Although the team had a dismal 2012 season, losing 101 games (the worst record since 1966) it was largely expected. The youth movement ushered in by Epstein and Hoyer began as longtime fan favorite Kerry Wood retired in May, followed by Ryan Dempster and Geovany Soto being traded to Texas at the All-Star break for a group of minor league prospects headlined by Christian Villanueva. The development of Castro, Anthony Rizzo, Darwin Barney, Brett Jackson and pitcher Jeff Samardzija as well as the replenishing of the minor-league system with prospects such as Javier Baez, Albert Almora, and Jorge Soler became the primary focus of the season, a philosophy which the new management said would carry over at least through the 2013 season.
The 2013 season resulted in much as the same the year before. Shortly before the trade deadline, the Cubs traded Matt Garza to the Texas Rangers for Mike Olt, C. J. Edwards, Neil Ramirez, and Justin Grimm. Three days later, the Cubs sent Alfonso Soriano to the New York Yankees for minor leaguer Corey Black. The mid season fire sale led to another last place finish in the NL Central, finishing with a record of 66-96. Although there was a five-game improvement in the record from the year before, Anthony Rizzo and Starlin Castro seemed to take steps backward in their development. On September 30, 2013, Theo Epstein made the decision to fire manager Dale Sveum after just two seasons at the helm of the Cubs. The regression of several young players was thought to be the main focus point, as the front office said Dale would not be judged based on wins and losses. In two seasons as skipper, Sveum finished with a record of 127-197.
On November 7, 2013, the Cubs hired San Diego Padres bench coach Rick Renteria to be the 53rd manager in team history. The Cubs relieved Renteria of his managerial duties on October 31, 2014. Epstein announced the Cubs are negotiating a deal with Joe Maddon. On November 2, 2014, the Cubs officially announced that Maddon had signed a five-year contract to be the 54th manager in team history. On December 10, 2014, newly hired manager Joe Maddon announced that the team had signed free agent Jon Lester to a 6-year, $155 million contract. As well, Rising stars such as Kris Bryant, Addison Russell, and Javier Baez show that the Cubs will be a great team in the future. Kris Bryant was called up to the Cubs MLB team on April 17 and Addison Russell played his first Major League game for the Cubs on April 22 making him the youngest player in the National League.
Memorable events and records.
Merkle's Boner.
On September 23, 1908, the Cubs and New York Giants were involved in a tight pennant race. The two clubs were tied in the bottom of the ninth inning at the Polo Grounds, and N.Y. had runners on first and third and two outs when Al Bridwell singled, scoring Moose McCormick from third with the Giants' apparent winning run, but the runner on first base, rookie Fred Merkle, left the field without touching second base. As fans swarmed the field, Cub infielder Johnny Evers retrieved the ball and touched second. Since there were two outs, a forceout was called at second base, ending the inning and the game. Because of the tie the Giants and Cubs ended up tied for first place. The Giants lost the ensuing one-game playoff and the Cubs went on to the World Series.
Babe Ruth's called shot.
On October 1, 1932, in game three of the World Series between the Cubs and the New York Yankees, Babe Ruth allegedly stepped to the plate, pointed his finger to Wrigley Field's center field bleachers and hit a long home run to center. There is speculation as to whether the "facts" surrounding the story are true or not, but nevertheless Ruth did help the Yankees secure a World Series win that year and the home run accounted for his 15th and last home run in the post season before he retired in 1935.
Riot at Wrigley.
Slugger Hack Wilson had a combative streak and frequently initiated fights with opposing players and fans. On June 22, 1928, a riot broke out in the ninth inning at Wrigley Field against the St. Louis Cardinals when Wilson jumped into the box seats to attack a heckling fan. An estimated 5,000 spectators swarmed the field before police could separate the combatants and restore order. The fan sued Wilson for $20,000, but a jury ruled in Wilson's favor.
RBI record.
Hack Wilson set a record of 56 home-runs and 190 runs-batted-in in 1930, breaking Lou Gehrig's MLB record of 176 RBI. (In 1999, a long-lost extra RBI mistakenly credited to Charlie Grimm had been found by Cooperstown researcher Cliff Kachline and verified by historian Jerome Holtzman, increasing the record number to 191.) As of 2013 the record still stands, with no serious threats coming since Gehrig (184) and Hank Greenberg (183) in the same era. The closest anyone has come to the mark in the last 75 years was Manny Ramirez's 165 RBI in 1999. In addition to the RBI record, Wilson 56 home-runs stood as the National League record until 1998, when Sammy Sosa and Mark McGwire hit 66 and 70, respectively. Wilson was named "Most Useful" player that year by the Baseball Writers Association of America, as the official N.L. Most Valuable Player Award was not awarded until the next season.
The Homer in the Gloamin'.
On September 28, 1938, with the Cubs and Pirates tied at 5, Gabby Hartnett stepped to the plate in a lightless Wrigley Field that was gradually being overcome by darkness and visibility was becoming difficult. With two outs in the bottom of the ninth and the umpires ready to end the game, Hartnett launched Pirate hurler Mace Brown's offering into the gloom and haze. This would be remembered as his "Homer in the Gloamin." It was ranked by ESPN as the 47th greatest home run of all time.
Rick Monday and the U.S. flag.
On April 25, 1976, at Dodger Stadium, father-and-son protestors ran into the outfield and tried to set fire to a U.S. flag. When Cubs outfielder Rick Monday noticed the flag on the ground and the man and boy fumbling with matches and lighter fluid, he dashed over and snatched the flag to thunderous applause. When he came up to bat in the next half-inning, he got a standing ovation from the crowd and the stadium titantron flashed the message, "RICK MONDAY... YOU MADE A GREAT PLAY..." Monday later said, "If you're going to burn the flag, don't do it around me. I've been to too many veterans' hospitals and seen too many broken bodies of guys who tried to protect it."
The Sandberg game.
On June 23, 1984, Chicago trailed St. Louis 9–8 in the bottom of the ninth on NBC's Game of the Week when Ryne Sandberg, known mostly for his glove, slugged a game-tying home run off ace closer Bruce Sutter. Despite this, the Cardinals scored two runs in the top of the tenth. Sandberg came up again facing Sutter with one man on base, and hit yet another game tying home run, and "Ryno" became a household name. The Cubs won what has become known as "The Sandberg Game" in the 11th inning.
Most home-runs in a month.
In June, 1998 Sammy Sosa exploded into the pursuit of Roger Maris' home run record. Sosa had 13 home runs entering the month, representing less than half of Mark McGwire's total. Sosa had his first of four multi-home run games that month on June 1, and went on to break Rudy York's record with 20 home runs in the month, a record that still stands. By the end of his historic month, the outfielder's 33 home runs tied him with Ken Griffey, Jr. and left him only four behind McGwire's 37. Sosa finished with 66 and won the NL MVP Award.
10,000th win.
On April 23, 2008, against the Colorado Rockies, the Cubs recorded the 10,000th regular-season win in their franchise's history dating back to the beginning of the National League in 1876. The Cubs reached the milestone with an overall National League record of 10,000 wins and 9,465 losses. Chicago was only the second club in Major League Baseball history to attain this milestone, the first having been the San Francisco Giants in mid-season 2005. The Cubs, however, hold the mark for victories for a team in a single city. The Chicago club's 77–77 record in the National Association (1871, 1874–1875) is not included in MLB record keeping. Post-season series are also not included in the totals. To honor the milestone, the Cubs flew an extra white flag displaying "10,000" in blue, along with the customary "W" flag.
Wood's 20K game.
In only his third career start, Kerry Wood struck out 20 batters against Houston on May 6, 1998. This is the franchise record and tied for the Major League record. The game is often considered the most dominant pitching performance of all time. Wood hit one batter, Craig Biggio, and allowed one hit, a scratch single by Ricky Gutiérrez off third baseman Kevin Orie's glove. The play was nearly scored an error, which would have given Wood a no-hitter.
Championship drought.
The Chicago Cubs have not won a World Series championship since 1908, and have not appeared in the Fall Classic since 1945, although between their postseason appearance in 1984 and their most recent in 2008, they have made the postseason six times. It is the longest championship drought in all four of the major North American professional sports leagues, which includes the NFL, NBA, NHL, as well as Major League Baseball. In fact, the Cubs' last World Series title occurred before those other three leagues even existed, and even the Cubs' last World Series appearance predates the founding of the NBA. The much publicized drought was concurrent to championship droughts by the Boston Red Sox and the Chicago White Sox, who both had over 80 years between championships. It is this unfortunate distinction that has led to the club often being known as "The Lovable Losers." The team was one win away from breaking what is often called "The Curse of the Billy Goat" in 1984 and 2003, but was unable get the victory that would send it to the World Series.
Tape-measure home runs.
 On May 11, 2000, Glenallen Hill, facing Brewers starter Steve Woodard, became the first, and thus far only player, to hit a pitched ball onto the roof of a five-story residential building across Waveland Ave, beyond Wrigley Field's left field wall. The shot was estimated at well over 500 ft, but the Cubs fell to Milwaukee 12–8. No batted ball has ever hit the center field scoreboard, although the original "Slammin' Sammy", golfer Sam Snead, hit it with a golf ball in an exhibition in the 1950s. In 1948, Bill Nicholson barely missed the scoreboard when he launched a home run ball onto Sheffield Avenue and in 1959, Roberto Clemente came even closer with a home run ball hit onto Waveland Avenue. In 2001, a Sammy Sosa shot landed across Waveland and bounced a block down Kenmore Avenue. Dave Kingman hit a shot in 1979 that hit the third porch roof on the east side of Kenmore, estimated at 555 ft, and is regarded as the longest home run in Wrigley Field history.
Individual awards.
Retired numbers.
The Chicago Cubs retired numbers are commemorated on pinstriped flags flying from the foul poles at Wrigley Field, with the exception of Jackie Robinson, the Brooklyn Dodgers player whose number 42 was retired for all clubs. The first retired number flag, Ernie Banks' number 14, was raised on the left field pole, and they have alternated since then. 14, 10 and 31 (Jenkins) fly on the left field pole; and 26, 23 and 31 (Maddux) fly on the right field pole.
Minor league affiliations.
Before signing a developmental agreement with the Kane County Cougars in 2012, the Cubs had a Class A minor league affiliation on two occasions with the Peoria Chiefs (1985-1995 and 2004-2012). Ryne Sandberg managed the Chiefs from 2006-2010. In the period between those associations with the Chiefs the club had affiliations with the Dayton Dragons and Lansing Lugnuts. The Lugnuts were often affectionately referred to by Chip Caray as "Steve Stone's favorite team." The 2007 developmental contract with the Tennessee Smokies was preceded by Double A affiliations with the Orlando Cubs and West Tenn Diamond Jaxx. On September 16, 2014 the Cubs announced a move of their top Class A affiliate from Daytona in the Florida State League to Myrtle Beach in the Carolina League for the 2015 season. Two days later, on the 18th, the Cubs signed a 4-year player development contract with the South Bend Silver Hawks of the Midwest League, ending their brief relationship with the Kane County Cougars and shortly thereafter renaming the Silver Hawks the South Bend Cubs.
Spring training history.
The Cubs spring training facility is located in Mesa, Arizona, where they play in the Cactus League. The club plays its games at Sloan Park The park seats 15,000, making it Major League Baseball's largest spring training ballpark by capacity, and the Cubs annually sell out most of their games both at home and on the road. Before Sloan Park opened in 2014, the team played games at HoHoKam Park (1979–2013), Dwight Patterson Field. "HoHoKam" is literally translated from Native American as "those who vanished." The Northsiders have called Mesa their spring home for most seasons since 1952.
In addition to Mesa, the club has held spring training in New Orleans (1870, 1907, 1911–1912); Champaign, Illinois (1901–02, 1906); Los Angeles (1903–04, 1948–1949), Santa Monica, California (1905); French Lick, Indiana (1908, 1943-1945); Hot Springs, Arkansas (1909–1910); Tampa, Florida (1913–1916); Pasadena, California (1917–1921); Santa Catalina Island, California (1922–1942, 1946–1947, 1950–1951); Rendezvous Park in Mesa (1952–1965); Blair Field in Long Beach, California (1966); and Scottsdale, Arizona (1967–1978).
The curious location on Catalina Island stemmed from Cubs owner William Wrigley Jr.'s then-majority interest in the island in 1919. Wrigley constructed a ballpark on the island to house the Cubs in spring training: it was built to the same dimensions as Wrigley Field. (The ballpark is long gone, but a clubhouse built by Wrigley to house the Cubs exists as the Catalina County Club.) However by 1951 the team chose to leave Catalina Island and spring training was shifted to Mesa, Arizona. The Cubs' 30-year association with Catalina is chronicled in the book, "The Cubs on Catalina," by Jim Vitti . . . which was named International 'Book of the Year' by "The Sporting News".
The former location in Mesa is actually the second HoHoKam Park; the first was built in 1976 as the spring-training home of the Oakland Athletics who left the park in 1979. Apart from HoHoKam Park and Sloan Park the Cubs also have another Mesa training facility called Fitch Park, this complex provides 25000 sqft of team facilities, including major league clubhouse, four practice fields, one practice infield, enclosed batting tunnels, batting cages, a maintenance facility, and administrative offices for the Cubs.
Media.
Radio.
The Cubs' flagship radio station is WBBM (AM) 780. The Chicago Cubs Radio Network consists of 45 stations and covers at least eleven states. The Cubs signed a deal with CBS Radio WBBM 780 in Chicago on June 4, 2014, ending a 90-year partnership with WGN (AM) 720. The Chicago Bears also make their radio home on WBBM 780. If both teams should be playing at conflicting times, the Cubs will be heard on WCFS (FM) 105.9, which is usually a simulcast of WBBM 780.
Pat Hughes has been the Cubs' radio play-by-play voice since 1996. Former Cubs third baseman and fan favorite Ron Santo was Hughes' partner in the booth until his death in 2010. Keith Moreland replaced Hall of Fame inductee Santo for three seasons, followed by Ron Coomer in 2014.
Print.
The club also produces its own print media; the Cubs' official magazine , which has 12 annual issues, is in its third decade, and spotlights players and events involving the club. The club also publishes a traditional media guide.
Television.
As of the 2015 season, Cubs games will air locally on the following outlets:
All of the team's current television contracts end in 2019. The "Chicago Tribune" reported that following the end of these contracts, the team may consider launching its own regional sports network.
Len Kasper has been the Cubs' television play-by-play announcer since 2005 and was joined by Jim Deshaies in 2013. Bob Brenly (analyst, 2005–12), Chip Caray (play-by-play, 1998-2004), Steve Stone (analyst, 1983-2000, 2003–04), Joe Carter (analyst for WGN-TV games, 2001–02) and Dave Otto (analyst for FSN Chicago games, 2001–02) also have spent time broadcasting from the Cubs booth since the death of Harry Caray in 1998.
Jack Brickhouse and Harry Caray.
Jack Brickhouse manned the Cubs radio and especially the TV booth for parts of five decades, the 34-season span from 1948 to 1981. He covered the games with a level of enthusiasm that often seemed unjustified by the team's poor performance on the field for many of those years. His trademark call "Hey Hey!" always followed a home run. That expression is spelled out in large letters vertically on both foul pole screens at Wrigley Field. "Whoo-boy!" and "Wheeee!" and "Oh, brother!" were among his other pet expressions. When he approached retirement age, he personally recommended his successor.
Harry Caray's stamp on the team is perhaps even deeper than that of Brickhouse, although his 17-year tenure, from 1982 to 1997, was half as long. First, Caray had already become a well-known Chicago figure by broadcasting White Sox games for a decade, after having been a St Louis Cardinals icon for 25 years. Caray also had the benefit of being in the booth during the NL East title run in 1984, which was widely seen due to WGN's status as a cable-TV superstation. His trademark call of "Holy Cow!" and his enthusiastic singing of "Take me out to the ballgame" during the 7th inning stretch (as he had done with the White Sox) made Caray a fan favorite both locally and nationally.
Caray had lively discussions with commentator Steve Stone, who was hand-picked by Harry himself, and producer Arne Harris. Caray often playfully quarreled with Stone over Stone's cigar and why Stone was single, while Stone would counter with poking fun at Harry being "under the influence." Stone disclosed in his book "Where's Harry" that most of this "arguing" was staged, and usually a ploy developed by Harry himself to add flavor to the broadcast. The Cubs still have a "guest conductor," usually a celebrity, lead the crowd in singing "Take me out to the ballgame" during the 7th inning stretch to honor Caray's memory.
Ownership.
Ownership history.
Al Spalding, who also owned Spalding sporting goods, played for the team for two seasons under club founder William Hulbert. After Hulbert's death Spalding owned the club for twenty one years, after which the Cubs were purchased by Albert Lasker and Charles Weeghman. That pair were followed by the Wrigley family, owners of Wrigley's chewing gum. In 1981, after 6 decades under the Wrigley family, the Cubs were purchased by Tribune Company for $20,500,000. Tribune, which also owned the "Chicago Tribune", "Los Angeles Times", "WGN Television", "WGN Radio" and many other media outlets, controlled the club until December 2007, when Sam Zell completed his purchase of the entire Tribune organization and announced his intention to sell the baseball team. After a nearly two-year process which involved potential buyers such as Mark Cuban and a group led by Hank Aaron, a family trust of TD Ameritrade Joe Ricketts won the bidding process as the 2009 season came to a close. Ultimately, the sale was unanimously approved by MLB owners and the Ricketts family took control on October 27, 2009.
Other information.
Tinker to Evers to Chance.
"Baseball's Sad Lexicon," also known as "Tinker to Evers to Chance" after its refrain, is a 1910 baseball poem by Franklin Pierce Adams. The poem is presented as a single, rueful stanza from the point of view of a New York Giants fan seeing the talented Chicago Cubs infield of shortstop Joe Tinker, second baseman Johnny Evers, and first baseman Frank Chance complete a double play. The trio began playing together with the Cubs in 1902, and formed a double play combination that lasted through April 1912. The Cubs won the pennant four times between 1906 and 1910, often defeating the Giants en route to the World Series.
The poem was first published in the "New York Evening Mail" on July 12, 1912. Popular among sportswriters, numerous additional verses were written. The poem gave Tinker, Evers, and Chance increased popularity and has been credited with their elections to the National Baseball Hall of Fame in 1946.
"White flag time at Wrigley!".
The term "White flag time at Wrigley!", coined by former play-by-play broadcaster Chip Caray, means the Cubs have won.
Beginning in the days of P.K. Wrigley and the 1937 bleacher/scoreboard reconstruction, and prior to modern media saturation, a flag with either a "W" or an "L" has flown from atop the scoreboard masthead, indicating the day's result(s) when baseball was played at Wrigley. In case of a doubleheader that results in a split, both the "win" and "loss" flags are flown.
Past Cubs media guides show that originally the flags were blue with a white "W" and white with a blue "L". In 1978, consistent with the dominant colors of the flags, blue and white lights were mounted atop the scoreboard, denoting "win" and "loss" respectively for the benefit of nighttime passers-by.
The flags were replaced by 1990, the first year in which the Cubs media guide reports the switch to the now familiar colors of the flags: White with blue "W" and blue with white "L". In addition to needing to replace the worn-out flags, by then the retired numbers of Banks and Williams were flying on the foul poles, as white with blue numbers; so the "good" flag was switched to match that scheme.
This long-established tradition has evolved to fans carrying the white-with-blue-W flags to both home and away games, and displaying them after a Cub win. The flags have become more and more popular each season since 1998, and are now even sold as T-shirts with the same layout. In 2009, the tradition spilled over to the NHL as Chicago Blackhawks fans adopted a red and black "W" flag of their own.
Mascots.
The official Cub mascot is a young bear cub, named Clark, described by the team's press release as a young and friendly Cub. Clark made his debut at Advocate Health Care on January 13, 2014, the same day as the press release announcing his installation as the club's first ever official physical mascot. The bear cub itself was used in the clubs since the early 1900s and was the inspiration of the Chicago Staleys changing their team's name to the Chicago Bears, due to the Cubs allowing the football team to play at Wrigley Field in the 1930s.
The Cubs had no official physical mascot prior to Clark, though a man in a 'polar bear' looking outfit, called "The Bear-man" (or Beeman), which was mildly popular with the fans, paraded the stands briefly in the early 1990s. There is no record of whether or not he was just a fan in a costume or employed by the club. Through the 2013 season, there were "Cubbie-bear" mascots outside of Wrigley on game day, but none are employed by the team. They pose for pictures with fans for tips. The most notable of these was "Billy Cub" who worked outside of the stadium until for over 6 years until July 2013, when the club asked him to stop. Billy Cub, who is played by fan John Paul Weier, had unsuccessfully petitioned the team to become the official mascot.
Another unofficial but much more well-known mascot is Ronnie "Woo Woo" Wickers who is a longtime fan and local celebrity in the Chicago area. He is known to Wrigley Field visitors for his idiosyncratic cheers at baseball games, generally punctuated with an exclamatory "Woo!" (e.g., "Cubs, woo! Cubs, woo! Big-Z, woo! Zambrano, woo! Cubs, woo!") Longtime Cubs announcer Harry Caray dubbed Wickers "Leather Lungs" for his ability to shout for hours at a time. He is not employed by the team, although the club has on two separate occasions allowed him into the broadcast booth and allow him some degree of freedom once he purchases or is given a ticket by fans to get into the games. He is largely allowed to roam the park and interact with fans by Wrigley Field security, although there have been numerous minor occurrences where Wickers has had confrontations with fans who do not approve of his antics, one in which Wickers was assaulted with a hula hoop by a fan named Greg Hoden, known as "The Stinger." Hoden was the belligerent sidekick popular heckler "Derek the Five Dollar Kid" in the late 1990s and early 2000s.
Wrigley Field and Wrigleyville.
The Cubs have played their home games at Wrigley Field, also known as "The Friendly Confines" since 1916. It was built in 1914 as Weeghman Park for the Chicago Whales, a Federal League baseball team. The Cubs also shared the park with the Chicago Bears of the NFL for 50 years. The ballpark includes a manual scoreboard, ivy-covered brick walls, and relatively small dimensions.
Located in Chicago's Lake View neighborhood, Wrigley Field sits on an irregular block bounded by Clark and Addison Streets and Waveland and Sheffield Avenues. The area surrounding the ballpark is typically referred to as Wrigleyville. There is a dense collection of sports bars and restaurants in the area, most with baseball inspired themes, including Sluggers, Murphy's Bleachers and The Cubby Bear. Many of the apartment buildings surrounding Wrigley Field on Waveland and Sheffield Avenues have built bleachers on their rooftops for fans to view games and other sell space for advertisement. One building on Sheffield Avenue has a sign atop its roof which says "Eamus Catuli!" which is Latin for "Let's Go Cubs!" and another chronicles the time since the last Division title, pennant, and World Series championship, as shown in the picture to the left. The 02 denotes two years since the 2008 NL Central title, 65 years since the 1945 pennant and 102 years since the 1908 World Series championship. On game days, many residents rent out their yards and driveways to people looking for parking spots. The uniqueness of the neighborhood itself has ingrained itself into the culture of the Chicago Cubs as well as the Wrigleyville neighborhood, and has led to being used for concerts and other sporting events, such as the 2010 NHL Winter Classic between the Chicago Blackhawks and Detroit Red Wings, as well as a 2010 NCAA men's football game between the Northwestern Wildcats and Illinois Fighting Illini.
In 2013, Tom Ricketts and team president Crane Kenney unveiled plans for a $300M renovation to Wrigley Field to Chicago mayor Rahm Emanuel. The proposed plans include a 6000 square foot jumbotron in left center field, and a return of some of the parks features from the 1930s, including recreating the green terra-cotta canopies and wrought-iron fencing that were part of Wrigley in that era. Previously, mostly all efforts to conduct large-scale renovations the field to former mayor Richard M. Daley (a staunch White Sox fan) had been opposed by the city and rooftop owners.
Bleacher Bums.
The "Bleacher Bums" is a name given to fans, many of whom spend much of the day heckling, who sit in the bleacher section at Wrigley Field. Initially, the group was called "bums" because it referred to a group of fans who were at most games, and since those games were all day games, it was assumed they did not work. Many of those fans were, and are still, students at Chicago area colleges, such as DePaul University, Loyola, Northwestern University, and Illinois-Chicago. A Broadway play, starring Joe Mantegna, Dennis Farina, Dennis Franz, and James Belushi ran for years and was based on a group of Cub fans who frequented the club's games. The group was started in 1967 by dedicated fans Ron Grousl, Tom Nall and "mad bugler" Mike Murphy, who was a sports radio host during mid days on Chicago-based WSCR AM 670 "The Score". Murphy alleges that Grousl started the Wrigley tradition of throwing back opposing teams' home run balls. The current group is headed by Derek Schaul (Derek the Five Dollar Kid). Prior to the 2006 season, they were updated, with new shops and private bar (The Batter's Eye) being added, and Bud Light bought naming rights to the bleacher section, dubbing them the Bud Light Bleachers. Bleachers at Wrigley are general admission, except during the playoffs. The bleachers have been referred to as the "World's Largest Beer Garden." A popular T-shirt (sold inside the park and licensed by the club) which says "Wrigley Bleachers" on the front and the phrase "Shut Up and Drink Your Beer" on the reverse fuels this stereotype.
Emil Verban Society.
In 1975, a group of Chicago Cubs fans based in Washington, D.C. formed the Emil Verban Society. The society is a select club of high profile Cub fans, currently headed by Illinois Senator Dick Durbin which is named for Emil Verban, who in three seasons with the Cubs in the 1940s batted .280 with 39 runs batted in and one home run. Verban was picked as the epitome of a Cub player, explains columnist George Will, because "He exemplified mediocrity under pressure, he was competent but obscure and typifying of the work ethics." Verban initially believed he was being ridiculed, but his ill feeling disappeared several years later when he was flown to Washington to meet President Ronald Reagan, also a society member, at the White House. Hillary Clinton, Jim Belushi, Joe Mantegna, Rahm Emanuel, Dick Cheney and many others have been included among its membership.
Music.
During the summer of 1969, a Chicago studio group produced a single record called "Hey Hey! Holy Mackerel! (The Cubs Song)" whose title and lyrics incorporated the catch-phrases of the respective TV and radio announcers for the Cubs, Jack Brickhouse and Vince Lloyd. Several members of the Cubs recorded an album called "Cub Power" which contained a cover of the song. The song received a good deal of local airplay that summer, associating it very strongly with that bittersweet season. It was played much less frequently thereafter, although it remained an unofficial Cubs theme song for some years after.
For many years, Cubs radio broadcasts started with "It's a Beautiful Day for a Ball Game" by the Harry Simeone Chorale. In 1979, Roger Bain released a 45 rpm record of his song "Thanks Mr. Banks," to honor "Mr. Cub" Ernie Banks.
The song "Go, Cubs, Go!" by Steve Goodman was recorded early in the 1984 season, and was heard frequently during that season. Goodman died in September of that year, four days before the Cubs clinched the National League Eastern Division title, their first title in 39 years. Since 1984, the song started being played from time to time at Wrigley Field; since 2007, the song has been played over the loudspeakers following each Cubs home victory.
The Mountain Goats recorded a song entitled "Cubs in Five" on its 1995 EP Nine Black Poppies which refers to the seeming impossibility of the Cubs winning a World Series in both its title and Chorus.
In 2007, Pearl Jam frontman Eddie Vedder composed a song dedicated to the team called "All the Way". Vedder, a Chicago native, and lifelong Cubs fan, composed the song at the request of Ernie Banks.
Pearl Jam has only played this song live one time, on August 2, 2007 at the Vic Theater in Chicago, IL. Eddie Vedder has played this song live twice, at his solo shows at the Chicago Auditorium on August 21 and 22, 2008.
An album entitled "Take Me Out to a Cubs Game" was released in 2008. It is a collection of 17 songs and other recordings related to the team, including Harry Caray's final performance of "Take Me Out to the Ball Game" on September 21, 1997, the Steve Goodman song mentioned above, and a newly recorded rendition of "Talkin' Baseball" (subtitled "Baseball and the Cubs") by Terry Cashman. The album was produced in celebration of the 100th anniversary of the Cubs' 1908 World Series victory and contains sounds and songs of the Cubs and Wrigley Field.
External links.
class="wikitable succession-box collapsible autocollapse" style="margin: 0 auto 0 auto; font-size:95%;clear:both;"
Achievements

</doc>
<doc id="6655" url="http://en.wikipedia.org/wiki?curid=6655" title="Coldcut">
Coldcut

Coldcut are an English electronic music duo composed of Matt Black and Jonathan More. Credited as pioneers for pop sampling in the ‘80s, Coldcut are also considered the first stars of UK electronic dance music due to their innovative style, which featured cut-up samples of hip-hop, breaks, jazz, spoken word and various other types of music, as well as video and multimedia. According to "Spin", "in ’87 Coldcut pioneered the British fad for ‘DJ records’".
Coldcut’s records first introduced the public to pop artists Yazz and Lisa Stansfield, through which these artists achieved pop chart success. In addition, Coldcut has remixed and created productions on tracks by the likes of Eric B & Rakim, Yazz, James Brown, Queen Latifah, Eurythmics, INXS, Steve Reich, Blondie, The Fall, Pierre Henri, Nina Simone, Fog, Red Snapper, and BBC Radiophonic Workshop.
Beyond their work as a production duo, Coldcut are the founders of Ninja Tune, an independent record label in London, England (with a satellite office in Montreal) with an overall emphasis on encouraging interactive technology and finding innovative uses of software. The label’s first releases (the first four volumes of DJ Food - 'Jazz Brakes') were produced by Coldcut in the early 90s, and composed of instrumental hip-hop cuts that led the duo to help pioneer the trip-hop genre, with artists such as Funky Porcini, The Herbaliser and DJ Vadim.
History.
1980s.
In 1986, computer programmer Matt Black and ex-art teacher Jonathan More were part-time DJs on the rare groove scene. More also DJed on pirate radio, hosting the "Meltdown Show" on Kiss FM and worked at the Reckless Records store on Berwick Street, London where Black visited as a customer. The first collaboration between the two artists was 'Say Kids What Time Is It?' on a white label in January 1987, which mixed Jungle Book's "King of the Swingers" with the break from James Brown's "Funky Drummer." The innovation of "Say Kids..." caused More and Black to be heralded by SPIN as "the first Brit artists to really get hip-hop’s class-cutup aesthetic". It’s regarded as the UK’s first breaks record, the first UK record to be built entirely of samples and "the final link in the chain connecting European collage-experiment with the dance-remix-scratch edit". This was later sampled in "Pump Up the Volume" by MARRS, a single that reached #1 in the UK in October 1987.
Though Black had joined Kiss FM with his own mix-based show, the pair eventually joined forces on its own show later in 1987 called Solid Steel. The eclectic show became a unifying force in underground experimental electronic music and is still running to date, celebrating 25 years in 2013.
The duo adopted the name Coldcut and set up a record label called Ahead Of Our Time to release the single Beats + Pieces (one of the formats also included "That Greedy Beat") in 1987. All of these tracks were assembled using cassette pause button edits and later spliced tape edits that would sometimes run "all over the room". The duo used sampling from Led Zeppelin to James Brown. Electronic act The Chemical Brothers have described ‘Beats + Pieces’ as the ‘first bigbeat record’, a style which appeared in the mid-90s.
Coldcut's first mainstream success came when Julian Palmer from Island Records asked them to remix Eric B. & Rakim's "Paid in Full". Released in October 1987, the landmark remix is said to have "laid the groundwork for hip hop’s entry into the UK mainstream", becoming a breakthrough hit for Eric B & Rakim outside the U.S., reaching #15 in the UK and the top 20 in a number of European countries. It featured a prominent Ofra Haza sample and many other vocal cut ups as well as a looped rhythm which later, when speeded up, proved popular in the Breakbeat genre. Off the back of its success in clubs, the Coldcut "Seven Minutes of Madness" remix ended up being promoted as the single in the UK.
In 1988, More and Black formed Hex, a self-titled "multimedia pop group," with Mile Visman and Rob Pepperell. While working on videos for artists such as Kevin Saunderson, Queen Latifah and Spiritualized, Hex’s collaborative work went on to incorporate 3D modelling, punk video art, and algorithmic visuals on desktop machines. The video for Coldcut’s ‘Christmas Break’ in 1989 is arguably one of the first pop promos produced entirely on microcomputers.
In 1988, Coldcut released ‘Out To Lunch With Ahead Of Our Time,’ a double LP of Coldcut productions and re-cuts, and the various aliases under which the duo had recorded. This continued the duo’s tradition of releasing limited available vinyl.
The next Coldcut single, released in February 1988, moved towards a more house-influenced style. "Doctorin' the House", which debuted singer Yazz, became a top ten hit, and peaked at #6. In the same year, under the guise Yazz and the Plastic Population, they produced "The Only Way Is Up", a cover of a Northern Soul song. The record reached #1 in the UK in August, and remained there for five weeks, becoming 1988’s second biggest selling single. Producer Youth of Killing Joke also helped Coldcut with this record. The duo had another top hit in September with "Stop This Crazy Thing", which featured reggae vocalist Junior Reid and reached number 21 in the UK.
The single "People Hold On" became another UK Top 20 hit. Released in March 1989, it helped launch the career of the then relatively unknown singer Lisa Stansfield. Coldcut and Mark Saunders produced her debut solo single "This Is the Right Time", which became another UK Top 20 hit in August as well as reaching #21 on the U.S. "Billboard" Hot 100 the following year.
As the duo started to enjoy critical and commercial success, their debut album What's That Noise? was released in April 1989 on Ahead of Our Time and distributed by Big Life Records. The album gave "breaks the full length treatment", and showcased "their heady blend of hip-hop production aesthetics and proto-acid house grooves". It also rounded up a heap of unconventional guest features, quoted by SPIN as having "somehow found room at the same table for Queen Latifah and Mark E. Smith". The album’s track ‘I’m in Deep’ (featuring Smith) prefigured the Indie-dance guitar-breaks crossover of such bands as the Stone Roses and Happy Mondays, utilizing Smith’s freestyle raucous vocals over an acid house backing, and also including psych guitar samples from British rock band Deep Purple. What’s That Noise? reached the Top 20 in the UK and was certified Silver.
1990s.
Coldcut's second album, Some Like It Cold released in 1990 on Ahead Of Our Time, featured a collaboration with Queen Latifah on the single "Find a Way". Though "Find a Way" was a minor hit in the UK, no more singles were released from the album. The duo was given the BPI "Producer of the Year Award" in 1990. Hex - alongside some other London visual experimenters such as iE - produced a series of videos for a longform VHS version of the album. This continued Coldcut and Hex’s pioneering of the use of microcomputers to synthesize electronic music visuals.
After their success with Lisa Stansfield, Coldcut signed with her label, Arista Conflicts arose with the major label, as Coldcut’s "vision extended beyond the formulae of house and techno" and mainstream pop culture (CITATION: The Virgin Encyclopedia Of Nineties Music, 2000). Eventually, the duo’s album Philosophy emerged in 1993. Singles "Dreamer" and "Autumn Leaves" (1994) were both minor hits but the album did not chart.
"Autumn Leaves" had strings recorded at Abbey Road, with a 30 piece string section and an arrangement by film composer Ed Shearmur. The leader of the string section was Simon Jeffes of Penguin Cafe Orchestra. Coldcut’s insistence on their friend Mixmaster Morris to remix "Autumn Leaves" led to one of Morris’ most celebrated remixes, which became a minor legend in ambient music. It has appeared on numerous compilations.
In 1990, whilst on their first tour in Japan (which also featured Norman Cook, who later became Fatboy Slim), Matt and Jon formed their second record label, Ninja Tune, as a self-titled ’technocoloured escape pod,’ and a way to escape the creative control of major labels. The label enabled them to release music under different aliases (e.g.. Bogus Order, DJ Food), which also helped them to avoid pigeonholing as producers. Ninja Tune’s first release was Bogus Order’s ‘Zen Brakes.’ The name Coldcut stayed with Arista so there were no official Coldcut releases for the next three years.
During this time, Coldcut still produced for artists on their new label, releasing a flood of material under different names and continuing to work with young groups. They additionally kept on with Solid Steel on Kiss FM and running the night club Stealth (Club of the Year in the NME, The Face, and Mixmag in 1996).
In 1991, Hex released their first video game, ‘Top Banana’, which was included on a Hex release for the Commodore CDTV machine in 1992, arguably the first complete purpose-designed multimedia system. ‘Top Banana’ was innovative in that it used sampled graphics, contained an ecological theme and a female lead character (dubbed ‘KT’), and its music changed through random processes. Coldcut and Hex presented this multimedia project as an example of the forthcoming convergence of pop music and computer game characters.
In 1992, Hex’s first single - ‘Global Chaos’ / ‘Digital Love Opus 1’ - combined rave visuals with techno and ambient interactive visuals.
In November of that year, Hex released Global Chaos CDTV, which took advantage of the possibilities of the new CD-ROM medium. The Global Chaos CDTV disk (which contained the ‘Top Banana’ game, interactive visuals and audio), was a forerunner of the "CD+" concept, uniting music, graphics, and video games into one. This multi-dimensional entertainment product received wide coverage in the national media, including features on Dance Energy, Kaleidoscope on BBC Radio 4, What's Up Doc? on ITV and Reportage on BBC 2. i-D Magazine was quoted as saying, "It's like your TV tripping".
Coldcut videos were made for most songs, often by Hexstatic, and used a lot of stock and sampled footage. Their ‘Timber’ video, which created an AV collage piece using analogous techniques to audio sample collage, was put on heavy rotation on MTV. Stuart Warren Hill of Hexstatic referred to this technique as: "What you see is what you hear." ‘Timber’ (which appears on both ‘Let Us Play’, Coldcut’s fourth album, and ‘Let Us Replay,’ their fifth) won awards for its innovative use of repetitive video clips synced to the music, including being shortlisted at the Edinburgh Television and Film Festival in their top five music videos of the year in 1998.
Coldcut began integrating video sampling into their live DJ gigs at the time, and incorporated multimedia content that caused press to credit the act as segueing "into the computer age". Throughout the 90s, Hex created visuals for Coldcut’s live performances, and developed the CD-ROM portion of Coldcut’s ‘Let Us Play’ and ‘Let Us Replay,’ in addition to software developed specifically for the album’s world tour. Hex’s inclusion of music videos and ‘playtools’ (playful art/music software programs) on Coldcut’s CD-Roms was completely ahead of the curve at that time, offering viewers/listeners a high level of interactivity. Playtools such as My Little Funkit and Playtime were the prototypes for , the app Coldcut designed and launched 16 years later. Playtime followed on from Coldcut and Hex’s Synopticon installation, developing the auto-cutup algorhythm, and using other random processes to generate surprising combinations. Coldcut and Hex performed live using Playtime at the 1st Sonar Festival in 1994. Playtime was also used to generate the backing track for Coldcut’s collaboration with Jello Biafra, ‘Every Home a Prison’.
In 1994 Coldcut and Hex contributed an installation to the Glasgow Gallery of Modern Art. The piece, called 'Generator' was installed in the Fire Gallery. Generator was an interactive installation which allowed users to mix sound, video, text and graphics and make their own audio-visual mix, modelled on the techniques and technology used by Coldcut in clubs and live performance events. It consisted of two consoles: the left controlling how the sounds are played, the right controlling how the images are played.
As part of the JAM exhibition of "Style, Music and Media" at the Barbican Art Gallery in 1996, Coldcut and Hex were commissioned to produce an interactive audiovisual piece called Synopticon. Conceived and designed by Robert Pepperell and Matt Black, the digital culture synthesiser allows users to "remix" sounds, images, text and music in a partially random, partially controlled way.
The year 1996 also brought the Coldcut name back to More and Black, and the pair celebrated with ‘70 Minutes of Madness,’ a mix CD that became part of the Journeys by DJ series. The release was credited with "bringing to wider attention the sort of freestyle mixing the pair were always known for through their radio show on KISS FM, Solid Steel, and their steady club dates". It was voted "Best Compilation of All Time" by Jockey Slut in 1998.
In February 1997, they released a double pack single "Atomic Moog 2000" / "Boot the System", the first Coldcut release on Ninja Tune. This was not eligible for the UK chart because time and format restrictions prevented the inclusion of the ‘Natural Rhythm’ video on the CD. In August 1997, a reworking of the early track "More Beats + Pieces" gave them their first UK Top 40 hit since 1989.
The album Let Us Play! followed in September and also made the Top 40. The fourth album by Coldcut, Let Us Play! paid homage to the greats that inspired them. Their first album to be released on Ninja Tune, it featured guest appearances by Grandmaster Flash, Steinski, Jello Biafra, Jimpster, The Herbaliser, Talvin Singh, Daniel Pemberton and Selena Saliva. Coldcut’s cut 'n' paste method on the album was compared to that of Dadaism and William Burroughs. Hex collaborated with Coldcut to produce the multimedia CD-Rom for the album. Hex later evolved the software into the engine that was used on the Let Us Play! world tour.
In 1997, Matt Black - alongside Cambridge based developers Camart - created real-time video manipulation software VJAMM. It allowed users to be a "digital video jockey,", remixing and collaging sound and images and trigger audio and visual samples simultaneously, subsequently bringing futuristic technology to the audio-visual field. VJAMM rivalled some of the features of high-end and high cost tech at the time. The VJAMM technology, praised as being proof of how far computers changed the face of live music, became seminal in both Coldcut's live sets (which were called a "revelaton" by Melody Maker and DJ sets. Their CCTV live show was featured at major festivals including Glastonbury, Roskilde, Sónar, the Montreux Jazz Festival, and John Peel's Meltdown. The "beautifully simple and devastatingly effective" software was deemed revolutionary, and became recognized as a major factor in the evolution of clubs. It eventually earned a place in the American Museum of the Moving Image's permanent collection. As quoted by The Independent: "Coldcut's motto? 'Don't hate the media, be the media." NME was quoted as saying: "Veteran duo Coldcut are so cool they invented the remix - now they are doing the same for television."
Also working with Camart, Black designed DJamm software in 1998, which Coldcut used on laptops for their live shows, providing the audio bed alongside VJAMM’s audiovisual samples. Matt Black explained they designed DJamm so they "could perform electronic music in a different way – i.e., not just taking a session band out to reproduce what you put together in the studio using samples. It had a relationship to DJing, but was more interactive and more effective." Excitingly at that time, DJamm was pioneering in its ability to shuffle sliced loops into intricate sequences, enabling users to split loops into any number of parts.
In 1999, Let Us Replay! was released, a double-disc remix album where Coldcut’s classic tunes were remixed by the likes of Cornelius (which was heralded as a highlight of the album, Irresistible Force, Shut Up And Dance, Carl Craig and J Swinscoe. Let Us Replay! pieces together "short sharp shocks that put the mental in ‘experimental’ and still bring the breaks till the breakadawn". It also includes a few live tracks from the duo’s innovative world tour. The CD-Rom of the album, which also contained a free demo disc of the VJamm software, was one of the earliest audiovisual CD- ROMs on the market, and Muzik claimed deserved to "have them canonized...it’s like buying an entire mini studio for under $15.".
2000s.
In 2000, the Solid Steel show moved to BBC London.
Coldcut continued to forge interesting collaborations, including 2001's "Revolution," an EP in which Coldcut created their own political party (The Guilty Party). Featuring scratches and samples of Tony Blair and William Hague speeches, the 3-track EP included Nautilus' "Space Journey," which won an Intermusic contest in 2000. The video was widely played on MTV. With ‘Space Journey,’ Coldcut were arguably the first group to give fans access to the multitrack parts, or "stems," of their songs, building on the idea of interactivity and sharing from Let Us Play.
In 2001, Coldcut produced tracks for the Sega music video game REZ. REZ replaced typical video game sound effect with electronic music; the player created sounds and melodies, intended to simulate a form of synesthesia. The soundtrack also featured Adam Freeland and Oval.
In 2002, while utilizing VJamm and Detraktor, Coldcut and Juxta remixed Herbie Hancock’s classic ‘Rockit,’ creating both an audio and video remix.
Working with Marcus Clements in 2002, Coldcut released the sample manipulation algorhythm from their DJamm software as a standalone VST plugin that could be used in other software, naming it the Coldcutter.
Also in 2002, Coldcut with UK VJs Headspace (now mainly performing as the VJamm Allstars developed Gridio, an interactive, immersive audio-visual installation for the Pompidou Centre as part of the ‘Sonic Process’ exhibition. The ‘Sonic Process’ exhibition was launched at the MACBA in Barcelona in conjunction with Sónar, featuring Gridio as its centerpiece. In 2003, a commission for Graz led to a specially built version of Gridio, in a cave inside the castle mountain in Austria. Gridio was later commissioned by O2 for two simultaneous customised installations at the O2 Wireless Festivals in Leeds and London in 2007. That same year, Gridio was featured as part of Optronica at the opening week of the new BFI Southbank development in London.
In 2003, Black worked with Penny Rimbaud (ex Crass) on Crass Agenda's Savage Utopia project. Black performed the piece with Rimbaud, Eve Libertine and other players at London’s Vortex Jazz Club.
In 2004, Coldcut collaborated with American video mashup artist TV Sheriff to produce their cut-up entitled ‘Revolution USA.’ The tactical-media project (coordinated with Canadian art duo NomIg) followed on from the UK version and extended the premise "into an open access participatory project". Through the multimedia political art project, over 12 gigabytes of footage from the last 40 years of US politics were made accessible to download, allowing participants to create a cut-up over a Coldcut beat. Coldcut also collaborated with TV Sheriff and NomIg to produce two audiovisual pieces "World of Evil" (2004) and "Revolution '08" (2008), both composed of footage from the United States presidential elections of respective years. The music used was composed by Coldcut, with "Revolution '08" featuring a remix by the Qemists.
Later that year, a collaboration with the British Antarctic Survey (BAS) led to the psychedelic art documentary 'Wavejammer.’ Coldcut was given access to the BAS archive in order to create sounds and visuals for the short film.
2004 also saw Coldcut produce a radio play in conjunction with renowned young author Hari Kunzru for BBC Radio 3 (incidentally called 'Sound Mirrors').
Coldcut returned with the single "Everything Is Under Control” at the end of 2005, featuring Jon Spencer (of Jon Spencer Blues Explosion) and Mike Ladd. It was followed in 2006 by their fifth studio album Sound Mirrors, which was quoted as being “one of the most vital and imaginative records Jon Moore and Matt Black have ever made”, and saw the duo "continue, impressively, to find new ways to present political statements through a gamut of pristine electronics and breakbeats" (CITATION: Future Music, 2007). The fascinating array of guest vocalists included Soweto Kinch, Annette Peacock, Ameri Baraka, and Saul Williams. The latter followed on from Coldcut’s remix of Williams’ ‘The Pledge’ for a project with DJ Spooky.
A 100-date audiovisual world tour commenced for ‘Sound Mirrors,’ which was considered "no small feat in terms of technology or human effort". Coldcut was accompanied by scratch DJ Raj and AV artist Juxta, in addition to guest vocalists from the album, including UK rapper Juice Aleem, Roots Manuva, Mpho Skeef, Jon Spencer and house legend Robert Owens.
Three further singles were released from the album including the Top 75 hit "True Skool" with Roots Manuva. The same track appeared on the soundtrack of the video game FIFA Street 2.
Sponsored by the British Council, in 2005 Coldcut introduced AV mixing to India with the Union project, alongside collaborators Howie B and Aki Nawaz of Fun-Da-Mental. Coldcut created an A/V remix of the Bollywood hit movie ‘Kal Ho Naa Ho’.
In 2006, Coldcut performed an A/V set based on "Music for 18 Musicians" as part of Steve Reich’s 70th birthday gig at the Barbican Centre in London.
Coldcut remixed another classic song in 2007: Nina Simone’s ‘Save Me.’ This was part of a remix album called ‘Nina Simone: Remixed & Re-imagined,’ featuring remixes from Tony Humphries, Francois K and Chris Coco.
In February 2007, Coldcut and Mixmaster Morris created a psychedelic AV obituary/tribute Coldcut, Mixmaster Morris, Ken Campbell, Bill Drummond and Alan Moore (18 March 2007). . Queen Elizabeth Hall, London: Mixmaster Morris. (28 August 2009) to Robert Anton Wilson, the 60s author of Illuminatus! Trilogy. The tribute featured graphic novel writer Alan Moore and artist Bill Drummond and a performance by experimental theatre legend Ken Campbell. Coldcut and Morris’ hour and a half performance resembled a documentary being remixed on the fly, cutting up nearly 15 hours’ worth of Wilson’s lectures.
In 2008, an international group of party organisers, activists and artists including Coldcut received a grant from the Intelligent Energy Department of the European Union, to create a project that promoted intelligent energy and environmental awareness to the youth of Europe. The result was Energy Union, a piece of VJ cinema, political campaign, music tour, party, art exhibition and social media hub. Energy Union toured 12 EU countries throughout 2009 and 2010, completing 24 events in total. Coldcut created the Energy Union show for the tour, a one-hour Audio/Visual montage on the theme of Intelligent Energy. In presenting new ideas for climate, environmental and energy communication strategies, the Energy Union tour was well received, and reached a widespread audience in cities across the UK, Germany, Belgium, The Netherlands, Croatia, Slovenia, Austria, Hungary, Bulgaria, Spain and the Czech Republic.
Also in 2008, Coldcut was asked to remix the theme song for British cult TV show Doctor Who for the program’s 40th anniversary. In October 2008, Coldcut celebrated the legacy of the BBC Radiophonic Workshop (the place where the Doctor Who theme was created) with a live DJ mix at London’s legendary Roundhouse. The live mix incorporated classic Radiophonic Workshop compositions with extended sampling of the original gear.
Additionally in 2008, Coldcut remixed "Ourselves", a Japanese #1 hit from the single "&" by Ayumi Hamasaki. This mix was included on the album .
Starting in 2009, Matt Black, with musician/artist/coder Paul Miller (creator of the TX Modular Open Source synth), developed Granul8, a new type of visual fx/source Black termed a ‘granular video synthesiser’. Granul8 allows the use of realtime VJ techniques including video feedback combined with VDMX VJ software.
From 2009 onwards, Black has been collaborating with coder and psychedelic mathematician William Rood to create a forthcoming project called Liveloom, a social media AV mixer.
Recent work.
In 2010, Coldcut celebrated 20 years of releasing music with its label, Ninja Tune. A book entitled Ninja Tune: 20 Years of Beats and Pieces was released on 12 August 2010, and an exhibition was held at Black Dog Publishing's Black Dog Space in London, showcasing artwork, design and photography from the label's 20-year history. A compilation album was released on 20 September in two formats: a regular version consisting of two 2-disc volumes, and a limited edition which contained six CDs, six 7" vinyl singles, a hardback copy of the book, a poster and additional items. Ninja Tune also incorporated a series of international parties. This repositioned Ninja as a continually compelling and influential label, being one of the "longest-running (and successful) UK indie labels to come out of the late-1980s/early-90s explosion in dance music and hip-hop" (Pitchfork, 28 September 2010). Pitchfork claimed it had a "right to show off a little".
In July 2013, Coldcut produced a piece entitled ‘D’autre’ based on the writings of French poet Arthur Rimbaud, for Forum Des Images in Paris.The following month, in August, Coldcut produced a new soundtrack for a section of André Sauvage’s classic film Études sur Paris, which was shown as part of Noise of Art at the BFI in London, which celebrated 100 years of Electronic Music and Silent Cinema. Coldcut put new music to films from the Russolo era, incorporating original recordings of Russolo's proto-synths.
In April 2013, Coldcut released Ninja Jamm, an iOS music remix app, in collaboration with London-based arts and technology firm Seeper. Geared toward both casual listeners and more experienced DJs and music producers, the freemium app allows users to download and remix "Tunepacks" that feature original tracks and mixes by Coldcut, as well as other Ninja artists, creating something new altogether. With the "intuitive yet deep" app, users can turn instruments on and off, swap between clips, add glitches and effects, trigger and pitch-bend stabs and one-off samples, and change the tempo of the track instantly. Users can additionally record as they mix and instantly upload to SoundCloud or save the mixes locally. Tunepack releases for Ninja Jamm are increasingly synchronised with Ninja Tune releases on conventional formats. To date over 20 tunepacks have been released, including Amon Tobin, Bonobo, Coldcut, DJ Food, Martyn, Emika, Machinedrum, Raffertie, Irresistible Force, FaltyDL, Shuttle, Starkey. Ninja Jamm was featured by Apple in the New and Noteworthy section of the App Store in the week of release and it received over 100,000 downloads in the first week. Coldcut intend to develop Ninja Jamm further.
In 2013, Coldcut are working on a new album, collaborating with producer Dave Taylor (a.k.a. Solid Groove a.k.a. Switch). This is planned for 2014 release.

</doc>
<doc id="6656" url="http://en.wikipedia.org/wiki?curid=6656" title="Cuisine">
Cuisine

A cuisine ( , from French "cuisine", "cooking; culinary art; kitchen"; ultimately from Latin "coquĕre", "to cook") is a style of cooking characterized by distinctive ingredients, techniques and dishes, and usually associated with a specific culture or geographic region. A cuisine is primarily influenced by the ingredients that are available locally or through trade. Religious food laws, such as Islamic and Jewish dietary laws, can also exercise a strong influence on cuisine. Regional food preparation traditions, customs and ingredients often combine to create dishes unique to a particular region.
Factors that affect a cuisine.
Some of the elements that have an influence on a region's cuisine include the area's climate, which in large measure determines the native foods that are available, the economic conditions, which affect trade and can affect food distribution, imports and exports, and religiousness or sumptuary laws, under which certain foods and food preparations are required or proscribed.
Climate also affects the supply of fuel for cooking; a common Chinese food preparation method was cutting food into small pieces to cook foods quickly and conserve scarce firewood and charcoal. Foods preserved for winter consumption by smoking, curing, and pickling have remained significant in world cuisines for their altered gustatory properties even when these preserving techniques are no longer strictly necessary to the maintenance of an adequate food supply.
History.
Cuisine dates back to the Antiquity. Rome was known for its cuisine, wealthy families would dine in the Triclinium on a variety of dishes, their diet consisted of eggs, cheese, bread, meat and honey.
New cuisines.
Cuisines evolve continually, and new cuisines are created by innovation and cultural interaction. One recent example is fusion cuisine, which combines elements of various culinary traditions while not being categorized per any one cuisine style, and generally refers to the innovations in many contemporary restaurant cuisines since the 1970s. "Nouvelle cuisine" (New cuisine) is an approach to cooking and food presentation in French cuisine that was popularized in the 1960s by the food critics Henri Gault, who invented the phrase, and his colleagues André Gayot and Christian Millau in a new restaurant guide, the Gault-Millau, or "Le Nouveau Guide".
Global cuisine.
A global cuisine is a cuisine that is practiced around the world, and can be categorized according to the common use of major foodstuffs, including grains, produce and cooking fats.
Regional cuisines.
Regional cuisines may vary based upon food availability and trade, cooking traditions and practices, and cultural differences. For example, in Central and South America, corn (maize), both fresh and dried, is a staple food. In northern Europe, wheat, rye, and fats of animal origin predominate, while in southern Europe olive oil is ubiquitous and rice is more prevalent. In Italy the cuisine of the north, featuring butter and rice, stands in contrast to that of the south, with its wheat pasta and olive oil. China likewise can be divided into rice regions and noodle & bread regions. Throughout the Middle East and Mediterranean there is a common thread marking the use of lamb, olive oil, lemons, peppers, and rice. The vegetarianism practiced in much of India has made pulses (crops harvested solely for the dry seed) such as chickpeas and lentils as significant as wheat or rice. From India to Indonesia the use of spices is characteristic; coconuts and seafood are used throughout the region both as foodstuffs and as seasonings.
African cuisine.
African cuisines use a combination of locally available fruits, cereal grains and vegetables, as well as milk and meat products. In some parts of the continent, the traditional diet features a preponderance of milk, curd and whey products. In much of tropical Africa, however, cow's milk is rare and cannot be produced locally (owing to various diseases that affect livestock). The continent's diverse demographic makeup is reflected in the many different eating and drinking habits, dishes, and preparation techniques of its manifold populations.
Asian cuisine.
Asian cuisines are many and varied. Ingredients common to many cultures in the east and Southeast regions of the continent include rice, ginger, garlic, sesame seeds, chilies, dried onions, soy, and tofu. Stir frying, steaming, and deep frying are common cooking methods. While rice is common to most Asian cuisines, different varieties are popular in the various regions; Basmati rice is popular in the subcontinent, Jasmine is often found across the southeast, while long-grain rice is popular in China and short-grain in Japan and Korea. Curry is also a common dish found in southern and eastern Asia, however they are not as popular in eastern cuisines. Those curry dishes with origins in India and other South Asian countries usually have a yogurt base while Southeastern and Eastern curries generally use coconut milk as their foundation.
European cuisine.
European cuisine (alternatively, "Western cuisine") include the cuisines of Europe and other Western countries. European cuisine includes that of Europe and to some extent Russia, as well as non-indigenous cuisines of North America, Australasia, Oceania, and Latin America. The term is used by East Asians to contrast with Asian styles of cooking. This is analogous to Westerners referring collectively to the cuisines of Asian countries as Asian cuisine. When used by Westerners, the term may refer more specifically to cuisine "in" Europe; in this context, a synonym is Continental cuisine, especially in British English.
Oceanian cuisine.
Oceanian cuisines include Australian cuisine, New Zealand cuisine, Tasmanian cuisine, and the cuisines from many other islands or island groups throughout Oceania.
Cuisines of the Americas.
The cuisines of the Americas are found across North and South America are based on the cuisines of the countries from which the immigrant peoples came, primarily Europe. However, the traditional European cuisine has been adapted by the addition of many local ingredients, and many techniques have been added to the tradition as well. In the case of Latin American cuisines, many pre-Columbian ingredients and techniques are still used. The regional cuisines are Canadian cuisine, American cuisine, Mexican cuisine, Central American cuisine, South American cuisine, and Caribbean cuisine.
See also.
Portals

</doc>
<doc id="6658" url="http://en.wikipedia.org/wiki?curid=6658" title="October 2003">
October 2003

The following events occurred in October 2003:

</doc>
<doc id="6660" url="http://en.wikipedia.org/wiki?curid=6660" title="Codec">
Codec

A codec is a device or computer program capable of encoding or decoding a digital data stream or signal. "Codec" is a portmanteau of "coder-decoder" or, less commonly, "compressor-decompressor".
A codec encodes a data stream or signal for transmission, storage or encryption, or decodes it for playback or editing. Codecs are used in videoconferencing, streaming media and video editing applications. A video camera's analog-to-digital converter (ADC) converts its analog signals into digital signals, which are then passed through a video compressor for digital transmission or storage. A receiving device then runs the signal through a video decompressor, then a digital-to-analog converter (DAC) for analog display.
Related concepts.
In the mid 20th century, a codec was a hardware device that coded analog signals into digital form using pulse-code modulation (PCM). Late in the century the name was also applied to a class of software for converting between different digital signal formats, including compander functions.
A modem is a contraction of "modulator-demodulator". The telecommunications industry referred to the device as a "dataset". It converts digital data from computers to analog signals for transmission over telephone lines. On the receiving end the analog signal is converted back to digital data.
An audio codec converts analog audio signals into digital signals for transmission or storage. A receiving device then converts the digital signals back to analog using an audio decompressor, for playback. An example of this is the codecs used in the sound cards of personal computers. A video codec accomplishes the same task for video signals.
Compression quality.
Media codecs.
Two principal techniques are used in codecs, pulse-code modulation and delta modulation. Codecs are often designed to emphasize certain aspects of the media to be encoded. For example, a digital video (using a DV codec) of a sports event needs to encode motion well but not necessarily exact colors, while a video of an art exhibit needs to encode color and surface texture well.
Audio codecs for cell phones need to have very low latency between source encoding and playback. In contrast, audio codecs for recording or broadcast can use high-latency audio compression techniques to achieve higher fidelity at a lower bit-rate.
There are thousands of audio and video codecs, ranging in cost from free to hundreds of dollars or more. This variety of codecs can create compatibility and obsolescence issues. The impact is lessened for older formats, for which free or nearly-free codecs have existed for a long time. The older formats are often ill-suited to modern applications, however, such as playback in small portable devices. For example, raw uncompressed PCM audio (44.1 kHz, 16 bit stereo, as represented on an audio CD or in a .wav or .aiff file) has long been a standard across multiple platforms, but its transmission over networks is slow and expensive compared with more modern compressed formats, such as Opus and MP3.
Many multimedia data streams contain both audio and video, and often some metadata that permit synchronization of audio and video. Each of these three streams may be handled by different programs, processes, or hardware; but for the multimedia data streams to be useful in stored or transmitted form, they must be encapsulated together in a container format. 
Lower bitrate codecs allow more users, but they also have more distortion. Beyond the initial increase in distortion, lower bit rate codecs also achieve their lower bit rates by using more complex algorithms that make certain assumptions, such as those about the media and the packet loss rate. Other codecs may not make those same assumptions. When a user with a low bitrate codec talks to a user with another codec, additional distortion is introduced by each transcoding.
AVI is sometimes erroneously described as a codec, but AVI is actually a container format, while a codec is a software or hardware tool that encodes or decodes audio or video into or from some audio or video format. Audio and video encoded with many codecs might be put into an AVI container, although AVI is not an ISO standard. There are also other well-known container formats, such as Ogg, ASF, QuickTime, RealMedia, Matroska, and DivX Media Format. Some container formats which are ISO standards are MPEG transport stream, MPEG program stream, MP4 and ISO base media file format.

</doc>
<doc id="6663" url="http://en.wikipedia.org/wiki?curid=6663" title="Clyde Tombaugh">
Clyde Tombaugh

Clyde William Tombaugh (February 4, 1906 – January 17, 1997) was an American astronomer. Although he is best known for discovering the dwarf planet Pluto in 1930, the first object to be discovered in what would later be identified as the Kuiper belt, Tombaugh also discovered many asteroids; he also called for the serious scientific research of unidentified flying objects, or UFOs.
Biography.
Tombaugh was born in Streator, Illinois. After his family moved to Burdett, Kansas in 1922, Tombaugh's plans for attending college were frustrated when a hailstorm ruined his family's farm crops. Starting in 1926, he built several telescopes with lenses and mirrors by himself. He sent drawings of Jupiter and Mars to the Lowell Observatory, which offered him a job. Tombaugh worked there from 1929 to 1945.
Following his discovery of Pluto, Tombaugh earned bachelor's and master's degrees in astronomy from the University of Kansas in 1936 and 1938. During World War II he taught naval personnel navigation at Northern Arizona University. He worked at White Sands Missile Range in the early 1950s, and taught astronomy at New Mexico State University from 1955 until his retirement in 1973.
The asteroid 1604 Tombaugh, discovered in 1931, is named after him. He discovered hundreds of asteroids, beginning with 2839 Annette in 1929, mostly as a by-product of his search for Pluto and his searches for other celestial objects. Tombaugh named some of them after his wife, children and grandchildren. The Royal Astronomical Society awarded him the Jackson-Gwilt Medal in 1931. 
In August 1992, JPL scientist Robert Staehle called Tombaugh, requesting permission to visit his planet. "I told him he was welcome to it," Tombaugh later remembered, "though he's got to go one long, cold trip." The call eventually led to the launch of the New Horizons space probe to Pluto in 2006.
Death.
Tombaugh died on January 17, 1997, when he was in Las Cruces, New Mexico, at the age of 90. A small portion of his ashes were placed aboard the New Horizons spacecraft. The container includes the inscription: "Interned (sic) herein are remains of American Clyde W. Tombaugh, discoverer of Pluto and the solar system's 'third zone'. Adelle and Muron's boy, Patricia's husband, Annette and Alden's father, astronomer, teacher, punster, and friend: Clyde W. Tombaugh (1906–1997)".
Tombaugh was survived by his wife, Patricia (1912–2012), and their children, Annette and Alden. Tombaugh was an active Unitarian-Universalist.
Family.
Through the daughter of his youngest brother, Robert M., Tombaugh is the great uncle of Los Angeles Dodgers pitcher Clayton Kershaw.
Discovery of Pluto.
While a young researcher working for the Lowell Observatory in Flagstaff, Arizona, Tombaugh was given the job to perform a systematic search for a trans-Neptunian planet (also called Planet X), which had been predicted by Percival Lowell and William Pickering.
Tombaugh used the observatory's 13-inch astrograph to take photographs of the same section of sky several nights apart. He then used a blink comparator to compare the different images. When he shifted between the two images, a moving object, such as a planet, would appear to jump from one position to another, while the more distant objects such as stars would appear stationary. Tombaugh noticed such a moving object in his search, near the place predicted by Lowell, and subsequent observations showed it to have an orbit beyond that of Neptune. This ruled out classification as an asteroid, and they decided this was the ninth planet that Lowell had predicted. The discovery was made on Tuesday, February 18, 1930, using images taken the previous month. The name "Pluto" was reportedly suggested by Venetia Burney, then an 11-year-old English schoolgirl, who died in April 2009, having lived to see the reclassification of Pluto as a dwarf planet. It won out over numerous other suggestions because it was the name of the Roman god of the underworld, who was able to render himself invisible, and because Percival Lowell's initials PL formed the first 2 letters. The name Pluto was officially adopted on May 1, 1930.
Following the discovery, starting in the 1990s, of other Kuiper belt objects, Pluto began to be seen not as a planet orbiting alone at 40 AU, but as the largest of a group of icy bodies in that region of space. After it was shown that at least one such body was larger than Pluto, on August 24, 2006 the International Astronomical Union (IAU) reclassified Pluto, grouping it with two similarly sized "dwarf planets" rather than with the eight "classical planets".
Tombaugh's widow Patricia stated after the IAU's decision that while Clyde may have been disappointed with the change since he had resisted attempts to remove Pluto's planetary status in his lifetime, he would have accepted the decision now if he were alive. She noted that he "was a scientist. He would understand they had a real problem when they start finding several of these things flying around the place." Hal Levison offered this perspective on Tombaugh's place in history: "Clyde Tombaugh discovered the Kuiper Belt. That's a helluva lot more interesting than the ninth planet."
Further search.
Tombaugh continued searching for some years after the discovery of Pluto, and the lack of further discoveries left him satisfied that no other object of a comparable apparent magnitude existed near the ecliptic. No more trans-Neptunian objects were discovered until (15760) 1992 QB1, in 1992.
However, more recently the relatively bright object Makemake has been discovered. It has a relatively high orbital inclination, but at the time of Tombaugh's discovery of Pluto, Makemake was only a few degrees from the ecliptic near the border of Taurus and Auriga at an apparent magnitude of 16. This position was also very near the galactic equator, making it almost impossible to find such an object within the dense concentration of background stars of the Milky Way.
Asteroids discovered.
Tombaugh is officially credited by the Minor Planet Center with discovering 15 asteroids, and he observed nearly 800 asteroids during his search for Pluto and years of follow-up searches looking for another candidate for the postulated Planet X. Tombaugh is also credited with the discovery of periodic comet 274P/Tombaugh–Tenagra. He also discovered hundreds of variable stars, as well as star clusters, galaxy clusters, and a galaxy supercluster.
Interest in UFOs.
Tombaugh was probably the most eminent astronomer to have reported seeing unidentified flying objects and to support the extraterrestrial hypothesis. On August 20, 1949, Tombaugh saw several unidentified objects near Las Cruces, New Mexico. He described them as six to eight rectangular lights, stating: "I doubt that the phenomenon was any terrestrial reflection, because... nothing of the kind has ever appeared before or since... I was so unprepared for such a strange sight that I was really petrified with astonishment."
Tombaugh observed these rectangles of light for about 3 seconds and his wife saw them for about 1 1⁄2 seconds. He never supported the interpretation as a spaceship that has often been attributed to him. He considered other possibilities, with a temperature inversion as the most likely cause.From my own studies of the solar system I cannot entertain any serious possibility for intelligent life on other planets, not even for Mars... The logistics of visitations from planets revolving around the nearer stars is staggering. In consideration of the hundreds of millions of years in the geologic time scale when such visits may have possibly occurred, the odds of a single visit in a given century or millennium are overwhelmingly against such an event.
A much more likely source of explanation is some natural optical phenomenon in our own atmosphere. In my 1949 sightings the faintness of the object, together with the manner of fading in intensity as it traveled away from the zenith towards the southeastern horizon, is quite suggestive of a reflection from an optical boundary or surface of slight contrast in refractive index, as in an inversion layer.
I have never seen anything like it before or since, and I have spent a lot of time where the night sky could be seen well. This suggests that the phenomenon involves a comparatively rare set of conditions or circumstances to produce it, but nothing like the odds of an interstellar visitation.
Another sighting by Tombaugh a year or two later while at a White Sands observatory was of an object of −6 magnitude, four times brighter than Venus at its brightest, going from the zenith to the southern horizon in about 3 seconds. The object executed the same maneuvers as in Tombaugh's first sighting.
Tombaugh later reported having seen three of the mysterious green fireballs, which suddenly appeared over New Mexico in late 1948 and continued at least through the early 1950s. Despite this, the final report of Project Twinkle claimed that he "... never observed an unexplainable aerial object despite his continuous and extensive observations of the sky."
In 1956 Tombaugh had the following to say about his various sightings: "I have seen three objects in the last seven years which defied any explanation of known phenomenon, such as Venus, atmospheric optic, meteors or planes. I am a professional, highly skilled, professional astronomer. In addition I have seen three green fireballs which were unusual in behavior from normal green fireballs... I think that several reputable scientists are being unscientific in refusing to entertain the possibility of extraterrestrial origin and nature."
Shortly after this in January 1957, in an Associated Press article in the "Alamogordo Daily News" titled "Celestial Visitors May Be Invading Earth's Atmosphere," Tombaugh was again quoted on his sightings and opinion about them. "Although our own solar system is believed to support no other life than on Earth, other stars in the galaxy may have hundreds of thousands of habitable worlds. Races on these worlds may have been able to utilize the tremendous amounts of power required to bridge the space between the stars..." Tombaugh stated that he had observed celestial phenomena which he could not explain, but had seen none personally since 1951 or 1952. "These things, which do appear to be directed, are unlike any other phenomena I ever observed. Their apparent lack of obedience to the ordinary laws of celestial motion gives credence."
In 1949, Tombaugh had also told the Naval missile director at White Sands Missile Range, Commander Robert McLaughlin, that he had seen a bright flash on Mars on August 27, 1941, which he now attributed to an atomic blast. Tombaugh also noted that the first atomic bomb tested in New Mexico would have lit up the dark side of the Earth like a neon sign and that Mars was coincidentally quite close at the time, the implication apparently being that the atomic test would have been visible from Mars.
In June 1952, Dr. J. Allen Hynek, an astronomer acting as a scientific consultant to the Air Force's Project Blue Book UFO study, secretly conducted a survey of fellow astronomers on UFO sightings and attitudes while attending an astronomy convention. Tombaugh and four other astronomers, including Dr. Lincoln LaPaz of the University of New Mexico, told Hynek about their sightings. Tombaugh also told Hynek that his telescopes were at the Air Force's disposal for taking photos of UFOs, if he was properly alerted.
Near-Earth satellite search.
Tombaugh's offer may have led to his involvement in a search for near-Earth satellites, first announced in late 1953 and sponsored by the Army Office of Ordnance Research. Another public statement was made on the search in March 1954 (photo at right), emphasizing the rationale that such an orbiting object would serve as a natural space station. However, according to Donald Keyhoe, later director of the National Investigations Committee on Aerial Phenomena (NICAP), the real reason for the sudden search was because two near-Earth orbiting objects had been picked up on new long-range radar in the summer of 1953, according to his Pentagon source.
By May 1954, Keyhoe was making public statements that his sources told him the search had indeed been successful, and either one or two objects had been found. However, the story did not break until August 23, 1954, when "Aviation Week" magazine stated that two satellites had been found only 400 and 600 miles out. They were termed "natural satellites" and implied that they had been recently captured, despite this being a virtual impossibility. The next day, the story was in many major newspapers. Dr. LaPaz was implicated in the discovery in addition to Tombaugh. LaPaz had earlier conducted secret investigations on behalf of the Air Force on the green fireballs and other unidentified aerial phenomena over New Mexico. The "New York Times" reported on August 29 that "a source close to the O. O. R. unit here described as 'quite accurate' the report in the magazine Aviation Week that two previously unobserved satellites had been spotted and identified by Dr. Lincoln Lepaz of the University of New Mexico as natural and not artificial objects. This source also said there was absolutely no connection between the reported satellites and flying saucer reports." However, in the October 10th issue, LaPaz said the magazine article was "false in every particular, in so far as reference to me is concerned."
Both LaPaz and Tombaugh were to issue public denials that anything had been found. The October 1955 issue of "Popular Mechanics" magazine reported: "Professor Tombaugh is closemouthed about his results. He won't say whether or not any small natural satellites have been discovered. He does say, however, that newspaper reports of 18 months ago announcing the discovery of natural satellites at 400 and 600 miles out are not correct. He adds that there is no connection between the search program and the reports of so-called flying saucers."
At a meteor conference in Los Angeles in 1957, Tombaugh reiterated that his four-year search for "natural satellites" had been unsuccessful. In 1959, Tombaugh was to issue a final report stating that nothing had been found in his search. His personal 16-inch telescope was reassembled and dedicated on September 17, 2009 at Rancho Hidalgo, New Mexico (near Animas, New Mexico), adjacent to "Astronomy" 's new observatory.

</doc>
<doc id="6666" url="http://en.wikipedia.org/wiki?curid=6666" title="Christopher Báthory">
Christopher Báthory

Christopher Báthory (Hungarian: "Báthory Kristóf") (1530, Szilágysomlyó – May 27, 1581, Gyulafehérvár) was a voivode of Transylvania. He succeeded his brother Stephen Báthory. He was the father of Sigismund Báthory and Gryzelda Bathory.

</doc>
<doc id="6667" url="http://en.wikipedia.org/wiki?curid=6667" title="CPAN">
CPAN

CPAN, the Comprehensive Perl Archive Network, is an archive of over 129,703 modules of software in 29,092 distributions, written by 11,274 authors, written in the Perl programming language, as well as documentation for them. It has a presence on the World Wide Web at or via the old interface and is mirrored worldwide at more than 260 locations. "CPAN" can denote either the archive network itself, or the Perl program that acts as an interface to the network and as an automated software installer (somewhat like a package manager). Most software on CPAN is free and open source software. CPAN was conceived in 1993, and the first web-accessible mirror was launched in January 1997.
Modules.
Like many programming languages, Perl has mechanisms to use external libraries of code, making one file contain common routines used by several programs. Perl calls these "modules". Perl modules are typically installed in one of several directories whose paths are placed in the Perl interpreter when it is first compiled; on Unix-like operating systems, common paths include "/usr/lib/perl5", "/usr/local/lib/perl5", and several of their subdirectories.
Perl comes with a small set of "core modules". Some of these perform bootstrapping tasks, such as , which is used for building and installing other extension modules; others, like CGI.pm, are merely commonly used. The authors of Perl do not expect this limited group to meet every need, however.
Role.
The CPAN's main purpose is to help programmers locate modules and programs not included in the Perl standard distribution. Its structure is decentralized. Authors maintain and improve their own modules. Forking, and creating competing modules for the same task or purpose is common. There is no formal bug tracking system, but there is a third-party bug tracking system that CPAN designated as the suggested official method of reporting issues with modules. Continuous development on modules is rare; many are abandoned by their authors, or go years between new versions being released. Sometimes a maintainer will be appointed to an abandoned module. They can release new versions of the module, and accept patches from the community to the module as their time permits. CPAN has no revision control system, although the source for the modules is often stored on GitHub. Also, the complete history of the CPAN and all its modules is available as the project, allowing to easily see the complete history for all the modules and for easy maintenance of forks. CPAN is also used to distribute new versions of Perl, as well as related projects, such as Parrot.
The CPAN is an important resource for the professional Perl programmer. With over 23,000 modules (containing 20,000,000 lines of code) as of July 2011, the CPAN can save programmers weeks of time, and large Perl programs often make use of dozens of modules. Some of them, such as the DBI family of modules used for interfacing with SQL databases, are nearly irreplaceable in their area of function; others, such as the module, are simply handy resources containing a few common functions.
Structure.
Files on the CPAN are referred to as "distributions". A distribution may consist of one or more modules, documentation files, or programs packaged in a common archiving format, such as a gzipped tar archive or a ZIP file. Distributions will often contain installation scripts (usually called "Makefile.PL" or "Build.PL") and test scripts which can be run to verify the contents of the distribution are functioning properly. New distributions are uploaded to the Perl Authors Upload Server, or PAUSE (see the section Uploading distributions with PAUSE).
In 2003, distributions started to include metadata files, called "META.yml", indicating the distribution's name, version, dependencies, and other useful information; however, not all distributions contain metadata. When metadata is not present in a distribution, the PAUSE's software will usually try to analyze the code in the distribution to look for the same information; this is not necessarily very reliable.
With thousands of distributions, CPAN needs to be structured to be useful. Distributions on the CPAN are divided into 24 broad "chapters" based on their purpose, such as "Internationalization and Locale"; "Archiving, Compression, And Conversion"; and "Mail and Usenet News". Distributions can also be browsed by author. Finally, the natural hierarchy of Perl module names (such as "Apache::DBI" or "Lingua::EN::Inflect") can sometimes be used to browse modules in the CPAN.
CPAN module distributions usually have names in the form of "CGI-Application-3.1" (where the :: used in the module's name has been replaced with a dash, and the version number has been appended to the name), but this is only a convention; many prominent distributions break the convention, especially those that contain multiple modules. Security restrictions prevent a distribution from ever being replaced, so virtually all distribution names do include a version number.
Components.
Mirrors.
The heart of CPAN is its worldwide network of more than 260 mirrors in more than 60 countries. CPAN's master site has over 149 direct public mirrors. Each site contains up to the full 3.9 gigabytes of data, or a subset of it if the mirror's maintainer wishes to selectively choose.
Most mirrors update themselves hourly, daily or bidaily from the CPAN master site. Some sites are major FTP servers which mirror lots of other software, but others are simply servers owned by companies that use Perl heavily. There are at least two mirrors on every continent except Antarctica.
For more information on CPAN mirrors, see .
Search engines.
Several search engines have been written to help Perl programmers sort through the CPAN. The most popular and official is , which includes textual search, a browsable index of modules, and extracted copies of all distributions currently on the CPAN. Other CPAN search engines that have been set up are:
Testers.
CPAN Testers are a group of volunteers, who will download and test distributions as they are uploaded to CPAN. This enables the authors to have their modules tested on many platforms and environments that they would otherwise not have access to, thus helping to promote portability, as well as a degree of quality. Smoke testers send reports, which are then collated and used for a variety of presentation websites, including the main reports site, statistics and dependencies.
Other supporting websites.
A family of other loosely integrated support websites have been created as the CPAN has grown in size and scale. These are created and managed by individual Perl developers, and provide data feeds to each other in various ad-hoc ways.
CPAN.pm and CPANPLUS.
There is also a Perl core module named CPAN; it is usually differentiated from the repository itself by using the name CPAN.pm. CPAN.pm is mainly an interactive shell which can be used to search for, download, and install distributions. An interactive shell called cpan is also provided in the Perl core, and is the usual way of running CPAN.pm. After a short configuration process and mirror selection, it uses tools available on the user's computer to automatically download, unpack, compile, test, and install modules. It is also capable of updating itself.
More recently, an effort to replace CPAN.pm with something cleaner and more modern has resulted in the CPANPLUS (or CPAN++) set of modules. CPANPLUS separates the back-end work of downloading, compiling, and installing modules from the interactive shell used to issue commands. It also supports several advanced features, such as cryptographic signature checking and test result reporting. Finally, CPANPLUS can uninstall a distribution. CPANPLUS was added to the Perl core in version 5.10.0.
Both modules can check a distribution's dependencies and can be set to recursively install any prerequisites, either automatically or with individual user approval. Both support FTP and HTTP and can work through firewalls and proxies.
Uploading distributions with PAUSE.
Authors can upload new distributions to the CPAN through the "Perl Authors Upload Server" (). To do so, they must . Registration information can be found at the 
Registrations are manually reviewed, so the process may take a week or longer.
Once registered, the new PAUSE account has a directory in the CPAN under "authors/id/(first letter)/(first two letters)/(author ID)". They may use a web interface at , or the PAUSE ftp server to upload files to their directory and delete them. PAUSE will warn an administrator if a user uploads a module that already exists, unless they are listed as a "co-maintainer". This can be specified through PAUSE's web interface.
Influence.
Experienced Perl programmers often comment that half of Perl's power is in the CPAN. It has been called Perl's killer app. Though the TeX typesetting language has an equivalent, the CTAN (and in fact the CPAN's name is based on the CTAN), few languages have an exhaustive central repository for libraries. The PHP language has PECL and PEAR, Python has a PyPI (Python Package Index) repository, Ruby has RubyGems, R has CRAN, Node.js has npm, Lua has LuaRocks, Haskell has Hackage and an associated installer/make clone cabal; but none of these are as large as the CPAN. Recently, Common Lisp has a de facto CPAN-like system—the Quicklisp repositories. Other major languages, such as Java and C++, have nothing similar to the CPAN (though for Java there is central Maven).
The CPAN has grown so large and comprehensive over the years that Perl users are known to express surprise when they start to encounter topics for which a CPAN module "doesn't" exist already.
The CPAN's influence on Perl's eclectic culture should not be underestimated either. As a hive of activity in the Perl world, the CPAN both shapes and is shaped by Perl culture. Its "self-appointed master librarian", Jarkko Hietaniemi, often takes part in the April Fools Day jokes so popular on the Internet; on 1 April 2002 the site was temporarily named to "CJAN", where the "J" stood for "Java". In 2003, the domain name was redirected to Matt's Script Archive, a site infamous in the Perl community for having badly written code.
Beyond April Fools', however, some of the distributions on the CPAN are jokes in themselves. The Acme:: hierarchy is reserved for joke modules; for instance, Acme::Don't adds a codice_1 function that doesn't run the code given to it (to complement the codice_2 built-in, which does). Even outside the Acme:: hierarchy, some modules are still written largely for amusement; one example is Lingua::Romana::Perligata, which can be used to write Perl programs in a subset of Latin.
Derivative works.
In 2005, a group of Perl developers who also had an interest in JavaScript got together to create JSAN, the JavaScript Archive Network. The JSAN is a near-direct port of the CPAN infrastructure for use with the JavaScript language, which for most of its lifespan did not have a cohesive "community".
In 2008, after a chance meeting with CPAN admin Adam Kennedy at the Open Source Developers Conference, Linux kernel developer Rusty Russell created the , the Comprehensive C Archive Network. The CCAN is a direct port of the CPAN architecture for use with the C language.

</doc>
<doc id="6669" url="http://en.wikipedia.org/wiki?curid=6669" title="Colorado Rockies">
Colorado Rockies

The Colorado Rockies is an American professional baseball franchise based in Denver, Colorado. They are currently members of Major League Baseball (MLB)'s National League (NL) West division. Their home venue is Coors Field. Their manager is Walt Weiss.
The Colorado Rockies have won one National League championship (2007). They mounted a rally in the last month of the 2007 regular season, winning 21 of their final 22 games, and reached the 2007 World Series. However, they were swept by the American League (AL) champion Boston Red Sox in four games.
History.
Denver had long been a hotbed of Denver Bears/Zephyrs Minor league baseball and many in the area desired a Major League team. Following the Pittsburgh drug trials, an unsuccessful attempt was made to purchase the Pittsburgh Pirates and relocate them. However, in 1991, as part of Major League Baseball's two-team expansion (they also added the Florida (now Miami) Marlins), an ownership group representing Denver led by John Antonucci and Michael I. Monus were granted a franchise; they took the name "Rockies" due to Denver's proximity to the Rocky Mountains, which is reflected in their logo. Monus and Antonucci were forced to drop out in 1992 after Monus' reputation was ruined by an accounting scandal. Trucking magnate Jerry McMorris stepped in at the eleventh hour to save the franchise, allowing the team to begin play in 1993. The Rockies shared Mile High Stadium (which had originally been built for the Zephyrs) with the National Football League's Denver Broncos their first two seasons while Coors Field was constructed. It was completed for the 1995 Major League Baseball season.
In 1993, they started play in the Western division of the National League. Since that date, the Rockies have reached the Major League Baseball postseason three times, each time as the National League wild card team. Twice (1995 and 2009) they were eliminated in the first round of the playoffs. In 2007, the Rockies advanced all the way to the World Series, only to be swept by the Boston Red Sox.
The Rockies have played their home games at Coors Field since 1995. Their newest Spring Training home, Salt River Fields at Talking Stick in Scottsdale, Arizona, opened in March 2011 and is shared with the Arizona Diamondbacks.
Uniform.
At the start of the 2012 season, the Rockies introduced "Purple Mondays" in which the team wears its purple uniform every Monday game day.
Baseball Hall of Famers.
No inducted members of the Baseball Hall of Fame have played for or managed the Rockies.
Retired numbers.
Todd Helton is the sole Colorado player to have his number (17) retired, which was done on Sunday, August 17, 2014. 
Jackie Robinson's number, 42, was retired throughout all of baseball in 1997.
Keli McGregor had worked with the Rockies since their inception in 1993, rising from senior director of operations to team president in 2002, until his death on April 20, 2010. He is honored at Coors Field alongside Helton and Robinson with his initials. 
Radio and television.
As of 2010, Rockies' flagship radio station is KOA 850AM, with some late-season games broadcast on KHOW 630 AM due to conflicts with Denver Broncos games. Jerry Schemmel and Jack Corrigan are the radio announcers which both serve as backup TV announcers whenever Drew Goodman is not on the broadcast. The Rockies Radio Network is composed of 38 affiliate stations in eight states.
As of 2013, Spanish broadcasts of the Rockies are heard on KNRV 1150 AM.
As of 2013, all games will be produced and televised by Root Sports Rocky Mountain. All 150 games produced by Root Sports Rocky Mountain will be broadcast in HD. Jeff Huson, Drew Goodman and George Frazier form the TV broadcast team with Marc Stout, Jenny Cavnar, Ryan Spilborghs, Jason Hirsh and Cory Sullivan handling the pre-game and post-game shows.
External links.
class="navbox collapsible autocollapse" style="width:100;"
!colspan="3" style="background-color:#DCDCDC;"

</doc>
<doc id="6670" url="http://en.wikipedia.org/wiki?curid=6670" title="Cement">
Cement

A cement is a binder, a substance that sets and hardens and can bind other materials together. The word "cement" traces to the Romans, who used the term "opus caementicium" to describe masonry resembling modern concrete that was made from crushed rock with burnt lime as binder. The volcanic ash and pulverized brick supplements that were added to the burnt lime, to obtain a hydraulic binder, were later referred to as "cementum", "cimentum", "cäment", and "cement".
Cements used in construction can be characterized as being either hydraulic or non-hydraulic, depending upon the ability of the cement to be used in the presence of water (see hydraulic and non-hydraulic lime plaster).
Non-hydraulic cement will not set in wet conditions or underwater, rather it sets as it dries and reacts with carbon dioxide in the air. It can be attacked by some aggressive chemicals after setting.
Hydraulic cement is made by replacing some of the cement in a mix with activated aluminium silicates, pozzolanas, such as fly ash. The chemical reaction results in hydrates that are not very water-soluble and so are quite durable in water and safe from chemical attack. This allows setting in wet condition or underwater and further protects the hardened material from chemical attack (e.g., Portland cement).
The chemical process for hydraulic cement found by ancient Romans used volcanic ash (activated aluminium silicates). Presently cheaper than volcanic ash, fly ash from power stations, recovered as a pollution control measure, or other waste or by products are used as pozzolanas with plain cement to produce hydraulic cement. Pozzolanas can constitute up to 40% of Portland cement.
The most important uses of cement are as a component in the production of mortar in masonry, and of concrete, a combination of cement and an aggregate to form a strong building material.
Chemistry.
Non-hydraulic cement, such as slaked lime (calcium hydroxide mixed with water), hardens by carbonation in the presence of carbon dioxide which is naturally present in the air. First calcium oxide is produced by lime calcination at temperatures above 825 °C (1,517 °F) for about 10 hours at atmospheric pressure: 
The calcium oxide is then "spent" (slaked) mixing it with water to make slaked lime:
Once the water in excess from the slaked lime is completely evaporated (this process is technically called "setting"), the carbonation starts:
This reaction takes a significant amount of time because the partial pressure of carbon dioxide in the air is low. The carbonation reaction requires the dry cement to be exposed to air, for this reason the slaked lime is a non-hydraulic cement and cannot be used under water. This whole process is called the "lime cycle".
Conversely, the chemistry ruling the action of the hydraulic cement is hydration. Hydraulic cements (such as Portland cement) are made of a mixture of silicates and oxides, the four main components being: 
The silicates are responsible of the mechanical properties of the cement, the celite and the brownmillerite are essential to allow the formation of the liquid phase during the kiln sintering (firing).
The chemistry of the above listed reactions is not completely clear and is still the object of research.
History.
Alternatives to cement used in antiquity.
Cement, chemically speaking, is a product including lime as the primary curing ingredient, but it is far from the first material used for cement"ation". The Babylonians and Assyrians used bitumen to bind together burnt brick or alabaster slabs. In Egypt stone blocks were cemented together with mortar, a combination of sand and roughly burnt gypsum, which often contained calcium carbonate.
Cements before the 18th century.
Lime was used on Crete and by the ancient Greeks. There is evidence that the Minoans of Crete used crushed potshards as an artificial pozzolan for hydraulic cement. It is uncertain where it was first discovered that a combination of hydrated non-hydraulic lime and a pozzolan produces a hydraulic mixture (see also: Pozzolanic reaction), but concrete made from such mixtures was used by the Ancient Macedonians and three centuries later on a large scale by Roman engineers. The Greeks used volcanic tuff from the island of Thera as their pozzolan and the Romans used volcanic tuff from around the Bay of Naples. In the absence of this, the Romans used powdered brick or pottery as a substitute and they may have used crushed tiles for this purpose before discovering natural sources near Rome. The huge dome of the Pantheon in Rome and the massive Baths of Caracalla are examples of ancient structures made from these concretes, many of which are still standing. The vast system of Roman aqueducts also made extensive use of hydraulic cement. Although any preservation of this knowledge in literary sources from the Middle Ages is unknown, medieval masons and some military engineers maintained an active tradition of using hydraulic cement in structures such as canals, fortresses, harbors, and shipbuilding facilities. This technical knowledge of making hydraulic cement was later formalized by French and British engineers in the 18th century. Tabby, a building material using oyster-shell lime, sand, and whole oyster shells to form a concrete, was introduced to the Americas by the Spanish in the sixteenth century.
Cements in the 18th century.
John Smeaton made an important contribution to the development of cements while planning the construction of the third Eddystone Lighthouse (1755–59) in the English Channel now known as Smeaton's Tower. He needed a hydraulic mortar that would set and develop some strength in the twelve hour period between successive high tides. He performed experiments with combinations of different limestones and additives including trass and pozzolanas and did exhaustive market research on the available hydraulic limes, visiting their production sites, and noted that the "hydraulicity" of the lime was directly related to the clay content of the limestone from which it was made. Smeaton was a civil engineer by profession, and took the idea no further.
In the South Atlantic seaboard of the United Sates, tabby relying upon the oyster-shell middens of earlier Native American populations was used in house construction from the 1730s to the 1860s.
In Britain particularly, good quality building stone became ever more expensive during a period of rapid growth, and it became a common practice to construct prestige buildings from the new industrial bricks, and to finish them with a stucco to imitate stone. Hydraulic limes were favored for this, but the need for a fast set time encouraged the development of new cements. Most famous was Parker's "Roman cement". This was developed by James Parker in the 1780s, and finally patented in 1796. It was, in fact, nothing like material used by the Romans, but was a "natural cement" made by burning septaria – nodules that are found in certain clay deposits, and that contain both clay minerals and calcium carbonate. The burnt nodules were ground to a fine powder. This product, made into a mortar with sand, set in 5–15 minutes. The success of "Roman cement" led other manufacturers to develop rival products by burning artificial hydraulic lime cements of clay and chalk.
Roman cement quickly became popular but was largely replaced by Portland cement in the 1850s.
Cements in the 19th century.
Apparently unaware of Smeaton's work, the same principle was identified by Frenchman Louis Vicat in the first decade of the nineteenth century. Vicat went on to devise a method of combining chalk and clay into an intimate mixture, and, burning this, produced an "artificial cement" in 1817 considered the "principal forerunner" of Portland cement and "...Edgar Dobbs of Southwark patented a cement of this kind in 1811."
In Russia, Egor Cheliev created a new binder by mixing lime and clay. His results were published in 1822 in his book "A Treatise on the Art to Prepare a Good Mortar" published in St. Petersburg. A few years later in 1825, he published another book, which described the various methods of making cement and concrete, as well as the benefits of cement in the construction of buildings and embankments.
James Frost, working in Britain, produced what he called "British cement" in a similar manner around the same time, but did not obtain a patent until 1822. In 1824, Joseph Aspdin patented a similar material, which he called "Portland cement", because the render made from it was in color similar to the prestigious Portland stone. However, Aspdins' cement was nothing like modern Portland cement but was a first step in its development, called a "proto-Portland cement". Joseph Aspdins' son William Aspdin had left his fathers company and in his cement manufacturing apparently accidentally produced calcium silicates in the 1840s, a middle step in the development of Portland cement. William Aspdin's innovation was counterintuitive for manufacturers of "artificial cements", because they required more lime in the mix (a problem for his father), a much higher kiln temperature (and therefore more fuel), and the resulting clinker was very hard and rapidly wore down the millstones, which were the only available grinding technology of the time. Manufacturing costs were therefore considerably higher, but the product set reasonably slowly and developed strength quickly, thus opening up a market for use in concrete. The use of concrete in construction grew rapidly from 1850 onward, and was soon the dominant use for cements. Thus Portland cement began its predominant role.
Isaac Charles Johnson further refined the production of "meso-Portland cement" (middle stage of development) and claimed to be the real father of Portland cement.
Setting time and "early strength" are important characteristics of cements. Hydraulic limes, "natural" cements, and "artificial" cements all rely upon their belite content for strength development. Belite develops strength slowly. Because they were burned at temperatures below 1250 C, they contained no alite, which is responsible for early strength in modern cements. The first cement to consistently contain alite was made by William Aspdin in the early 1840s: This was what we call today "modern" Portland cement. Because of the air of mystery with which William Aspdin surrounded his product, others ("e.g.," Vicat and Johnson) have claimed precedence in this invention, but recent analysis of both his concrete and raw cement have shown that William Aspdin's product made at Northfleet, Kent was a true alite-based cement. However, Aspdin's methods were "rule-of-thumb": Vicat is responsible for establishing the chemical basis of these cements, and Johnson established the importance of sintering the mix in the kiln.
In the US the first large-scale use of cement was Rosendale cement, a natural cement mined from a massive deposit of a large dolostone rock deposit discovered in the early 19th century near Rosendale, New York. Rosendale cement was extremely popular for the foundation of buildings ("e.g.", Statue of Liberty, Capitol Building, Brooklyn Bridge) and lining water pipes.
Sorel cement was patented in 1867 by Frenchman Stanislas Sorel and was stronger than Portland cement but its poor water restive and corrosive qualities limited its use in building construction. The next development with the manufacture of Portland cement was the introduction of the rotary kiln which allowed a stronger, more homogeneous mixture and a continuous manufacturing process.
Cements in the 20th century.
Calcium aluminate cements were patented in 1908 in France by Jules Bied for better resistance to sulfates.
In the US, the long curing time of at least a month for Rosendale cement made it unpopular after World War One in the construction of highways and bridges and many states and construction firms turned to the use of Portland cement. Because of the switch to Portland cement, by the end of the 1920s of the 15 Rosendale cement companies, only one had survived. But in the early 1930s it was discovered that, while Portland cement had a faster setting time it was not as durable, especially for highways, to the point that some states stopped building highways and roads with cement. Bertrain H. Wait, an engineer whose company had worked on the construction of the New York City's Catskill Aqueduct, was impressed with the durability of Rosendale cement, and came up with a blend of both Rosendale and synthetic cements which had the good attributes of both: it was highly durable and had a much faster setting time. Mr. Wait convinced the New York Commissioner of Highways to construct an experimental section of highway near New Paltz, New York, using one sack of Rosendale to six sacks of synthetic cement. It was proved a success and for decades the Rosendale-synthetic cement blend became common use in highway and bridge construction.
Modern cements.
Modern hydraulic cements began to be developed from the start of the Industrial Revolution (around 1800), driven by three main needs:
Types of modern cement.
Portland cement.
Portland cement is by far the most common type of cement in general use around the world. This cement is made by heating limestone (calcium carbonate) with small quantities of other materials (such as clay) to 1450 °C in a kiln, in a process known as calcination, whereby a molecule of carbon dioxide is liberated from the calcium carbonate to form calcium oxide, or quicklime, which is then blended with the other materials that have been included in the mix. The resulting hard substance, called 'clinker', is then ground with a small amount of gypsum into a powder to make 'Ordinary Portland Cement', the most commonly used type of cement (often referred to as OPC).
Portland cement is a basic ingredient of concrete, mortar and most non-specialty grout. The most common use for Portland cement is in the production of concrete. Concrete is a composite material consisting of aggregate (gravel and sand), cement, and water. As a construction material, concrete can be cast in almost any shape desired, and once hardened, can become a structural (load bearing) element. Portland cement may be grey or white.
Portland cement blends.
Portland cement blends are often available as inter-ground mixtures from cement producers, but similar formulations are often also mixed from the ground components at the concrete mixing plant.
Portland blastfurnace cement contains up to 70% ground granulated blast furnace slag, with the rest Portland clinker and a little gypsum. All compositions produce high ultimate strength, but as slag content is increased, early strength is reduced, while sulfate resistance increases and heat evolution diminishes. Used as an economic alternative to Portland sulfate-resisting and low-heat cements.
Portland flyash cement contains up to 35% fly ash. The fly ash is pozzolanic, so that ultimate strength is maintained. Because fly ash addition allows a lower concrete water content, early strength can also be maintained. Where good quality cheap fly ash is available, this can be an economic alternative to ordinary Portland cement.
Portland pozzolan cement includes fly ash cement, since fly ash is a pozzolan, but also includes cements made from other natural or artificial pozzolans. In countries where volcanic ashes are available (e.g. Italy, Chile, Mexico, the Philippines) these cements are often the most common form in use.
Portland silica fume cement. Addition of silica fume can yield exceptionally high strengths, and cements containing 5–20% silica fume are occasionally produced. However, silica fume is more usually added to Portland cement at the concrete mixer.
Masonry cements are used for preparing bricklaying mortars and stuccos, and must not be used in concrete. They are usually complex proprietary formulations containing Portland clinker and a number of other ingredients that may include limestone, hydrated lime, air entrainers, retarders, waterproofers and coloring agents. They are formulated to yield workable mortars that allow rapid and consistent masonry work. Subtle variations of Masonry cement in the US are Plastic Cements and Stucco Cements. These are designed to produce controlled bond with masonry blocks.
Expansive cements contain, in addition to Portland clinker, expansive clinkers (usually sulfoaluminate clinkers), and are designed to offset the effects of drying shrinkage that is normally encountered with hydraulic cements. This allows large floor slabs (up to 60 m square) to be prepared without contraction joints.
White blended cements may be made using white clinker and white supplementary materials such as high-purity metakaolin.
Colored cements are used for decorative purposes. In some standards, the addition of pigments to produce "colored Portland cement" is allowed. In other standards (e.g. ASTM), pigments are not allowed constituents of Portland cement, and colored cements are sold as "blended hydraulic cements".
Very finely ground cements are made from mixtures of cement with sand or with slag or other pozzolan type minerals that are extremely finely ground together. Such cements can have the same physical characteristics as normal cement but with 50% less cement particularly due to their increased surface area for the chemical reaction. Even with intensive grinding they can use up to 50% less energy to fabricate than ordinary Portland cements.
Pozzolan-lime cements. Mixtures of ground pozzolan and lime are the cements used by the Romans, and can be found in Roman structures still standing (e.g. the Pantheon in Rome). They develop strength slowly, but their ultimate strength can be very high. The hydration products that produce strength are essentially the same as those produced by Portland cement.
Slag-lime cements. Ground granulated blast furnace slag is not hydraulic on its own, but is "activated" by addition of alkalis, most economically using lime. They are similar to pozzolan lime cements in their properties. Only granulated slag (i.e. water-quenched, glassy slag) is effective as a cement component.
Supersulfated cements contain about 80% ground granulated blast furnace slag, 15% gypsum or anhydrite and a little Portland clinker or lime as an activator. They produce strength by formation of ettringite, with strength growth similar to a slow Portland cement. They exhibit good resistance to aggressive agents, including sulfate.
Calcium aluminate cements are hydraulic cements made primarily from limestone and bauxite. The active ingredients are monocalcium aluminate CaAl2O4 (CaO · Al2O3 or CA in Cement chemist notation, CCN) and mayenite Ca12Al14O33 (12 CaO · 7 Al2O3, or C12A7 in CCN). Strength forms by hydration to calcium aluminate hydrates. They are well-adapted for use in refractory (high-temperature resistant) concretes, e.g. for furnace linings.
Calcium sulfoaluminate cements are made from clinkers that include ye'elimite (Ca4(AlO2)6SO4 or C4A3S in Cement chemist's notation) as a primary phase. They are used in expansive cements, in ultra-high early strength cements, and in "low-energy" cements. Hydration produces ettringite, and specialized physical properties (such as expansion or rapid reaction) are obtained by adjustment of the availability of calcium and sulfate ions. Their use as a low-energy alternative to Portland cement has been pioneered in China, where several million tonnes per year are produced. Energy requirements are lower because of the lower kiln temperatures required for reaction, and the lower amount of limestone (which must be endothermically decarbonated) in the mix. In addition, the lower limestone content and lower fuel consumption leads to a CO2 emission around half that associated with Portland clinker. However, SO2 emissions are usually significantly higher.
"Natural" cements correspond to certain cements of the pre-Portland era, produced by burning argillaceous limestones at moderate temperatures. The level of clay components in the limestone (around 30–35%) is such that large amounts of belite (the low-early strength, high-late strength mineral in Portland cement) are formed without the formation of excessive amounts of free lime. As with any natural material, such cements have highly variable properties.
Geopolymer cements are made from mixtures of water-soluble alkali metal silicates and aluminosilicate mineral powders such as fly ash and metakaolin.
Curing (setting).
Cement sets or cures when mixed with water which causes a series of hydration chemical reactions. The constituents slowly hydrate and crystallize; the interlocking of the crystals gives cement its strength. Maintaining a high moisture content in cement during curing increases both the speed of curing, and its final strength. Gypsum is often added to Portland cement to prevent early hardening or "flash setting", allowing a longer working time. The time it takes for cement to cure varies depending on the mixture and environmental conditions; initial hardening can occur in as little as twenty minutes, while full cure can take over a month. Cement typically cures to the extent that it can be put into service within 24 hours to a week.
Safety issues.
Bags of cement routinely have health and safety warnings printed on them because not only is cement highly alkaline, but the setting process is exothermic. As a result, wet cement is strongly caustic and can easily cause severe skin burns if not promptly washed off with water. Similarly, dry cement powder in contact with mucous membranes can cause severe eye or respiratory irritation. Some ingredients can be specifically allergenic and may cause allergic dermatitis. Reducing agents are sometimes added to cement to prevent the formation of carcinogenic chromate in cement. Cement users should wear protective clothing.
Cement industry in the world.
In 2010, the world production of hydraulic cement was 3,300 million tonnes. The top three producers were China with 1,800, India with 220, and USA with 63.5 million tonnes for a combined total of over half the world total by the world's three most populated states.
For the world capacity to produce cement in 2010, the situation was similar with the top three states (China, India, and USA) accounting for just under half the world total capacity.
Over 2011 and 2012, global consumption continued to climb, rising to 3585 Mt in 2011 and 3736 Mt in 2012, while annual growth rates eased to 8.3% and 4.2%, respectively.
China, representing an increasing share of world cement consumption, continued to be the main engine of global growth. By 2012, Chinese demand was recorded at 2160 Mt, representing 58% of world consumption. Annual growth rates, which reached 16% in 2010, appear to have softened, slowing to 5–6% over 2011 and 2012, as China’s economy targets a more sustainable growth rate.
Outside of China, worldwide consumption climbed by 4.4% to 1462 Mt in 2010, 5% to 1535 Mt in 2011, and finally 2.7% to 1576 Mt in 2012.
Iran is now the 3rd largest cement producer in the world and has increased its output by over 10% from 2008 to 2011. Due to climbing energy costs in Pakistan and other major cement-producing countries, Iran is a unique position as a trading partner, utilizing its own surplus petroleum to power clinker plants. Now a top producer in the Middle-East, Iran is further increasing its dominant position in local markets and abroad.
The performance in North America and Europe over the 2010–12 period contrasted strikingly with that of China, as the global financial crisis evolved into a sovereign debt crisis for many economies in this region and recession. Cement consumption levels for this region fell by 1.9% in 2010 to 445 Mt, recovered by 4.9% in 2011, then dipped again by 1.1% in 2012.
The performance in the rest of the world, which includes many emerging economies in Asia, Africa and Latin America and representing some 1020 Mt cement demand in 2010, was positive and more than offset the declines in North America and Europe. Annual consumption growth was recorded at 7.4% in 2010, moderating to 5.1% and 4.3% in 2011 and 2012, respectively.
As at year-end 2012, the global cement industry consisted of 5673 cement production facilities, including both integrated and grinding, of which 3900 were located in China and 1773 in the rest of the world.
Total cement capacity worldwide was recorded at 5245 Mt in 2012, with 2950 Mt located in China and 2295 Mt in the rest of the world.
China.
"For the past 18 years, China consistently has produced more cement than any other country in the world. [...] (However,) China's cement export peaked in 1994 with 11 million tonnes shipped out and has been in steady decline ever since. Only 5.18 million tonnes were exported out of China in 2002. Offered at $34 a ton, Chinese cement is pricing itself out of the market as Thailand is asking as little as $20 for the same quality."
In 2006, it was estimated that China manufactured 1.235 billion tonnes of cement, which was 44% of the world total cement production. "Demand for cement in China is expected to advance 5.4% annually and exceed 1 billion tonnes in 2008, driven by slowing but healthy growth in construction expenditures. Cement consumed in China will amount to 44% of global demand, and China will remain the world's largest national consumer of cement by a large margin."
In 2010, 3.3 billion tonnes of cement was consumed globally. Of this, China accounted for 1.8 billion tonnes.
Environmental impacts.
Cement manufacture causes environmental impacts at all stages of the process. These include emissions of airborne pollution in the form of dust, gases, noise and vibration when operating machinery and during blasting in quarries, and damage to countryside from quarrying. Equipment to reduce dust emissions during quarrying and manufacture of cement is widely used, and equipment to trap and separate exhaust gases are coming into increased use. Environmental protection also includes the re-integration of quarries into the countryside after they have been closed down by returning them to nature or re-cultivating them.
CO2 emissions.
Carbon concentration in cement spans from ≈5% in cement structures to ≈8% in the case of roads in cement. Cement manufacturing releases CO2 in the atmosphere both directly when calcium carbonate is heated, producing lime and carbon dioxide, and also indirectly through the use of energy if its production involves the emission of CO2. The cement industry produces about 5% of global man-made CO2 emissions, of which 50% is from the chemical process, and 40% from burning fuel.
The amount of CO2 emitted by the cement industry is nearly 900 kg of CO2 for every 1000 kg of cement produced.
In the European union the specific energy consumption for the production of cement clinker has been reduced by approximately 30% since the 1970s. This reduction in primary energy requirements is equivalent to approximately 11 million tonnes of coal per year with corresponding benefits in reduction of CO2 emissions. This accounts for approximately 5% of anthropogenic CO2.
The high proportion of carbon dioxide produced in the chemical reaction leads to a large decrease in mass in the conversion from limestone to cement. So, to reduce the transport of heavier raw materials and to minimize the associated costs, it is more economical for cement plants to be closer to the limestone quarries rather than to the consumer centers.
In certain applications, lime mortar reabsorbs the same amount of CO2 as was released in its manufacture, and has a lower energy requirement in production than mainstream cement. Newly developed cement types from Novacem and Eco-cement can absorb carbon dioxide from ambient air during hardening. Use of the Kalina cycle during production can also increase energy efficiency.
Heavy metal emissions in the air.
In some circumstances, mainly depending on the origin and the composition of the raw materials used, the high-temperature calcination process of limestone and clay minerals can release in the atmosphere gases and dust rich in volatile heavy metals, a.o, thallium, cadmium and mercury are the most toxic. Heavy metals (Tl, Cd, Hg, ...) are often found as trace elements in common metal sulfides (pyrite (FeS2), zinc blende (ZnS), galena (PbS), ...) present as secondary minerals in most of the raw materials. Environmental regulations exist in many countries to limit these emissions. As of 2011 in the United States, cement kilns are "legally allowed to pump more toxins into the air than are hazardous-waste incinerators."
Heavy metals present in the clinker.
The presence of heavy metals in the clinker arises both from the natural raw materials and from the use of recycled by-products or alternative fuels. The high pH prevailing in the cement porewater (12.5 < pH < 13.5) limits the mobility of many heavy metals by decreasing their solubility and increasing their sorption onto the cement mineral phases. Nickel, zinc and lead are commonly found in cement in non-negligible concentrations.
Use of alternative fuels and by-products materials.
A cement plant consumes 3 to 6 GJ of fuel per tonne of clinker produced, depending on the raw materials and the process used. Most cement kilns today use coal and petroleum coke as primary fuels, and to a lesser extent natural gas and fuel oil. Selected waste and by-products with recoverable calorific value can be used as fuels in a cement kiln (referred to as co-processing), replacing a portion of conventional fossil fuels, like coal, if they meet strict specifications. Selected waste and by-products containing useful minerals such as calcium, silica, alumina, and iron can be used as raw materials in the kiln, replacing raw materials such as clay, shale, and limestone. Because some materials have both useful mineral content and recoverable calorific value, the distinction between alternative fuels and raw materials is not always clear. For example, sewage sludge has a low but significant calorific value, and burns to give ash containing minerals useful in the clinker matrix.
Normal operation of cement kilns provides combustion conditions which are more than adequate for the destruction of even the most difficult to destroy organic substances. This is primarily due to the very high temperatures of the kiln gases (2000 °C in the combustion gas from the main burners and 1100 °C in the gas from the burners in the precalciner). The gas residence time at high temperature in the rotary kiln is of the order of 5–10 seconds and in the precalciner more than 3 seconds.
Due to bovine spongiform encephalopathy (BSE) in the European beef industry, the use of animal-derived products to feed cattle is now severely restricted. Large quantities of waste animal meat and bone meal (MBM), also known as animal flour, have to be safely disposed of or transformed. The production of cement kilns, together with the incineration, is to date one of the two main ways to treat this solid effluent of the food industry.
Green cement.
Green cement is a cementitious material that meets or exceeds the functional performance capabilities of ordinary Portland cement by incorporating and optimizing recycled materials, thereby reducing consumption of natural raw materials, water, and energy, resulting in a more sustainable construction material.
The manufacturing process for green cement succeeds in reducing, and even eliminating, the production and release of damaging pollutants and greenhouse gasses, particularly CO2.
Growing environmental concerns and increasing cost of fuels of fossil origin have resulted in many countries in sharp reduction of the resources needed to produce cement and effluents (dust and exhaust gases).
Peter Trimble, a design student at the University of Edinburgh has proposed 'DUPE' based on sporosarcina pasteurii, a bacterium with binding qualities which, when mixed with sand and urine produces a concrete said to be 70% as strong as conventional materials.

</doc>
<doc id="6671" url="http://en.wikipedia.org/wiki?curid=6671" title="Cincinnati Reds">
Cincinnati Reds

The Cincinnati Reds are an American professional baseball team based in Cincinnati, Ohio. As a member of Major League Baseball, they compete in the Central Division of the National League. The Cincinnati Reds organization was officially established in 1881 as an independent club and became a charter member of the American Association in 1882. The team later joined the National League in 1890. The club traces its origins back to the 1869 Cincinnati Red Stockings, widely recognized as baseball's first all-professional team, which disbanded in 1870.
Following the introduction of divisions, the Reds played in the National League West from 1969 to 1993 before joining the Central Division when it was formed in 1994. The Reds have won five World Series titles accompanied by nine National League pennants, one American Association pennant and ten division titles. They play at Great American Ball Park, opened in 2003 as a replacement for the team's previous home, Riverfront Stadium. Bob Castellini was appointed the chief executive officer of the organization in 2006.
Franchise history.
The Birth of the Reds and the American Association (1881–1889).
The origins of the modern Cincinnati Reds can be traced to the expulsion of an earlier team bearing that name. In 1876, Cincinnati became one of the charter members of the new National League, but the club ran afoul of league organizer and long-time president William Hulbert for selling beer at the ballpark and playing games on Sunday, both important activities to entice the city's large German population. While Hulbert made clear his distaste for both beer and Sunday baseball at the founding of the league, neither practice was actually against league rules in those early years. On October 6, 1880, however, seven of the eight team owners pledged at a special league meeting to formally ban both beer and Sunday baseball at the regular league meeting that December. Only Cincinnati president W. H. Kennett refused to sign the pledge, so the other owners formally expelled Cincinnati for violating a rule that would not actually go into effect for two more months.
Cincinnati's expulsion from the National League incensed "Cincinnati Enquirer" sports editor O. P. Caylor, who made two attempts to form a new league on behalf of the receivers for the now bankrupt Reds franchise. When these attempts failed, he formed a new independent ballclub known as the Red Stockings in the Spring of 1881, and brought the team to St. Louis for a weekend exhibition. The Reds' first game was a 12–3 victory over the St. Louis club. The Reds have played each year since then, without going on hiatus (as the Cubs did, out of necessity) or relocating (like the Braves), making Cincinnati the oldest club to have played every year of its existence in one city. After the 1881 series proved a success, Caylor and a former president of the old Reds named Justus Thorner received an invitation from Philadelphia businessman Horace Phillips to attend a meeting of several clubs in Pittsburgh with the intent of establishing a rival to the National League. Upon arriving in the city, however, Caylor and Thorner discovered that no other owners had decided to accept the invitation, with even Phillips not bothering to attend his own meeting. By chance, the duo met a former pitcher named Al Pratt, who hooked them up with former Pittsburgh Alleghenys president H. Denny McKnight. Together, the three men hatched a scheme to form a new league by sending a telegram to each of the other owners who were supposed to attend the meeting stating that he was the only person who did not attend and that everyone else was enthusiastic about the new venture and eager to attend a second meeting in Cincinnati. The ploy worked, and the American Association was officially formed at the Hotel Gibson in Cincinnati with the new Reds a charter member with Thorner as president.
Led by the hitting of third baseman Hick Carpenter, the defense of future Hall of Fame second baseman Bid McPhee, and the pitching of 40-game-winner Will White, the Reds won the inaugural AA pennant in 1882. With the establishment of the Union Association Justus Thorner left the club to finance the Cincinnati Outlaw Reds and managed to acquire the lease on the Reds Bank Street Grounds playing field, forcing new president Aaron Stern to relocate three blocks away at the hastily built League Park. The club never placed higher than second or lower than fifth for the rest of its tenure in the American Association.
The National League returns to Cincinnati (1890–1911).
The Cincinnati Red Stockings left the American Association on November 14, 1889 and joined the National League along with the Brooklyn Bridegrooms after a dispute with St. Louis Browns owner Chris Von Der Ahe over the selection of a new league president. The National League was happy to accept the teams in part due to the emergence of the new Player's League. This new league, an early failed attempt to break the reserve clause in baseball, threatened both existing leagues. Because the National League decided to expand while the American Association was weakening, the team accepted an invitation to join the National League. It was also at this time that the team first shortened their name from "Red Stockings" to "Reds". The Reds wandered through the 1890s signing local stars and aging veterans. During this time, the team never finished above third place (1897) and never closer than 10½ games (1890).
At the start of the 20th century, the Reds had hitting stars Sam Crawford and Cy Seymour. Seymour's .377 average in 1905 was the first individual batting crown won by a Red. In 1911, Bob Bescher stole 81 bases, which is still a team record. Like the previous decade, the 1900s (decade) were not kind to the Reds, as much of the decade was spent in the league's second division.
Redland Field to the Great Depression (1912–1932).
In 1912, the club opened a new steel-and-concrete ballpark, Redland Field (later to be known as Crosley Field). The Reds had been playing baseball on that same site, the corner of Findlay and Western Avenues on the city's west side, for 28 years, in wooden structures that had been occasionally damaged by fires. By the late 1910s the Reds began to come out of the second division. The 1918 team finished fourth, and new manager Pat Moran led the Reds to an NL pennant in 1919, in what the club advertised as its "Golden Anniversary". The 1919 team had hitting stars Edd Roush and Heinie Groh while the pitching staff was led by Hod Eller and left-hander Harry "Slim" Sallee. The Reds finished ahead of John McGraw's New York Giants, and then won the world championship in eight games over the Chicago White Sox.
By 1920, the "Black Sox" scandal had brought a taint to the Reds' first championship. After 1926, and well into the 1930s, the Reds were second division dwellers. Eppa Rixey, Dolf Luque and Pete Donohue were pitching stars, but the offense never lived up to the pitching. By 1931, the team was bankrupt, the Great Depression was in full swing and Redland Field was in a state of disrepair.
Championship Baseball and revival (1933–1940).
Powel Crosley, Jr., an electronics magnate who, with his brother Lewis M. Crosley, produced radios, refrigerators, and other household items, bought the Reds out of bankruptcy in 1933, and hired Larry MacPhail to be the General Manager. Crosley had started WLW radio, the Reds flagship radio broadcaster, and the Crosley Broadcasting Corporation in Cincinnati, where he was also a prominent civic leader. MacPhail began to develop the Reds' minor league system and expanded the Reds' fan base. The Reds, throughout the 1930s, became a team of "firsts". The now-renamed Crosley Field became the host of the first night game in 1935, which was also the first baseball fireworks night, the fireworks at the game were shot by Joe Rozzi of Rozzi's Famous Fireworks. Johnny Vander Meer became the only pitcher in major league history to throw back-to-back no-hitters in 1938. Thanks to Vander Meer, Paul Derringer and second baseman/third baseman-turned-pitcher Bucky Walters, the Reds had a solid pitching staff. The offense came around in the late 1930s. By 1938 the Reds, now led by manager Bill McKechnie, were out of the second division finishing fourth. Ernie Lombardi was named the National League's Most Valuable Player in 1938. By 1939, they were National League champions, but in the World Series, they were swept by the New York Yankees. In 1940, they repeated as NL Champions, and for the first time in 21 years, the Reds captured a World championship, beating the Detroit Tigers 4 games to 3. Frank McCormick was the 1940 NL MVP. Other position players included Harry Craft, Lonny Frey, Ival Goodman, Lew Riggs and Bill Werber.
1941–1969.
World War II and age finally caught up with the Reds. Throughout the 1940s and early 1950s, Cincinnati finished mostly in the second division. In 1944, Joe Nuxhall (who was later to become part of the radio broadcasting team), at age 15, pitched for the Reds on loan from Wilson Junior High school in Hamilton, Ohio. He became the youngest player ever to appear in a major league game—a record that still stands today. Ewell "The Whip" Blackwell was the main pitching stalwart before arm problems cut short his career. Ted Kluszewski was the NL home run leader in 1954. The rest of the offense was a collection of over-the-hill players and not-ready-for-prime-time youngsters.
In April 1953, the Reds, fearing that their traditional club nickname would associate them with the threat of Communism, officially changed the name of the team to the "Cincinnati Redlegs". From 1956 to 1960, the club's logo was altered to remove the term "REDS" from the inside of the "wishbone "C"" symbol. The "REDS" reappeared on the 1961 uniforms, but the point of the "C" was removed, leaving a smooth, non-wishbone curve. The traditional home-uniform logo was restored in 1967.
In 1956, led by National League Rookie of the Year Frank Robinson, the Redlegs hit 221 HR to tie the NL record. By 1961, Robinson was joined by Vada Pinson, Wally Post, Gordy Coleman, and Gene Freese. Pitchers Joey Jay, Jim O'Toole, and Bob Purkey led the staff.
The Reds captured the 1961 National League pennant, holding off the Los Angeles Dodgers and the San Francisco Giants, only to be defeated by the perennially powerful New York Yankees in the World Series.
The Reds had winning teams during the rest of the 1960s, but did not produce any championships. They won 98 games in 1962, paced by Purkey's 23, but finished third. In 1964, they lost the pennant by one game to the Cardinals after having taken first place when the Phillies collapsed in September. Their beloved manager Fred Hutchinson died of cancer just weeks after the end of the 1964 season. The failure of the Reds to win the 1964 pennant led to owner Bill DeWitt's selling off key components of the team, in anticipation of relocating the franchise. In response to DeWitt's threatened move, the women of Cincinnati banded together to form the Rosie Reds to urge DeWitt to keep the franchise in Cincinnati. The Rosie Reds are still in existence, and are currently the oldest fan club in Major League Baseball. After the 1965 season he executed what may be the most lopsided trade in baseball history, sending former Most Valuable Player Frank Robinson to the Baltimore Orioles for pitchers Milt Pappas and Jack Baldschun, and outfielder Dick Simpson. Robinson went on to win the MVP and triple crown in the American league for 1966, and lead Baltimore to its first ever World Series title in a sweep of the Los Angeles Dodgers. The Reds did not recover from this trade until the rise of the "Big Red Machine" of the 1970s.
Starting in the early 1960s, the Reds' farm system began producing a series of stars, including Jim Maloney (the Reds' pitching ace of the 1960s), Pete Rose, Tony Pérez, Johnny Bench, Lee May, Tommy Helms, Bernie Carbo, Hal McRae, Dave Concepción, and Gary Nolan. The tipping point came in 1967 with the appointment of Bob Howsam as general manager. That same year the Reds avoided a move to San Diego when the city of Cincinnati and Hamilton County agreed to build a state of the art, downtown stadium on the edge of the Ohio River. The Reds entered into a 30-year lease in exchange for the stadium commitment keeping the franchise in its original home city. In a series of strategic moves, Howsam brought in key personnel to complement the homegrown talent. The Reds' final game at Crosley Field, home to more than 4,500 baseball games, was played on June 24, 1970, a 5–4 victory over the San Francisco Giants.
Under Howsam's administration starting in the late 1960s, the Reds instituted a strict rule barring the team's players from wearing facial hair and long hair. The clean cut look was meant to present the team as wholesome in an era of turmoil. All players coming to the Reds were required to shave and cut their hair for the next three decades. Over the years, the rule was controversial, but persisted well into the ownership of Marge Schott. On at least one occasion, in the early 1980s, enforcement of this rule lost them the services of star reliever and Ohio native Rollie Fingers, who would not shave his trademark handlebar mustache in order to join the team. The rule was not officially rescinded until 1999 when the Reds traded for slugger Greg Vaughn, who had a goatee. The New York Yankees continue to have a similar rule today, though unlike the Reds during this period, Yankees players are permitted to have mustaches. Much like when players leave the Yankees today, players who left the Reds took advantage with their new teams; Pete Rose, for instance, grew his hair out much longer than would be allowed by the Reds once he signed with the Philadelphia Phillies in 1979.
The Reds' rules also included conservative uniforms. In Major League Baseball, a club generally provides most of the equipment and clothing needed for play. However, players are required to supply their gloves and shoes themselves. Many players enter into sponsorship arrangements with shoe manufacturers, but through the mid-1980s, the Reds had a strict rule that players were to wear only plain black shoes with no prominent logo. Reds players decried what they considered to be the boring color choice as well as the denial of the opportunity to earn more money through shoe contracts. A compromise was struck in which players were allowed to wear red shoes.
The Big Red Machine (1970–1976).
In 1970, little known George "Sparky" Anderson was hired as manager, and the Reds embarked upon a decade of excellence, with a team that came to be known as "The Big Red Machine". Playing at Crosley Field until June 30, 1970, when the Reds moved into brand-new Riverfront Stadium, a 52,000 seat multi-purpose venue on the shores of the Ohio River, the Reds began the 1970s with a bang by winning 70 of their first 100 games. Johnny Bench, Tony Pérez, Pete Rose, Lee May and Bobby Tolan were the early Red Machine offensive leaders; Gary Nolan, Jim Merritt, Wayne Simpson and Jim McGlothlin led a pitching staff which also contained veterans Tony Cloninger and Clay Carroll and youngsters Pedro Borbón and Don Gullett. The Reds breezed through the 1970 season, winning the NL West and captured the NL pennant by sweeping the Pittsburgh Pirates in three games. By the time the club got to the World Series, however, the Reds pitching staff had run out of gas and the veteran Baltimore Orioles beat the Reds in five games.
After the disastrous 1971 season (the only season of the 1970s during which the Reds finished with a losing record) the Reds reloaded by trading veterans Jimmy Stewart, May, and Tommy Helms for Joe Morgan, César Gerónimo, Jack Billingham, Ed Armbrister, and Denis Menke. Meanwhile, Dave Concepción blossomed at shortstop. 1971 was also the year a key component of the future world championships was acquired in George Foster from the San Francisco Giants in a trade for shortstop Frank Duffy.
The 1972 Reds won the NL West in baseball's first ever strike-shortened season and defeated the Pittsburgh Pirates in an exciting five-game playoff series. They then faced the Oakland Athletics in the World Series. Six of the seven games were won by one run. With powerful slugger Reggie Jackson sidelined due to an injury incurred during Oakland's playoff series, Ohio native Gene Tenace got a chance to play in the series, delivering four home runs that tied the World Series record for homers, propelling Oakland to a dramatic seven-game series win. This was one of the few World Series in which no starting pitcher for either side pitched a complete game.
The Reds won a third NL West crown in 1973 after a dramatic second half comeback, that saw them make up 10 1⁄2 games on the Los Angeles Dodgers after the All-Star break. However they lost the NL pennant to the New York Mets in five games in the NLCS. In game one, Tom Seaver faced Jack Billingham in a classic pitching duel, with all three runs of the 2–1 margin being scored on home runs. John Milner provided New York's run off Billingham, while Pete Rose tied the game in the seventh inning off Seaver, setting the stage for a dramatic game ending home run by Johnny Bench in the bottom of the ninth. The New York series provided plenty of controversy with the riotous behavior of Shea Stadium fans towards Pete Rose when he and Bud Harrelson scuffled after a hard slide by Rose into Harrelson at second base during the fifth inning of Game 3. A full bench-clearing fight resulted after Harrelson responded to Rose's aggressive move to prevent him from completing a double play by calling him a name. This also led to two more incidents in which play was stopped. The Reds trailed 9–3 and New York's manager, Yogi Berra, and legendary outfielder Willie Mays, at the request of National League president Warren Giles, appealed to fans in left field to restrain themselves. The next day the series was extended to a fifth game when Rose homered in the 12th inning to tie the series at two games each.
The Reds won 98 games in 1974 but they finished second to the 102-win Los Angeles Dodgers. The 1974 season started off with much excitement, as the Atlanta Braves were in town to open the season with the Reds. Hank Aaron entered opening day with 713 home runs, one shy of tying Babe Ruth's record of 714. The first pitch Aaron swung at in the 1974 season was the record tying home run off Jack Billingham. The next day the Braves benched Aaron, hoping to save him for his record breaking home run on their season opening homestand. The commissioner of baseball, Bowie Kuhn, ordered Braves management to play Aaron the next day, where he narrowly missed the historic home run in the fifth inning. Aaron went on to set the record in Atlanta two nights later. 1974 also was the debut of Hall of Fame radio announcer Marty Brennaman, who replaced Al Michaels, after Michaels left the Reds to broadcast for the San Francisco Giants.
With 1975, the Big Red Machine lineup solidified with the "Great Eight" starting team of Johnny Bench (catcher), Tony Pérez (first base), Joe Morgan (second base), Dave Concepción (shortstop), Pete Rose (third base), Ken Griffey (right field), César Gerónimo (center field), and George Foster (left field). The starting pitchers included Don Gullett, Fred Norman, Gary Nolan, Jack Billingham, Pat Darcy, and Clay Kirby. The bullpen featured Rawly Eastwick and Will McEnaney combining for 37 saves, and veterans Pedro Borbón and Clay Carroll. On Opening Day, Rose still played in left field, Foster was not a starter, while John Vukovich, an off-season acquisition, was the starting third baseman. While Vuckovich was a superb fielder, he was a weak hitter. In May, with the team off to a slow start and trailing the Dodgers, Sparky Anderson made a bold move by moving Rose to third base, a position where he had very little experience, and inserting Foster in left field. This was the jolt that the Reds needed to propel them into first place, with Rose proving to be reliable on defense, while adding Foster to the outfield gave the offense some added punch. During the season, the Reds compiled two notable streaks: (1) by winning 41 out of 50 games in one stretch, and (2) by going a month without committing any errors on defense.
In the 1975 season, Cincinnati clinched the NL West with 108 victories, then swept the Pittsburgh Pirates in three games to win the NL pennant. In the World Series, the Boston Red Sox were the opponents. After splitting the first four games, the Reds took Game 5. After a three-day rain delay, the two teams met in Game 6, one of the most memorable baseball games ever played and considered by many to be the best World Series game ever. The Reds were ahead 6–3 with 5 outs left, when the Red Sox tied the game on former Red Bernie Carbo's three-run home run. It was Carbo's second pinch-hit three-run homer in the series. After a few close-calls either way, Carlton Fisk hit a dramatic 12th inning home run off the foul pole in left field to give the Red Sox a 7–6 win and force a deciding Game 7. Cincinnati prevailed the next day when Morgan's RBI single won Game 7 and gave the Reds their first championship in 35 years. The Reds have not lost a World Series game since Carlton Fisk's home run, a span of 9 straight wins.
1976 saw a return of the same starting eight in the field. The starting rotation was again led by Nolan, Gullett, Billingham, and Norman, while the addition of rookies Pat Zachry and Santo Alcalá comprised an underrated staff in which four of the six had ERAs below 3.10. Eastwick, Borbon, and McEnaney shared closer duties, recording 26, 8, and 7 saves respectively. The Reds won the NL West by ten games. They went undefeated in the postseason, sweeping the Philadelphia Phillies (winning Game 3 in their final at-bat) to return to the World Series. They continued to dominate by sweeping the Yankees in the newly renovated Yankee Stadium, the first World Series games played in Yankee Stadium since 1964. This was only the second ever sweep of the Yankees in the World Series. In winning the Series, the Reds became the first NL team since the 1921–22 New York Giants to win consecutive World Series championships, and the Big Red Machine of 1975–76 is considered one of the best teams ever. So far in MLB history, the 1975 and '76 Reds were the last NL team to repeat as champions.
Beginning with the 1970 National League pennant, the Reds beat either the Philadelphia Phillies or the Pittsburgh Pirates to win their pennants (Pirates in 1970, 1972, 1975, and 1990, Phillies in 1976), making The Big Red Machine part of the rivalry between the two Pennsylvania teams. In 1979, Pete Rose added further fuel in The Big Red Machine being part of the rivalry when he signed with the Phillies and helped them win their first World Series championship in 1980.
The Machine dismantled (1977–1989).
The later years of the 1970s brought turmoil and change. Popular Tony Pérez was sent to Montreal after the 1976 season, breaking up the Big Red Machine's starting lineup. Manager Sparky Anderson and General Manager Bob Howsam later considered this trade the biggest mistake of their careers. Starting pitcher Don Gullett left via free agency and signed with the New York Yankees. In an effort to fill that gap, a trade with the Oakland A's for starting ace Vida Blue was arranged during the 1976–77 off-season. However, Bowie Kuhn, the Commissioner of Baseball, vetoed the trade for the stated reason of maintaining competitive balance in baseball. Some have suggested that the actual reason had more to due with Kuhn's continued feud with Oakland A's owner Charlie Finley. On June 15, 1977, the Reds acquired Mets' franchise pitcher Tom Seaver for Pat Zachry, Doug Flynn, Steve Henderson, and Dan Norman. In other deals that proved to be less successful, the Reds traded Gary Nolan to the Angels for Craig Hendrickson, Rawly Eastwick to St. Louis for Doug Capilla and Mike Caldwell to Milwaukee for Rick O'Keeffe and Garry Pyka, and got Rick Auerbach from Texas. The end of the Big Red Machine era was heralded by the replacement of General Manager Bob Howsam with Dick Wagner.
In Rose's last season as a Red, he gave baseball a thrill as he challenged Joe DiMaggio's 56-game hitting streak, tying for the second-longest streak ever at 44 games. The streak came to an end in Atlanta after striking out in his fifth at bat in the game against Gene Garber. Rose also earned his 3,000th hit that season, on his way to becoming baseball's all-time hits leader when he rejoined the Reds in the mid-1980s. The year also witnessed the only no-hitter of Hall of Fame pitcher Tom Seaver's career, coming against the St. Louis Cardinals on June 16, 1978.
After the 1978 season and two straight second-place finishes, Wagner fired manager Anderson—an unpopular move. Pete Rose, who since 1963 had played almost every position for the team except pitcher and catcher, signed with Philadelphia as a free agent. By 1979, the starters were Bench (c), Dan Driessen (1b), Morgan (2b), Concepción (ss), Ray Knight (3b), with Griffey, Foster, and Geronimo again in the outfield. The pitching staff had experienced a complete turnover since 1976 except for Fred Norman. In addition to ace starter Tom Seaver; the remaining starters were Mike LaCoss, Bill Bonham, and Paul Moskau. In the bullpen, only Borbon had remained. Dave Tomlin and Mario Soto worked middle relief with Tom Hume and Doug Bair closing. The Reds won the 1979 NL West behind the pitching of Tom Seaver but were dispatched in the NL playoffs by Pittsburgh. Game 2 featured a controversial play in which a ball hit by Pittsburgh's Phil Garner was caught by Cincinnati outfielder Dave Collins but was ruled a trap, setting the Pirates up to take a 2–1 lead. The Pirates swept the series 3 games to 0 and went on to win the World Series against the Baltimore Orioles.
The 1981 team fielded a strong lineup, but with only Concepción, Foster, and Griffey retaining their spots from the 1975–76 heyday. After Johnny Bench was able to play only a few games at catcher each year after 1980 due to ongoing injuries, Joe Nolan took over as starting catcher. Driessen and Bench shared 1st base, and Knight starred at third. Morgan and Geronimo had been replaced at second base and center field by Ron Oester and Dave Collins. Mario Soto posted a banner year starting on the mound, only surpassed by the outstanding performance of Seaver's Cy Young runner-up season. La Coss, Bruce Berenyi, and Frank Pastore rounded out the starting rotation. Hume again led the bullpen as closer, joined by Bair and Joe Price. In 1981, Cincinnati had the best overall record in baseball, but they finished second in the division in both of the half-seasons that were created after a mid-season players' strike, and missed the playoffs. To commemorate this, a team photo was taken, accompanied by a banner that read "Baseball's Best Record 1981".
By 1982, the Reds were a shell of the original Red Machine; they lost 101 games that year. Johnny Bench, after an unsuccessful transition to 3rd base, retired a year later.
After the heartbreak of 1981, General Manager Dick Wagner pursued the strategy of ridding the team of veterans including third-baseman Knight and the entire starting outfield of Griffey, Foster, and Collins. Bench, after being able to catch only seven games in 1981, was moved from platooning at first base to be the starting third baseman; Alex Treviño became the regular starting catcher. The outfield was staffed with Paul Householder, César Cedeño, and future Colorado Rockies & Pittsburgh Pirates manager Clint Hurdle on opening day. Hurdle was an immediate bust, and rookie Eddie Milner took his place in the starting outfield early in the year. The highly touted Householder struggled throughout the year despite extensive playing time. Cedeno, while providing steady veteran play, was a disappointment, and was unable to recapture his glory days with the Houston Astros. The starting rotation featured the emergence of a dominant Mario Soto, and featured strong years by Pastore and Bruce Berenyi, but Seaver was injured all year, and their efforts were wasted without a strong offensive lineup. Tom Hume still led the bullpen, along with Joe Price. But the colorful Brad "The Animal" Lesley was unable to consistently excel, and former all-star Jim Kern was a big disappointment. Kern was also publicly upset over having to shave off his prominent beard to join the Reds, and helped force the issue of getting traded during mid-season by growing it back.
The Reds fell to the bottom of the Western Division for the next few years. After his injury-riddled 1982 season, Seaver was traded back to the Mets. The year 1983 found Dann Bilardello behind the plate, Bench returning to part-time duty at first base, rookies Nick Esasky taking over at third base and Gary Redus taking over from Cedeno. Tom Hume's effectiveness as a closer had diminished, and no other consistent relievers emerged. Dave Concepción was the sole remaining starter from the Big Red Machine era.
Wagner's "reign of terror" ended in 1983, when Howsam, the architect of the Big Red Machine, was brought back. The popular Howsam began his second term as Reds' General Manager by signing Cincinnati native Dave Parker as a free agent from Pittsburgh. In 1984 the Reds began to move up, depending on trades and some minor leaguers. In that season Dave Parker, Dave Concepción and Tony Pérez were in Cincinnati uniforms. In August 1984, Pete Rose was reacquired and hired to be the Reds player-manager. After raising the franchise from the grave, Howsam gave way to the administration of Bill Bergesch, who attempted to build the team around a core of highly regarded young players in addition to veterans like Parker. However, he was unable to capitalize on an excess of young and highly touted position players including Kurt Stillwell, Tracy Jones, and Kal Daniels by trading them for pitching. Despite the emergence of Tom Browning as rookie of the year in 1985 when he won 20 games, the rotation was devastated by the early demise of Mario Soto's career to arm injury.
Under Bergesch, from 1985–89 the Reds finished second four times. Among the highlights, Rose became the all-time hits leader, Tom Browning threw a perfect game, Eric Davis became the first player in baseball history to hit at least 35 home runs and steal 50 bases, and Chris Sabo was the 1988 National League Rookie of the Year. The Reds also had a bullpen star in John Franco, who was with the team from 1984 to 1989. Rose once had Concepcion pitch late in a game at Dodger Stadium. Following the release of the Dowd Report which accused Rose for betting on baseball games, in 1989 Rose was banned from baseball by Commissioner Bart Giamatti, who declared Rose guilty of "conduct detrimental to baseball". Controversy also swirled around Reds owner Marge Schott, who was accused several times of ethnic and racial slurs.
World Championship and the End of an Era (1990–2002).
In 1987, General Manager Bergesch was replaced by Murray Cook, who initiated a series of deals that would finally bring the Reds back to the championship, starting with acquisitions of Danny Jackson and José Rijo. An aging Dave Parker was let go after a revival of his career in Cincinnati following the Pittsburgh drug trials. Barry Larkin emerged as the starting shortstop over Kurt Stillwell, who along with reliever Ted Power, was traded for Jackson. In 1989, Cook was succeeded by Bob Quinn, who put the final pieces of the championship puzzle together, with the acquisitions of Hal Morris, Billy Hatcher, and Randy Myers.
In 1990, the Reds under new manager Lou Piniella shocked baseball by leading the NL West from wire-to-wire. They started off 33–12, winning their first nine games, and maintained their lead throughout the year. Led by Chris Sabo, Barry Larkin, Eric Davis, Paul O'Neill and Billy Hatcher in the field, and by José Rijo, Tom Browning and the "Nasty Boys" of Rob Dibble, Norm Charlton and Randy Myers on the mound, the Reds took out the Pirates in the NLCS. The Reds swept the heavily favored Oakland Athletics in four straight, and extended a Reds winning streak in the World Series to 9 consecutive games. The World Series, however, saw Eric Davis severely bruise a kidney diving for a fly ball in Game 4, and his play was greatly limited the next year. In winning the World Series the Reds became the only National League team to go wire to wire.
In 1992, Quinn was replaced in the front office by Jim Bowden. On the field, manager Lou Piniella wanted outfielder Paul O'Neill to be a power-hitter to fill the void Eric Davis left when he was traded to the Los Angeles Dodgers in exchange for Tim Belcher. However, O'Neill only hit .246 and 14 homers. The Reds returned to winning after a losing season in 1991, but 90 wins was only enough for 2nd place behind the division-winning Atlanta Braves. Before the season ended, Piniella got into an altercation with reliever Rob Dibble. In the off season, Paul O'Neill was traded to the New York Yankees for outfielder Roberto Kelly. Kelly was a disappointment for the Reds over the next couple of years, while O'Neill blossomed, leading a down-trodden Yankees franchise to a return to glory. Also, the Reds would replace their outdated "Big Red Machine" era uniforms in favor of a pinstriped uniform with no sleeves.
For the 1993 season Piniella was replaced by fan favorite Tony Pérez, but he lasted only 44 games at the helm, replaced by Davey Johnson. With Johnson steering the team, the Reds made steady progress upward. In 1994, the Reds were in the newly created National League Central Division with the Chicago Cubs, St. Louis Cardinals, as well as fellow rivals Pittsburgh Pirates and Houston Astros. By the time the strike hit, the Reds finished a half-game ahead of the Astros for first-place in the NL Central. By 1995, the Reds won the division thanks to Most Valuable Player Barry Larkin. After defeating the NL West champion Dodgers in the first NLDS since 1981, they lost to the Atlanta Braves.
Team owner Marge Schott announced mid-season that Johnson would be gone by the end of the year, regardless of outcome, to be replaced by former Reds third baseman Ray Knight. Johnson and Schott had never gotten along and she did not approve of Johnson living with his fiancée before they were married, In contrast, Knight, along with his wife, professional golfer Nancy Lopez, were friends of Schott. The team took a dive under Knight and he was unable to complete two full seasons as manager, subject to complaints in the press about his strict managerial style.
In 1999 the Reds won 96 games, led by manager Jack McKeon, but lost to the New York Mets in a one game playoff. Earlier that year, Schott sold controlling interest in the Reds to Cincinnati businessman Carl Lindner. Despite an 85–77 finish in 2000, and being named 1999 NL manager of the year, McKeon was fired after the 2000 season. The Reds did not have another winning season until 2010.
Contemporary era (2003–).
Riverfront Stadium, then known as Cinergy Field, was demolished in 2002. Great American Ball Park opened in 2003 with high expectations for a team led by local favorites, including outfielder Ken Griffey, Jr., shortstop Barry Larkin, and first baseman Sean Casey. Although attendance improved considerably with the new ballpark, the team continued to lose. Schott had not invested much in the farm system since the early 1990s, leaving the team relatively thin on talent. After years of promises that the club was rebuilding toward the opening of the new ballpark, General Manager Jim Bowden and manager Bob Boone were fired on July 28. This broke up the father-son combo of manager Bob Boone and third baseman Aaron Boone, and Aaron was soon traded to the New York Yankees. Following the season Dan O'Brien was hired as the Reds' 16th General Manager.
The 2004 and 2005 seasons continued the trend of big hitting, poor pitching, and poor records. Griffey, Jr. joined the 500 home run club in 2004, but was again hampered by injuries. Adam Dunn emerged as consistent home run hitter, including a 535 ft home run against José Lima. He also broke the major league record for strikeouts in 2004. Although a number of free agents were signed before 2005, the Reds were quickly in last place and manager Dave Miley was forced out in the 2005 mid season and replaced by Jerry Narron. Like many other small market clubs, the Reds dispatched some of their veteran players and began entrusting their future to a young nucleus that included Adam Dunn and Austin Kearns.
Late summer 2004 saw the opening of the Cincinnati Reds Hall of Fame (HOF). The Reds HOF had been in existence in name only since the 1950s, with player plaques, photos and other memorabilia scattered throughout their front offices. Ownership and management desired a stand-alone facility, where the public could walk through inter-active displays, see locker room recreations, watch videos of classic Reds moments and peruse historical items. The first floor houses a movie theater which resembles an older, ivy-covered brick wall ball yard. The hallways contain many vintage photographs. The rear of the building features a three-story wall containing a baseball for every hit Pete Rose had during his career. The third floor contains interactive exhibits including a pitcher's mound, radio booth, and children's area where the fundamentals of baseball are taught through videos featuring former Reds players.
Robert Castellini took over as controlling owner from Lindner in 2006. Castellini promptly fired general manager Dan O'Brien and hired Wayne Krivsky. The Reds made a run at the playoffs but ultimately fell short. The 2007 season was again mired in mediocrity. Midway through the season Jerry Narron was fired as manager and replaced by Pete Mackanin. The Reds ended up posting a winning record under Mackanin, but finished the season in 5th place in the Central Division. Mackanin was manager in an interim capacity only, and the Reds, seeking a big name to fill the spot, ultimately brought in Dusty Baker. Early in the 2008 season, Krivsky was fired and replaced by Walt Jocketty. Though the Reds did not win under Krivsky, he is credited with revamping the farm system and signing young talent that could potentially lead the Reds to success in the future.
The Reds failed to post winning records in both 2008 and 2009. In 2010, with NL MVP Joey Votto and Gold Glovers Brandon Phillips and Scott Rolen the Reds posted a 91-71 record and were NL Central champions. The following week, the Reds became only the second team in MLB history to be no-hit in a postseason game when Philadelphia's Roy Halladay shut down the National League's number one offense in game one of the NLDS. The Reds lost in a 3-game sweep of the NLDS for Philadelphia.
After coming off their surprising 2010 NL Central Division Title, the Reds fell short of many expectations for the 2011 season. Multiple injuries and inconsistent starting pitching played a big role in their mid-season collapse, along with a less productive offense as compared to the previous year. The Reds ended the season at 79-83. The Reds won the 2012 NL Central Division Title. On September 28, Homer Bailey threw a 1-0 no-hitter against the Pittsburgh Pirates at PNC Park, this was the first Reds no-hitter since Tom Browning's perfect game in September of the 1988 season. Finishing with a 97–65 record, they earned the second seed in the Division Series and a match-up with the eventual World Series champion San Francisco Giants. After taking a 2–0 lead with road victories at AT&T Park, they headed home looking to win the series. However, they lost three straight at their home ballpark to become the first National League team since the Cubs in 1984 to lose a division series after leading 2–0.
In the off-season, the team traded outfielder Drew Stubbs, as part of a three team deal with the Arizona Diamondbacks and Cleveland Indians, to the Indians, and in turn received right fielder Shin-Soo Choo. On July 2, 2013, Homer Bailey pitched a no-hitter against the San Francisco Giants for a 4-0 Reds victory, making Bailey the third pitcher in Reds history with two complete game no-hitters in their career.
Following six consecutive losses to close out the 2013 season, including a loss to the Pittsburgh Pirates, at PNC Park, in the National League wild-card playoff game, the Reds decided to fire Dusty Baker. During his six years as manager, Baker led the Reds to the playoff three times; however, they never advanced beyond the first round.
On October 22, 2013, the Reds hired pitching coach Bryan Price to replace Baker as manager.
Ballpark.
The Cincinnati Reds play their home games at Great American Ball Park, located at 100 Joe Nuxhall Way, in downtown Cincinnati. Great American Ball Park opened in 2003 at the cost of $290 million and has a capacity of 42,271. Along with serving as the home field for the Reds, the stadium also holds the Cincinnati Reds Hall of Fame. The Hall of Fame was added as a part of Reds tradition allowing fans to walk through the history of the franchise as well as participating in many interactive baseball features.
Great American Ball Park is the seventh home of the Cincinnati Reds, built immediately to the north of the site on which Riverfront Stadium, later named Cinergy Field, once stood. The first ballpark the Reds occupied was Bank Street Grounds from 1882–1883 until they moved to League Park I in 1884, where they would remain until 1893. Through the late 1890s and early 1900s (decade), the Reds moved to two different parks where they stayed for less than ten years. League Park II was the third home field for the Reds from 1894–1901, and then moved to the Palace of the Fans which served as the home of the Reds in the 1910s. It was in 1912 that the Reds moved to Crosley Field which they called home for fifty-eight years. Crosley served as the home field for the Reds for two World Series titles and five National League pennants. Beginning June 30, 1970, and during the dynasty of the Big Red Machine, the Reds played in Riverfront Stadium, appropriately named due to its location right by the Ohio River. Riverfront saw three World Series titles and five National League pennants. It was in the late 1990s that the city agreed to build two separate stadiums on the riverfront for the Reds and the Cincinnati Bengals. Thus, in 2003, the Reds began a new era with the opening of the current stadium.
The Reds hold their spring training in Goodyear, Arizona at Goodyear Ballpark. The Reds moved into this stadium and the Cactus League in 2010 after staying in the Grapefruit League for most of their history. The Reds share Goodyear Park with their rivals in Ohio, the Cleveland Indians.
Logos and uniforms.
Logo.
Throughout the history of the Cincinnati Reds, many different variations of the classic wishbone "C" logo have been introduced. For most of the history of the Reds, especially during the early history, the Reds logo has been simply the wishbone "C" with the word "REDS" inside, the only colors used being red and white. However, during the 1950s, during the renaming and re-branding of the team as the Cincinnati Redlegs because of the connections to communism of the word 'Reds', the color blue was introduced as part of the Reds color combination. During the 1960s and 1970s the Reds saw a move towards the more traditional colors, abandoning the navy blue. A new logo also appeared with the new era of baseball in 1972, when the team went away from the script "REDS" inside of the "C", instead, putting their mascot Mr. Redlegs in its place as well as putting the name of the team inside of the wishbone "C". In the 1990s the more traditional, early logos of Reds came back with the current logo reflecting more of what the team's logo was when they were first founded.
Uniform.
Along with the logo, the Reds' uniforms have been changed many different times throughout their history. Following their departure from being called the "Redlegs" in 1956 the Reds made a groundbreaking change to their uniforms with the use of sleeveless jerseys, seen only once before in the Major Leagues by the Chicago Cubs. At home and away, the cap was all-red with a white wishbone C insignia. The long-sleeved undershirts were red. The uniform was plain white with a red wishbone C logo on the left and the uniform number on the right. On the road the wishbone C was replaced by the mustachioed "Mr. Red" logo, the pillbox-hat-wearing man with a baseball for a head. The home stockings were red with six white stripes. The away stockings had only three white stripes.
The Reds changed uniforms again in 1961, when they replaced the traditional wishbone C insignia with an oval C logo, but continued to use the sleeveless jerseys. At home, the Reds wore white caps with the red bill with the oval C in red, white sleeveless jerseys with red pinstripes, with the oval C-REDS logo in black with red lettering on the left breast and the number in red on the right. The gray away uniform included a gray cap with the red oval C and a red bill. Their gray away uniforms, which also included a sleeveless jersey, bore CINCINNATI in an arched block style across with the number below on the left. In 1964, players' last names were placed on the back of each set of uniforms, below the numbers. Those uniforms were scrapped after the 1966 season.
However, the Cincinnati uniform design most familiar to baseball enthusiasts is the one whose basic form, with minor variations, held sway for the 26 seasons from 1967 to 1992. Most significantly, the point was restored to the C insignia, making it a wishbone again. During this era, the Reds wore all-red caps both at home and on the road. The caps bore the simple wishbone C insignia in white. The uniforms were standard short-sleeved jerseys and standard trousers—white at home and grey on the road. The home uniform featured the Wishbone C-REDS logo in red with white type on the left breast and the uniform number in red on the right. The away uniform bore CINCINNATI in an arched block style across the front with the uniform number below on the left. Red, long-sleeved undershirts and plain red stirrups over white sanitary stockings completed the basic design.
The 1993 uniforms (which did away with the pullovers and brought back button-down jerseys) kept white and gray as the base colors for the home and away uniforms, but added red pinstripes. The home jerseys were sleeveless, showing more of the red undershirts. The color scheme of the C-REDS logo on the home uniform was reversed, now red lettering on a white background. A new home cap was created that had a red bill and a white crown with red pinstripes and a red wishbone C insignia. The away uniform kept the all-red cap, but moved the uniform number to the left, to more closely match the home uniform. The only additional change to these uniforms was the introduction of black as a primary color of the Reds in 1999, especially on their road uniforms.
The Reds latest uniform change came in December 2006 which differed significantly from the uniforms worn during the previous eight seasons. The home caps returned to an all-red design with a white wishbone C, lightly outlined in black. Caps with red crowns and a black bill became the new road caps. Additionally, the sleeveless jersey was abandoned for a more traditional design. The numbers and lettering for the names on the backs of the jerseys were changed to an early-1900s style typeface, and a handlebar mustached "Mr. Redlegs" – reminiscent of the logo used by the Reds in the 1950s and 1960s – was placed on the left sleeve.
Awards and accolades.
Retired numbers.
The Cincinnati Reds have retired nine numbers in franchise history, as well as honoring Jackie Robinson, retired in all major league baseball. Since Pete Rose was banned from baseball, the Reds have not retired his #14. However, they have not reissued it except for Pete Rose, Jr. in his 11-game tenure in 1997.
On April 15, 1997, #42 was retired throughout Major League Baseball in honor of Jackie Robinson.
All of the retired numbers are located at Great American Ball Park behind home-plate on the outside of the press box. Along with the retired player and manager number, the following broadcasters are honored with microphones by the broadcast booth: Marty Brennaman, Waite Hoyt, and Joe Nuxhall.
Ohio Cup.
The Ohio Cup was an annual pre-season baseball game, which pitted the Ohio rivals Cleveland Indians and Cincinnati Reds. In its first series it was a single-game cup, played each year at minor-league Cooper Stadium in Columbus, Ohio, was staged just days before the start of each new Major League Baseball season. A total of eight Ohio Cup games were played, in 1989 to 1996, with the Indians winning six of them. The winner of the game each year was awarded the Ohio Cup in postgame ceremonies. The Ohio Cup was a favorite among baseball fans in Columbus, with attendances regularly topping 15,000. 
The Ohio Cup games ended with the introduction of regular-season interleague play in 1997. Thereafter, the two teams competed annually in the regular-season Battle of Ohio or Buckeye Series. The Ohio Cup was revived in 2008 as a reward for the team with the better overall record in the Reds-Indians series each year. The Indians currently lead the interleague series 36–35. 
Media.
Radio.
The Reds' flagship radio station has been WLW, 700AM since 1969. Prior to that, the Reds were heard over: WKRC, WCPO, WSAI and WCKY. WLW, a 50,000-watt station, is "clear channel" in more than one way, as Clear Channel Communications owns the "blowtorch" outlet which is also known as "The Nation's Station".
Marty Brennaman has been the Reds' play-by-play voice since 1974 and has won the Ford C. Frick Award for his work, which includes his famous call of "... and this one belongs to the Reds!" after a win. Joining him for years on color was former Reds pitcher Joe Nuxhall, who worked in the radio booth from 1967 (the year after his retirement as an active player) until 2004, plus three more seasons doing select home games until his death, in 2007.
In 2007, Thom Brennaman, a veteran announcer seen nationwide on Fox Sports, joined his father Marty in the radio booth. Retired relief pitcher Jeff Brantley, formerly of ESPN, also joined the network in 2007. As of 2010, Brantley and Thom Brennaman's increased TV schedule (see below) has led to more appearances for Jim Kelch, who had filled in on the network since 2008.
Television.
Televised games are seen exclusively on Fox Sports Ohio (in Cincinnati, Dayton, Columbus and Kentucky) and Fox Sports Indiana. Fox Sports South also pipes in Fox Sports Ohio broadcasts of Reds games to Tennessee and western North Carolina. George Grande, who hosted the first "SportsCenter" on ESPN in 1979, was the play-by-play announcer from 1993 until his retirement during the final game of the '09 season, usually alongside Chris Welsh. Since 2009 Grande has worked part-time for the Reds as play-by-play announcer in September when Thom is working for Fox Sports covering the NFL. He also has made guest appearances throughout the season. Thom Brennaman has been the head play-by-play commentator since 2010, and Welsh and Brantley, share time as the color commentator. Paul Keels, the current radio play-by-play announcer for The Ohio State University Buckeyes Radio Network, was the Reds backup play-by-play television announcer for 2010. Jim Kelch will replaced Keels for the 2011 season. The Reds also added former Cincinnati First Baseman Sean Casey, "The Mayor", as Cincinnatians call him, to do color commentary for 15 games in 2011.
NBC affiliate WLWT carried Reds games from 1948–1995. Among those that have called games for WLWT include Waite Hoyt, Ray Lane, Steve Physioc, Johnny Bench, Joe Morgan, and Ken Wilson. Al Michaels, who later went on to a long career with ABC and NBC, spent three years in Cincinnati before being drafted by NBC. WSTR-TV aired games from 1996–1998, and the Reds have not broadcast over-the-air locally on a regular basis since then. Since 2010, WKRC-TV has simulcast Fox Sports Ohio's feed of the Opening Day game; they were the first games broadcast locally over-the-air since Opening Day 2002.

</doc>
<doc id="6672" url="http://en.wikipedia.org/wiki?curid=6672" title="Caribbean cuisine">
Caribbean cuisine

Caribbean cuisine is a fusion of African, Amerindian, European, East Indian, Arab and Chinese cuisine. These traditions were brought from many different countries when they came to the Caribbean. In addition, the population has created styles that are unique to the region.
Ingredients which are common in most islands' dishes are rice, plantains, beans, cassava, cilantro (coriander), bell peppers, chickpeas, tomatoes, sweet potatoes, coconut, and any of various meats that are locally available like beef, poultry, pork or fish. A characteristic seasoning for the region is a green herb and oil based marinade which imparts a flavor profile which is quintessentially Caribbean in character. Ingredients may include garlic, onions, scotch bonnet peppers, celery, green onions, and herbs like cilantro, marjoram, rosemary, tarragon and thyme. This green seasoning is used for a variety of dishes like curries, stews and roasted meats.
Traditional dishes are so important to regional culture that, for example, the local version of Caribbean goat stew has been chosen as the official national dish of Montserrat and is also one of the signature dishes of St. Kitts and Nevis. Another popular dish in the Anglophone Caribbean is called "Cook-up", or Pelau. Ackee and saltfish is another popular dish that is unique to Jamaica. Callaloo is a dish containing leafy vegetables and sometimes okra amongst others, widely distributed in the Caribbean, with a distinctively mixed African and indigenous character.
The variety of dessert dishes in the area also reflects the mixed origins of the recipes. In some areas, Black Cake, a derivative of English Christmas pudding may be served, especially on special occasions.

</doc>
<doc id="6673" url="http://en.wikipedia.org/wiki?curid=6673" title="Central Powers">
Central Powers

The Central Powers (German: "Mittelmächte"; Hungarian: "Központi hatalmak"‍; Turkish: "İttifak Devletleri" or "Bağlaşma Devletleri"; Bulgarian: Централни сили "Tsentralni sili"), consisting of Germany, Austria-Hungary, the Ottoman Empire and Bulgaria – hence also known as the Quadruple Alliance (German: "Vierbund") – was one of the two main factions during World War I (1914–18). It faced and was defeated by the Allied Powers that had formed around the Triple Entente, after which it was dissolved.
The Powers' origin was the alliance of Germany and Austria-Hungary in 1879. The Ottoman Empire and Bulgaria did not join until after World War I had begun.
Member states.
The Central Powers consisted of the German Empire and the Austro-Hungarian Empire at the beginning of the war. The Ottoman Empire joined the Central Powers later in 1914. In 1915, the Kingdom of Bulgaria joined the alliance. The name "Central Powers" is derived from the location of these countries; all four (including the other groups that supported them except for Finland and Lithuania) were located between the Russian Empire in the east and France and the United Kingdom in the west. Finland, Azerbaijan, and Lithuania joined them in 1918 before the war ended and after the Russian Empire collapsed.
The Central Powers were composed of the following nations:
Combatants.
Germany.
War justifications.
In early July 1914, in the aftermath of the assassination of Austro-Hungarian Archduke Franz Ferdinand and the immediate likelihood of war between Austria-Hungary and Serbia, Kaiser Wilhelm II and the German government informed the Austro-Hungarian government that Germany would uphold its alliance with Austria-Hungary and defend it from possible Russia intervention if a war between Austria-Hungary and Serbia took place. When Russia enacted a general mobilization, Germany viewed the act as provocative. The Russian government promised Germany that its general mobilization did not mean preparation for war with Germany but was a reaction to the events between Austria-Hungary and Serbia. The German government regarded the Russian promise of no war with Germany to be nonsense in light of its general mobilization, and Germany in turn mobilized for war. On August 1, Germany sent an ultimatum to Russia stating that since both Germany and Russia were in a state of military mobilization, an effective state of war existed between the two countries. Later that day, France, an ally of Russia, declared a state of general mobilization,
In August 1914, Germany waged war on Russia, the German government justified military action against Russia as necessary because of Russian aggression as demonstrated by the mobilization of the Russian army that had resulted in Germany mobilizing in response.
After Germany declared war on Russia, France with its alliance with Russia prepared a general mobilization in expectation of war. On 3 August 1914, Germany responded to this action by declaring war on France. Germany facing a two-front war enacted what was known as the Schlieffen Plan, that involved German armed forces needing to move through Belgium and swing south into France and towards the French capital of Paris. This plan was hoped to quickly gain victory against the French and allow German forces to concentrate on the Eastern Front. Belgium was a neutral country and would not accept German forces crossing its territory. Germany disregarded Belgian neutrality and invaded the country to launch an offensive towards Paris. This caused Great Britain to declare war against the German Empire, as the action violated the Treaty of London that both nations signed in 1839 guaranteeing Belgian neutrality and defense of the kingdom if a nation reneged.
Subsequently several states declared war on Germany, including: Japan declaring war on Germany in late August 1914; Italy declaring war on Austria-Hungary in 1915 and Germany on August 27, 1916; the United States declaring war on Germany on April 6, 1917 and Greece declaring war on Germany in July 1917.
Colonies and dependencies.
Upon its founding in 1871, the German Empire controlled Alsace-Lorraine as an "imperial territory" incorporated from France after the Franco-Prussian War. It was held as part of Germany's sovereign territory.
Germany held multiple African colonies at the time of World War I. All of Germany's African colonies were invaded and occupied by Allied forces during the war.
Cameroon, German East Africa, and German Southwest Africa were German colonies in Africa. Togoland was a German protectorate in Africa.
German New Guinea was a German protectorate in the Pacific. It was occupied by Australian forces in 1914.
The Kiautschou Bay concession was a German dependency in East Asia leased from China in 1898. It was occupied by Japanese forces following the Siege of Tsingtao.
Austria-Hungary.
War justifications.
Austria-Hungary regarded the assassination of Arch Duke Franz Ferdinand as being orchestrated with the assistance of Serbia. The country viewed the assassination as setting a dangerous precedent of encouraging the country's South Slav population to rebel and threaten to tear apart the multinational country. Austria-Hungary formally sent an ultimatum to Serbia demanding a full-scale investigation of Serbian government complicity in the assassination, and complete compliance by Serbia in agreeing to the terms demanded by Austria-Hungary. Serbia submitted to accept most of the demands, however Austria-Hungary viewed this as insufficient and used this lack of full compliance to justify military intervention. These demands have been viewed as a diplomatic cover for what was going to be an inevitable Austro-Hungarian declaration of war on Serbia.
Austria-Hungary had been warned by Russia that the Russian government would not tolerate Austria-Hungary crushing Serbia. However with Germany supporting Austria-Hungary's actions, the Austro-Hungarian government hoped that Russia would not intervene and that the conflict with Serbia would be a regional conflict.
Austria-Hungary's invasion of Serbia resulted in Russia declaring war on the country and Germany in turn declared war on Russia, setting off the beginning of the clash of alliances that resulted in the World War.
Austria-Hungary was internally divided into two states with their own governments, joined in communion through the Habsburg throne. Austrian Cisleithania contained various duchies and principalities but also the Kingdom of Bohemia, the Kingdom of Dalmatia, the Kingdom of Galicia and Lodomeria. Hungarian Transleithania comprised the Kingdom of Hungary and the Kingdom of Croatia-Slavonia. In Bosnia and Herzegovina sovereign authority was shared by both Austria and Hungary.
Ottoman Empire.
War justifications.
The Ottoman Empire joined the war on the side of the Central Powers in November 1914. The Ottoman Empire had gained strong economic connections with Germany through the Berlin-to-Baghdad railway project that was still incomplete at the time. The Ottoman Empire made a formal alliance with Germany signed on 2 August 1914. The alliance treaty expected that the Ottoman Empire would become involved in the conflict in a short amount of time. However, for the first several months of the war the Ottoman Empire maintained neutrality though it allowed a German naval squadron to enter and stay near the strait of Bosphorus. Ottoman officials informed the German government that the country needed time to prepare for conflict. Germany provided financial aid and weapons shipments to the Ottoman Empire.
After pressure escalated from the German government demanding that the Ottoman Empire fulfill its treaty obligations, or else Germany would expel the country from the alliance and terminate economic and military assistance, the Ottoman government entered the war with the recently acquired cruisers from Germany, the "Yavuz Sultan Selim" (formerly "SMS Goeben") and the "Midilli" (formerly "SMS Breslau") launching a naval raid on the Russian port of Odessa, thus engaging in a military action in accordance with its alliance obligations with Germany. Russia and the Triple Entente declared war on the Ottoman Empire.
Bulgaria.
War justifications.
Bulgaria was still resentful after its defeat in July 1913 at the hands of Serbia, Greece and Romania. It signed a treaty of defensive alliance with the Ottoman Empire on 19 August 1914. It was the last country to join the Central Powers, which Bulgaria did in October 1915 by declaring war on Serbia. It invaded Serbia in conjunction with German and Austro-Hungarian forces. Bulgaria held irredentist aims on the region of Vardar Macedonia held by Serbia.
Co-belligerents.
Dervish State.
The Dervish State was a rebel Somali state seeking independence of Somali territories. Dervish forces fought against Italian and British forces in Italian Somaliland and British Somaliland during World War I in the Somaliland Campaign. The Dervish State received support from Germany and the Ottoman Empire.
Sultanate of Darfur.
The Sultanate of Darfur forces fought against British forces in Anglo-Egyptian Sudan during World War I in the Anglo-Egyptian Darfur Expedition.
Client states.
During 1917 and 1918, the Finns under Carl Gustaf Emil Mannerheim and Lithuanian nationalists fought Russia for a common cause. With the Bolshevik attack of late 1917, the General Secretariat of Ukraine sought military protection first from the Central Powers and later from the armed forces of the Entente.
The Ottoman Empire also had its own allies in Azerbaijan and the Northern Caucasus. The three nations fought alongside each other under the Army of Islam in the Battle of Baku.
Non-state combatants.
Other movements supported the efforts of the Central Powers for their own reasons, such as the Irish Nationalists who launched the Easter Rising in Dublin in April 1916; they referred to their "gallant allies in Europe". In 1914, Józef Piłsudski was permitted by Germany and Austria-Hungary to form independent Polish legions. Piłsudski wanted his legions to help the Central Powers defeat Russia and then side with France and the UK and win the war with them.
Armistice and treaties.
Bulgaria signed an armistice with the Allies on 29 September 1918, following a successful Allied advance in Macedonia. The Ottoman Empire followed suit on 30 October 1918 in the face of British and Arab gains in Palestine and Syria. Austria and Hungary concluded ceasefires separately during the first week of November following the disintegration of the Habsburg Empire and the Italian offensive at Vittorio Veneto; Germany signed the armistice ending the war on the morning of 11 November 1918 after the Hundred Days Offensive, and a succession of advances by New Zealand, Australian, Canadian, Belgian, British, French and US forces in north-eastern France and Belgium. There was no unified treaty ending the war; the Central Powers were dealt with in separate treaties.

</doc>
<doc id="6675" url="http://en.wikipedia.org/wiki?curid=6675" title="Conservatism">
Conservatism

Conservatism as a political and social philosophy promotes retaining traditional social institutions in the context of the culture and civilization. Some conservatives seek to preserve things as they are, emphasizing stability and continuity, while others, called reactionaries, oppose modernism and seek a return to "the way things were". The first established use of the term in a political context originated with François-René de Chateaubriand in 1818,
during the period of Bourbon restoration that sought to roll back the policies of the French Revolution. The term, historically associated with right-wing politics, has since been used to describe a wide range of views. There is no single set of policies that are universally regarded as conservative, because the meaning of conservatism depends on what is considered traditional in a given place and time. Thus conservatives from different parts of the world—each upholding their respective traditions—may disagree on a wide range of issues.
Edmund Burke, an 18th-century politician who opposed the French Revolution but supported the American Revolution, is credited as one of the main theorists of conservatism in Great Britain in the 1790s. According to Quintin Hogg, the chairman of the British Conservative Party in 1959, "Conservatism is not so much a philosophy as an attitude, a constant force, performing a timeless function in the development of a free society, and corresponding to a deep and permanent requirement of human nature itself."
Development of Western conservatism.
Great Britain.
In Britain, conservative ideas (though not yet called that) emerged in the Tory movement during the Restoration period (1660–1688). Toryism supported a hierarchical society with a monarch who ruled by divine right. Tories opposed the idea that sovereignty derived from the people, and rejected the authority of parliament and freedom of religion. Robert Filmer's "Patriarcha: or the Natural Power of Kings", published posthumously in 1680 but written before the English Civil War of 1642-1651, became accepted as the statement of their doctrine. However, the Glorious Revolution of 1688 destroyed this principle to some degree by establishing a constitutional government in England, leading to the hegemony of the Tory-opposed Whig ideology. Faced with defeat, the Tories reformed their movement, now holding that sovereignty was vested in the three estates of Crown, Lords, and Commons rather than solely in the Crown. Toryism became marginalized during the long period of Whig ascendancy in the 18th century.
Conservatives typically see Richard Hooker (1554-1600) as the founding father of conservatism, along with the Marquess of Halifax (1633-1695), David Hume (1711-1776) and Edmund Burke (1729-1797). Halifax promoted pragmatism in government, whilst Hume argued against political rationalism and utopianism. Burke served as the private secretary to the Marquis of Rockingham and as official pamphleteer to the Rockingham branch of the Whig party. Together with the Tories, they were the conservatives in the late 18th century United Kingdom. Burke's views were a mixture of liberal and conservative. He supported the American Revolution of 1765-1783 but abhorred the violence of the French Revolution (1789- ). He accepted the liberal ideals of private property and the economics of Adam Smith (1723-1790), but thought that economics should remain subordinate to the conservative social ethic, that capitalism should be subordinate to the medieval social tradition and that the business class should be subordinate to aristocracy. He insisted on standards of honor derived from the medieval aristocratic tradition, and saw the aristocracy as the nation's natural leaders. That meant limits on the powers of the Crown, since he found the institutions of Parliament to be better informed than commissions appointed by the executive. He favored an established church, but allowed for a degree of religious toleration. Burke justified the social order on the basis of tradition: tradition represented the wisdom of the species and he valued community and social harmony over social reforms. Burke was a leading theorist in his day, finding extreme idealism (either Tory or Whig) an endangerment to broader liberties, and (like Hume) rejecting abstract reason as an unsound guide for political theory. Despite their influence on future conservative thought, none of these early contributors were explicitly involved in Tory politics. Hooker lived in the 16th century, long before the advent of toryism, whilst Hume was an apolitical philosopher and Halifax similarly politically independent. Burke described himself as a Whig.
Shortly after Burke's death in 1797, conservatism revived as a mainstream political force as the Whigs suffered a series of internal divisions. This new generation of conservatives derived their politics not from Burke but from his predecessor, the Viscount Bolingbroke (1678-1751), who was a Jacobite and traditional Tory, lacking Burke's sympathies for Whiggish policies such as Catholic Emancipation and American independence (famously attacked by Samuel Johnson in "Taxation No Tyranny"). In the first half of the 19th century many newspapers, magazines, and journals promoted loyalist or right-wing attitudes in religion, politics, and international affairs. Burke was seldom mentioned but William Pitt the Younger (1759-1806) became a conspicuous hero. The most prominent journals included "The Quarterly Review," founded in 1809 as a counterweight to the Whigs' "Edinburgh Review", and the even more conservative "Blackwood's Edinburgh Magazine". Sack finds that the "Quarterly Review" promoted a balanced Canningite toryism; was neutral on Catholic emancipation and only mildly critical of Nonconformist Dissent; it opposed slavery and supported the current poor laws. It was "aggressively imperialist". The high-church clergy of the Church of England read the "Orthodox Churchman's Magazine" which was equally hostile to Jewish, Catholic, Jacobin, Methodist, and Unitarian spokesmen. Anchoring the ultra tories, "Blackwood's Edinburgh Magazine" stood firmly against Catholic emancipation, and favoured slavery, cheap money, mercantilism, the Navigation acts, and the Holy Alliance.
In the 19th century, conflict between wealthy businessmen and the aristocracy split the British conservative movement, with the aristocracy calling for a return to medieval ideas while the business classes advocated laissez-faire capitalism.
Although conservatives opposed attempts to allow greater representation of the middle class in parliament, in 1834 they conceded that electoral reform could not be reversed and promised to support further reforms so long as they did not erode the institutions of church and state. These new principles were presented in the Tamworth Manifesto of 1834, which historians regard as the basic statement of the beliefs of the new Conservative Party.
Some conservatives lamented the passing of a pastoral world where the ethos of "noblesse oblige" had promoted respect from the lower classes. They saw the Anglican Church and the aristocracy as balances against commercial wealth. They worked toward legislation for improved working conditions and urban housing. This viewpoint would later be called Tory Democracy. However since Burke there has always been tension between traditional aristocratic conservatism and the wealthy business class.
In 1834 Tory Prime Minister Robert Peel issued the Tamworth Manifesto in which he pledged to endorse moderate political reform. This marked the beginning of the transformation of British conservatism from High Tory reactionism towards a more modern form based on "conservation". The party became known as the Conservative Party as a result, a name it has retained to this day. Peel, however, would also be the root of a split in the party between the traditional Tories (led by the Earl of Derby and Benjamin Disraeli) and the 'Peelites' (led first by Peel himself, then by the Earl of Aberdeen). The split occurred in 1846 over the issue of free trade, which Peel supported, versus protectionism, supported by Derby. The majority of the party sided with Derby, whilst about a third split away, eventually merging with the Whigs and the radicals to form the Liberal Party. Despite the split, the mainstream Conservative Party accepted the doctrine of free trade in 1852.
In the second half of the 19th century the Liberal Party faced political schisms, especially over Irish Home Rule. Leader William Gladstone (himself a former Peelite) sought to give Ireland a degree of autonomy, a move that elements in both the left and right wings of his party opposed. These split off to become the Liberal Unionists (led by Joseph Chamberlain), forming a coalition with the Conservatives before merging with them in 1912. The Liberal Unionist influence dragged the Conservative Party towards the left; Conservative governments passing a number of progressive reforms at the turn of the 20th century. By the late 19th century the traditional business supporters of the UK Liberal Party had joined the Conservatives, making them the party of business and commerce.
After a period of Liberal dominance before the First World War, the Conservatives gradually became more influential in government, regaining full control of the cabinet in 1922. In the interwar period conservatism was the major ideology in Britain, as the Liberal Party vied with the Labour Party for control of the left. After the Second World War, the first Labour government (1945-1951) under Clement Attlee embarked on a program of nationalization of industry and the promotion of social welfare. The Conservatives generally accepted those policies until the 1980s. In the 1980s the Conservative government of Margaret Thatcher, guided by neoliberal economics, reversed many of Labour's programmes.
Small conservative political parties -such as the United Kingdom Independence Party (founded in 1993) and the Democratic Unionist Party (founded in 1971) - began to appear, although they have yet to make any significant impact at Westminster (as of 2014[ [update]] the DUP comprises the largest political party in the ruling coalition in the Northern Ireland Assembly).
Germany.
Conservative thought developed alongside nationalism in Germany, culminating in Germany's victory over France in the Franco-Prussian War, the creation of the unified German Empire in 1871, and the simultaneous rise of Otto von Bismarck on the European political stage. Bismarck's "balance of power" model maintained peace in Europe for decades at the end of the 19th century. His "revolutionary conservatism" was a conservative state-building strategy designed to make ordinary Germans—not just the Junker elite—more loyal to state and emperor, he created the modern welfare state in Germany in the 1880s. According to Kees van Kersbergen and Barbara Vis, his strategy was: 
Bismarck also enacted universal male suffrage in the new German Empire in 1871. He became a great hero to German conservatives, who erected many monuments to his memory after he left office in 1890.
With the rise of Nazism in 1933, agrarian movements faded and was supplanted by a more command-based economy and forced social integration. Though Adolf Hitler succeeded in garnering the support of many German industrialists, prominent traditionalists openly and secretly opposed his policies of euthanasia, genocide, and attacks on organized religion, including Claus von Stauffenberg, Dietrich Bonhoeffer, Henning von Tresckow, Bishop Clemens August Graf von Galen, and the monarchist Carl Friedrich Goerdeler.
More recently, the work of conservative CDU leader Helmut Kohl helped bring about German Reunification, along with the closer integration of Europe in the form of the Maastricht Treaty. Today, German conservatism is often associated with Chancellor Angela Merkel, whose tenure has been marked by attempts to save the common European currency (EURO) from demise.
United States.
In the United States, conservatism is rooted in the American Revolution and its commitment to republicanism, sovereignty of the people, and the rights and liberties of Englishmen while expelling the king and his supporters. Most European conservative writers do not accept American conservatism as genuine; they consider it to be a variety of liberalism. Modern American liberals in the New Deal do not disagree with that consensus view, but conservatives spend much more emphasis on the Revolutionary origins, with the Tea Party advocates using an episode from the 1770s for their name and some even dress in costumes from that era at their rallies.
Historian Gregory Schneider identifies several constants in American conservatism: respect for tradition, support of republicanism, "the rule of law and the Christian religion," and a defense of "Western civilization from the challenges of modernist culture and totalitarian governments."
Latin Europe.
Another form of conservatism developed in France in parallel to conservatism in Britain. It was influenced by Counter-Enlightenment works by men such as Joseph de Maistre and Louis de Bonald. Latin conservatism was less pragmatic and more reactionary than the conservatism of Burke. Many Continental or Traditionalist conservatives do not support separation of Church and state, with most supporting state recognition of and cooperation with the Catholic Church, such as had existed in France before the Revolution.
Eventually conservatives added patriotism and nationalism to the list of traditional values they support. German conservatives were the first to embrace nationalism, which was previously associated with liberalism and the Revolution in France.
Forms of conservatism.
Liberal conservatism.
Liberal conservatism is a variant of conservatism that combines conservative values and policies with classical liberal stances. As these latter two terms have had different meanings over time and across countries, liberal conservatism also has a wide variety of meanings. Historically, the term often referred to the combination of economic liberalism, which champions laissez-faire markets, with the classical conservatism concern for established tradition, respect for authority and religious values. It contrasted itself with classical liberalism, which supported freedom for the individual in both the economic and social spheres.
Over time, the general conservative ideology in many countries adopted economic liberal arguments, and the term "liberal conservatism" was replaced with "conservatism". This is also the case in countries where liberal economic ideas have been the tradition, such as the United States, and are thus considered conservative. In other countries where liberal conservative movements have entered the political mainstream, such as Italy and Spain, the terms "liberal" and "conservative" may be synonymous. The liberal conservative tradition in the United States combines the economic individualism of the classical liberals with a Burkean form of conservatism (which has also become part of the American conservative tradition, such as in the writings of Russell Kirk).
A secondary meaning for the term "liberal conservatism" that has developed in Europe is a combination of more modern conservative (less traditionalist) views with those of social liberalism. This has developed as an opposition to the more collectivist views of socialism. Often this involves stressing what are now conservative views of free-market economics and belief in individual responsibility, with social liberal views on defence of civil rights, environmentalism and support for a limited welfare state. In continental Europe, this is sometimes also translated into English as social conservatism.
Conservative liberalism.
Conservative liberalism is a variant of liberalism that combines liberal values and policies with conservative stances, or, more simply, the right wing of the liberal movement. The roots of conservative liberalism are found at the beginning of the history of liberalism. Until the two World Wars, in most European countries the political class was formed by conservative liberals, from Germany to Italy. Events after World War I brought the more radical version of classical liberalism to a more conservative (i.e. more moderate) type of liberalism.
Libertarian conservatism.
Libertarian conservatism describes certain political ideologies within the United States and Canada which combine libertarian economic issues with aspects of conservatism. Its five main branches are Constitutionalism, paleolibertarianism, neolibertarianism, small government conservatism and Christian libertarianism. They generally differ from paleoconservatives, in that they are in favor of more personal and economic freedom.
Agorists such as Samuel Edward Konkin III labeled libertarian conservatism right-libertarianism.
In contrast to paleoconservatives, libertarian conservatives support strict laissez-faire policies such as free trade, opposition to any national bank and opposition to business regulations. They are vehemently opposed to environmental regulations, corporate welfare, subsidies, and other areas of economic intervention.
Many conservatives, especially in the United States, believe that the government should not play a major role in regulating business and managing the economy. They typically oppose efforts to charge high tax rates and to redistribute income to assist the poor. Such efforts, they argue, do not properly reward people who have earned their money through hard work.
Fiscal conservatism.
Fiscal conservatism is the economic philosophy of prudence in government spending and debt. Edmund Burke, in his "Reflections on the Revolution in France", argued that a government does not have the right to run up large debts and then throw the burden on the taxpayer:
...[I]t is to the property of the citizen, and not to the demands of the creditor of the state, that the first and original faith of civil society is pledged. The claim of the citizen is prior in time, paramount in title, superior in equity. The fortunes of individuals, whether possessed by acquisition or by descent or in virtue of a participation in the goods of some community, were no part of the creditor's security, expressed or implied...[T]he public, whether represented by a monarch or by a senate, can pledge nothing but the public estate; and it can have no public estate except in what it derives from a just and proportioned imposition upon the citizens at large.
Most conservatives believe that government action cannot solve society's problems, such as poverty and inequality. Many believe that government programs that seek to provide services and opportunities for the poor actually encourage dependence and reduce self-reliance. Most conservatives oppose "affirmative action" policies-that is, policies in employment, education, and other areas that aim to counteract past discrimination by giving special help to members of disadvantaged groups. Conservatives believe that the government should not give special treatment to individuals on the basis of group identity.
Many conservatives, especially in the United States, believe that the government should not play a major role in regulating business and managing the economy. They typically oppose efforts to charge high tax rates and to redistribute income to assist the poor. Such efforts, they argue, do not properly reward people who have earned their money through hard work.
National and traditional conservatism.
National conservatism is a political term used primarily in Europe to describe a variant of conservatism which concentrates more on national interests than standard conservatism as well as upholding cultural and ethnic identity, while not being outspokenly nationalist or supporting a far-right approach. In Europe, national conservatives are usually eurosceptics.
National conservatism is heavily oriented towards the traditional family and social stability as well as in favour of limiting immigration. As such, national conservatives can be distinguished from economic conservatives, for whom free market economic policies, deregulation and fiscal conservatism are the main priorities. Some commentators have identified a growing gap between national and economic conservatism: "most parties of the Right [today] are run by economic conservatives who, in varying degrees, have marginalized social, cultural, and national conservatives." National conservatism is also related to traditionalist conservatism.
Traditionalist conservatism is a political philosophy emphasizing the need for the principles of natural law and transcendent moral order, tradition, hierarchy and organic unity, agrarianism, classicism and high culture, and the intersecting spheres of loyalty. Some traditionalists have embraced the labels "reactionary" and "counterrevolutionary", defying the stigma that has attached to these terms since the Enlightenment. Having a hierarchical view of society, many traditionalist conservatives, including a few Americans, defend the monarchical political structure as the most natural and beneficial social arrangement.
Cultural and social conservatism.
Cultural conservatives support the preservation of the heritage of one nation, or of a shared culture that is not defined by national boundaries. The shared culture may be as divergent as Western culture or Chinese culture. In the United States, the term "cultural conservative" may imply a conservative position in the culture war. Cultural conservatives hold fast to traditional ways of thinking even in the face of monumental change. They believe strongly in traditional values and traditional politics, and often have an urgent sense of nationalism.
Social conservatism is distinct from cultural conservatism, although there are some overlaps. Social conservatives may believe that the government has a role in encouraging or enforcing traditional values or behaviours. A social conservative wants to preserve traditional morality and social mores, often by opposing what they consider radical policies or social engineering. Social change is generally regarded as suspect.
A second meaning of the term "social conservatism" developed in the Nordic countries and continental Europe. There it refers to liberal conservatives supporting modern European welfare states.
Social conservatives (in the first meaning of the word) in many countries generally favour the pro-life position in the abortion controversy and oppose human embryonic stem cell research (particularly if publicly funded); oppose both eugenics and human enhancement (transhumanism) while supporting bioconservatism; support a traditional definition of marriage as being one man and one woman; view the nuclear family model as society's foundational unit; oppose expansion of civil marriage and child adoption rights to couples in same-sex relationships; promote public morality and traditional family values; oppose atheism, especially militant atheism, secularism and the separation of church and state; support the prohibition of drugs, prostitution, and euthanasia; and support the censorship of pornography and what they consider to be obscenity or indecency. Most conservatives in the U.S. support the death penalty.
Religious conservatism.
Religious conservatives principally seek to apply the teachings of particular religions to politics, sometimes by merely proclaiming the value of those teachings, at other times by having those teachings influence laws.
In most modern democracies, political conservatism seeks to uphold traditional family structures and social values. Religious conservatives typically oppose abortion, homosexual behavior, drug use, and sexual activity outside of marriage. In some cases, conservative values are grounded in religious beliefs, and some conservatives seek to increase the role of religion in public life.
Progressive conservatism.
Progressive conservatism incorporates progressive policies alongside conservative policies. It stresses the importance of a social safety net to deal with poverty, support of limited redistribution of wealth along with government regulation to regulate markets in the interests of both consumers and producers. Progressive conservatism first arose as a distinct ideology in the United Kingdom under Prime Minister Benjamin Disraeli's "One Nation" Toryism.
There have been a variety of progressive conservative governments. In the UK, the Prime Ministers Disraeli, Stanley Baldwin, Neville Chamberlain, Winston Churchill, Harold Macmillan, and present Prime Minister David Cameron are progressive conservatives. 
In the United States, the administration of President William Howard Taft was progressive conservative and he described himself as "a believer in progressive conservatism" and President Dwight D. Eisenhower declared himself an advocate of "progressive conservatism". In Germany, Chancellor Leo von Caprivi promoted a progressive conservative agenda called the "New Course". In Canada, a variety of conservative governments have been progressive conservative, with Canada's major conservative movement being officially named the Progressive Conservative Party of Canada from 1942 to 2003. In Canada, the Prime Ministers Arthur Meighen, R.B. Bennett, John Diefenbaker, Joe Clark, Brian Mulroney, and Kim Campbell led progressive conservative federal governments.
Historic conservatism in different countries.
Conservative political parties vary widely from country to country in the goals they wish to achieve. Both conservative and liberal parties tend to favor private ownership of property, in opposition to communist, socialist and green parties, which favor communal ownership or laws requiring social responsibility on the part of property owners. Where conservatives and liberals differ is primarily on social issues. Conservatives tend to reject behavior that does not conform to some social norm. Modern conservative parties often define themselves by their opposition to liberal or labor parties. The United States usage of the term "conservative" is unique to that country.
According to Alan Ware, Belgium, Denmark, Iceland, Finland, France, Greece, Iceland, Luxembourg, Netherlands, Norway, Sweden, Switzerland, and the UK retained viable conservative parties into the 1980s. Ware said that Australia, Germany, Israel, Italy, Japan, Malta, New Zealand, Spain and the US had no conservative parties, although they had either Christian Democrats or liberals as major right-wing parties. Canada, Ireland, and Portugal had right-wing political parties that defied categorization: the Progressive Conservative Party of Canada; Fianna Fáil, Fine Gael, and Progressive Democrats in Ireland; and the Social Democratic Party of Portugal. Since then, the Swiss People's Party has moved to the extreme right and is no longer considered to be conservative.
Klaus von Beyme, who developed the method of party categorization, found that no modern Eastern European parties could be considered conservative, although the communist and communist-successor parties had strong similarities.
In Italy, which was united by liberals and radicals ("risorgimento"), liberals not conservatives emerged as the party of the Right. In the Netherlands, conservatives merged into a new Christian democratic party in 1980. In Austria, Germany, Portugal and Spain, conservatism was transformed into and incorporated into fascism or the far right. In 1940, all Japanese parties were merged into a single fascist party. Following the war, Japanese conservatives briefly returned to politics but were largely purged from public office.
Louis Hartz explained the absence of conservatism in Australia or the United States as a result of their settlement as radical or liberal fragments of Great Britain. Although he said English Canada had a negligible conservative influence, subsequent writers claimed that loyalists opposed to the American Revolution brought a Tory ideology into Canada. Hartz explained conservatism in Quebec and Latin America as a result of their settlement as feudal societies. The American conservative writer Russell Kirk provided the opinion that conservatism had been brought to the US and interpreted the American revolution as a "conservative revolution".
Conservative elites have long dominated Latin American nations. Mostly this has been achieved through control of and support for civil institutions, the church and the armed forces, rather than through party politics. Typically the church was exempt from taxes and its employees immune from civil prosecution. Where national conservative parties were weak or non-existent, conservatives were more likely to rely on military dictatorship as a preferred form of government. However in some nations where the elites were able to mobilize popular support for conservative parties, longer periods of political stability were achieved. Chile, Colombia and Venezuela are examples of nations that developed strong conservative parties. Argentina, Brazil, El Salvador and Peru are examples of nations where this did not occur. The Conservative Party of Venezuela disappeared following the Federal Wars of 1858-1863. Chile's conservative party, the National Party disbanded in 1973 following a military coup and did not re-emerge as a political force following the subsequent return to democracy.
Belgium.
Founded in 1945 as the Christian People's Party, the Flemish Christian Democrats (CD&V) dominated politics in post-war Belgium. In 1999, the party's support collapsed and it became the country's fifth largest party. Currently the N-VA (nieuw-vlaamse alliantie / new-Flemish alliance) is the largest party in Belgium.
Canada.
Canada's "Conservatives" had their roots in the Loyalists – Tories – who left America after the American Revolution. They developed in the socio-economic and political cleavages that existed during the first three decades of the 19th century, and had the support of the business, professional and established Church (Anglican) elites in Ontario and to a lesser extent in Quebec. Holding a monopoly over administrative and judicial offices, they were called the "Family Compact" in Ontario and the "Chateau Clique" in Quebec. John A. Macdonald's successful leadership of the movement to confederate the provinces and his subsequent tenure as prime minister for most of the late 19th century rested on his ability to bring together the English-speaking Protestant oligarchy and the ultramontane Catholic hierarchy of Quebec and to keep them united in a conservative coalition.
The Conservatives combined pro-market liberalism and Toryism. They generally supported an activist government and state intervention in the marketplace, and their policies were marked by "noblesse oblige", a paternalistic responsibility of the elites for the less well-off. From 1942, the party was known as the Progressive Conservatives, until 2003, when the national party merged with the Canadian Alliance to form the Conservative Party of Canada.
The conservative Union Nationale governed the province of Quebec in periods from 1936 to 1960, in a close alliance with English Canadian business elites and the Catholic Church. This period, known as the Great Darkness ended with the Quiet Revolution and the party went into terminal decline.
Colombia.
The Colombian Conservative Party, founded in 1849, traces its origins to opponents of General Francisco de Paula Santander's 1833–37 administration. While the term "liberal" had been used to describe all political forces in Colombia, the conservatives began describing themselves as "conservative liberals" and their opponents as "red liberals". From the 1860s until the present, the party has supported strong central government, and supported the Catholic Church, especially its role as protector of the sanctity of the family, and opposed separation of church and state. Its policies include the legal equality of all men, the citizen's right to own property and opposition to dictatorship. It has usually been Colombia's second largest party, with the Colombian Liberal Party being the largest.
Denmark.
Founded in 1915, the Conservative People's Party of Denmark. was the successor of "Højre" (literally "right"). In the 2005 election it won 18 out of 179 seats in the "Folketing" and became a junior partner in coalition with the Liberals. The party is preceded by 11 years by the Young Conservatives (KU), today the youth movement of the party. The Party suffered a major defeat in the parliamentary elections of September 2011 in which the party lost more than half of its seat and also lost governmental power. A liberal cultural policy dominated during the postwar period. However, by the 1990s disagreements regarding immigrants from entirely different cultures ignited a conservative backlash.
Finland.
The conservative party in Finland is the National Coalition Party (in Finnish "Kansallinen Kokoomus", "Kok"). The party was founded in 1918 when several monarchist parties united. Although in the past the party was right-wing, today it is a moderate party. While the party advocates economic liberalism, it is committed to the social market economy.
France.
Conservatism in France focused on The rejection of the French Revolution, support for the Catholic Church, and the restoration of the monarchy. The monarchist cause was on the verge of victory in the 1870s but then collapsed because of disagreements on who would be king, and what the national flag should be. Religious tensions heightened in the 1890-1910 era, but moderated after the spirit of unity in fighting the First World War. An extreme form of conservatism characterized the Vichy regime of 1940-1944 with heightened anti-Semitism, opposition to individualism, emphasis on family life, and national direction of the economy.
Following the Second World War, conservatives in France supported Gaullist groups and have been nationalistic, and emphasized tradition, order, and the regeneration of France. Gaullists held divergent views on social issues. The number of Conservative groups, their lack of stability, and their tendency to be identified with local issues defy simple categorization. Conservatism has been the major political force in France since the second world war. Unusually, post-war French conservatism was formed around the personality of a leader, Charles de Gaulle, and did not draw on traditional French conservatism, but on the Bonapartism tradition. Gaullism in France continues under the Union for a Popular Movement. The word "conservative" itself is a term of abuse in France.
Greece.
The main interwar conservative party was called the People's Party (PP), which supported constitutional monarchy and opposed the republican Liberal Party. It was able to re-group after the Second World War as part of a "United Nationalist Front" which achieved power campaigning on a simple anticommunist, ultranationalist platform. However, the vote received by the PP declined, leading them to create an expanded party, the Greek Rally, under the leadership of the charismatic General Alexandros Papagos. The conservatives opposed the far right dictatorship of the colonels (1967–1974) and established the New Democratic Party following the fall of the dictatorship. The new party had four objectives: to confront Turkish expansionism in Cyprus, to reestablish and solidify democratic rule, to give the country a strong government, and to make a powerful moderate party a force in Greek politics.
The Independent Greeks, a newly formed political party in Greece has also supported conservatism, particularly national and religious conservatism. The Founding Declaration of the Independent Greeks strongly emphasises in the preservation of the Greek state and its sovereignty, the Greek people and the Greek Orthodox Church.
Iceland.
Founded in 1926 as the Conservative Party, Iceland's Independence Party adopted its current name in 1929. From the beginning they have been the largest vote-winning party, averaging around 40%. They combine liberalism and conservatism, supporting nationalization and opposed to class conflict. While mostly in opposition during the 1930s, they embraced economic liberalism, but accepted the welfare state after the war and participated in governments supportive of state intervention and protectionism. Unlike other Scandanivian conservative (and liberal) parties, it has always had a large working-class following.
Italy.
After WW2 in Italy the conservative theories were mainly represented by the Christian Democracy, which government form the foundation of the Republic until party's dissolution in 1994. Officially DC refused the ideology of Conservatism, but in many aspects, for example family values, it was a typical social conservative party.
In 1994 the media tycoon and entrepreneur Silvio Berlusconi founded the liberal conservative Forza Italia movement. Berlusconi won three elections in 1994, 2001 and 2008 governing the country for almost ten years as Prime Minister.
Besides FI, now the conservative ideas are mainly expressed by the New Centre-Right party led by Angelino Alfano, former Berlusconi's protégé who split from the reborn Forza Italia founding a new conservative movement. Alfano is the current Minister of the Interior in the government of Matteo Renzi.
Luxembourg.
Luxembourg's major conservative party, the Christian Social People's Party (CSV or PCS) was formed as the Party of the Right in 1914, and adopted its present name in 1945. It was consistently the largest political party in Luxembourg and dominated politics throughout the 20th century.
Norway.
The Conservative Party of Norway (Norwegian: Høyre, literally "right") was formed by the old upper class of state officials and wealthy merchants to fight the populist democracy of the Liberal Party, but lost power in 1884 when parliamentarian government was first practised. It formed its first government under parliamentarism in 1889, and continued to alternate in power with the Liberals until the 1930s, when Labour became the dominant political party. It has elements both of paternalism, stressing the responsibilities of the state, and of economic liberalism. It first returned to power in the 1960s. During Kåre Willoch's premiership in the 1980s, much emphasis was laid on liberalizing the credit- and housing market and abolishing the NRK TV and radio monopoly, while supporting law and order in criminal justice and traditional norms in education
Sweden.
Sweden's conservative party, the Moderate Party, was formed in 1904, two years after the founding of the liberal party. The party emphasizes tax reductions, deregulation of private enterprise, and privatization of schools, hospitals and kindergartens.
Switzerland.
There are a number of conservative parties in Switzerland's parliament, the Federal Assembly. These include the largest, the Swiss People's Party (SVP), the Christian Democratic People's Party (CVP), represented in the Federal Council or cabinet by Doris Leuthard (in 2011), and the Conservative Democratic Party of Switzerland (BDP), which is a splinter of the SVP created after a failed attempt to expel Eveline Widmer-Schlumpf from the SVP.
The Swiss People's Party (SVP or UDC) was formed from the 1971 merger of the Party of Farmers, Traders, and Citizens, formed in 1917 and the smaller Swiss Democratic Party, formed in 1942. The SVP emphasized agricultural policy, and was strong among farmers in German-speaking Protestant areas. As Switzerland considered closer relations with the European Union in the 1990s, the SVP adopted a more militant protectionist and isolationist stance. This stance has allowed it to expand into German-speaking Catholic mountainous areas. The Anti-Defamation League, a non-Swiss lobby group based in the USA has accused them of manipulating issues such as immigration, Swiss neutrality and welfare benefits, awakening anti-Semitism and racism. The Council of Europe has called the SVP "extreme right", although some scholars dispute this classification. Hans-Georg Betz for example describes it as "populist radical right".
Modern conservatism in different countries.
While conservatism has been seen as an appeal to traditional, hierarchical society, some writers, such as Samuel P. Huntington, see it as situational. Under this definition, conservatives are seen as defending the established institutions of their time.
Australia.
The Liberal Party of Australia adheres to the principles of social conservatism and liberal conservatism. It is Liberal in the sense of economics. The party is considered to be more right-wing than the Conservative Party (UK). Other conservative parties are the National Party of Australia, a sister party of the Liberals, Family First Party, Democratic Labor Party, Shooters Party and the Katter's Australian Party.
The second largest party in the country, the Australian Labor Party's dominant faction is Labor Right, a socially conservative element. Australia is generally considered one of the most conservative western nations. Australia undertook significant economic reform under the Australian Labor Party in the mid-1980s. Consequently issues like protectionism, welfare reform, privatization and deregulation are no longer debated in the political space as they are in Europe or North America. Moser and Catley explain, "In America, 'liberal' means left-of-center, and it is a pejorative term when used by conservatives in adversarial political debate. In Australia, of course, the conservatives are in the Liberal Party." Jupp points out that, "[the] decline in English influences on Australian reformism and radicalism, and appropriation of the symbols of Empire by conservatives continued under the Liberal Party leadership of Sir Robert Menzies, which lasted until 1966."
South Korea.
South Korea's major conservative party, the Saenuri Party or 새누리당, changed its form throughout its history. First it was the Democratic-Republican Party(1963~1980); its head was Park Chung-hee who seized power in a 1961 military coup d'état and ruled as an unelected military strongman until his formal election as president in 1963. He was president for 16 years, until his assassination on October 26, 1979. The Democratic Justice Party inherited the same ideology as the Democrati-Republican Party. Its head, Chun Doo-hwan, also gained power through a coup. His followers called themselves the Hanahoe. The Democratic Justice Party changed its form and acted to suppress the opposition party and to follow the people's demand for direct elections. The party's Roh Tae-woo became the first president who was elected through direct election. The next form of the major conservative party was the Democratic-Liberal Party. Again, through election, its second leader, Kim Young-sam, became the fourteenth president of Korea. When the conservative party was beaten by the opposition party in the general election, it changed its form again to follow the party members' demand for reforms. It became the New Korean Party. It changed again one year later since the President Kim Young-sam was blamed by the citizen for the IMF. It changed its name to Grand National Party (Hannara-dang) (1998~2011). Since the late Kim Dae-jung assumed the presidency in 1998, GNP had not been the ruling party until Lee Myung-bak won the presidential election of 2007. It renamed to Saenoori Party (새누리당) in 2011.
United States.
The meaning of "conservatism" in America has little in common with the way the word is used elsewhere. As Ribuffo (2011) notes, "what Americans now call conservatism much of the world calls liberalism or neoliberalism." Since the 1950s conservatism in the United States has been chiefly associated with the Republican Party. However, during the era of segregation many Southern Democrats were conservatives, and they played a key role in the Conservative Coalition that largely controlled domestic policy in Congress from 1937 to 1963.
Major priorities within American conservatism include support for tradition, law-and-order, Christianity, anti-communism, and a defense of "Western civilization from the challenges of modernist culture and totalitarian governments." Economic conservatives and libertarians favor small government, low taxes, limited regulation, and free enterprise. Some social conservatives see traditional social values threatened by secularism, so they support school prayer and oppose abortion and homosexuality. Neoconservatives want to expand American ideals throughout the world and show a strong support for Israel. Paleoconservatives, in opposition to multiculturalism, press for restrictions on immigration. Most U.S. conservatives prefer Republicans over Democrats, and most factions favor a strong foreign policy and a strong military. The conservative movement of the 1950s attempted to bring together these divergent strands, stressing the need for unity to prevent the spread of "Godless Communism", which Reagan later labeled an "evil empire". During the Reagan administration, conservatives also supported the so-called "Reagan Doctrine" under which the U.S., as part of a Cold War strategy, provided military and other support to guerrilla insurgencies that were fighting governments identified as socialist or communist.
Other modern conservative positions include opposition to world government and opposition to environmentalism. On average, American conservatives desire tougher foreign policies than liberals do.
Most recently, the Tea Party movement, founded in 2009, has proven a large outlet for populist American conservative ideas. Their stated goals include rigorous adherence to the U.S. Constitution, lower taxes, and opposition to a growing role for the federal government in health care. Electorally, it was considered a key force in Republicans reclaiming control of the U.S. House of Representatives in 2010.
Characteristics of conservatism in France, Italy, Russia, UK and US.
This is a broad Checklist of modern conservatism in five major countries.
Psychology.
Following the Second World War, psychologists conducted research into the different motives and tendencies that account for ideological differences between left and right. The early studies focused on conservatives, beginning with Theodor W. Adorno's "The Authoritarian Personality" (1950) based on the F-scale personality test. This book has been heavily criticized on theoretical and methodological grounds, but some of its findings have been confirmed by further empirical research.
In 1973, British psychologist Glenn Wilson published an influential book providing evidence that a general factor underlying conservative beliefs is "fear of uncertainty". A meta-analysis of research literature by Jost, Glaser, Kruglanski, and Sulloway in 2003 found that many factors, such as intolerance of ambiguity and need for cognitive closure, contribute to the degree of one's political conservatism. A study by Kathleen Maclay stated these traits "might be associated with such generally valued characteristics as personal commitment and unwavering loyalty." The research also suggested that while most people are resistant to change, liberals are more tolerant of it.
According to psychologist Bob Altemeyer, individuals who are politically conservative tend to rank high in right-wing authoritarianism on his RWA scale. This finding was echoed by Theodor Adorno. A study done on Israeli and Palestinian students in Israel found that RWA scores of right-wing party supporters were significantly higher than those of left-wing party supporters. However, a 2005 study by H. Michael Crowson and colleagues suggested a moderate gap between RWA and other conservative positions. "The results indicated that conservatism is not synonymous with RWA."
Psychologist Felicia Pratto and her colleagues have found evidence to support the idea that a high social dominance orientation (SDO) is strongly correlated with conservative political views, and opposition to social engineering to promote equality, though Pratto's findings have been highly controversial. Pratto and her colleagues found that high SDO scores were highly correlated with measures of prejudice. They were refuted in this claim by David J. Schneider, who wrote that "correlations between prejudice and political conservative are reduced virtually to zero when controls for SDO are instituted". Kenneth Minogue criticized Pratto's work, saying "It is characteristic of the conservative temperament to value established identities, to praise habit and to respect prejudice, not because it is irrational, but because such things anchor the darting impulses of human beings in solidities of custom which we do not often begin to value until we are already losing them. Radicalism often generates youth movements, while conservatism is a condition found among the mature, who have discovered what it is in life they most value."
A 1996 study on the relationship between racism and conservatism found that the correlation was stronger among more educated individuals, though "anti-Black affect had essentially no relationship with political conservatism at any level of educational or intellectual sophistication". They also found that the correlation between racism and conservatism could be entirely accounted for by their mutual relationship with social dominance orientation.
A 2008 research report found that conservatives are happier than liberals, and that as income inequality increases, this difference in relative happiness increases, because conservatives (more than liberals) possess an ideological buffer against the negative hedonic effects of economic inequality.
References.
</dl>

</doc>
<doc id="6677" url="http://en.wikipedia.org/wiki?curid=6677" title="Classical liberalism">
Classical liberalism

Classical liberalism is a political ideology, a branch of liberalism which advocates civil liberties and political freedom with representative democracy under the rule of law and emphasizes economic freedom.
Classical liberalism developed in the 19th century in Europe and the United States. Although classical liberalism built on ideas that had already developed by the end of the 18th century, it advocated a specific kind of society, government and public policy as a response to the Industrial Revolution and urbanization. Notable individuals whose ideas have contributed to classical liberalism include John Locke, Jean-Baptiste Say, Thomas Malthus, and David Ricardo. It drew on the economics of Adam Smith and on a belief in natural law, utilitarianism, and progress.
In the late 19th century, classical liberalism developed into neo-classical liberalism, which argued for government to be as small as possible to allow the exercise of individual freedom. In its most extreme form, it advocated Social Darwinism. Libertarianism is a modern form of neo-classical liberalism.
Meaning of the term.
The term "classical liberalism" was applied in retrospect to distinguish earlier 19th-century liberalism from the newer social liberalism. The phrase "classical liberalism" is also sometimes used to refer to all forms of liberalism before the 20th century, and some conservatives and libertarians use the term classical liberalism to describe their belief in the primacy of individual freedom and minimal government. It is not always clear which meaning is intended.
Evolution of core beliefs.
Core beliefs of classical liberals included new ideas—which departed from both the older conservative idea of society as a family and from later sociological concept of society as complex set of social networks—that individuals were "egoistic, coldly calculating, essentially inert and atomistic" and that society was no more than the sum of its individual members.
These beliefs were complemented by a belief that labourers could be best motivated by financial incentive. This led classical liberal politicians at the time to pass the Poor Law Amendment Act 1834, which limited the provision of social assistance, because classical liberals believed in markets as the mechanism that would most efficiently lead to wealth. Adopting Thomas Malthus's population theory, they saw poor urban conditions as inevitable; they believed population growth would outstrip food production, and they regarded that consequence desirable, because starvation would help limit population growth. They opposed any income or wealth redistribution, which they believed would be dissipated by the lowest orders.
Classical liberals agreed with Thomas Hobbes that government had been created by individuals to protect themselves from one another. They thought that individuals should be free to pursue their self-interest without control or restraint by government. Classical liberals believed that individuals should be free to obtain work from the highest-paying employers, while the profit motive would ensure that products that people desired were produced at prices they would pay. In a free market, both labour and capital would receive the greatest possible reward, while production would be organised efficiently to meet consumer demand.
Drawing on selected ideas of Adam Smith, classical liberals believed that all individuals are able to equally freely pursue their own economic self-interest, without government direction, serving the common good. They were critical of welfare state as interfering in a free market. They criticised labour's group rights being pursued at the expense of individual rights, while they accepted big corporations' rights being pursued at the expense of inequality of bargaining power noted by Adam Smith:
A landlord, a farmer, a master manufacturer, a merchant, though they did not employ a single workman, could generally live a year or two upon the stocks which they have already acquired. Many workmen could not subsist a week, few could subsist a month, and scarce any a year without employment. In the long run the workman may be as necessary to his master as his master is to him; but the necessity is not so immediate.
It was not until emergence of social liberalism that child labour was forbidden, minimum standards of worker safety were introduced, a minimum wage and old age pensions were established, and financial institutions regulations with the goal of fighting cyclic depressions, monopolies, and cartels, were introduced. They were met by classical liberalism as an unjust interference of the state. So called slim state was argued for, instead, serving only the following functions:
They assert that rights are of a "negative" nature which require other individuals (and governments) to refrain from interfering with the free market, whereas social liberals asserts that individuals have positive rights, such as the right to vote, the right to an education, the right to health care, and the right to a living wage. For society to guarantee positive rights requires taxation over and above the minimum needed to enforce negative rights.
Core beliefs of classical liberals did not necessarily include democracy where law is made by majority vote by citizens, because "there is nothing in the bare idea of majority rule to show that majorities will always respect the rights of property or maintain rule of law." For example, James Madison argued for a constitutional republic with protections for individual liberty over a pure democracy, reasoning that, in a pure democracy, a "common passion or interest will, in almost every case, be felt by a majority of the whole...and there is nothing to check the inducements to sacrifice the weaker party..."
Hayek's typology of beliefs.
Friedrich Hayek identified two different traditions within classical liberalism: the "British tradition" and the "French tradition". Hayek saw the British philosophers Bernard Mandeville, David Hume, Adam Smith, Adam Ferguson, Josiah Tucker and William Paley as representative of a tradition that articulated beliefs in empiricism, the common law, and in traditions and institutions which had spontaneously evolved but were imperfectly understood. The French tradition included Rousseau, Condorcet, the Encyclopedists and the Physiocrats. This tradition believed in rationalism and sometimes showed hostility to tradition and religion. Hayek conceded that the national labels did not exactly correspond to those belonging to each tradition: Hayek saw the Frenchmen Montesquieu, Constant and Tocqueville as belonging to the "British tradition" and the British Thomas Hobbes, Priestley, Richard Price and Thomas Paine as belonging to the "French tradition". Hayek also rejected the label "laissez faire" as originating from the French tradition and alien to the beliefs of Hume and Smith.
History.
Classical liberalism in Britain developed from Whiggery and radicalism, and represented a new political ideology. Whiggery had become a dominant ideology following the Glorious Revolution of 1688, and was associated with the defence of Parliament, upholding the rule of law and defending landed property. The origins of rights were seen as being in an ancient constitution, which had existed from time immemorial. These rights, which some Whigs considered to include freedom of the press and freedom of speech, were justified by custom rather than by natural rights. They believed that the power of the executive had to be constrained. While they supported limited suffrage, they saw voting as a privilege, rather than as a right. However there was no consistency in Whig ideology, and diverse writers including John Locke, David Hume, Adam Smith and Edmund Burke were all influential among Whigs, although none of them was universally accepted.
British radicals, from the 1790s to the 1820s, concentrated on parliamentary and electoral reform, emphasising natural rights and popular sovereignty. Richard Price and Joseph Priestley adapted the language of Locke to the ideology of radicalism. The radicals saw parliamentary reform as a first step toward dealing with their many grievances, including the treatment of Protestant Dissenters, the slave trade, high prices and high taxes.
There was greater unity to classical liberalism ideology than there had been with Whiggery. Classical liberals were committed to individualism, liberty and equal rights. They believed that required a free economy with minimal government interference. Writers such as John Bright and Richard Cobden opposed both aristocratic privilege and property, which they saw as an impediment to the development of a class of yeoman farmers. Some elements of Whiggery opposed this new thinking, and were uncomfortable with the commercial nature of classical liberalism. These elements became associated with conservatism.
Classical liberalism was the dominant political theory in Britain from the early 19th century until the First World War. Its notable victories were the Catholic Emancipation Act of 1829, the Reform Act of 1832, and the repeal of the Corn Laws in 1846. The Anti-Corn Law League brought together a coalition of liberal and radical groups in support of free trade under the leadership of Richard Cobden and John Bright, who opposed militarism and public expenditure. Their policies of low public expenditure and low taxation were adopted by William Ewart Gladstone when he became chancellor of the exchequer and later prime minister. Classical liberalism was often associated with religious dissent and nonconformism.
Although classical liberals aspired to a minimum of state activity, they accepted the principle of government intervention in the economy from the early 19th century with passage of the Factory Acts. From around 1840 to 1860, "laissez-faire" advocates of the Manchester School and writers in "The Economist" were confident that their early victories would lead to a period of expanding economic and personal liberty and world peace but would face reversals as government intervention and activity continued to expand from the 1850s. Jeremy Bentham and James Mill, although advocates of "laissez faire", non-intervention in foreign affairs, and individual liberty, believed that social institutions could be rationally redesigned through the principles of Utilitarianism. The Conservative prime minister, Benjamin Disraeli, rejected classical liberalism altogether and advocated Tory Democracy. By the 1870s, Herbert Spencer and other classical liberals concluded that historical development was turning against them. By the First World War, the Liberal Party had largely abandoned classical liberal principles.
The changing economic and social conditions of the 19th century led to a division between neo-classical and social (or welfare) liberals who, while agreeing on the importance of individual liberty, differed on the role of the state. Neo-classical liberals, who called themselves "true liberals", saw Locke's "Second Treatise" as the best guide, and emphasised "limited government", while social liberals supported government regulation and the welfare state. Herbert Spencer in Britain and William Graham Sumner were the leading neo-classical liberal theorists of the 19th century. Neo-classical liberalism has continued into the contemporary era, with writers such as John Rawls. The evolution from classical to social/welfare liberalism is reflected in Britain in, for example, the evolution of the thought of John Maynard Keynes.
In the United States, liberalism took a strong root because it had little opposition to its ideals, whereas in Europe liberalism was opposed by many reactionary interests. In a nation of farmers, especially farmers whose workers were slaves, little attention was paid to the economic aspects of liberalism. Thomas Jefferson adopted many of the ideals of liberalism but, in the Declaration of Independence, changed Locke's "life, liberty, and property" to the more socially liberal "life, liberty, and the pursuit of happiness". As America grew, industry became a larger and larger part of American life; and, during the term of America's first populist president, Andrew Jackson, economic questions came to the forefront. The economic ideas of the Jacksonian era were almost universally the ideas of classical liberalism. Freedom was maximised when the government took a "hands off" attitude toward industrial development and supported the value of the currency by freely exchanging paper money for gold. The ideas of classical liberalism remained essentially unchallenged until a series of depressions, thought to be impossible according to the tenets of classical economics, led to economic hardship from which the voters demanded relief. In the words of William Jennings Bryan, "You shall not crucify the American farmer on a cross of gold." Classical liberalism remained the orthodox belief among American businessmen until the Great Depression. The Great Depression saw a sea change in liberalism, leading to the development of modern liberalism. In the words of Arthur Schlesinger Jr.:
When the growing complexity of industrial conditions required increasing government intervention in order to assure more equal opportunities, the liberal tradition, faithful to the goal rather than to the dogma, altered its view of the state," and "there emerged the conception of a social welfare state, in which the national government had the express obligation to maintain high levels of employment in the economy, to supervise standards of life and labour, to regulate the methods of business competition, and to establish comprehensive patterns of social security.
Intellectual sources.
John Locke.
Central to classical liberal ideology was their interpretation of John Locke's "Second Treatise of Government" and "A Letter Concerning Toleration", which had been written as a defence of the Glorious Revolution of 1688. Although these writings were considered too radical at the time for Britain's new rulers, they later came to be cited by Whigs, radicals and supporters of the American Revolution. However, much of later liberal thought was absent in Locke's writings or scarcely mentioned, and his writings have been subject to various interpretations. There is little mention, for example, of constitutionalism, the separation of powers, and limited government.
James L. Richardson identified five central themes in Locke's writing: individualism, consent, the concepts of the rule of law and government as trustee, the significance of property, and religious toleration. Although Locke did not develop a theory of natural rights, he envisioned individuals in the state of nature as being free and equal. The individual, rather than the community or institutions, was the point of reference. Locke believed that individuals had given consent to government and therefore authority derived from the people rather than from above. This belief would influence later revolutionary movements.
As a trustee, Government was expected to serve the interests of the people, not the rulers, and rulers were expected to follow the laws enacted by legislatures. Locke also held that the main purpose of men uniting into commonwealths and governments was for the preservation of their property. Despite the ambiguity of Locke's definition of property, which limited property to "as much land as a man tills, plants, improves, cultivates, and can use the product of", this principle held great appeal to individuals possessed of great wealth.
Locke held that the individual had the right to follow his own religious beliefs and that the state should not impose a religion against Dissenters. But there were limitations. No tolerance should be shown for atheists, who were seen as amoral, or to Catholics, who were seen as owing allegiance to the Pope over their own national government.
Adam Smith.
Adam Smith's "The Wealth of Nations", published in 1776, was to provide most of the ideas of economics, at least until the publication of J. S. Mill's "Principles" in 1848. Smith addressed the motivation for economic activity, the causes of prices and the distribution of wealth, and the policies the state should follow to maximise wealth.
Smith wrote that as long as supply, demand, prices, and competition were left free of government regulation, the pursuit of material self-interest, rather than altruism, would maximise the wealth of a society through profit-driven production of goods and services. An "invisible hand" directed individuals and firms to work toward the nation's good as an unintended consequence of efforts to maximise their own gain. This provided a moral justification for the accumulation of wealth, which had previously been viewed by some as sinful.
He assumed that workers could be paid wages as low as was necessary for their survival, which was later transformed by Ricardo and Malthus into the "Iron Law of Wages". His main emphasis was on the benefit of free internal and international trade, which he thought could increase wealth through specialisation in production. He also opposed restrictive trade preferences, state grants of monopolies, and employers' organisations and trade unions. Government should be limited to defence, public works and the administration of justice, financed by taxes based on income.
Smith's economics was carried into practice in the nineteenth century with the lowering of tariffs in the 1820s, the repeal of the Poor Relief Act, that had restricted the mobility of labour, in 1834, and the end of the rule of the East India Company over India in 1858.
Say, Malthus, and Ricardo.
In addition to Adam Smith's legacy, Say's law, Malthus theories of population and Ricardo's iron law of wages became central doctrines of classical economics. The pessimistic nature of these theories provided a basis for criticism of capitalism by its opponents and helped perpetuate the tradition of calling economics the dismal science.
Jean-Baptiste Say was a French economist who introduced Adam Smith's economic theories into France and whose commentaries on Smith were read in both France and Britain. Say challenged Smith's labour theory of value, believing that prices were determined by utility and also emphasised the critical role of the entrepreneur in the economy. However neither of those observations became accepted by British economists at the time. His most important contribution to economic thinking was Say's law, which was interpreted by classical economists that there could be no overproduction in a market, and that there would always be a balance between supply and demand. This general belief influenced government policies until the 1930s. Following this law, since the economic cycle was seen as self-correcting, government did not intervene during periods of economic hardship because it was seen as futile.
Thomas Malthus wrote two books, "An essay on the principle of population", published in 1798, and "Principles of political economy", published in 1820. The second book which was a rebuttal of Say's law had little influence on contemporary economists. His first book however became a major influence on classical liberalism. In that book, Malthus claimed that population growth would outstrip food production, because population grew geometrically, while food production grew arithmetically. As people were provided with food, they would reproduce until their growth outstripped the food supply. Nature would then provide a check to growth in the forms of vice and misery. No gains in income could prevent this, and any welfare for the poor would be self-defeating. The poor were in fact responsible for their own problems which could have been avoided through self-restraint.
David Ricardo, who was an admirer of Adam Smith, covered many of the same topics but while Smith drew conclusions from broadly empirical observations, Ricardo used induction, drawing conclusions by reasoning from basic assumptions. While Ricardo accepted Smith's labour theory of value, he acknowledged that utility could influence the price of some rare items. Rents on agricultural land were seen as the production that was surplus to the subsistence required by the tenants. Wages were seen as the amount required for workers' subsistence and to maintain current population levels. According to his Iron Law of Wages, wages could never rise beyond subsistence levels. Ricardo explained profits as a return on capital, which itself was the product of labour. But a conclusion many drew from his theory was that profit was a surplus appropriated by capitalists to which they were not entitled.
Utilitarianism.
Utilitarianism provided the political justification for implementation of economic liberalism by British governments, which was to dominate economic policy from the 1830s. Although utilitarianism prompted legislative and administrative reform and John Stuart Mill's later writings on the subject foreshadowed the welfare state, it was mainly used as a justification for "laissez faire".
The central concept of utilitarianism, which was developed by Jeremy Bentham, was that public policy should seek to provide "the greatest happiness of the greatest number". While this could be interpreted as a justification for state action to reduce poverty, it was used by classical liberals to justify inaction with the argument that the net benefit to all individuals would be higher.
Political economy.
Classical liberals saw utility as the foundation for public policies. This broke both with conservative "tradition" and Lockean "natural rights", which were seen as irrational. Utility, which emphasises the happiness of individuals, became the central ethical value of all liberalism. Although utilitarianism inspired wide-ranging reforms, it became primarily a justification for "laissez-faire" economics. However, classical liberals rejected Adam Smith's belief that the "invisible hand" would lead to general benefits and embraced Thomas Robert Malthus' view that population expansion would prevent any general benefit and David Ricardo's view of the inevitability of class conflict. Laissez faire was seen as the only possible economic approach, and any government intervention was seen as useless and harmful. The Poor Law Amendment Act 1834 was defended on "scientific or economic principles" while the authors of the Elizabethan Poor Law of 1601 were seen as not having had the benefit of reading Malthus.
Commitment to "laissez faire", however, was not uniform. Some economists advocated state support of public works and education. Classical liberals were also divided on free trade. Ricardo, for example, expressed doubt that the removal of grain tariffs advocated by Richard Cobden and the Anti-Corn Law League would have any general benefits. Most classical liberals also supported legislation to regulate the number of hours that children were allowed to work and usually did not oppose factory reform legislation.
Despite the pragmatism of classical economists, their views were expressed in dogmatic terms by such popular writers as Jane Marcet and Harriet Martineau. The strongest defender of "laissez faire" was "The Economist" founded by James Wilson in 1843. "The Economist" criticised Ricardo for his lack of support for free trade and expressed hostility to welfare, believing that the lower orders were responsible for their economic circumstances. "The Economist" took the position that regulation of factory hours was harmful to workers and also strongly opposed state support for education, health, the provision of water, and granting of patents and copyrights.
"The Economist" also campaigned against the Corn Laws that protected landlords in the United Kingdom of Great Britain and Ireland against competition from less expensive foreign imports of cereal products. A rigid belief in "laissez faire" guided the government response in 1846–1849 to the Great Famine in Ireland, during which an estimated 1.5 million people died. The minister responsible for economic and financial affairs, Charles Wood, expected that private enterprise and free trade, rather than government intervention, would alleviate the famine. The Corn Laws were finally repealed in 1846 by removal tariffs on grain which kept the price of bread artificially high. However, repeal of the Corn Laws came too late to stop Irish famine, partly because it was done in stages over three years.
Free trade and world peace.
Several liberals, including Adam Smith and Richard Cobden, argued that the free exchange of goods between nations could lead to world peace. Erik Gartzke states, "Scholars like Montesquieu, Adam Smith, Richard Cobden, Norman Angell, and Richard Rosecrance have long speculated that free markets have the potential to free states from the looming prospect of recurrent warfare." American political scientists John R. Oneal and Bruce M. Russett, well known for their work on the democratic peace theory, state:
The classical liberals advocated policies to increase liberty and prosperity. They sought to empower the commercial class politically and to abolish royal charters, monopolies, and the protectionist policies of mercantilism so as to encourage entrepreneurship and increase productive efficiency. They also expected democracy and laissez-faire economics to diminish the frequency of war.
Adam Smith argued in the "Wealth of Nations" that, as societies progressed from hunter gatherers to industrial societies, the spoils of war would rise but that the costs of war would rise further, making war difficult and costly for industrialised nations.
... the honours, the fame, the emoluments of war, belong not to [the middle and industrial classes]; the battle-plain is the harvest field of the aristocracy, watered with the blood of the people...Whilst our trade rested upon our foreign dependencies, as was the case in the middle of the last century...force and violence, were necessary to command our customers for our manufacturers...But war, although the greatest of consumers, not only produces nothing in return, but, by abstracting labour from productive employment and interrupting the course of trade, it impedes, in a variety of indirect ways, the creation of wealth; and, should hostilities be continued for a series of years, each successive war-loan will be felt in our commercial and manufacturing districts with an augmented pressure—Richard Cobden
When goods cannot cross borders, armies will.—Frédéric Bastiat
By virtue of their mutual interest does nature unite people against violence and war…the spirit of trade cannot coexist with war, and sooner or later this spirit dominates every people. For among all those powers…that belong to a nation, financial power may be the most reliable in forcing nations to pursue the noble cause of peace…and wherever in the world war threatens to break out, they will try to head it off through mediation, just as if they were permanently leagued for this purpose.
Cobden believed that military expenditures worsened the welfare of the state and benefited a small but concentrated elite minority, summing up British imperialism, which he believed was the result of the economic restrictions of mercantilist policies. To Cobden, and many classical liberals, those who advocated peace must also advocate free markets.
The belief that free trade would promote peace was widely shared by English liberals of the 19th and early 20th century, leading the economist John Maynard Keynes (1883–1946), who was a classical liberal in his early life, to say that this was a doctrine on which he was "brought up" and which he held unquestioned until at least the 1920s. A related manifestation of this idea was the argument of Norman Angell (1872–1967), most famously before World War I in "The Great Illusion" (1909), that the interdependence of the economies of the major powers was now so great that war between them was futile and irrational (and therefore unlikely).
Relationship to modern liberalism.
Many modern scholars of liberalism argue that no particularly meaningful distinction between classical and modern liberalism exists. Alan Wolfe summarises this viewpoint, which:
reject(s) any such distinction and argue(s) instead for the existence of a continuous liberal understanding that includes both Adam Smith and John Maynard Keynes... The idea that liberalism comes in two forms assumes that the most fundamental question facing mankind is how much government intervenes into the economy... When instead we discuss human purpose and the meaning of life, Adam Smith and John Maynard Keynes are on the same side. Both of them possessed an expansive sense of what we are put on this earth to accomplish. Both were on the side of enlightenment. Both were optimists who believed in progress but were dubious about grand schemes that claimed to know all the answers. For Smith, mercantilism was the enemy of human liberty. For Keynes, monopolies were. It makes perfect sense for an eighteenth-century thinker to conclude that humanity would flourish under the market. For a twentieth century thinker committed to the same ideal, government was an essential tool to the same end... [M]odern liberalism is instead the logical and sociological outcome of classical liberalism.
According to William J. Novak, however, "liberalism" in the United States shifted, "between 1877 and 1937...from laissez-faire constitutionalism to New Deal statism, from classical liberalism to democratic social-welfarism".
L. T. Hobhouse in "" (1911) attributed this purported shift, which included qualified acceptance of government intervention in the economy and the collective right to equality in dealings, to an increased desire for "just consent". Hayek wrote that Hobhouse's book would have been more accurately titled "Socialism", and Hobhouse himself called his beliefs "liberal socialism".
References.
</dl>

</doc>
<doc id="6678" url="http://en.wikipedia.org/wiki?curid=6678" title="Cat">
Cat

The domestic cat (Felis catus or Felis silvestris catus) is a small, usually furry, domesticated, and carnivorous mammal. They are often called a housecat when kept as an indoor pet or simply a cat when there is no need to distinguish them from other felids and felines. Cats are often valued by humans for companionship and their ability to hunt pests.
Cats are similar in anatomy to the other felids, with strong, flexible bodies, quick reflexes, sharp retractable claws, and teeth adapted to killing small prey. Cat senses fit a crepuscular and predatory ecological niche. Cats can hear sounds too faint or too high in frequency for human ears, such as those made by mice and other small animals. They can see in near darkness. Like most other mammals, cats have poorer color vision and a better sense of smell than humans.
Despite being solitary hunters, cats are a social species and cat communication includes the use of a variety of vocalizations (mewing, purring, trilling, hissing, growling and grunting), as well as cat pheromones and types of cat-specific body language.
Cats have a high breeding rate. Under controlled breeding, they can be bred and shown as registered pedigree pets, a hobby known as cat fancy. Failure to control the breeding of pet cats by neutering and the abandonment of former household pets, has resulted in large numbers of feral cats worldwide, requiring population control. This has led to extinction of many bird species.
Since cats were cult animals in ancient Egypt, they were commonly believed to have been domesticated there, but there may have been instances of domestication as early as the Neolithic from around 9,500 years ago (7,500 BC). A genetic study in 2007, concluded that domestic cats are descended from African wildcats ("Felis silvestris lybica"), having diverged around 8,000 BC in West Asia. Cats are the most popular pet in the world, and are now found in almost every place where humans live.
Nomenclature and etymology.
The English word 'cat' (Old English "catt") is in origin a loanword, introduced to many languages of Europe from Latin "cattus" and Byzantine Greek κάττα, including Portuguese and Spanish "gato", French "chat", German "Katze", Lithuanian "katė" and Old Church Slavonic "kotka", among others. The ultimate source of the word is Afroasiatic, presumably from Late Egyptian "čaute", the feminine of "čaus" "wildcat". The word was introduced, together with the domestic animal itself, to the Roman Republic by the first century BC.
An alternative word with cognates in many languages is English 'puss' ('pussycat'). Attested only from the 16th century, it may have been introduced from Dutch "poes" or from Low German "puuskatte", related to Swedish "kattepus", or Norwegian "pus", "pusekatt". Similar forms exist in Lithuanian "puižė" and Irish "puiscín". The etymology of this word is unknown, but it may have simply arisen from a sound used to attract a cat.
A group of cats is referred to as a "clowder" or a "glaring", a male cat is called a "tom" or "tomcat" (or a "gib", if neutered), an unaltered female is called a "queen", and a prepubescent juvenile is referred to as a "kitten". Although spayed females have no commonly used name, in some rare instances, an immature or spayed female is referred to as a "molly". The male progenitor of a cat, especially a pedigreed cat, is its "sire", and its female progenitor is its "dam". In Early Modern English, the word 'kitten' was interchangeable with the now-obsolete word 'catling'.
A pedigreed cat is one whose ancestry is recorded by a cat fancier organization. A purebred cat is one whose ancestry contains only individuals of the same breed. Many pedigreed and especially purebred cats are exhibited as show cats. Cats of unrecorded, mixed ancestry are referred to as domestic short-haired or domestic long-haired cats, by coat type, or commonly as random-bred, moggies (chiefly British), or (using terms borrowed from dog breeding) mongrels or mutt-cats.
While the African wildcat is the ancestral subspecies from which domestic cats are descended, and wildcats and domestic cats can completely interbreed, several intermediate stages occur between domestic pet and pedigree cats on one hand and those entirely wild animals on the other. The semiferal cat, a mostly outdoor cat, is not owned by any one individual, but is generally friendly to people and may be fed by several households. Feral cats are associated with human habitation areas and may be fed by people or forage in rubbish, but are typically wary of human interaction.
Taxonomy and evolution.
The felids are a rapidly evolving family of mammals that share a common ancestor only 10–15 million years ago and include lions, tigers, cougars and many others. Within this family, domestic cats ("Felis catus") are part of the genus "Felis", which is a group of small cats containing about seven species (depending upon classification scheme). Members of the genus are found worldwide and include the jungle cat ("Felis chaus") of southeast Asia, European wildcat ("F. silvestris silvestris"), African wildcat ("F. s. lybica"), the Chinese mountain cat ("F. bieti"), and the Arabian sand cat ("F. margarita"), among others.
The domestic cat was first classified as "Felis catus" by Carolus Linnaeus in the 10th edition of his "Systema Naturae" in 1758. Because of modern phylogenetics, domestic cats are usually regarded as another subspecies of the wildcat, "F. silvestris". This has resulted in mixed usage of the terms, as the domestic cat can be called by its subspecies name, "Felis silvestris catus". Wildcats have also been referred to as various subspecies of "F. catus", but in 2003, the International Commission on Zoological Nomenclature fixed the name for wildcats as "F. silvestris". The most common name in use for the domestic cat remains "F. catus", following a convention for domesticated animals of using the earliest (the senior) synonym proposed. Sometimes, the domestic cat has been called "Felis domesticus" or "Felis domestica", as proposed by German naturalist J. C. P. Erxleben in 1777 but these are not valid taxonomic names and have been used only rarely in scientific literature, because Linnaeus's binomial takes precedence. A population of Transcaucasian black feral cats was once classified as "Felis daemon" but now this population is considered to be a part of domestic cat.
All the cats in this genus share a common ancestor that probably lived around 6–7 million years ago in Asia. The exact relationships within the Felidae are close but still uncertain, e.g. the Chinese mountain cat is sometimes classified (under the name "Felis silvestris bieti") as a subspecies of the wildcat, like the North African variety "F. s. lybica".
In comparison to dogs cats have not undergone major changes during the domestication process, as the form and behavior of the domestic cat is not radically different from those of wildcats and domestic cats are perfectly capable of surviving in the wild. Fully domesticated house cats often interbreed with feral "F. catus" populations.
This limited evolution during domestication, means that hybridisation can occur with many other felids notably the Asian leopard cat. Several natural behaviors and characteristics of wildcats may have preadapted them for domestication as pets. These traits include their small size, social nature, obvious body language, love of play and relatively high intelligence;:12–17 they may also have an inborn tendency towards tameness.
Cats have either a mutualistic or commensal relationship with humans. 
Two main theories are given about how cats were domesticated. In one, people deliberately tamed cats in a process of artificial selection, as they were useful predators of vermin. This has been criticized as implausible, because the reward for such an effort may have been too little; cats generally do not carry out commands and although they do eat rodents, other species such as ferrets or terriers may be better at controlling these pests. The alternative idea is that cats were simply tolerated by people and gradually diverged from their wild relatives through natural selection, as they adapted to hunting the vermin found around humans in towns and villages.
Genetics.
The domesticated cat and its closest wild ancestor are both diploid organisms that possess 38 chromosomes and roughly 20,000 genes. About 250 heritable genetic disorders have been identified in cats, many similar to human inborn errors. The high level of similarity among the metabolism of mammals allows many of these feline diseases to be diagnosed using genetic tests that were originally developed for use in humans, as well as the use of cats as animal models in the study of the human diseases.
Anatomy.
Domestic cats are similar in size to the other members of the genus "Felis", typically weighing between 4 and. However, some breeds, such as the Maine Coon, can occasionally exceed 11 kg (25 lb). Conversely, very small cats (less than 1.8 kg) have been reported. The world record for the largest cat is 21.3 kg. The smallest adult cat ever officially recorded weighed around 1.36 kg. Feral cats tend to be lighter as they have more limited access to food than house cats. In the Boston area, the average feral adult male will weigh 3.9 kg and average feral female 3.3 kg. Cats average about 23–25 cm (9–10 in) in height and 46 cm (18.1 in) in head/body length (males being larger than females), with tails averaging 30 cm (11.8 in) in length.
Cats have seven cervical vertebrae, as do almost all mammals; 13 thoracic vertebrae (humans have 12); seven lumbar vertebrae (humans have five); three sacral vertebrae like most mammals (humans have five); and a variable number of caudal vertebrae in the tail (humans retain three to five caudal vertebrae, fused into an internal coccyx).:11 The extra lumbar and thoracic vertebrae account for the cat's spinal mobility and flexibility. Attached to the spine are 13 ribs, the shoulder, and the pelvis. :16 Unlike human arms, cat forelimbs are attached to the shoulder by free-floating clavicle bones which allow them to pass their bodies through any space into which they can fit their heads.
The cat skull is unusual among mammals in having very large eye sockets and a powerful and specialized jaw.:35 Within the jaw, cats have teeth adapted for killing prey and tearing meat. When it overpowers its prey, a cat delivers a lethal neck bite with its two long canine teeth, inserting them between two of the prey's vertebrae and severing its spinal cord, causing irreversible paralysis and death. Compared to other felines, domestic cats have narrowly spaced canine teeth, which is an adaptation to their preferred prey of small rodents, which have small vertebrae. The premolar and first molar together compose the carnassial pair on each side of the mouth, which efficiently shears meat into small pieces, like a pair of scissors. These are vital in feeding, since cats' small molars cannot chew food effectively.:37
Cats, like dogs, are digitigrades. They walk directly on their toes, with the bones of their feet making up the lower part of the visible leg. Cats are capable of walking very precisely, because like all felines, they directly register; that is, they place each hind paw (almost) directly in the print of the corresponding fore paw, minimizing noise and visible tracks. This also provides sure footing for their hind paws when they navigate rough terrain. Unlike most mammals, when cats walk, they use a "pacing" gait; that is, they move the two legs on one side of the body before the legs on the other side. This trait is shared with camels and giraffes. As a walk speeds up into a trot, a cat's gait changes to be a "diagonal" gait, similar to that of most other mammals (and many other land animals, such as lizards): the diagonally opposite hind and fore legs move simultaneously.
Like almost all members of the Felidae, cats have protractable and retractable claws. In their normal, relaxed position, the claws are sheathed with the skin and fur around the paw's toe pads. This keeps the claws sharp by preventing wear from contact with the ground and allows the silent stalking of prey. The claws on the fore feet are typically sharper than those on the hind feet. Cats can voluntarily extend their claws on one or more paws. They may extend their claws in hunting or self-defense, climbing, kneading, or for extra traction on soft surfaces. Most cats have five claws on their front paws, and four on their rear paws. The fifth front claw (the dewclaw) is proximal to the other claws. More proximally is a protrusion which appears to be a sixth "finger". This special feature of the front paws, on the inside of the wrists, is the carpal pad, also found on the paws of big cats and of dogs. It has no function in normal walking, but is thought to be an antiskidding device used while jumping. Some breeds of cats are prone to polydactyly (extra toes and claws). These are particularly common along the northeast coast of North America.
Physiology.
Cats are familiar and easily kept animals, and their physiology has been particularly well studied; it generally resembles that of other carnivorous mammals, but displays several unusual features probably attributable to cats' descent from desert-dwelling species. For instance, cats are able to tolerate quite high temperatures: Humans generally start to feel uncomfortable when their skin temperature passes about 38 °C (100 °F), but cats show no discomfort until their skin reaches around 52 °C (126 °F),:46 and can tolerate temperatures of up to 56 °C (133 °F) if they have access to water.
Cats conserve heat by reducing the flow of blood to their skin and lose heat by evaporation through their mouths. Cats have minimal ability to sweat, with glands located primarily in their paw pads, and pant for heat relief only at very high temperatures (but may also pant when stressed). A cat's body temperature does not vary throughout the day; this is part of cats' general lack of circadian rhythms and may reflect their tendency to be active both during the day and at night.:1 Cats' feces are comparatively dry and their urine is highly concentrated, both of which are adaptations to allow cats to retain as much water as possible. Their kidneys are so efficient, they can survive on a diet consisting only of meat, with no additional water, and can even rehydrate by drinking seawater.:29
Cats are obligate carnivores: their physiology has evolved to efficiently process meat, and they have difficulty digesting plant matter. In contrast to omnivores such as rats, which only require about 4% protein in their diet, about 20% of a cat's diet must be protein. Cats are unusually dependent on a constant supply of the amino acid arginine, and a diet lacking arginine causes marked weight loss and can be rapidly fatal. Another unusual feature is that the cat cannot produce taurine, with taurine deficiency causing macular degeneration, wherein the cat's retina slowly degenerates, causing irreversible blindness. Since cats tend to eat all of their prey, they obtain minerals by digesting animal bones, and a diet composed only of meat may cause calcium deficiency.
A cat's gastrointestinal tract is adapted to meat eating, being much shorter than that of omnivores and having low levels of several of the digestive enzymes needed to digest carbohydrates. These traits severely limit the cat's ability to digest and use plant-derived nutrients, as well as certain fatty acids. Despite the cat's meat-oriented physiology, several vegetarian or vegan cat foods have been marketed that are supplemented with chemically synthesized taurine and other nutrients, in attempts to produce a complete diet. However, some of these products still fail to provide all the nutrients cats require, and diets containing no animal products pose the risk of causing severe nutritional deficiencies.
Cats do eat grass occasionally. A proposed explanations is that cats use grass as a source of folic acid. Another proposed explanation is that it is used to supply dietary fiber.
Senses.
Cats have excellent night vision and can see at only one-sixth the light level required for human vision.:43 This is partly the result of cat eyes having a tapetum lucidum, which reflects any light that passes through the retina back into the eye, thereby increasing the eye's sensitivity to dim light. Another adaptation to dim light is the large pupils of cats' eyes. Unlike some big cats, such as tigers, domestic cats have slit pupils. These slit pupils can focus bright light without chromatic aberration, and are needed since the domestic cat's pupils are much larger, relative to their eyes, than the pupils of the big cats. Indeed, at low light levels a cat's pupils will expand to cover most of the exposed surface of its eyes. However, domestic cats have rather poor color vision and (like most nonprimate mammals) have only two types of cones, optimized for sensitivity to blue and yellowish green; they have limited ability to distinguish between red and green. A 1993 paper reported a response to middle wavelengths from a system other than the rods which might be due to a third type of cone. However, this appears to be an adaptation to low light levels rather than representing true trichromatic vision.
Cats have excellent hearing and can detect an extremely broad range of frequencies. They can hear higher-pitched sounds than either dogs or humans, detecting frequencies from 55 Hz to 79,000 Hz, a range of 10.5 octaves, while humans and dogs both have ranges of about 9 octaves. Cats can hear ultrasound, which is important in hunting because many species of rodents make ultrasonic calls.However, they do not communicate using ultrasound like rodents do. Cats' hearing is also sensitive and among the best of any mammal, being most acute in the range of 500 Hz to 32 kHz. This sensitivity is further enhanced by the cat's large movable outer ears (their pinnae), which both amplify sounds and help detect the direction of a noise.
Cats have an acute sense of smell, due in part to their well-developed olfactory bulb and a large surface of olfactory mucosa, about 5.8 cm2 in area, which is about twice that of humans. Cats are sensitive to pheromones such as 3-mercapto-3-methylbutan-1-ol, which they use to communicate through urine spraying and marking with scent glands. Many cats also respond strongly to plants that contain nepetalactone, especially catnip, as they can detect that substance at less than one part per billion. About 70—80% of cats are affected by nepetalactone. This response is also produced by other plants, such as silver vine ("Actinidia polygama") and the herb valerian; it may be caused by the smell of these plants mimicking a pheromone and stimulating cats' social or sexual behaviors.
Cats have relatively few taste buds compared to humans. Domestic and wild cats share a gene mutation that keeps their sweet taste buds from binding to sugary molecules, leaving them with no ability to taste sweetness. Their taste buds instead respond to amino acids, bitter tastes, and acids.
To aid with navigation and sensation, cats have dozens of movable whiskers (vibrissae) over their bodies, especially their faces. These provide information on the width of gaps and on the location of objects in the dark, both by touching objects directly and by sensing air currents; they also trigger protective blink reflexes to protect the eyes from damage.:47
Health.
The average lifespan of pet cats has risen in recent years. In the early 1980s it was about seven years,:33 rising to 9.4 years in 1995:33 and 12-15 years in 2014.
However, cats have been reported as surviving into their 30s, with the oldest known cat, Creme Puff, dying at a verified age of 38.
Spaying or neutering increases life expectancy: one study found neutered male cats live twice as long as intact males, while spayed female cats live 62% longer than intact females.:35 Non-neutered cats in the U.S. are four times as likely to be hit by a car as a neutered cat, and are three times more likely to require treatment for an animal bite.
Having a cat neutered confers health benefits, because castrated males cannot develop testicular cancer, spayed females cannot develop uterine or ovarian cancer, and both have a reduced risk of mammary cancer.
Despite widespread concern about the welfare of free-roaming cats, the lifespans of neutered feral cats in managed colonies compare favourably with those of pet cats.:45:1358 Neutered cats in managed colonies can also live long lives.
Diseases.
Cats can suffer from a wide range of health problems, including infectious diseases, parasites, injuries, and chronic disease. Vaccinations are available for many of these diseases, and domestic cats are regularly given treatments to eliminate parasites such as worms and fleas.
Poisoning.
In addition to obvious dangers such as rodenticides, insecticides, and herbicides, cats may be poisoned by many chemicals usually considered safe by their human guardians, because their livers are less effective at some forms of detoxification than those of many other animals, including humans and dogs. Some of the most common causes of poisoning in cats are antifreeze and rodent baits. Cats may be particularly sensitive to environmental pollutants. When a cat has a sudden or prolonged serious illness without any obvious cause, it has possibly been exposed to a toxin.
Many human medicines should never be given to cats. For example, the painkiller paracetamol (or acetaminophen, sold as Tylenol and Panadol) is extremely toxic to cats: even very small doses need immediate treatment and can be fatal. Even aspirin, which is sometimes used to treat arthritis in cats, is much more toxic to them than to humans and must be administered cautiously. Similarly, application of minoxidil (Rogaine) to the skin of cats, either accidentally or by well-meaning guardians attempting to counter loss of fur, has sometimes been fatal. Essential oils can be toxic to cats and cases have been reported of serious illnesses caused by tea tree oil, including flea treatments and shampoos containing it.
Other common household substances that should be used with caution around cats include mothballs and other naphthalene products. Phenol-based products (e.g. Pine-Sol, Dettol (Lysol) or hexachlorophene) are often used for cleaning and disinfecting near cats' feeding areas or litter boxes, but these can sometimes be fatal. Ethylene glycol, often used as an automotive antifreeze, is particularly appealing to cats, and as little as a teaspoonful can be fatal. Some human foods are toxic to cats; for example chocolate can cause theobromine poisoning, although (unlike dogs) few cats will eat chocolate. Large amounts of onions or garlic are also poisonous to cats. Many houseplants are also dangerous, such as "Philodendron" species and the leaves of the Easter lily ("Lilium longiflorum"), which can cause permanent and life-threatening kidney damage.
Behavior.
Free-ranging cats are active both day and night, although they tend to be slightly more active at night. The timing of cats' activity is quite flexible and varied, which means house cats may be more active in the morning and evening (crepuscular behavior), as a response to greater human activity at these times. Although they spend the majority of their time in the vicinity of their home, housecats can range many hundreds of meters from this central point, and are known to establish territories that vary considerably in size, in one study ranging from 7 to(-).
Cats conserve energy by sleeping more than most animals, especially as they grow older. The daily duration of sleep varies, usually 12–16 hours, with 13–14 being the average. Some cats can sleep as much as 20 hours in a 24-hour period. The term "cat nap" for a short rest refers to the cat's tendency to fall asleep (lightly) for a brief period. While asleep, cats experience short periods of rapid eye movement sleep often accompanied by muscle twitches, which suggests they are dreaming.
Sociability.
Although wildcats are solitary, the social behavior of domestic cats is much more variable and ranges from widely dispersed individuals to feral cat colonies that form around a food source, based on groups of co-operating females. Within such groups, one cat is usually dominant over the others. Each cat in a colony holds a distinct territory, with sexually active males having the largest territories, which are about 10 times larger than those of female cats and may overlap with several females' territories. These territories are marked by urine spraying, by rubbing objects at head height with secretions from facial glands, and by defecation. Between these territories are neutral areas where cats watch and greet one another without territorial conflicts. Outside these neutral areas, territory holders usually chase away stranger cats, at first by staring, hissing, and growling, and if that does not work, by short but noisy and violent attacks. Despite some cats cohabiting in colonies, they do not have a social survival strategy, or a pack mentality, and always hunt alone.
Domestic cats use many vocalizations for communication, including purring, trilling, hissing, growling/snarling, grunting, and several different forms of meowing. By contrast, feral cats are generally silent.:208 Their types of body language, including position of ears and tail, relaxation of whole body, and kneading of paws, are all indicators of mood. The tail and ears are particularly important social signal mechanisms in cats, e.g. with a raised tail acting as a friendly greeting, and flattened ears indicating hostility. Tail-raising also indicates the cat's position in the group's social hierarchy, with dominant individuals raising their tails less often than subordinate animals. Nose-to-nose touching is also a common greeting and may be followed by social grooming, which is solicited by one of the cats raising and tilting its head.
However, some pet cats are poorly socialized. In particular, older cats may show aggressiveness towards newly arrived kittens, which may include biting and scratching; this type of behavior is known as feline asocial aggression.
Though cats and dogs are believed to be natural enemies, they can live together if correctly socialized.
For cats, life in proximity to humans and other animals kept by them amounts to a symbiotic social adaptation. They may express great affection towards their human (and even other) companions, especially if they psychologically imprint on them at a very young age and are treated with consistent affection. Ethologically, the human keeper of a cat may function as a sort of surrogate for the cat's mother, and adult housecats live their lives in a kind of extended kittenhood, a form of behavioral neoteny. The high-pitched sounds housecats make to solicit food may mimic the cries of a hungry human infant, making them particularly hard for humans to ignore.
Grooming.
Cats are known for spending considerable amounts of time licking their coat to keep it clean. The cat's tongue has backwards-facing spines about 500 μm long, which are called papillae. These contain keratin which makes them quite rigid so the papillae act like a hairbrush. Some cats, particularly longhaired cats, occasionally regurgitate hairballs of fur that have collected in their stomachs from grooming. These clumps of fur are usually sausage-shaped and about 2–3 cm long. Hairballs can be prevented with remedies that ease elimination of the hair through the gut, as well as regular grooming of the coat with a comb or stiff brush. Some cats can develop a compulsive behavior known as psychogenic alopecia, or excessive grooming.
Fighting.
Among domestic cats, males are more likely to fight than females. Among feral cats, the most common reason for cat fighting is competition between two males to mate with a female. In such cases, most fights are won by the heavier male. Another common reason for fighting in domestic cats is the difficulty of establishing territories within a small home. Female cats also fight over territory or to defend their kittens. Neutering will decrease or eliminate this behavior in many cases, suggesting that the behavior is linked to sex hormones.
When cats become aggressive, they try to make themselves appear larger and more threatening by raising their fur, arching their backs, turning sideways and hissing or spitting. Often, the ears are pointed down and back to avoid damage to the inner ear and potentially listen for any changes behind them while focused forward. They may also vocalize loudly and bare their teeth in an effort to further intimidate their opponent. Fights usually consist of grappling and delivering powerful slaps to the face and body with the forepaws as well as bites. Cats also throw themselves to the ground in a defensive posture to rake their opponent's belly with their powerful hind legs.
Serious damage is rare, as the fights are usually short in duration, with the loser running away with little more than a few scratches to the face and ears. However, fights for mating rights are typically more severe and injuries may include deep puncture wounds and lacerations. Normally, serious injuries from fighting are limited to infections of scratches and bites, though these can occasionally kill cats if untreated. In addition, bites are probably the main route of transmission of feline immunodeficiency virus. Sexually active males are usually involved in many fights during their lives, and often have decidedly battered faces with obvious scars and cuts to their ears and noses.
Hunting and feeding.
Cats hunt small prey, primarily birds and rodents, and are often used as a form of pest control. Domestic cats are a major predator of wildlife in the United States, killing an estimated 1.4–3.7 billion birds and 6.9–20.7 billion mammals annually. The bulk of predation in the United States is done by 80 million feral and stray cats. Effective measures to reduce this population are elusive, meeting opposition from cat enthusiasts. In the case of free-ranging pets, equipping cats with bells and not letting them out at night will reduce wildlife predation. Free-fed feral cats and house cats tend to consume many small meals in a single day, although the frequency and size of meals varies between individuals. Cats use two hunting strategies, either stalking prey actively, or waiting in ambush until an animal comes close enough to be captured. Although it is not certain, the strategy used may depend on the prey species in the area, with cats waiting in ambush outside burrows, but tending to actively stalk birds.:153
Most breeds of cat have a noted fondness for settling in high places, or perching. In the wild, a higher place may serve as a concealed site from which to hunt; domestic cats may strike prey by pouncing from a perch such as a tree branch, as does a leopard. Another possible explanation is that height gives the cat a better observation point, allowing it to survey its territory. During a fall from a high place, a cat can reflexively twist its body and right itself using its acute sense of balance and flexibility. This is known as the cat righting reflex. An individual cat always rights itself in the same way, provided it has the time to do so, during a fall. The height required for this to occur is around 90 cm (3 ft). Cats without a tail (e.g. Manx cats) also have this ability, since a cat mostly moves its hind legs and relies on conservation of angular momentum to set up for landing, and the tail is little used for this feat. This leads to the proverb "a cat always lands on its feet".
The perhaps best known element of cats' hunting behavior, which is commonly misunderstood and often appalls cat owners because it looks like torture, is that cats often appear to "play" with prey by releasing it after capture. This behavior is due to an instinctive imperative to ensure that the prey is weak enough to be killed without endangering the cat. This behavior is referred to in the idiom "cat-and-mouse game" or simply "cat and mouse".
Another poorly understood element of cat hunting behavior is the presentation of prey to human guardians. Ethologist Paul Leyhausen proposed that cats adopt humans into their social group and share excess kill with others in the group according to the dominance hierarchy, in which humans are reacted to as if they are at, or near, the top. Anthropologist and zoologist Desmond Morris, in his 1986 book "Catwatching", suggests, when cats bring home mice or birds, they are attempting to teach their human to hunt, or trying to help their human as if feeding "an elderly cat, or an inept kitten". Morris's theory is inconsistent with the fact that male cats also bring home prey, despite males having no involvement with raising kittens.:153
Domestic cats select food based on its temperature, smell, and texture, strongly disliking chilled foods and responding most strongly to moist foods rich in amino acids, which are similar to meat. Cats may reject novel flavors (a response termed neophobia) and learn quickly to avoid foods that have tasted unpleasant in the past. They may also avoid sugary foods and milk; since they are lactose intolerant, these sugars are not easily digested and may cause soft stools or diarrhea. They can also develop odd eating habits. Some cats like to eat or chew on other things, most commonly wool, but also plastic, cables, paper, string, aluminum foil/Christmas tree tinsel, or even coal. This condition, pica, can threaten their health, depending on the amount and toxicity of the items eaten.
Though cats usually prey on animals more than half their size, a feral cat in Australia had been photographed to kill an adult pademelon weighing around the cat's size at 4 kg (8.8 lb).
Since cats cannot fully close their lips around something to create suction, they use a lapping method with the tongue to draw liquid upwards into their mouths. Lapping at a rate of four times a second, the cat touches the smooth tip of its tongue to the surface of the water, and quickly retracts it, drawing water upwards.
Play.
Domestic cats, especially young kittens, are known for their love of play. This behavior mimics hunting and is important in helping kittens learn to stalk, capture, and kill prey. Cats also engage in play fighting, with each other and with humans. This behavior may be a way for cats to practice the skills needed for real combat, and might also reduce any fear they associate with launching attacks on other animals.
Owing to the close similarity between play and hunting, cats prefer to play with objects that resemble prey, such as small furry toys that move rapidly, but rapidly lose interest (they become habituated) in a toy they have played with before. Cats also tend to play with toys more when they are hungry. String is often used as a toy, but if it is eaten, it can become caught at the base of the cat's tongue and then move into the intestines, a medical emergency which can cause serious illness, even death. Owing to the risks posed by cats eating string, it is sometimes replaced with a laser pointer's dot, which cats may chase. While concerns have been raised about the safety of playing with lasers, John Marshall, an ophthalmologist at St Thomas' Hospital, has stated it would be "virtually impossible" to blind a cat with a laser pointer.
Reproduction.
Female cats are seasonally polyestrous, which means they may have many periods of heat over the course of a year, the season beginning in spring and ending in late autumn. Heat periods occur about every two weeks and last about 4 to 7 days. Multiple males will be attracted to a female in heat. The males will fight over her, and the victor wins the right to mate. At first, the female rejects the male, but eventually the female allows the male to mate. The female utters a loud yowl as the male pulls out of her because a male cat's penis has a band of about 120–150 backwards-pointing penile spines, which are about 1 mm long; upon withdrawal of the penis, the spines rake the walls of the female's vagina, which is a trigger for ovulation. This act also occurs to clear the vagina of other sperm in the context of a second (or more) mating, thus giving the later males a larger chance of conception.
After mating, the female washes her vulva thoroughly. If a male attempts to mate with her at this point, the female will attack him. After about 20 to 30 minutes, once the female is finished grooming, the cycle will repeat.
Because ovulation is not always triggered by a single mating, females may not be impregnated by the first male with which they mate. Furthermore, cats are superfecund; that is, a female may mate with more than one male when she is in heat, with the result that different kittens in a litter may have different fathers.
At 124 hours after conception, the morula forms. At 148 hours, early blastocysts form. At 10–12 days, implantation occurs.
The gestation period for cats is between 64 and 67 days, with an average of 66 days. The size of a litter usually is three to five kittens, with the first litter usually smaller than subsequent litters. Kittens are weaned between six and seven weeks old, and cats normally reach sexual maturity at 5–10 months (females) and to 5–7 months (males), although this can vary depending on breed. Females can have two to three litters per year, so may produce up to 150 kittens in their breeding span of around ten years.
Cats are ready to go to new homes at about 12 weeks of age, when they are ready to leave their mother. They can be surgically sterilized (spayed or castrated) as early as 7 weeks to limit unwanted reproduction. This surgery also prevents undesirable sex-related behavior, such as aggression, territory marking (spraying urine) in males and yowling (calling) in females. Traditionally, this surgery was performed at around six to nine months of age, but it is increasingly being performed prior to puberty, at about three to six months. In the US, about 80% of household cats are neutered.
Vocalizations.
The cat is a very vocal animal. Known for its trademark purring, it also produces a wide variety of other sounds.
The mechanism by which cats purr is elusive. The cat has no unique anatomical feature that is clearly responsible for the sound. It was, until recent times, believed that only the cats of the "Felis" genus could purr. However, felids of the "Panthera" genus (tiger, lion, jaguar and leopard) also produce sounds similar to purring, but only when exhaling.
Ecology.
Habitats.
Cats are a cosmopolitan species and are found across much of the world. Geneticist Stephen James O'Brien, of the National Cancer Institute in Frederick, Maryland, remarked on how successful cats have been in evolutionary terms: "Cats are one of evolution's most charismatic creatures. They can live on the highest mountains and in the hottest deserts." They are extremely adaptable and are now present on all continents except Antarctica, and on 118 of the 131 main groups of islands—even on sub-Antarctic islands such as the Kerguelen Islands.
Feral cats can live in forests, grasslands, tundra, coastal areas, agricultural land, scrublands, urban areas, and wetlands. Their habitats even include small oceanic islands with no human inhabitants. Further, the close relatives of domestic cats, the African wildcat ("Felis silvestris lybica") and the Arabian sand cat ("Felis margarita") both inhabit desert environments, and domestic cats still show similar adaptations and behaviors. The cat's ability to thrive in almost any terrestrial habitat has led to its designation as one of the world's worst invasive species.
As domestic cats are little altered from wildcats, they can readily interbreed. This hybridization poses a danger to the genetic distinctiveness of some wildcat populations, particularly in Scotland and Hungary and possibly also the Iberian Peninsula.
Feral cats.
Feral cats are domestic cats that were born in or have reverted to a wild state. They are unfamiliar with and wary of humans and roam freely in urban and rural areas. The numbers of feral cats is not known, but estimates of the US feral population range from 25 to 60 million. Feral cats may live alone, but most are found in large colonies, which occupy a specific territory and are usually associated with a source of food. Famous feral cat colonies are found in Rome around the Colosseum and Forum Romanum, with cats at some of these sites being fed and given medical attention by volunteers.
Public attitudes towards feral cats vary widely, ranging from seeing them as free-ranging pets, to regarding them as vermin. One common approach to reducing the feral cat population is termed 'trap-neuter-return', where the cats are trapped, neutered, immunized against rabies and the feline leukemia virus, and then released. Before releasing them back into their feral colonies, the attending veterinarian often nips the tip off one ear to mark it as neutered and inoculated, since these cats may be trapped again. Volunteers continue to feed and give care to these cats throughout their lives. Given this support, their lifespans are increased, and behavior and nuisance problems caused by competition for food are reduced.
Impact on prey species.
To date, few scientific data are available to assess the impact of cat predation on prey populations. Even well-fed domestic cats may hunt and kill, mainly catching small mammals, but also birds, amphibians, reptiles, fish, and invertebrates. Hunting by domestic cats may be contributing to the decline in the numbers of birds in urban areas, although the importance of this effect remains controversial. In the wild, the introduction of feral cats during human settlement can threaten native species with extinction. In many cases, controlling or eliminating the populations of non-native cats can produce a rapid recovery in native animals. However, the ecological role of introduced cats can be more complicated. For example, cats can control the numbers of rats, which also prey on birds' eggs and young, so a cat population can protect an endangered bird species by suppressing mesopredators.
In the Southern Hemisphere, cats are a particular problem in landmasses such as Australasia, where cat species have never been native and few equivalent native medium-sized mammalian predators occurred. Native species such as the New Zealand kakapo and the Australian bettong, for example, tend to be more ecologically vulnerable and behaviorally "naive" to predation by feral cats. Feral cats have had a major impact on these native species and have played a leading role in the endangerment and extinction of many animals.
Cat numbers in the UK are growing and their abundance is far above the "natural" carrying capacity, because their population sizes are independent of their prey's dynamics: i.e. cats are "recreational" hunters, with other food sources. Population densities can be as high as 2,000 individuals per km2 and the trend is an increase of 0.5 million cats annually.
Impact on birds.
The domestic cat is probably a significant predator of birds. UK assessments indicate they may be accountable for an estimated 64.8 million bird deaths each year. Certain species appear more susceptible than others; for example, 30% of house sparrow mortality is linked to the domestic cat. In the recovery of ringed robins ("Erithacus rubecula") and dunnocks ("Prunella modularis"), 31% of deaths were a result of cat predation. The presence of larger carnivores such as coyotes which prey on cats and other small predators reduces the effect of predation by cats and other small predators such as opossums and raccoons on bird numbers and variety. The proposal that cat populations will increase when the numbers of these top predators decline is called the mesopredator release hypothesis. However, a new study suggests cats are a much greater menace than previously thought and feral cats kill several billion birds each year in the United States.
On islands, birds can contribute as much as 60% of a cat's diet. In nearly all cases, however, the cat cannot be identified as the sole cause for reducing the numbers of island birds, and in some instances, eradication of cats has caused a 'mesopredator release' effect; where the suppression of top carnivores creates an abundance of smaller predators that cause a severe decline in their shared prey. Domestic cats are, however, known to be a contributing factor to the decline of many species, a factor that has ultimately led, in some cases, to extinction. The South Island piopio, Chatham Islands rail, the Auckland Islands Merganser, and the common diving petrel are a few from a long list, with the most extreme case being the flightless Stephens Island wren, which was driven to extinction only a few years after its discovery.
Some of the same factors that have promoted adaptive radiation of island avifauna over evolutionary time appear to promote vulnerability to non-native species in modern time. The susceptibility of many island birds is undoubtedly due to evolution in the absence of mainland predators, competitors, diseases, and parasites, in addition to lower reproductive rates and extended incubation periods. The loss of flight, or reduced flying ability is also characteristic of many island endemics. These biological aspects have increased vulnerability to extinction in the presence of introduced species, such as the domestic cat. Equally, behavioral traits exhibited by island species, such as "predatory naivety" and ground-nesting, have also contributed to their susceptibility.
Cats and humans.
Cats are common pets in Europe and North America, and their worldwide population exceeds 500 million. Although cat guardianship has commonly been associated with women, a 2007 Gallup poll reported that men and women were equally likely to own a cat.
According to the Humane Society of the United States, as well as being kept as pets, cats are also used in the international fur trade, for making coats, gloves, hats, shoes, blankets, and stuffed toys. About 24 cats are needed to make a cat fur coat. This use has now been outlawed in the United States, Australia, and the European Union. However, some cat furs are still made into blankets in Switzerland as folk remedies believed to help rheumatism.
A few attempts to build a cat census have been made over the years, both through associations or national and international organizations (such as the Canadian Federation of Humane Societies's one) and over the net, but such a task does not seem simple to achieve. General estimates for the global population of domestic cats range widely from anywhere between 200 million to 600 million.
History and mythology.
Traditionally, historians tended to think ancient Egypt was the site of cat domestication, owing to the clear depictions of house cats in Egyptian paintings about 3,600 years old. However, in 2004, a Neolithic grave excavated in Shillourokambos, Cyprus, contained the skeletons, laid close to one another, of both a human and a cat. The grave is estimated to be 9,500 years old, pushing back the earliest known feline–human association significantly. The cat specimen is large and closely resembles the African wildcat ("F. s. lybica"), rather than present-day domestic cats. This discovery, combined with genetic studies, suggests cats were probably domesticated in the Middle East, in the Fertile Crescent around the time of the development of agriculture and then they were brought to Cyprus and Egypt.
Direct evidence for the domestication of cats 5,300 years ago in Quanhucun in China has been published. The cats are believed to have been attracted to the village by rodents, which in turn were attracted by grain cultivated and stored by humans.
In ancient Egypt, cats were sacred animals, with the goddess Bastet often depicted in cat form, sometimes taking on the war-like aspect of a lioness.:220 The Romans are often credited with introducing the domestic cat from Egypt to Europe;:223 in Roman Aquitaine, a first- or second-century epitaph of a young girl holding a cat is one of two earliest depictions of the Roman domesticated cat. However, cats possibly were already kept in Europe prior to the Roman Empire, as they may have been present in Britain in the late Iron Age. Domestic cats were spread throughout much of the rest of the world during the Age of Discovery, as they were carried on sailing ships to control shipboard rodents and as good-luck charms.:223
Several ancient religions believed cats are exalted souls, companions or guides for humans, that are all-knowing but mute so they cannot influence decisions made by humans. In Japan, the "maneki neko" cat is a symbol of good fortune.
Although no species are sacred in Islam, cats are revered by Muslims. Some Western writers have stated Muhammad had a favorite cat, Muezza. He is reported to have loved cats so much, "he would do without his cloak rather than disturb one that was sleeping on it". The story has no origin in early Muslim writers, and seems to confuse a story of a later Sufi saint, Ahmed ar-Rifa'i centuries after Muhammad.
Freyja, the goddess of love, beauty, and fertility in Norse mythology, is depicted as riding a chariot drawn by cats.
Many cultures have negative superstitions about cats. An example would be the belief that a black cat "crossing one's path" leads to bad luck, or that cats are witches' familiars used to augment a witch's powers and skills. The killing of cats in Medieval Ypres, Belgium, is commemorated in the innocuous present-day Kattenstoet (cat parade).
According to a myth in many cultures, cats have multiple lives. In many countries, they are believed to have nine lives, but in Italy, Germany, Greece, and some Spanish-speaking regions, they are said to have seven lives, while in Turkish and Arabic traditions, the number of lives is six. The myth is attributed to the natural suppleness and swiftness cats exhibit to escape life-threatening situations. Also lending credence to this myth is the fact that falling cats often land on their feet, using an instinctive righting reflex to twist their bodies around. Nonetheless, cats can still be injured or killed by a high fall.
External links.
Listen to this article (3 parts) · 
This audio file was created from a revision of the "Cat" article dated 2007-05-13, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="6681" url="http://en.wikipedia.org/wiki?curid=6681" title="Crank">
Crank

Crank may refer to:

</doc>
<doc id="6682" url="http://en.wikipedia.org/wiki?curid=6682" title="Clade">
Clade

A clade (from Ancient Greek: κλάδος, "klados", "branch") or monophylum (see monophyletic) is a life-form group consisting of a common ancestor and all its descendants—representing a single "branch" on the "tree of life".
The common ancestor may be an individual, a population or even a species (extinct or extant). Clades are nested, one in another, as each branch in turn splits into smaller branches. These splits reflect evolutionary history as populations diverged and evolved independently. 
Many commonly named groups are clades, for example, rodents, or insects; because in each case, their name comprises a common ancestor with all its descendant branches. Rodents, for example, are a branch of mammals that split off after the end of the dinosaurs 66 million years ago. The original population and all its descendants are a clade. The rodent clade corresponds to the order Rodentia, and insects to the class Insecta. These clades include smaller clades, such as "chipmunk" or "ant," each of which comprises even smaller clades. The clade "rodent" is in turn included in the mammal, vertebrate and animal clades. 
Often, however, common words for kinds of living things name only part of a clade. "Lizards" is not a clade, and neither is "monkeys". Snakes are a clade, and lizards and snakes together make a bigger clade, but "lizard" excludes snakes based on anatomy. Snakes are still part of that clade even though they have evolved to look different. The term "lizard" puts geckos and Gila monsters in the same category but excludes snakes. Since Gila monsters are more closely related to snakes than they are to geckos, the term "lizard" does not name a clade. In the same way, "monkeys" is not a clade because it excludes apes. Apes are more closely related to Old World monkeys than they are to New World monkeys, so any clade big enough to include New World and Old World monkeys has to include apes, too. Finally, many definitive clades lack common terms for them. For example, there is no term for the clade that includes all monkeys and apes (humans included). Terms such as "vertebrate" and "mammal" resulted from modern taxonomy, since folk taxonomy did not have terms for these clades. 
Increasingly, taxonomists try to avoid naming taxa that are not clades.
Etymology.
The term "clade" was coined in 1957 by the biologist Julian Huxley to refer to the result of cladogenesis, a concept Huxley borrowed from Bernhard Rensch.
Definitions.
Clade and ancestor.
A clade is by definition monophyletic, meaning it contains one ancestor (which can be an organism, a population, or a species) and all its descendants. The ancestor can be known or unknown; any and all members of a clade can be extant or extinct.
Clades and phylogenetic trees.
The science that tries to reconstruct phylogenetic trees and thus discover clades is called phylogenetics or cladistics, the latter term being derived from "clade" by Ernst Mayr (1965). The results of phylogenetic/cladistic analyses are tree-shaped diagrams called "cladograms"; they, and all their branches, are phylogenetic hypotheses.
Three methods of defining clades are featured in phylogenetic nomenclature: node-, stem-, and apomorphy-based (see here for detailed definitions).
Terminology.
The relationship between clades can be described in several ways:
Nomenclature and taxonomy.
The idea of a clade did not exist in pre-Darwinian Linnaean taxonomy, which was based by necessity only on internal or external morphological similarities between organisms – although as it happens, many of the better known animal groups in Linnaeus' original Systema Naturae (notably among the vertebrate groups) do represent clades. The phenomenon of convergent evolution is however responsible for many cases where there are misleading similarities in the morphology of groups that evolved from different lineages.
With the publication of Darwin's theory of evolution in 1859, the idea was born that groups used in a system of classification should represent branches on the evolutionary tree of life. In the century and a half since then, taxonomists have increasingly worked to make the taxonomic system reflect evolution. When it comes to naming, however, this principle is not always compatible with the traditional rank-based nomenclature. In the latter, only taxa associated with a rank can be named, yet there are not enough ranks to name a long series of nested clades; also, taxon names cannot be defined in a way that guarantees them to refer to clades. For these and other reasons, phylogenetic nomenclature has been developed; it is still controversial.
Cultural references.
"Clade" is the title of a novel by James Bradley, who chose it both because of its biological meaning but also because of the larger implications of the word.

</doc>
<doc id="6684" url="http://en.wikipedia.org/wiki?curid=6684" title="Communications in Afghanistan">
Communications in Afghanistan

Communications in Afghanistan is under the control of the Ministry of Communications and Information Technology (MCIT). It has rapidly expanded after the Karzai administration took over in late 2001, and has embarked on wireless companies, internet, radio stations and television channels. The Afghan government signed a $64.5 agreement in 2006 with China's ZTE on the establishment of a countrywide optical fiber cable network. The project began to improve telephone, internet, television and radio broadcast services throughout Afghanistan. As of 2014, about 90% of the country's population has access to communication services.
There are about 18 million mobile phone users in the country. Etisalat, Roshan, Afghan Wireless and MTN are the leading telecom companies. Etisalat became the first company to launch 4G services in 2014. It is predicted that over 50% of the population will have access to the internet by 2015. In 2014, Afghanistan leased a space satellite from Eutelsat, called AFGHANSAT 1.
Telephone.
There are about 18 million GSM mobile phone subscribers in Afghanistan as of 2009, with over 75,000 fixed-telephone-lines and little over 190,000 CDMA subscribers. Mobile communications have improved because of the introduction of wireless carriers into this developing country. The first was Afghan Wireless, which is US based that was founded by Ehsan Bayat. The second was Roshan, which began providing services to all major cities within Afghanistan. There are also a number of VSAT stations in major cities such as Kabul, Kandahar, Herat, Mazari Sharif, and Jalalabad, providing international and domestic voice/data connectivity. The international calling code for Afghanistan is +93. The following is a partial list of mobile phone companies in the country:
All the companies providing communication services are obligated to deliver 2.5% of their income to the communication development fund annually. According to the Ministry of Communication and Information Technology there are 4760 active towers throughout the country which covers 85% of the population. The Ministry of Communication and Information Technology plans to expand its services in remote parts of the country where the remaining 15% of the population will be covered with the installation of 700 new towers.
Phone calls in Afghanistan have been monitored by the National Security Agency according to Wikileaks.
Internet.
Afghanistan was given legal control of the ".af" domain in 2003, and the Afghanistan Network Information Center (AFGNIC) was established to administer domain names. As of 2010, there are at least 46 internet service providers (ISPs) in the country. Internet in Afghanistan is also at the peak with 1 million users as of 2009.
According to the Ministry of Communications, the following are some of the different ISPs operating in Afghanistan:
Television.
There are over 50 Afghan television channels worldwide, many of which are based inside Afghanistan while others are broadcast from North America and Europe. Selected foreign channels are also shown to the public in Afghanistan, but with the use of the internet, over 3,500 international TV channels may be accessed in Afghanistan.
Radio.
As of 2007, there are an estimated 50 private radio stations throughout the country. Broadcasts are in Dari, Pashto, English, Uzbeki and many other languages.
The number of radio listeners are decreasing and are being slowly outnumbered by television. Of Afghanistan's 6 main cities, Kandahar and Khost have a lot of radio listeners. Kabul and Jalalabad have moderate number of listeners. However, Mazar-e-Sharif and especially Herat have very few radio listeners.
Postal service.
In 1870, a central post office was established at Bala Hissar in Kabul and a post office in the capital of each province. The service was slowly being expanded over the years as more postal offices were established in each large city by 1918. Afghanistan became a member of the Universal Postal Union in 1928, and the postal administration elevated to the Ministry of Communication in 1934. Civil war caused a disruption in issuing official stamps during the 1980s-90s war but in 1999 postal service was operating again. Postal services to/from Kabul worked remarkably well all throughout the war years. Postal services to/from Herat resumed in 1997. The Afghan government has reported to the UPU several times about illegal stamps being issued and sold in 2003 and 2007.
Afghanistan Post has been reorganizing the postal service in 2000s with the help of Pakistan Post. The Afghanistan Postal commission was formed to prepare a written policy for the development of the postal sector, which will form the basis of a new postal services law governing licensing of postal services providers. The project was expected to finish by 2008.
Satellite.
It was announced in 2013 that Afghanistan would soon have its own satellite in space. In January 2014 the Afghan Ministry of Communications and Information Technology signed an agreement with Eutelsat for the use of satellite resources to enhance deployment of Afghanistan's national broadcasting and telecommunications infrastructure as well as its international connectivity. AFGHANSAT 1 was officially launched in May 2014, with expected service for at least seven years in Afghanistan. The Afghan government plans to launch AFGHANSAT 2 after the AFGHANSAT 1 lease ends.

</doc>
<doc id="6689" url="http://en.wikipedia.org/wiki?curid=6689" title="Christian of Oliva">
Christian of Oliva

Christian of Oliva (Polish: "Christian z Oliwy"), also Christian of Prussia (German: "Christian von Preußen") (died 4 December(?) 1245) was the first missionary bishop of Prussia. 
Christian was born about 1180 in the Duchy of Pomerania, possibly in the area of Chociwel (according to Johannes Voigt). Probably as a juvenile he joined the Cistercian Order at newly established Kołbacz ("Kolbatz") Abbey and in 1209 entered Oliwa Abbey near Gdańsk, founded in 1178 by the Samboride dukes of Pomerelia. At this time the Piast duke Konrad I of Masovia with the consent of Pope Innocent III had started the first of several unsuccessful Prussian Crusades into the adjacent Chełmno Land and Christian acted as a missionary among the Prussians east of the Vistula River. In 1215 he is recorded as abbot of Łekno Abbey near Gniezno in Greater Poland, most but not all authors identify him with Godfrey of Łękno. 
In 1209, Christian was commissioned by the Pope to be responsible for the Prussian missions between the Vistula and Neman Rivers and in 1212 he was appointed bishop. In 1215 he went to Rome in order to report to the Curia on the condition and prospects of his mission, and was consecrated first "Bishop of Prussia" at the Fourth Council of the Lateran. His seat as a bishop remained at Oliwa Abbey on the western side of the Vistula, whereas the pagan Prussian (later East Prussian) territory was on the eastern side of it.
The attempts by Konrad of Masovia to subdue the Prussian lands had picked long-term and intense border quarrels, whereby the Polish lands of Masovia, Cuyavia and even Greater Poland became subject to continuous Prussian raids. Bishop Christian asked the new Pope Honorius III for the consent to start another Crusade, however a first campaign in 1217 proved a failure and even the joint efforts by Duke Konrad with the Polish High Duke Leszek I the White and Duke Henry I the Bearded of Silesia in 122/23 only led to the reconquest of Chełmno Land but did not stop the Prussian invasions. At least Christian was able to establish the Diocese of Chełmno east of the Vistula, adopting the episcopal rights from the Masovian Bishop of Płock, confirmed by both Duke Konrad and the Pope.
Duke Konrad of Masovia still was not capable to end the Prussian attacks on his territory and in 1224 began to conduct negotiations with the Teutonic Knights under Grand Master Hermann von Salza in order to strengthen his forces. As von Salza initially hesitated to offer his services, Christian created the military Order of Dobrzyń ("Fratres Milites Christi") in 1228, however to little avail.
Meanwhile von Salza had to abandon his hope to establish an Order's State in the Burzenland region of Transylvania, which had led to an éclat with King Andrew II of Hungary. He obtained a charter by Emperor Frederick II issued in the 1226 Golden Bull of Rimini, whereby Chełmno Land would be the unshared possession of the Teutonic Knights, which was confirmed by Duke Konrad of Masovia in the 1230 Treaty of Kruszwica. Christian ceded his possessions to the new State of the Teutonic Order and in turn was appointed Bishop of Chełmno the next year.
Bishop Christian continued his mission in Sambia ("Samland"), where from 1233 to 1239 he was held captive by pagan Prussians, and freed in trade for five other hostages who then in turn were released for a ransom of 800 Marks, granted to him by Pope Gregory IX. He had to deal with the constant cut-back of his autonomy by the Knights and asked the Roman Curia for mediation. In 1243, the Papal legate William of Modena divided the Prussian lands of the Order's State into four dioceses, whereby the bishops retained the secular rule over about on third of the diocesan territory:
all suffragan dioceses under the Archbishopric of Riga. Christian was supposed to choose one of them, but did not agree to the division. He possibly retired to the Cistercians Abbey in Sulejów, where he died before the conflict was solved.

</doc>
<doc id="6690" url="http://en.wikipedia.org/wiki?curid=6690" title="Coca-Cola">
Coca-Cola

Coca-Cola is a carbonated soft drink sold in stores, restaurants, and vending machines throughout the world. It is produced by The Coca-Cola Company of Atlanta, Georgia, and is often referred to simply as Coke (a registered trademark of The Coca-Cola Company in the United States since March 27, 1944). Originally intended as a patent medicine when it was invented in the late 19th century by John Pemberton, Coca-Cola was bought out by businessman Asa Griggs Candler, whose marketing tactics led Coke to its dominance of the world soft-drink market throughout the 20th century.
The company produces concentrate, which is then sold to licensed Coca-Cola bottlers throughout the world. The bottlers, who hold territorially exclusive contracts with the company, produce finished product in cans and bottles from the concentrate in combination with filtered water and sweeteners. The bottlers then sell, distribute and merchandise Coca-Cola to retail stores and vending machines. The Coca-Cola Company also sells concentrate for soda fountains to major restaurants and food service distributors.
The Coca-Cola Company has, on occasion, introduced other cola drinks under the Coke brand name. The most common of these is Diet Coke, with others including Caffeine-Free Coca-Cola, Diet Coke Caffeine-Free, Coca-Cola Cherry, Coca-Cola Zero, Coca-Cola Vanilla, and special versions with lemon, lime, or coffee. In 2013, Coke products could be found in over 200 countries worldwide, with consumers downing more than 1.8 billion company beverage servings each day.
Based on Interbrand's best global brand study of 2011, Coca-Cola was the world's most valuable brand.
History.
19th-century historical origins.
Confederate Colonel John Pemberton who was wounded in the Civil War, became addicted to morphine, and began a quest to find a substitute for the dangerous opiate. The prototype Coca-Cola recipe was formulated at Pemberton's Eagle Drug and Chemical House, a drugstore in Columbus, Georgia, originally as a coca wine. He may have been inspired by the formidable success of Vin Mariani, a European coca wine.
In 1885, Pemberton registered his French Wine Coca nerve tonic. In 1886, when Atlanta and Fulton County passed prohibition legislation, Pemberton responded by developing Coca-Cola, essentially a nonalcoholic version of French Wine Coca.
The first sales were at Jacob's Pharmacy in Atlanta, Georgia, on May 8, 1886. It was initially sold as a patent medicine for five cents a glass at soda fountains, which were popular in the United States at the time due to the belief that carbonated water was good for the health. Pemberton claimed Coca-Cola cured many diseases, including morphine addiction, dyspepsia, neurasthenia, headache, and impotence. Pemberton ran the first advertisement for the beverage on May 29 of the same year in the "Atlanta Journal".
By 1888, three versions of Coca-Cola – sold by three separate businesses – were on the market. A copartnership had been formed on January 14, 1888 between Pemberton and four Atlanta businessmen: J.C. Mayfield, A.O. Murphey; C.O. Mullahy and E.H. Bloodworth. Not codified by any signed document, a verbal statement given by Asa Candler years later asserted under testimony that he had acquired a stake in Pemberton's company as early as 1887. John Pemberton declared that the "name" "Coca-Cola" belonged to his son, Charley, but the other two manufacturers could continue to use the "formula".
Charley Pemberton's record of control over the "Coca-Cola" name was the underlying factor that allowed for him to participate as a major shareholder in the March 1888 Coca-Cola Company incorporation filing made in his father's place. Charley's exclusive control over the "Coca Cola" name became a continual thorn in Asa Candler's side.
Candler's oldest son, Charles Howard Candler, authored a book in 1950 published by Emory University. In this definitive biography about his father, Candler specifically states: "..., on April 14, 1888, the young druggist [Asa Griggs Candler] purchased a one-third interest in the formula of an almost completely unknown proprietary elixir known as Coca-Cola."
The deal was actually between John Pemberton's son Charley and Walker, Candler & Co. – with John Pemberton acting as cosigner for his son. For $50 down and $500 in 30 days, Walker, Candler & Co. obtained all of the one-third interest in the Coca-Cola Company that Charley held, all while Charley still held on to the name. After the April 14 deal, on April 17, 1888, one-half of the Walker/Dozier interest shares were acquired by Candler for an additional $750.
The Coca-Cola Company.
In 1892, Candler set out to incorporate a second company; "The Coca-Cola Company" (the current corporation). When Candler had the earliest records of the "Coca-Cola Company" burned in 1910, the action was claimed to have been made during a move to new corporation offices around this time.
After Candler had gained a better foothold on Coca-Cola in April 1888, he nevertheless was forced to sell the beverage he produced with the recipe he had under the names "Yum Yum" and "Koke". This was while Charley Pemberton was selling the elixir, although a cruder mixture, under the name "Coca-Cola", all with his father's blessing. After both names failed to catch on for Candler, by the summer of 1888, the Atlanta pharmacist was quite anxious to establish a firmer legal claim to Coca-Cola, and hoped he could force his two competitors, Walker and Dozier, completely out of the business, as well.
On August 16, 1888, Dr. John Stith Pemberton suddenly died, Asa G. Candler then sought to move swiftly forward to attain his vision of taking full control of the whole Coca-Cola operation.
Charley Pemberton, an alcoholic, was the one obstacle who unnerved Asa Candler more than anyone else. Candler is said to have quickly maneuvered to purchase the exclusive rights to the name "Coca-Cola" from Pemberton's son Charley right after Dr. Pemberton's death. One of several stories was that Candler bought the title to the name from Charley's mother for $300; approaching her at Dr. Pemberton's funeral. Eventually, Charley Pemberton was found on June 23, 1894, unconscious, with a stick of opium by his side. Ten days later, Charley died at Atlanta's Grady Hospital at the age of 40.
In Charles Howard Candler's 1950 book about his father, he stated: "On August 30th [1888], he [Asa Candler] became sole proprietor of Coca-Cola, a fact which was stated on letterheads, invoice blanks and advertising copy."
With this action on August 30, 1888, Candler's sole control became technically all true. Candler had negotiated with Margaret Dozier and her brother Woolfolk Walker a full payment amounting to $1,000, which all agreed Candler could pay off with a series of notes over a specified time span. By May 1, 1889, Candler was now claiming full ownership of the Coca-Cola beverage, with a total investment outlay by Candler for the drink enterprise over the years amounting to $2,300.
In 1914, Margaret Dozier, as co-owner of the original Coca-Cola Company in 1888, came forward to claim that her signature on the 1888 Coca-Cola Company bill of sale had been forged. Subsequent analysis of certain similar transfer documents had also indicated John Pemberton's signature was most likely a forgery, as well, which some accounts claim was precipitated by his son Charley.
On September 12, 1919, Coca-Cola Co. was purchased from the Candler family by a group of investors led by the Trust Company of Georgia's (predecessor of SunTrust Bank) president Ernest Woodruff for $25 million. The new Coca-Cola Co. was reorganized and reincorporated in the state of Delaware, but the company’s headquarters would remain in Atlanta. Five hundred thousand shares of stock in the new company were issued and offered to the public for $40 a share. The sale of Coca-Cola and subsequent chance for the public to purchase stock made national and international news.
In 1986, The Coca-Cola Company merged with the John T. Lupton franchises and BCI Holding Corporation's bottling holdings – forming Coca-Cola Enterprises Inc. (CCE).
In 1988, Berkshire Hathaway purchased 14,172,500 shares of The Coca Cola Company at a cost of $592,540,000.
In December 1991, a merger between Coca-Cola Enterprises and the Johnston Coca-Cola Bottling Group, Inc. created a larger, stronger organization with Johnston’s senior management team assuming management responsibilities.
Origins of bottling.
The first bottling of Coca-Cola occurred in Vicksburg, Mississippi, at the Biedenharn Candy Company in 1891. The proprietor of the bottling works was Joseph A. Biedenharn. The original bottles were Biedenharn bottles, very different from the much later hobble-skirt design of 1915 now so familiar.
It was then a few years later that two entrepreneurs from Chattanooga, Tennessee, namely; Benjamin F. Thomas and Joseph B. Whitehead, proposed the idea of bottling and were so persuasive that Candler signed a contract giving them control of the procedure for only one dollar. Candler never collected his dollar, but in 1899, Chattanooga became the site of the first Coca-Cola bottling company. Candler remained very content just selling his company's syrup. The loosely termed contract proved to be problematic for The Coca-Cola Company for decades to come. Legal matters were not helped by the decision of the bottlers to subcontract to other companies, effectively becoming parent bottlers.
20th century.
The first outdoor wall advertisement that promoted the Coca-Cola drink was painted in 1894 in Cartersville, Georgia. Cola syrup was sold as an over-the-counter dietary supplement for upset stomach. By the time of its 50th anniversary, the soft drink had reached the status of a national icon in the USA. In 1935, it was certified kosher by Atlanta Rabbi Tobias Geffen, after the company made minor changes in the sourcing of some ingredients.
The longest running commercial Coca-Cola soda fountain anywhere was Atlanta's Fleeman's Pharmacy, which first opened its doors in 1914. Jack Fleeman took over the pharmacy from his father and ran it until 1995; closing it after 81 years. On July 12, 1944, the one-billionth gallon of Coca-Cola syrup was manufactured by The Coca-Cola Company. Cans of Coke first appeared in 1955.
New Coke.
On April 23, 1985, Coca-Cola, amid much publicity, attempted to change the formula of the drink with "New Coke". Follow-up taste tests revealed most consumers preferred the taste of New Coke to both Coke and Pepsi but Coca-Cola management was unprepared for the public's nostalgia for the old drink, leading to a backlash. The company gave in to protests and returned to a variation of the old formula using high fructose corn syrup instead of cane sugar as the main sweetener, under the name Coca-Cola Classic, on July 10, 1985.
21st century.
On July 5, 2005, it was revealed that Coca-Cola would resume operations in Iraq for the first time since the Arab League boycotted the company in 1968.
In April 2007, in Canada, the name "Coca-Cola Classic" was changed back to "Coca-Cola". The word "Classic" was removed because "New Coke" was no longer in production, eliminating the need to differentiate between the two. The formula remained unchanged. In January 2009, Coca-Cola stopped printing the word "Classic" on the labels of 16 USfloz bottles sold in parts of the southeastern United States. The change is part of a larger strategy to rejuvenate the product's image. The word "Classic" was removed from all Coca-Cola products by 2011.
In November 2009, due to a dispute over wholesale prices of Coca-Cola products, Costco stopped restocking its shelves with Coke and Diet Coke for two months; a separate pouring rights deal in 2013 saw Coke products removed from Costco food courts in favor of Pepsi. Some Costco locations (such as the ones in Tucson, Arizona) additionally sell imported Coca-Cola from Mexico with cane sugar instead of corn syrup from separate distributors. Coca-Cola introduced the 7.5-ounce mini-can in 2009, and on September 22, 2011, the company announced price reductions, asking retailers to sell eight-packs for $2.99. That same day, Coca-Cola announced the 12.5-ounce bottle, to sell for 89 cents. A 16-ounce bottle has sold well at 99 cents since being re-introduced, but the price was going up to $1.19.
In 2012, Coca-Cola resumed business in Myanmar after 60 years of absence due to U.S.-imposed investment sanctions against the country. Coca-Cola's bottling plant will be located in Yangon and is part of the company's five-year plan and $200 million investment in Myanmar. Coca-Cola with its partners is to invest USD 5 billion in its operations in India by 2020. In 2013, it was announced that Coca-Cola Life would be introduced in Argentina that would contain stevia and sugar.
In August 2014 the company announced it was forming a long-term partnership with Monster Beverage, with the two forging a strategic marketing and distribution alliance, and product line swap. As part of the deal Coca-Cola was to acquire a 16.7% stake in Monster for $2.15 billion, with an option to increase it to 25%.
Production.
Ingredients.
A typical can of Coca-Cola (12 fl ounces/355 ml) contains 38 grams of sugar (usually in the form of high fructose corn syrup), 50 mg of sodium, 0 grams fat, 0 grams potassium, and 140 calories.
Formula of natural flavorings.
The exact formula of Coca-Cola's natural flavorings (but not its other ingredients, which are listed on the side of the bottle or can) is a trade secret. The original copy of the formula was held in SunTrust Bank's main vault in Atlanta for 86 years. Its predecessor, the Trust Company, was the underwriter for the Coca-Cola Company's initial public offering in 1919. On December 8, 2011, the original secret formula was moved from the vault at SunTrust Banks to a new vault containing the formula which will be on display for visitors to its World of Coca-Cola museum in downtown Atlanta.
According to Snopes, a popular myth states that only two executives have access to the formula, with each executive having only half the formula. However, several sources state that while Coca-Cola does have a rule restricting access to only two executives, each knows the entire formula and others, in addition to the prescribed duo, have known the formulation process.
On February 11, 2011, Ira Glass revealed on his PRI radio show, "This American Life", that the secret formula to Coca-Cola had been uncovered in a 1979 newspaper. The formula found basically matched the formula found in Pemberton's diary.
Use of stimulants in formula.
When launched, Coca-Cola's two key ingredients were cocaine and caffeine. The cocaine was derived from the coca leaf and the caffeine from kola nut, leading to the name Coca-Cola (the "K" in Kola was replaced with a "C" for marketing purposes).
Coca – cocaine.
Pemberton called for five ounces of coca leaf per gallon of syrup, a significant dose; in 1891, Candler claimed his formula (altered extensively from Pemberton's original) contained only a tenth of this amount. Coca-Cola once contained an estimated nine milligrams of cocaine per glass. In 1903, it was removed.
After 1904, instead of using fresh leaves, Coca-Cola started using "spent" leaves – the leftovers of the cocaine-extraction process with trace levels of cocaine. Coca-Cola now uses a cocaine-free coca leaf extract prepared at a Stepan Company plant in Maywood, New Jersey.
In the United States, the Stepan Company is the only manufacturing plant authorized by the Federal Government to import and process the coca plant, which it obtains mainly from Peru and, to a lesser extent, Bolivia. Besides producing the coca flavoring agent for Coca-Cola, the Stepan Company extracts cocaine from the coca leaves, which it sells to Mallinckrodt, a St. Louis, Missouri, pharmaceutical manufacturer that is the only company in the United States licensed to purify cocaine for medicinal use.
Kola nuts – caffeine.
Kola nuts act as a flavoring and the source of caffeine in Coca-Cola. In Britain, for example, the ingredient label states "Flavourings (Including Caffeine)." Kola nuts contain about 2.0 to 3.5% caffeine, are of bitter flavor and are commonly used in cola soft drinks. In 1911, the U.S. government initiated "United States v. Forty Barrels and Twenty Kegs of Coca-Cola", hoping to force Coca-Cola to remove caffeine from its formula. The case was decided in favor of Coca-Cola. Subsequently, in 1912, the U.S. Pure Food and Drug Act was amended, adding caffeine to the list of "habit-forming" and "deleterious" substances which must be listed on a product's label.
Coca-Cola contains 34 mg of caffeine per 12 fluid ounces (9.8 mg per 100 ml).
Franchised production model.
The actual production and distribution of Coca-Cola follows a franchising model. The Coca-Cola Company only produces a syrup concentrate, which it sells to bottlers throughout the world, who hold Coca-Cola franchises for one or more geographical areas. The bottlers produce the final drink by mixing the syrup with filtered water and sweeteners, and then carbonate it before putting it in cans and bottles, which the bottlers then sell and distribute to retail stores, vending machines, restaurants and food service distributors.
The Coca-Cola Company owns minority shares in some of its largest franchises, such as Coca-Cola Enterprises, Coca-Cola Amatil, Coca-Cola Hellenic Bottling Company and Coca-Cola FEMSA, but fully independent bottlers produce almost half of the volume sold in the world.
Independent bottlers are allowed to sweeten the drink according to local tastes.
The bottling plant in Skopje, Macedonia, received the 2009 award for "Best Bottling Company".
On May 5, 2014, Coca-Cola said they are working to remove a controversial ingredient, brominated vegetable oil, from all of their drinks.
Geographic spread.
Since it announced its intention to begin distribution in Burma in June 2012, Coca-Cola has been officially available in every country in the world except Cuba and North Korea. However, it is reported to be available in both countries as a grey import.
Coca-Cola has been a point of legal discussion in the Middle East. In the early 20th century, a fatwa was created in Egypt to discuss the question of "whether Muslims were permitted to drink Coca-Cola and Pepsi cola." The fatwa states: "According to the Muslim Hanefite, Shafi'ite, etc., the rule in Islamic law of forbidding or allowing foods and beverages is based on the presumption that such things are permitted unless it can be shown that they are forbidden on the basis of the Qur'an." The Muslim jurists stated that, unless the Qu'ran specifically prohibits the consumption of a particular product, it is permissible to consume. Another clause was discussed, whereby the same rules apply if a person is unaware of the condition or ingredients of the item in question.
Brand portfolio.
This is a list of variants of Coca-Cola introduced around the world. In addition to the caffeine-free version of the original, additional fruit flavors have been included over the years. Not included here are versions of Diet Coke and Coca-Cola Zero; variant versions of those no-calorie colas can be found at their respective articles.
Logo design.
The Coca-Cola logo was created by John Pemberton's bookkeeper, Frank Mason Robinson, in 1885. Robinson came up with the name and chose the logo's distinctive cursive script. The typeface used, known as Spencerian script, was developed in the mid-19th century and was the dominant form of formal handwriting in the United States during that period.
Robinson also played a significant role in early Coca-Cola advertising. His promotional suggestions to Pemberton included giving away thousands of free drink coupons and plastering the city of Atlanta with publicity banners and streetcar signs.
Contour bottle design.
The Coca-Cola bottle, called the "contour bottle" within the company, was created by bottle designer Earl R. Dean. In 1915, the Coca-Cola Company launched a competition among its bottle suppliers to create a new bottle for their beverage that would distinguish it from other beverage bottles, "a bottle which a person could recognize even if they felt it in the dark, and so shaped that, even if broken, a person could tell at a glance what it was."
Chapman J. Root, president of the Root Glass Company of Terre Haute, Indiana, turned the project over to members of his supervisory staff, including company auditor T. Clyde Edwards, plant superintendent Alexander Samuelsson, and Earl R. Dean, bottle designer and supervisor of the bottle molding room. Root and his subordinates decided to base the bottle's design on one of the soda's two ingredients, the coca leaf or the kola nut, but were unaware of what either ingredient looked like. Dean and Edwards went to the Emeline Fairbanks Memorial Library and were unable to find any information about coca or kola. Instead, Dean was inspired by a picture of the gourd-shaped cocoa pod in the Encyclopædia Britannica. Dean made a rough sketch of the pod and returned to the plant to show Root. He explained to Root how he could transform the shape of the pod into a bottle. Root gave Dean his approval.
Faced with the upcoming scheduled maintenance of the mold-making machinery, over the next 24 hours Dean sketched out a concept drawing which was approved by Root the next morning. Dean then proceeded to create a bottle mold and produced a small number of bottles before the glass-molding machinery was turned off.
Chapman Root approved the prototype bottle and a design patent was issued on the bottle in November 1915. The prototype never made it to production since its middle diameter was larger than its base, making it unstable on conveyor belts. Dean resolved this issue by decreasing the bottle's middle diameter. During the 1916 bottler's convention, Dean's contour bottle was chosen over other entries and was on the market the same year. By 1920, the contour bottle became the standard for the Coca-Cola Company. A revised version was also patented in 1923. Because the Patent Office releases the "Patent Gazette" on Tuesday, the bottle was patented on December 25, 1923, and was nicknamed the "Christmas bottle." Today, the contour Coca-Cola bottle is one of the most recognized packages on the planet..."even in the dark!".
As a reward for his efforts, Dean was offered a choice between a $500 bonus or a lifetime job at the Root Glass Company. He chose the lifetime job and kept it until the Owens-Illinois Glass Company bought out the Root Glass Company in the mid-1930s. Dean went on to work in other Midwestern glass factories.
One alternative depiction has Raymond Loewy as the inventor of the unique design, but, while Loewy did serve as a designer of Coke cans and bottles in later years, he was in the French Army the year the bottle was invented and did not emigrate to the United States until 1919. Others have attributed inspiration for the design not to the cocoa pod, but to a Victorian hooped dress.
In 1944, Associate Justice Roger J. Traynor of the Supreme Court of California took advantage of a case involving a waitress injured by an exploding Coca-Cola bottle to articulate the doctrine of strict liability for defective products. Traynor's concurring opinion in "Escola v. Coca-Cola Bottling Co." is widely recognized as a landmark case in U.S. law today.
In 2007, the company's logo on cans and bottles changed. The cans and bottles retained the red color and familiar typeface, but the design was simplified, leaving only the logo and a plain white swirl (the "dynamic ribbon").
Designer bottles.
Karl Lagerfeld is the latest designer to have created a collection of aluminum bottles for Coca-Cola. Lagerfeld is not the first fashion designer to create a special version of the famous Coca-Cola Contour bottle. A number of other limited edition bottles by fashion designers for Coca Cola Light soda have been created in the last few years.
In 2009, in Italy, Coca-Cola Light had a Tribute to Fashion to celebrate 100 years of the recognizable contour bottle. Well known Italian designers Alberta Ferretti, Blumarine, Etro, Fendi, Marni, Missoni, Moschino, and Versace each designed limited edition bottles.
Competitors.
Pepsi, the flagship product of PepsiCo, The Coca-Cola Company's main rival in the soft drink industry, is usually second to Coke in sales, and outsells Coca-Cola in some markets. RC Cola, now owned by the Dr Pepper Snapple Group, the third largest soft drink manufacturer, is also widely available.
Around the world, many local brands compete with Coke. In South and Central America Kola Real, known as Big Cola in Mexico, is a growing competitor to Coca-Cola. On the French island of Corsica, Corsica Cola, made by brewers of the local Pietra beer, is a growing competitor to Coca-Cola. In the French region of Brittany, Breizh Cola is available. In Peru, Inca Kola outsells Coca-Cola, which led The Coca-Cola Company to purchase the brand in 1999. In Sweden, Julmust outsells Coca-Cola during the Christmas season. In Scotland, the locally produced Irn-Bru was more popular than Coca-Cola until 2005, when Coca-Cola and Diet Coke began to outpace its sales.
In India, Coca-Cola ranked third behind the leader, Pepsi-Cola, and local drink Thums Up. The Coca-Cola Company purchased Thums Up in 1993. As of 2004, Coca-Cola held a 60.9% market-share in India. Tropicola, a domestic drink, is served in Cuba instead of Coca-Cola, due to a United States embargo. French brand Mecca Cola and British brand Qibla Cola are competitors to Coca-Cola in the Middle East.
In Turkey, Cola Turka, in Iran and the Middle East, Zamzam Cola and Parsi Cola, in some parts of China, China Cola, in Slovenia, Cockta and the inexpensive Mercator Cola, sold only in the country's biggest supermarket chain, Mercator, are some of the brand's competitors. Classiko Cola, made by Tiko Group, the largest manufacturing company in Madagascar, is a serious competitor to Coca-Cola in many regions. Laranjada is the top-selling soft drink on Madeira.
Advertising.
Coca-Cola's advertising has significantly affected American culture, and it is frequently credited with inventing the modern image of Santa Claus as an old man in a red-and-white suit. Although the company did start using the red-and-white Santa image in the 1930s, with its winter advertising campaigns illustrated by Haddon Sundblom, the motif was already common. Coca-Cola was not even the first soft drink company to use the modern image of Santa Claus in its advertising: White Rock Beverages used Santa in advertisements for its ginger ale in 1923, after first using him to sell mineral water in 1915. Before Santa Claus, Coca-Cola relied on images of smartly dressed young women to sell its beverages. Coca-Cola's first such advertisement appeared in 1895, featuring the young Bostonian actress Hilda Clark as its spokeswoman.
1941 saw the first use of the nickname "Coke" as an official trademark for the product, with a series of advertisements informing consumers that "Coke means Coca-Cola". In 1971 a song from a Coca-Cola commercial called "I'd Like to Teach the World to Sing", produced by Billy Davis, became a hit single.
Coke's advertising is pervasive, as one of Woodruff's stated goals was to ensure that everyone on Earth drank Coca-Cola as their preferred beverage. This is especially true in southern areas of the United States, such as Atlanta, where Coke was born.
Some Coca-Cola television commercials between 1960 through 1986 were written and produced by former Atlanta radio veteran Don Naylor (WGST 1936–1950, WAGA 1951–1959) during his career as a producer for the McCann Erickson advertising agency. Many of these early television commercials for Coca-Cola featured movie stars, sports heroes and popular singers.
During the 1980s, Pepsi-Cola ran a series of television advertisements showing people participating in taste tests demonstrating that, according to the commercials, "fifty percent of the participants who said they preferred Coke "actually" chose the Pepsi." Statisticians pointed out the problematic nature of a 50/50 result: most likely, the taste tests showed that in blind tests, most people cannot tell the difference between Pepsi and Coke. Coca-Cola ran ads to combat Pepsi's ads in an incident sometimes referred to as the "cola wars"; one of Coke's ads compared the so-called Pepsi challenge to two chimpanzees deciding which tennis ball was furrier. Thereafter, Coca-Cola regained its leadership in the market.
Selena was a spokesperson for Coca-Cola from 1989 till the time of her death. She filmed three commercials for the company. During 1994, to commemorate her five years with the company, Coca-Cola issued special Selena coke bottles.
The Coca-Cola Company purchased Columbia Pictures in 1982, and began inserting Coke-product images into many of its films. After a few early successes during Coca-Cola's ownership, Columbia began to under-perform, and the studio was sold to Sony in 1989.
Coca-Cola has gone through a number of different advertising slogans in its long history, including "The pause that refreshes," "I'd like to buy the world a Coke," and "Coke is it" (see Coca-Cola slogans).
In 2006, Coca-Cola introduced My Coke Rewards, a customer loyalty campaign where consumers earn points by entering codes from specially marked packages of Coca-Cola products into a website. These points can be redeemed for various prizes or sweepstakes entries.
In Australia in 2011, Coca-Cola began the "share a Coke" campaign, where the Coca-Cola logo was replaced on the bottles and replaced with first names. Coca-Cola used the 150 most popular names in Australia to print on the bottles. The campaign was paired with a website page, Facebook page and an online "share a virtual Coke". The same campaign was introduced to Coca-Cola, Diet Coke & Coke Zero bottles and cans in the UK in 2013.
Coca-Cola has also advertised its product to be consumed as a breakfast beverage, instead of coffee or tea for the morning caffeine.
Holiday campaigns.
The "Holidays are coming!" advertisement features a train of red delivery trucks, emblazoned with the Coca-Cola name and decorated with Christmas lights, driving through a snowy landscape and causing everything that they pass to light up and people to watch as they pass through.
The advertisement fell into disuse in 2001, as the Coca-Cola company restructured its advertising campaigns so that advertising around the world was produced locally in each country, rather than centrally in the company's headquarters in Atlanta, Georgia. In 2007, the company brought back the campaign after, according to the company, many consumers telephoned its information center saying that they considered it to mark the beginning of Christmas. The advertisement was created by U.S. advertising agency Doner, and has been part of the company's global advertising campaign for many years.
Keith Law, a producer and writer of commercials for Belfast CityBeat, was not convinced by Coca-Cola's reintroduction of the advertisement in 2007, saying that "I don't think there's anything Christmassy about HGVs and the commercial is too generic."
In 2001, singer Melanie Thornton recorded the campaign's advertising jingle as a single, "Wonderful Dream (Holidays are Coming)", which entered the pop-music charts in Germany at no. 9. In 2005, Coca-Cola expanded the advertising campaign to radio, employing several variations of the jingle.
In 2011, Coca-Cola launched a campaign for the Indian holiday Diwali. The campaign included commercials, a song and an integration with Shah Rukh Khan’s film Ra.One.
Sports sponsorship.
Coca-Cola was the first commercial sponsor of the Olympic games, at the 1928 games in Amsterdam, and has been an Olympics sponsor ever since. This corporate sponsorship included the 1996 Summer Olympics hosted in Atlanta, which allowed Coca-Cola to spotlight its hometown. Most recently, Coca-Cola has released localized commercials for the 2010 Winter Olympics in Vancouver; one Canadian commercial referred to Canada's hockey heritage and was modified after Canada won the gold medal game on February 28, 2010 by changing the ending line of the commercial to say "Now they know whose game they're playing".
Since 1978, Coca-Cola has sponsored the FIFA World Cup, and other competitions organised by FIFA. One FIFA tournament trophy, the FIFA World Youth Championship from Tunisia in 1977 to Malaysia in 1997, was called "FIFA — Coca Cola Cup". In addition, Coca-Cola sponsors the annual Coca-Cola 600 and Coke Zero 400 for the NASCAR Sprint Cup Series at Charlotte Motor Speedway in Concord, North Carolina and Daytona International Speedway in Daytona, Florida.
Coca-Cola has a long history of sports marketing relationships, which over the years have included Major League Baseball, the National Football League, the National Basketball Association, and the National Hockey League, as well as with many teams within those leagues. Coca-Cola has had a longtime relationship with the NFL's Pittsburgh Steelers, due in part to the now-famous 1979 television commercial featuring "Mean Joe" Greene, leading to the two opening the Coca-Cola Great Hall at Heinz Field in 2001 and a more recent Coca-Cola Zero commercial featuring Troy Polamalu.
Coca-Cola is the official soft drink of many collegiate football teams throughout the nation, partly due to Coca-Cola providing those schools with upgraded athletic facilities in exchange for Coca-Cola's sponsorship. This is especially prevalent at the high school level, which is more dependent on such contracts due to tighter budgets.
Coca-Cola was one of the official sponsors of the 1996 Cricket World Cup held on the Indian subcontinent. Coca Cola is also one of the associate sponsor of Delhi Daredevils in Indian Premier League.
In England, Coca-Cola was the main sponsor of The Football League between 2004 and 2010, a name given to the three professional divisions below the Premier League in football (soccer). In 2005, Coca-Cola launched a competition for the 72 clubs of the football league — it was called "Win a Player". This allowed fans to place one vote per day for their favorite club, with one entry being chosen at random earning £250,000 for the club; this was repeated in 2006. The "Win A Player" competition was very controversial, as at the end of the 2 competitions, Leeds United A.F.C. had the most votes by more than double, yet they did not win any money to spend on a new player for the club. In 2007, the competition changed to "Buy a Player". This competition allowed fans to buy a bottle of Coca-Cola or Coca-Cola Zero and submit the code on the wrapper on the Coca-Cola website. This code could then earn anything from 50p to £100,000 for a club of their choice. This competition was favored over the old "Win a Player" competition, as it allowed all clubs to win some money. Between 1992 and 1998, Coca-Cola was the title sponsor of the Football League Cup (Coca-Cola Cup), the secondary cup tournament of England.
Between 1994 and 1997, Coca-Cola was also the title sponsor of the Scottish League Cup, renaming it the Coca-Cola Cup like its English counterpart.
Coca-Cola is the presenting sponsor of the Tour Championship, the final event of the PGA Tour held each year at East Lake Golf Club in Atlanta, GA.
Introduced March 1, 2010, in Canada, to celebrate the 2010 Winter Olympics, Coca Cola sold gold colored cans in packs of 12 355 mL each, in select stores.
In 2012, Coca-Cola (Philippines) hosted/sponsored the Coca-Cola PBA Youngstars in the Philippines.
In mass media.
Coca-Cola has been prominently featured in countless films and television programs. Since its creation, it remains as one of the most prominent elements of the popular culture. It was a major plot element in films such as "One, Two, Three", "The Coca-Cola Kid", and "The Gods Must Be Crazy", among many others. It provides a setting for comical corporate shenanigans in the novel "Syrup" by Maxx Barry. In music, in the Beatles' song, "Come Together", the lyrics say, "He shoot Coca-Cola, he say...". The Beach Boys also referenced Coca-Cola in their 1964 song "All Summer Long" (i.e. "'Member when you spilled Coke all over your blouse?")
The best selling artist of all time and worldwide cultural icon, Elvis Presley, promoted Coca-Cola during his last tour of 1977. The Coca-Cola Company used Elvis' image to promote the product. For example, the company used a song performed by Presley, A Little Less Conversation, in a Japanese Coca-Cola commercial.
Other artists that promoted Coca-Cola include the Beatles, David Bowie, George Michael, Elton John and Whitney Houston, who appeared in the Diet Coca-Cola commercial, among many others.
Not all musical references to Coca-Cola went well. A line in "Lola" by the Kinks was originally recorded as "You drink champagne and it tastes just like Coca-Cola." When the British Broadcasting Corporation refused to play the song because of the commercial reference, lead singer Ray Davies re-recorded the lyric as "it tastes just like cherry cola" to get airplay for the song.
Political cartoonist Michel Kichka satirized a famous Coca-Cola billboard in his 1982 poster "And I Love New York." On the billboard, the Coca-Cola wave is accompanied by the words "Enjoy Coke." In Kichka's poster, the lettering and script above the Coca-Cola wave instead read "Enjoy Cocaine."
Criticism.
Health effects.
A link has been shown between long-term regular cola intake and osteoporosis in older women (but not men). This was thought to be due to the presence of phosphoric acid, and the risk was found to be same for caffeinated and noncaffeinated colas, as well as the same for diet and sugared colas.
In India, there was a controversy whether there are pesticides and other harmful chemicals in bottled products, including Coca-Cola. In 2003 the Centre for Science and Environment (CSE), a non-governmental organization in New Delhi, said aerated waters produced by soft drinks manufacturers in India, including multinational giants PepsiCo and Coca-Cola, contained toxins including lindane, DDT, malathion and chlorpyrifos — pesticides. CSE found that the Indian-produced Pepsi's soft drink products had 36 times the level of pesticide residues permitted under European Union regulations; Coca-Cola's soft drink was found to have 30 times the permitted amount. CSE said it had tested the same products sold in the U.S. and found no such residues. A study performed by the Indian Health Ministry failed to reproduce these results and concluded that the product was safe.
After the pesticide allegations were made in 2003, Coca-Cola sales in India declined by 15 percent. In 2004 an Indian parliamentary committee backed up CSE's findings and a government-appointed committee was tasked with developing the world's first pesticide standards for soft drinks. The Coca-Cola Company has responded that its plants filter water to remove potential contaminants and that its products are tested for pesticides and must meet minimum health standards before they are distributed. In the Indian state of Kerala sale and production of Coca-Cola, along with other soft drinks, was initially banned after the allegations, until the High Court in Kerala overturned ruled that only the federal government can ban food products. Coca-Cola has also been accused of excessive water usage in India.
Use as political and corporate symbol.
Coca-Cola has a high degree of identification with the United States, being considered by some an "American Brand" or as an item representing America. During World War II, this gave rise to brief production of the White Coke as a neutral brand. The drink is also often a metonym for the Coca-Cola Company.
There are some consumer boycotts of Coca-Cola in Arab countries due to Coke's early investment in Israel during the Arab League boycott of Israel (its competitor Pepsi stayed out of Israel). Mecca Cola and Pepsi have been successful alternatives in the Middle East.
A Coca-Cola fountain dispenser (officially a Fluids Generic Bioprocessing Apparatus-2 or FGBA-2) was developed for use on the Space Shuttle as a test bed to determine if carbonated beverages can be produced from separately stored carbon dioxide, water and flavored syrups and determine if the resulting fluids can be made available for consumption without bubble nucleation and resulting foam formation. The unit flew in 1996 aboard STS-77 and held 1.65 liters each of Coca-Cola and Diet Coke.
Social causes.
In 2012, Coca-Cola is listed as a partner of the (RED) campaign, together with other brands such as Nike, Girl, American Express and Converse. The campaign's mission is to prevent the transmission of the HIV virus from mother to child by 2015 (the campaign's byline is "Fighting For An AIDS Free Generation").

</doc>
<doc id="6693" url="http://en.wikipedia.org/wiki?curid=6693" title="Cofinality">
Cofinality

In mathematics, especially in order theory, the cofinality cf("A") of a partially ordered set "A" is the least of the cardinalities of the cofinal subsets of "A".
This definition of cofinality relies on the axiom of choice, as it uses the fact that every non-empty set of cardinal numbers has a least member. The cofinality of a partially ordered set "A" can alternatively be defined as the least ordinal "x" such that there is a function from "x" to "A" with cofinal image. This second definition makes sense without the axiom of choice. If the axiom of choice is assumed, as will be the case in the rest of this article, then the two definitions are equivalent.
Cofinality can be similarly defined for a directed set and is used to generalize the notion of a subsequence in a net.
Properties.
If "A" admits a totally ordered cofinal subset, then we can find a subset "B" which is well-ordered and cofinal in "A". Any subset of "B" is also well-ordered. If two cofinal subsets of "B" have minimal cardinality (i.e. their cardinality is the cofinality of "B"), then they are order isomorphic to each other.
Cofinality of ordinals and other well-ordered sets.
The cofinality of an ordinal α is the smallest ordinal δ which is the order type of a cofinal subset of α. The cofinality of a set of ordinals or any other well-ordered set is the cofinality of the order type of that set.
Thus for a limit ordinal, there exists a δ-indexed strictly increasing sequence with limit α. For example, the cofinality of ω² is ω, because the sequence ω·"m" (where "m" ranges over the natural numbers) tends to ω²; but, more generally, any countable limit ordinal has cofinality ω. An uncountable limit ordinal may have either cofinality ω as does ωω or an uncountable cofinality.
The cofinality of 0 is 0. The cofinality of any successor ordinal is 1. The cofinality of any nonzero limit ordinal is an infinite regular cardinal.
Regular and singular ordinals.
A regular ordinal is an ordinal which is equal to its cofinality. A singular ordinal is any ordinal which is not regular.
Every regular ordinal is the initial ordinal of a cardinal. Any limit of regular ordinals is a limit of initial ordinals and thus is also initial but need not be regular. Assuming the Axiom of choice, formula_1 is regular for each α. In this case, the ordinals 0, 1, formula_2, formula_3, and formula_4 are regular, whereas 2, 3, formula_5, and ωω·2 are initial ordinals which are not regular.
The cofinality of any ordinal "α" is a regular ordinal, i.e. the cofinality of the cofinality of "α" is the same as the cofinality of "α". So the cofinality operation is idempotent.
Cofinality of cardinals.
If κ is an infinite cardinal number, then cf(κ) is the least cardinal such that there is an unbounded function from cf(κ) to κ; cf(κ) is also the cardinality of the smallest set of strictly smaller cardinals whose sum is κ; more precisely
That the set above is nonempty comes from the fact that
i.e. the disjoint union of κ singleton sets. This implies immediately that cf(κ) ≤ κ.
The cofinality of any totally ordered set is regular, so one has cf(κ) = cf(cf(κ)).
Using König's theorem, one can prove κ < κcf(κ) and κ < cf(2κ) for any infinite cardinal κ.
The last inequality implies that the cofinality of the cardinality of the continuum must be uncountable. On the other hand,
the ordinal number ω being the first infinite ordinal, so that the cofinality of formula_9 is card(ω) = formula_10. (In particular, formula_9 is singular.) Therefore,
Generalizing this argument, one can prove that for a limit ordinal δ

</doc>
<doc id="6695" url="http://en.wikipedia.org/wiki?curid=6695" title="Citadel">
Citadel

A citadel is the core fortified area of a town or city. It may be a fortress, castle, or fortified center. The term is a diminutive of "city" and thus means "little city" so called because it is a smaller part of the city of which it is the defensive core.
In a fortification with bastions, the citadel is the strongest part of the system, sometimes well inside the outer walls and bastions, but often forming part of the outer wall for the sake of economy. It is positioned to be the last line of defense, should the enemy breach the other components of the fortification system. A citadel is also a term of the third part of a medieval castle, with higher walls than the rest. It was to be the last line of defence before the keep itself.
In various countries, the citadels gained a specific name such as "Kremlin" in Russia or "Alcázar" in the Iberian Peninsula. In European cities, the term "Citadel" and "City Castle" are often used interchangeably. The term "tower" is also used in some cases such as the Tower of London and Jerusalem's Tower of David. However, the Haitian citadel, which is the largest citadel in the Western Hemisphere, is called Citadelle Laferrière or simply the "Citadel" in English.
History.
8000 BC–600 AD.
In Ancient Greece, the Acropolis (literally: "peak of the city"), placed on a commanding eminence, was important in the life of the people, serving as a refuge and stronghold in peril and containing military and food supplies, the shrine of the god and a royal palace. The most well-known is the Acropolis of Athens, but nearly every Greek city-state had one – the Acrocorinth famed as a particularly strong fortress. In a much later period, when Greece was ruled by the Latin Empire, the same strongpoints were used by the new feudal rulers for much the same purpose.
167–160 BC.
Rebels who took power in the city but with the citadel still held by the former rulers could by no means regard their tenure of power as secure. One such incident played an important part in the history of the Maccabean Revolt against the Seleucid Empire. The Hellenistic garrison of Jerusalem and local supporters of the Seleucids held out for many years in the Acra citadel, making Maccabean rule in the rest of Jerusalem precarious. When finally gaining possession of the place, the Maccabeans pointedly destroyed and razed the Acra, though they constructed another citadel for their own use in a different part of Jerusalem.
3300–1300 BC.
Some of the oldest known structures which have served as citadels were built by the Indus Valley Civilization, where the citadel represented a centralised authority. The main citadel in Indus Valley was almost 12 meters tall. The purpose of these structures, however, remains debated. Though the structures found in the ruins of Mohenjo-daro were walled, it is far from clear that these structures were defensive against enemy attacks. Rather, they may have been built to divert flood waters.
500–1500 AD.
At various periods, and particularly during the Middle Ages, the citadel – having its own fortifications, independent of the city walls – was the last defence of a besieged army, often held after the town had been conquered. A city where the citadel held out against an invading army was not considered conquered. For example, in the 1543 Siege of Nice the Ottoman forces led by Barbarossa conquered and pillaged the town itself and took many captives – but the city castle held out, due to which the townspeople were accounted the victors.
1600–1860 AD.
As late as the 19th century, a similar situation developed at Antwerp, where a Dutch garrison under General David Hendrik Chassé held out in the city's citadel between 1830 and 1832, while the city itself had already become part of the independent Belgium.
In time of war the citadel in many cases afforded retreat to the people living in the areas around the town. However, Citadels were often used also to protect a garrison or political power from the inhabitants of the town where it was located, being designed to ensure loyalty from the town that they defended.
For example Barcelona had a great citadel built in 1714 to intimidate the Catalans against repeating their mid-17th- and early-18th-century rebellions against the Spanish central government. In the 19th century, when the political climate had liberalised enough to permit it, the people of Barcelona had the citadel torn down, and replaced it with the city's main central park, the Parc de la Ciutadella. A similar example is the Citadella in Budapest, Hungary.
The attack on the Bastille in the French Revolution – though afterwards remembered mainly for the release of the handful of prisoners incarcerated there – was to considerable degree motivated by the structure being a Royal citadel in the midst of revolutionary Paris.
Similarly, after Garibaldi's overthrow of Bourbon rule in Palermo, during the 1860 Unification of Italy, Palermo's Castellamare Citadel – symbol of the hated and oppressive former rule – was ceremoniously demolished.
The Siege of the Alcázar in the Spanish Civil War, in which the Nationalists held out against a much larger Republican force for two months until relieved, shows that in some cases a citadel can be effective even in modern warfare; a similar case is the Battle of Huế during the Vietnam war, where a North Vietnamese Army division held the citadel of Huế for 26 days against roughly their own numbers of much better-equipped US and South Vietnamese troops.
1820–present AD.
The Citadelle of Québec (construction started 1673, completed 1820) still survives as the largest citadel still in official military operation in North America. It is home to the Royal 22nd Regiment of Canada; and forms part of the fortified walls of Vieux-Québec dating back to 1608.
Modern usage.
Citadels since the mid 20th century, are commonly military command and control centres built to resist attack commonly aerial or nuclear bombardment. The Military citadels under London such as the massive underground complex beneath the Ministry of Defence called Pindar is one such example, as is the Cheyenne Mountain nuclear bunker in the US.
Naval term.
On armored warships, the heavily armored section of the ship that protects the ammunition and machinery spaces is called the citadel.
The safe room on a ship is also called a citadel.

</doc>
<doc id="6696" url="http://en.wikipedia.org/wiki?curid=6696" title="Mail (armour)">
Mail (armour)

Mail (chainmail, maille) is a type of armour consisting of small metal rings linked together in a pattern to form a mesh.
History.
The earliest example of mail was found in a Dacian chieftain's burial located in Ciumești, Romania. Its invention is commonly credited to the Celts, but there are examples of Etruscan pattern mail dating from at least the 4th century BC. Mail may have been inspired by the much earlier scale armour. Mail spread to North Africa, the Middle East, Central Asia, India, Tibet, South East Asia, and Japan.
Mail continues to be used in the 21st century as a component of stab-resistant body armour, cut-resistant gloves for butchers and woodworkers, shark-resistant wetsuits for defense against shark bites, and a number of other applications.
Etymology.
The origins of the word “mail” are not fully known. One theory is that it originally derives from the Latin word "macula", meaning "spot" or “opacity” (as in macula of retina). Another theory relates the word to the old French “maillier”, meaning “to hammer” (related to the modern English word “malleable”).
The first attestations of the word “mail” are in Old French and Anglo-Norman: “maille” “maile”, or “male” or other variants, which became “mailye” “maille” “maile”, “male”, or “meile” in Middle English.
The modern usage of terms for mail armour is highly contested in popular and, to a lesser degree, academic culture. Medieval sources referred to armour of this type simply as “mail”, however “chain-mail” has become a commonly used, if incorrect neologism first attested in Sir Walter Scott’s 1822 novel "The Fortunes of Nigel". Since then the word “mail” has been commonly, if incorrectly, applied to other types of armour, such as in “plate-mail” (first attested in 1835). The more correct term is “plate armour”.
Civilizations that used mail invented specific terms for each garment made from it. The standard terms for European mail armour derive from French: leggings are called chausses, a hood is a coif, and mittens, mitons. A mail collar hanging from a helmet is a camail or aventail. A shirt made from mail is a hauberk if knee-length and a haubergeon if mid-thigh length. A layer (or layers) of mail sandwiched between layers of fabric is called a jazerant.
A waist-length coat in medieval Europe was called a byrnie, although the exact construction of a byrnie is unclear, including whether it was constructed of mail or other armour-types. Noting that the byrnie was the "most highly valued piece of armour" to the Carolingian soldier, Bennet, Bradbury, DeVries, Dickie, and Jestice indicate that:
There is some dispute among historians as to what exactly constituted the Carolingian byrnie. Relying... only on artistic and some literary sources because of the lack of archaeological examples, some believe that it was a heavy leather jacket with metal scales sewn onto it. It was also quite long, reaching below the hips and covering most of the arms. Other historians claim instead that the Carolingian byrnie was nothing more than a coat of mail, but longer and perhaps heavier than traditional early medieval mail. Without more certain evidence, this dispute will continue.
Mail armour in Europe.
The use of mail as battlefield armour was common during the Iron Age and the Middle Ages, becoming less common over the course of the 16th and 17th centuries. It is believed that the Roman Republic first came into contact with mail fighting the Gauls in Cisalpine Gaul, now Northern Italy, but a different pattern of mail was already in use among the Etruscans. The Roman army adopted the technology for their troops in the form of the lorica hamata which was used as a primary form of armour through the Imperial period.
After the fall of the Western Empire much of the infrastructure needed to create plate armour diminished. Eventually the word "mail" came to be synonymous with armour. It was typically an extremely prized commodity as it was expensive and time consuming to produce and could mean the difference between life and death in a battle. Mail from dead combatants was frequently looted and was used by the new owner or sold for a lucrative price. As time went on and infrastructure improved it came to be used by more soldiers. Eventually with the rise of the lanced cavalry charge, impact warfare, and high-powered crossbows, mail came to be used as a secondary armour to plate for the mounted nobility.
By the 14th century, plate armour was commonly used to supplement mail. Eventually mail was supplanted by plate for the most part as it provided greater protection against windlass crossbows, bludgeoning weapons, and lance charges. However, mail was still widely used by many soldiers as well as brigandines and padded jacks. These three types of armour made up the bulk of the equipment used by soldiers with mail being the most expensive. It was sometimes more expensive than plate armour. Mail typically persisted longer in less technologically advanced areas such as Eastern Europe but was in use everywhere into the 16th century.
During the late 19th and early 20th century mail was used as a material for bulletproof vests, most notably by the Wilkinson Sword Company. Results were unsatisfactory; Wilkinson mail worn by the Khedive of Egypt's regiment of "Iron Men" was manufactured from split rings which proved to be too brittle, and the rings would fragment when struck by bullets and aggravate the damage. The riveted mail armour worn by the opposing Sudanese Madhists did not have the same problem but also proved to be relatively useless against the firearms of British forces at the battle of Omdurman. During World War I Wilkinson Sword transitioned from mail to a lamellar design which was the precursor to the flak jacket.
Also during World War I a mail fringe, designed by Captain Cruise of the British Infantry, was added to helmets to protect the face. This proved unpopular with soldiers, in spite of being proven to defend against a three-ounce (100 g) shrapnel round fired at a distance of one hundred yards (90 m).
Mail armour in Asia.
Mail Armour was introduced to the Middle East and Asia through the Romans and was adopted by the Sassanid Persians starting in the 3rd century AD, where it was supplemental to the scale and lamellar armours already used. Mail was commonly also used as horse armour for cataphracts and heavy cavalry as well as armour for the soldiers themselves. Asian mail was typically lighter than the European variety and sometimes had prayer symbols stamped on the rings as a sign of their craftsmanship as well as for divine protection. Indeed, mail armour is mentioned in the Koran as being a gift revealed by Allah to David:
21:80 "It was We Who taught him the making of coats of mail for your benefit, to guard you from each other's violence: will ye then be grateful?" (Yusuf Ali's translation).
From the Middle East mail was quickly adopted in Central Asia by the Sogdians and by India in the South. It was not commonly used in Mongol armies due to its weight and the difficulty of its maintenance, but it eventually became the armour of choice in India. Indian mail was often used with plate protection. Plated mail was in common use in India until the Battle of Plassey and the subsequent British conquest of the sub-continent.
The Ottoman Empire used plated mail widely and it was used in their armies until the 18th century by heavy cavalry and elite units such as the Janissaries. They spread its use into North Africa where it was adopted by Mamluk Egyptians and the Sudanese who produced it until the early 20th century.
Mail was introduced to China when its allies in Central Asia paid tribute to the Tang Emperor in 718 by giving him a coat of "link armour" assumed to be mail. China first encountered the armour in 384 when its allies in the nation of Kuchi arrived wearing "armour similar to chains". Once in China mail was imported but was not produced widely. Due to its flexibility and comfort, it was typically the armour of high-ranking guards and those who could afford the import rather than the armour of the rank and file, who used the easier to produce and maintain brigandine and lamellar types. However, it was one of the only military products that China imported from foreigners. Mail spread to Korea slightly later where it was imported as the armour of imperial guards and generals.
Mail armour (kusari) in Japan.
The Japanese had more varieties of mail than all the rest of the world put together. In Japan mail is called ' which means chain. When the word "kusari" is used in conjunction with an armoured item it usually means that the "kusari" makes up the majority of the armour defence. An example of this would be "kusari gusoku" which means chain armour. "Kusari" ', ', ', ', ', shoulder, ', and other armoured clothing were produced, even ' socks.
"" was used in samurai armour at least from the time of the Mongol invasion (1270s) but particularly from the Nambokucho period (1336–1392). The Japanese used many different weave methods including: a square 4-in-1 pattern ("so gusari"), a hexagonal 6-in-1 pattern ("hana gusari") and a European 4-in-1 ("nanban gusari"). Kusari was typically made with rings that were much smaller than their European counterparts, and patches of kusari were used to link together plates and to drape over vulnerable areas such as the underarm.
Riveted kusari was known and used in Japan. On page 58 of the book "Japanese Arms & Armor: Introduction" by H. Russell Robinson, there is a picture of Japanese riveted kusari, and
this quote from the translated reference of Sakakibara Kozan's 1800 book, "The Manufacture of Armour and Helmets in Sixteenth Century Japan", shows that the Japanese not only knew of and used riveted kusari but that they manufactured it as well.
... karakuri-namban (riveted namban), with stout links each closed by a rivet. Its invention is credited to Fukushima Dembei Kunitaka, pupil, of Hojo Awa no Kami Ujifusa, but it is also said to be derived directly from foreign models. It is heavy because the links are tinned (biakuro-nagashi) and these are also sharp edged because they are punched out of iron plate
Butted and or split (twisted) links made up the majority of "kusari" links used by the Japanese. Links were either "butted" together meaning that the ends touched each other and were not riveted, or the "kusari" was constructed with links where the wire was turned or twisted two or more times, these split links are similar to the modern split ring commonly used on keychains. The rings were lacquered black to prevent rusting, and were always stitched onto a backing of cloth or leather. The kusari was sometimes concealed entirely between layers of cloth.
Kusari gusoku or chain armour was commonly used during the Edo period 1603 to 1868 as a stand-alone defence. According to George Cameron Stone
Entire suits of mail "kusari gusoku" were worn on occasions, sometimes under the ordinary clothing
Ian Bottomley in his book "Arms and Armor of the Samurai: The History of Weaponry in Ancient Japan" shows a picture of a kusari armour and mentions "" (chain jackets) with detachable arms being worn by samurai police officials during the Edo period. The end of the samurai era in the 1860s, along with the 1876 ban on wearing swords in public, marked the end of any practical use for mail and other armour in Japan. Japan turned to a conscription army and uniforms replaced armour.
Effectiveness.
Mail armour provided an effective defence against slashing blows by an edged weapon and penetration by thrusting and piercing weapons; in fact, a study conducted at the Royal Armouries at Leeds concluded that "it is almost impossible to penetrate using any conventional medieval weapon" Generally speaking, mail's resistance to weapons is determined by four factors: linkage type (riveted, butted, or welded), material used (iron versus bronze or steel), weave density (a tighter weave needs a thinner weapon to surpass), and ring thickness (generally ranging from 18 to 14 gauge in most examples). Mail, if a warrior could afford it, provided a significant advantage to a warrior when combined with competent fighting techniques. When the mail was not riveted, a well-placed thrust from a spear or thin sword could penetrate, and a pollaxe or halberd blow could break through the armour. In India, punching daggers known as katars were developed that could pierce the light butted mail used in the area. Some evidence indicates that during armoured combat, the intention was to actually get around the armour rather than through it—according to a study of skeletons found in Visby, Sweden, a majority of the skeletons showed wounds on less well protected legs.
The flexibility of mail meant that a blow would often injure the wearer, potentially causing serious bruising or fractures, and it was a poor defence against head trauma. Mail-clad warriors typically wore separate rigid helms over their mail coifs for head protection. Likewise, blunt weapons such as maces and warhammers could harm the wearer by their impact without penetrating the armour; usually a soft armour, such as gambeson, was worn under the hauberk. Medieval surgeons were very well capable of setting and caring for bone fractures resulting from blunt weapons. With the poor understanding of hygiene however, cuts that could get infected were much more of a problem. Thus mail armour proved to be sufficient protection in most situations.
Manufacture.
Several patterns of linking the rings together have been known since ancient times, with the most common being the 4-to-1 pattern (where each ring is linked with four others). In Europe, the 4-to-1 pattern was completely dominant. Mail was also common in East Asia, primarily Japan, with several more patterns being utilised and an entire nomenclature developing around them.
Historically, in Europe, from the pre-Roman period on, the rings composing a piece of mail would be riveted closed to reduce the chance of the rings splitting open when subjected to a thrusting attack or a hit by an arrow.
Up until the 14th century European mail was made of alternating rows of riveted rings and solid rings. After that point mail was almost all made from riveted rings only. Both were commonly made of wrought iron, but some later pieces were made of heat-treated steel. Wire for the riveted rings was formed by either of two methods. One was to hammer out wrought iron into plates and cut or slit the plates. These thin pieces were then pulled through a draw plate repeatedly until the desired diameter was achieved. Waterwheel powered drawing mills are pictured in several period manuscripts. Another method was to simply forge down an iron billet into a rod and then proceed to draw it out into wire. The solid links would have been made by punching from a sheet. Guild marks were often stamped on the rings to show their origin and craftsmanship. Forge welding was also used to create solid links, but there are few possible examples known, the only well documented example from Europe is that of the camail (mail neck-defence) of the 7th century Coppergate helmet. Outside of Europe this practice was more common such as "theta" links from India. Very few examples of historic butted mail have been found and it is generally accepted that butted mail was never in wide use historically except in Japan where mail ("kusari") was commonly made from "butted" links.
Modern uses.
Practical uses.
Mail is used as protective clothing for butchers against meat-packing equipment. Workers may wear up to 8 lb of mail under their white coats. Butchers also commonly wear a single mail glove to protect themselves from self-inflicted injury while cutting meat.
Woodcarvers sometimes use similar mail gloves to protect their hands from cuts and punctures.
The British police use mail gloves for dealing with knife-armed aggressors.
Scuba divers use mail to protect them from sharkbite, as do animal control officers for protection against the animals they handle. Shark expert and underwater filmmaker Valerie Taylor was among the first to develop and test the mail suit in 1979 while diving with sharks.
Mail is widely used in industrial settings as shrapnel guards and splash guards in metal working operations.
Electrical applications for mail include RF leakage testing and being worn as a faraday cage suit by tesla coil enthusiasts and high voltage electrical workers.
Stab-proof vests.
Conventional textile-based ballistic vests are designed to stop soft-nosed bullets but offer little defense from knife attacks. Knife resistant armours are designed to defend against knife attacks; some of these use layers of metal plates, mail and metallic wires.
Historical re-enactment.
Many historical reenactment groups, especially those whose focus is Antiquity or the Middle Ages, commonly use mail both as practical armour and for costuming. Mail is especially popular amongst those groups which use steel weapons. A modern hauberk made from 1.5 mm diameter wire with 10 mm inner diameter rings weighs roughly 10 kg and contains 15,000–45,000 rings.
One of the drawbacks of mail is the uneven weight distribution; the stress falls mainly on shoulders. Weight can be better distributed by wearing a belt over the mail, which provides another point of support.
Mail worn today for re-enactment and recreational use can be made in a variety of styles and materials. Most recreational mail today is made of butted links which are galvanized or stainless steel. This is historically inaccurate but is much less expensive to procure and maintain than historically accurate reproductions. Mail can also be made of titanium, aluminium, bronze, or copper. Riveted mail offers significantly better protection ability as well as historical accuracy than mail constructed with butted links. Riveted mail can be more labour-intensive and expensive to manufacture. Japanese mail ("kusari") is one of the few historically correct examples of mail being constructed with such "butted links".
Decorative uses.
Mail remained in use as a decorative and possibly high-status symbol with military overtones long after its practical usefulness had passed. It was frequently used for the epaulettes of military uniforms. It is still used in this form by the British Territorial Army, and the Royal Canadian Armoured Corps of the Canadian Army.
Mail has applications in sculpture and jewellery, especially when made out of precious metals or colourful anodized metals. Mail artwork includes headdresses, Christmas ornaments, chess sets, and jewelry. For these non-traditional applications, hundreds of weaves or patterns have been invented.
In film.
In some films, knitted string spray-painted with a metallic paint is used instead of actual mail in order to cut down on cost (an example being "Monty Python and the Holy Grail", which was filmed on a very small budget). Films more dedicated to costume accuracy often use ABS plastic rings, for the lower cost and weight. Such ABS mail coats were made for "The Lord of the Rings" film trilogy, in addition to many metal coats. The metal coats are used rarely because of their weight, except in close-up filming where the appearance of ABS rings is distinguishable. A large scale example of the ABS mail used in the "Lord of the Rings" can be seen in the entrance to the Royal Armouries museum in Leeds in the form of a large curtain bearing the logo of the museum. It was acquired from the makers of the film's armour, Weta Workshop, when the museum hosted an exhibition of WETA armour from their films. For the film "Mad Max Beyond Thunderdome", Tina Turner is said to have worn actual mail and she complained how heavy this was.

</doc>
<doc id="6697" url="http://en.wikipedia.org/wiki?curid=6697" title="Cerberus">
Cerberus

Cerberus (; Greek: Κέρβερος "Kerberos" ]) in Greek and Roman mythology, is a multi-headed (usually three-headed) dog, or "hellhound" with a serpent's tail, a mane of snakes, and a lion's claws. He guards the entrance of the Greek underworld to prevent the dead from escaping and the living from entering. Cerberus is featured in many works of ancient Greek and Roman literature and in works of both ancient and modern art and architecture, although the depiction of Cerberus differs across various renditions. The most notable difference is the number of his heads: Most sources describe or depict three heads; others show Cerberus with two or even just one; a smaller number of sources show a variable number, sometimes as many as fifty or even a hundred.
In myth.
Cerberus is the offspring of Echidna, a hybrid half-woman and half-serpent, and Typhon, a gigantic monster even the Greek gods feared. Its siblings are the Lernaean Hydra; Orthrus, a two-headed hellhound; and the Chimera, a three-headed monster. The common depiction of Cerberus in Greek mythology and art is as having three heads. In most works, the three heads respectively see and represent the past, the present, and the future, while other sources suggest the heads represent birth, youth, and old age. Each of Cerberus' heads is said to have an appetite only for live meat and thus allow only the spirits of the dead to freely enter the underworld, but allow none to leave. Cerberus was always employed as Hades' loyal watchdog, and guarded the gates that granted access and exit to the underworld.
The Twelfth Labour of Heracles.
Capturing Cerberus was the final labour assigned to Heracles (Hercules) by King Eurystheus, in recompense for the killing of his own children by Megara after he was driven insane by Hera, and therefore was the most dangerous and difficult.
After having been given the task, Heracles went to Eleusis to be initiated in the Eleusinian Mysteries so he could learn how to enter and exit the underworld alive, and in passing absolve himself for killing centaurs. He found the entrance to the underworld at Tanaerum, and Athena and Hermes helped him to traverse the entrance in each direction. He passed Charon with Hestia's assistance and his own heavy and fierce frowning.
Whilst in the underworld, Heracles met Theseus and Pirithous. The two companions had been imprisoned by Hades for attempting to kidnap Persephone. One tradition tells of snakes coiling around their legs then turning into stone; another tells that Hades feigned hospitality and prepared a feast inviting them to sit. They unknowingly sat in chairs of forgetfulness and were permanently ensnared. When Heracles had pulled Theseus first from his chair, some of his thigh stuck to it (this explains the supposedly lean thighs of Athenians), but the earth shook at the attempt to liberate Pirithous, whose desire to have the wife of a god for himself was so insulting, he was doomed to stay behind.
Heracles found Hades and asked permission to bring Cerberus to the surface, to which Hades agreed if Heracles could overpower the beast without using weapons. Heracles was able to overpower Cerberus and proceeded to sling the beast over his back, dragging it out of the underworld through a cavern entrance in the Peloponnese and bringing it to Eurystheus. Later Heracles brought Cerberus to a secret grove belonging to Demeter. Which later Cerberus escapes and returns to the underworld to once again guard the entrance.
In literature.
Cerberus is featured in many prominent works of Greek and Roman literature, most famously in Virgil's "Aeneid", Peisandros of Rhodes' epic poem the "Labours of Hercules", the story of Orpheus in Plato's "Symposium", and in Homer's "Iliad", which is the only known reference to one of Heracles' labours which first appeared in a literary source.
The depiction of Cerberus is relatively consistent between different works and authors, the common theme of the mane of serpents is kept across works, as is the serpent's tail, most literary works of the era describe Cerberus as having three heads with the only notable exception being Hesiod's "Theogony" in which he had 50 heads.
Most occurrences in ancient literature revolve around the basis of the threat of Cerberus being overcome to allow a living being access to the underworld; in the "Aeneid" Cerberus was lulled to sleep after being tricked into eating drugged honeycakes and Orpheus put the creature to sleep with his music. Capturing Cerberus alive was the twelfth and final labour of Heracles. In Dante Alighieri's "Inferno", Canto VI, the "great worm" Cerberus is found in the Third Circle of Hell, where he oversees and rends to pieces those who have succumbed to gluttony, one of the seven deadly sins.
In the constellation Cerberus introduced by Johannes Hevelius in 1687, Cerberus is sometimes substituted for the "branch from the tree of the golden apples" fetched by Atlas from the garden of the Hesperides.
In art.
Numerous references to Cerberus have appeared in ancient Greek and Roman art, found in archaeological ruins and often including in statues and architecture, inspired by the mythology of the creature. Cerberus' depiction in ancient art is not as definitive as in literature; the poets and linguists of ancient Greece and Rome mostly agreed on the physical appearance (with the notable exception in Hesiod's "Theogony" in which he had 50 heads). His depiction in classical art mostly shows the recurring motif of serpents, but the number of heads differs. A statue in the Galleria Borghese depicts Cerberus with three heads sitting by the side of Hades, while a bronze sculpture depicting Heracles' twelfth labour shows the demi-god leading a two-headed Cerberus from the underworld. The majority of vases depicting the twelfth task also show Cerberus as having two heads. Classical critics have identified one of the earliest works of Cerberus as "the most imaginative," that being a Laconian vase created around 560 BC in which Cerberus is shown with three-heads and with rows of serpents covering his body and heads.
Etymology.
The name "Cerberus" is a Latinised version of the Greek "Kerberos". The etymology of this name prior to Greek is disputed. It has been claimed to be related to the Sanskrit word सर्वरा "sarvarā", used as an epithet of one of the dogs of Yama, from a Proto-Indo-European word *"k̑érberos", meaning "spotted" 
The use of a dog is uncertain, although mythologists have speculated that the association was first made in the city of Trikarenos in Phliasia. Bruce Lincoln (1991), among others, critiques this etymology; and Ogden (2013) refers to it as "dismissed".
Lincoln notes a similarity between Cerberus and the Norse mythological dog Garmr, relating both names to a Proto-Indo-European root "*ger-" "to growl" (perhaps with the suffixes "-*m/*b" and "-*r"). However, as Ogden observes, this analysis actually requires "Cerberus" and "Garmr" to be derived from two "different" Indo-European roots (*"ger-" and *"gher-" respectively), and so does not actually establish a relationship between the two names.
Explanations.
There have been many attempts to explain the depiction of Cerberus. A 2nd century CE Greek known as Heraclitus the paradoxographer - not to be confused with the 5th century BCE Greek philosopher Heraclitus - claimed euhemeristically that Cerberus had two pups that were never away from their father, and that Cerberus was in fact a normal (though very large) dog, but that artists incorporating the two pups into their work made it appear as if his two children were in fact extra heads. Classical historians have dismissed Heraclitus the paradoxographer's explanation as "feeble". Mythologers have speculated that if Cerberus were given his name in Trikarenos it could be interpreted as "three karenos". Certain experts believe that the monster was inspired by the golden jackal.

</doc>
<doc id="6698" url="http://en.wikipedia.org/wiki?curid=6698" title="CamelCase">
CamelCase

camelCase (Camelcase, camelCase, camel case, camel caps or medial capitals) is the practice of writing compound words or phrases such that each word or abbreviation begins with a capital letter. Camel case may start with a capital or, especially in programming languages, with a lowercase letter. Common examples are LibreOffice, PowerPoint, iPhone or usernames such as DylanStuart.
In Microsoft documentation, camel case always starts with a lower case letter (e.g. backColor), and it is contrasted with Pascal case which always begins with a capital letter (e.g. BackColor).
Variations and synonyms.
Although the first letter of a camel case compound word may or may not be capitalized, the term camel case generally implies lowercase first letter. For clarity, this article calls the two alternatives upper camel case and lower camel case. Some people and organizations use the term "camel case" only for lower camel case. Other synonyms include:
StudlyCaps encompasses all such variations, and more, including even random mixed capitalization, as in "MiXeD CaPitALiZaTioN" (typically a stereotyped allusion to online culture).
Camel case is also distinct from title case, which is traditionally used for book titles and headlines. Title case capitalizes most of the words yet retains the spaces between the words. Camel case is also distinct from Tall Man lettering, which uses capitals to emphasize the differences between similar-looking words.
History.
Chemical formulae.
The first systematic and widespread use of medial capitals for technical purposes was the notation for chemical formulae invented by the Swedish chemist Berzelius in 1813. To replace the multitude of naming and symbol conventions used by chemists until that time, he proposed to indicate each chemical element by a symbol of one or two letters, the first one being capitalized. The capitalization allowed formulae like 'NaCl' to be written without spaces and still be parsed without ambiguity.
Berzelius's system remains in use to this day, augmented with three-letter symbols like 'Uut' for unnamed elements and abbreviations for some common substituents (especially in the field of organic chemistry, for instance 'Et' for 'ethyl-'). This has been further extended to describe the amino acid sequences of proteins and other similar domains.
"The King's English".
In their English style guide "The King's English", first published in 1906, H. W. Fowler and F. G. Fowler suggested that medial capitals could be used in triple compound words where hyphens would cause ambiguity—the examples they give are "KingMark-like" (as against "King Mark-like") and "Anglo-SouthAmerican" (as against "Anglo-South American"). However, they described the system as "too hopelessly contrary to use at present."
Early use in trademarks.
Since the early 20th century, medial capitals have occasionally been used for corporate names and product trademarks, such as
Computer programming.
In the 1970s and 1980s, medial capitals were adopted as a standard or alternative naming convention for multi-word identifiers in several programming languages. The origin of this convention has not yet been settled. However, a 1954 conference proceedings informally referred to IBM's Speedcoding system as "SpeedCo". Christopher Strachey's paper on GPM (1965), shows a program that includes some medial capital identifiers, including "NextCh" and "WriteSymbol". Charles Simonyi, who oversaw the creation of Microsoft's flagship Office suite of applications, invented and taught the use of Hungarian Notation, in which the lower case letter at the start of a variable name denotes its type.
Background: multi-word identifiers.
Computer programmers often need to write descriptive (hence multi-word) identifiers, like codice_1 or codice_2, in order to improve the readability of their code. However, most popular programming languages forbid the use of spaces inside identifiers, since they are interpreted as delimiters between tokens, and allowing spaces inside of identifiers would significantly complicate lexical analysis (breaking the source code into tokens). The alternative of writing the words together as in codice_3 or codice_4 is not satisfactory, since the word boundaries may be quite difficult to discern in the result or it may even be misleading (e.g. codice_4 may be used to mean chart-able, not "char table": something can be displayed in a chart).
Some early programming languages, notably Lisp (1958) and COBOL (1959), addressed this problem by allowing a hyphen ("-") to be used between words of compound identifiers, as in "END-OF-FILE"—Lisp because it worked well with prefix notation; a Lisp parser would not treat a hyphen in the middle of a symbol as a subtraction operator; COBOL because its operators were English words. This convention remains in use in these languages, and is also common in program names entered at a command line, as in Unix.
However, this solution was not adequate for algebra-oriented languages such as FORTRAN (1955) and ALGOL (1958), which used the hyphen as an intuitively obvious subtraction operator, and did not wish to require spaces around the hyphen, so one could write codice_6 as codice_7. These early languages instead allowed identifiers to have spaces in them, determining the end of the identifier by context; this was abandoned in later languages due to the complexity it adds to tokenization. In addition, common punched card character sets of the time were uppercase only and lacked other special characters. Further, FORTRAN initially restricted identifiers to six characters or fewer at the time, preventing multi-word identifiers except those made of very short words.
It was only in the late 1960s that the widespread adoption of the ASCII character set made both lower case and the underscore character codice_8 universally available. Some languages, notably C, promptly adopted underscores as word separators; and underscore-separated compounds like codice_9 are still prevalent in C programs and libraries, as well as other languages such as Python. However, some languages and programmers chose to avoid underscores, among other reasons to prevent confusing them with whitespace, and adopted camel case instead. Two accounts are commonly given for the origin of this convention.
"Lazy programmer" hypothesis.
One hypothesis for the origin of the camel case convention holds that C programmers and hackers simply found it more convenient than the snake case style (as in "this_is_an_example").
The underscore key is inconveniently placed on American QWERTY keyboards. Furthermore, early compilers severely restricted the length of identifiers (e.g., to 8 or 14 letters) or silently truncated all identifiers to that length (for example, FORTRAN 77 limited identifiers to 6 characters; even in C99, characters after the first 31 could be ignored). Finally, the small size of computer displays available in the 1970s (e.g., 80-character by 24-line VT52 and similar terminals) encouraged the use of short identifiers. Some programmers opted to use camel case instead of underscores to get legible compound names with fewer keystrokes and fewer characters.
"Alto Keyboard" hypothesis.
Another account claims that the camel case style first became popular at Xerox PARC around 1978, with the Mesa programming language developed for the Xerox Alto computer. This machine lacked an underscore key, and the hyphen and space characters were not permitted in identifiers, leaving camel case as the only viable scheme for readable multiword names. The PARC Mesa Language Manual (1979) included a coding standard with specific rules for Upper- and lowerCamelCase that was strictly followed by the Mesa libraries and the Alto operating system.
The Smalltalk language, which was developed originally on the Alto and became quite popular in the early 1980s, may have been instrumental in spreading the style outside PARC. Camel case was also used by convention for many names in the PostScript page description language (invented by Adobe Systems founder and ex-PARC scientist John Warnock), as well as for the language itself. A further boost was provided by Niklaus Wirth (the inventor of Pascal) who acquired a taste for camel case during a sabbatical at PARC and used it in Modula, his next programming language.
Spread to mainstream usage.
Whatever its origins within the computing world, the practice spread in the 1980s and 1990s, when the advent of the personal computer exposed hacker culture to the world. Camel case then became fashionable for corporate trade names, initially in technical fields; mainstream usage was well established by 1990:
During the dot-com bubble of the late 1990s, the lowercase prefixes "e" (for "electronic") and "i" (for "Internet", "information", "intelligent", etc.) became quite common, giving rise to names like Apple's iMac and the eBox software platform.
In 1998, Dave Yost suggested that chemists use medial capitals to aid readability of long chemical names, e.g. write AmidoPhosphoRibosylTransferase instead of amidophosphoribosyltransferase. This usage was still rare in 2012.
The practice is sometimes used for abbreviated names of certain neighborhoods, e.g. New York City neighborhoods "SoHo" ("So"uth of "Ho"uston Street) and "TriBeCa" ("Tri"angle "Be"low "Ca"nal Street) and San Francisco's "SoMa" ("So"uth of "Ma"rket). Such usages erode quickly, so the neighborhoods are now typically rendered as "Soho", "Tribeca", and "Soma".
Internal capitalization has also been used for other technical codes like HeLa (1983).
History of the name "camel case".
The original name of the practice, used in media studies, grammars and the "Oxford English Dictionary", was "medial capitals". Other names such as "InterCaps", "CamelCase" and variations thereof are relatively recent and seem more common in computer-related communities.
The earliest known occurrence of the term "InterCaps" on Usenet is in an April 1990 post to the group alt.folklore.computers by Avi Rappoport, with "BiCapitalization" appearing slightly later in a 1991 post by Eric S. Raymond to the same group. The earliest use of the name "CamelCase" occurs in 1995, in a post by Newton Love. ""With the advent of programming languages having these sorts of constructs, the humpiness of the style made me call it HumpyCase at first, before I settled on CamelCase. I had been calling it CamelCase for years," said Love, "The citation above was just the first time I had used the name on USENET.""
The name "CamelCase" is not related to the "Camel Book" ("Programming Perl"), which uses all-lowercase identifiers with underscores in its sample code. However, in Perl programming camel case is also commonly used.
Current usage in computing.
Programming and coding.
The use of medial caps for compound identifiers is recommended by the coding style guidelines of many organizations or software projects. For some languages (such as Mesa, Pascal, Modula, Java and Microsoft's .NET) this practice is recommended by the language developers or by authoritative manuals and has therefore become part of the language's "culture".
Style guidelines often distinguish between upper and lower camel case, typically specifying which variety should be used for specific kinds of entities: variables, record fields, methods, procedures, types, etc. These rules are sometimes supported by static analysis tools that check source code for adherence.
The original Hungarian notation for programming, for example, specifies that a lowercase abbreviation for the "usage type" (not data type) should prefix all variable names, with the remainder of the name in upper camel case; as such it is a form of lower camel case.
Programming identifiers often need to contain acronyms and initialisms that are already in upper case, such as "old HTML file". By analogy with the title case rules, the natural camel case rendering would have the abbreviation all in upper case, namely "oldHTMLFile". However, this approach is problematic when two acronyms occur together (e.g., "parse DBM XML" would become "parseDBMXML") or when the standard mandates lower camel case but the name begins with an abbreviation (e.g. "SQL server" would become "sQLServer"). For this reason, some programmers prefer to treat abbreviations as if they were lower case words and write "oldHtmlFile", "parseDbmXml" or "sqlServer".
Wiki link markup.
Camel case is used in some wiki markup languages for terms that should be automatically linked to other wiki pages. This convention was originally used in Ward Cunningham's original wiki software, WikiWikiWeb, and can be activated in most other wikis. Some wiki engines such as TiddlyWiki, Trac and PMWiki make use of it in the default settings, but usually also provide a configuration mechanism or plugin to disable it. Wikipedia formerly used camel case linking as well, but switched to explicit link markup using square brackets and many other wiki sites have done the same. Some wikis that do not use camel case linking may still use the camel case as a naming convention, such as AboutUs.
Other uses.
The NIEM registry requires that XML data elements use upper camel case and XML attributes use lower camel case.
Most popular command-line interfaces and scripting languages cannot easily handle file names that contain embedded spaces (usually requiring the name to be put in quotes). Therefore, users of those systems often resort to camel case (or underscores, hyphens and other "safe" characters) for compound file names like MyJobResume.pdf.
Microblogging and social networking sites that limit the number of characters in a message (most famously Twitter, where the 140-character limit can be quite restrictive in languages that rely on alphabets, including English) are potential outlets for medial capitals. Using CamelCase between words reduces the number of spaces, and thus the number of characters, in a given message, allowing more content to fit into the limited space. Hashtags, especially long ones, often use CamelCase to maintain readability (e.g. #CollegeStudentProblems is easier to read than #collegestudentproblems).
In website URLs, spaces are percent-encoded as "%20", making the address longer and less human readable. By omitting spaces, CamelCase does not have this problem.
Current usage in natural languages.
Camel case has been used in languages other than English for a variety of purposes, including the ones below:
Orthographic markings.
Camel case is sometimes used in the transcription of certain scripts, to differentiate letters or markings. An example is the rendering of Tibetan proper names like "rLobsang": the "r" here stands for a prefix glyph in the original script that functions as tone marker rather than a normal letter. Another example is "tsIurku", a Latin transcription of the Chechen term for the capping stone of the characteristic Medieval defensive towers of Chechenia and Ingushetia; the capital letter "I" here denoting a phoneme distinct from the one transcribed as "i".
Inflection prefixes.
Camel case may also be used when writing proper names in languages that inflect words by attaching prefixes to them. In some of those languages, the custom is to leave the prefix in lower case and capitalize the root.
This convention is used in Irish orthography as well as Scots Gaelic orthography; e.g., "i nGaillimh" ("in Galway"), from "Gaillimh" ("Galway"); "an tAlbanach" ("the Scottish person"), from "Albanach" ("Scottish person"); "go hÉireann" ("to Ireland"), from "Éire" ("Ireland).
Similarly, in transliteration of the Hebrew language, "haIvri" means "the Hebrew person" and "biYerushalayim" means "in Jerusalem".
This convention is also used by several Bantu languages (e.g., "kiSwahili" = "Swahili language", "isiZulu" = "Zulu language") and several indigenous languages of Mexico (e.g. Nahuatl, Totonacan, Mixe–Zoque and some Oto-Manguean languages).
In abbreviations and acronyms.
Abbreviations of some academic qualifications are sometimes presented in camel case without punctuation, e.g. PhD or BSc.
In French, camel case acronyms such as OuLiPo (1960) were favored for a time as alternatives to initialisms.
Camel case is often used to transliterate initialisms into alphabets where two letters may be required to represent a single character of the original alphabet, e.g., DShK from Cyrillic ДШК. 
Honorifics within compound words.
In several languages, including English, pronouns and possessives may be capitalized to indicate respect, e.g., when referring to the reader of a formal letter or to God. In some of those languages, the capitalization is customarily retained even when those words occur within compound words or suffixed to a verb. For example, in Italian one would write "porgendoLe distinti saluti" ("offering to You respectful salutations") or "adorarLo" ("adore Him").
Other uses.
In German nouns carry a grammatical gender—which, for roles or job titles, is felt usually as masculine. Since the feminist movement of the 1980s, some writers and publishers have been using the feminine title suffixes "-in" (singular) and "-innen" (plural) to emphasize the inclusion of females; but written with a capital 'I', to indicate that males are not excluded. Example: "MitarbeiterInnen" ("co-workers, male or female") instead of "Mitarbeiter" ("co-workers", masculine grammatical gender) or "Mitarbeiterinnen" ("female co-workers"). This use is analogous to the use of parentheses in English, for example in the phrase "congress(wo)man."
In German, the names to statutes are abbreviated using embedded capitals, e.g. StGB (Strafgesetzbuch) for criminal code, PatG (Patentgesetz) for Patent Act or the very common GmbH (Gesellschaft mit beschränkter Haftung) for Company with Limited Liability.
Criticism.
CamelCase has been criticised as negatively impacting readability due to the removing of spaces and upcasing of every word. One natural language study found that replacing spaces between words with letters or digits made it harder to recognise individual words, which resulted in increased reading times. However, a study that specifically compared snake case and CamelCase found that camel case identifiers could be recognised with higher accuracy among both programmers and non-programmers, and that programmers already trained in CamelCase were able to recognise CamelCase identifiers faster than underscored identifiers.
Another study with use of eye-tracking equipment, while results indicate no difference in accuracy between the two styles, subjects recognize identifiers in the underscore style more quickly.
Use of CamelCase can conflict with the regular use of uppercase letters for all caps acronyms e.g. to represent a concept like "the TCP IP socket ID" the writer must choose to either retain the capitalisation of the acronyms ("TCPIPSocketID"), which harms readability, or to retain capitalisation of only the first letter ("TcpIpSocketId"), which makes it harder to recognise that a given word is intended as an acronym. An alternative is to follow any instance of acronymic capitalization with a re-initialization of lower case camel, as TCPIPsocketID. This has the effect of enforcing the lower camel case standard.

</doc>
<doc id="6700" url="http://en.wikipedia.org/wiki?curid=6700" title="Cereal">
Cereal

A cereal is a grass, a member of the monocot family Poaceae, cultivated for the edible components of its grain (botanically, a type of fruit called a caryopsis), composed of the endosperm, germ, and bran. Cereal grains are grown in greater quantities and provide more food energy worldwide than any other type of crop; they are therefore staple crops.
In their natural form (as in "whole grain"), they are a rich source of vitamins, minerals, carbohydrates, fats, oils, and protein. When refined by the removal of the bran and germ, the remaining endosperm is mostly carbohydrate. In some developing nations, grain in the form of rice, wheat, millet, or maize constitutes a majority of daily sustenance. In developed nations, cereal consumption is moderate and varied but still substantial.
The word "cereal" derives from "Ceres", the name of the Roman goddess of harvest and agriculture.
History.
The first cereal grains were domesticated about 8,000 years ago by ancient farming communities in the Fertile Crescent region. Emmer wheat, einkorn wheat, and barley were three of the so-called Neolithic founder crops in the development of agriculture. Around the same time, millets and rices were starting to become domesticated in east Asia. Sorghum and millets were also being domesticated in sub-Saharan West Africa.
Production.
The following table shows the annual production of cereals in 1961, 2010, 2011, 2012, and 2013 ranked by 2013 production. All but buckwheat and quinoa are true grasses (these two are pseudocereals).
Maize, wheat, and rice together accounted for 89% of all cereal production worldwide in 2012, and 43% of all food calories in 2009, while the production of oats and triticale have drastically fallen from their 1960s levels.
Other grains that are important in some places, but that have little production globally (and are not included in FAO statistics), include:
Several other species of wheat have also been domesticated, some very early in the history of agriculture:
In 2013 global cereal production reached a record 2,521 million tonnes. A slight dip to 2,498 million tonnes was forecast for 2014 by the FAO in July 2014.
Farming.
While each individual species has its own peculiarities, the cultivation of all cereal crops is similar. Most are annual plants; consequently one planting yields one harvest. Wheat, rye, triticale, oats, barley, and spelt are the "cool-season" cereals. These are hardy plants that grow well in moderate weather and cease to grow in hot weather (approximately 30 °C, but this varies by species and variety). The "warm-season" cereals are tender and prefer hot weather. Barley and rye are the hardiest cereals, able to overwinter in the subarctic and Siberia. Many cool-season cereals are grown in the tropics. However, some are only grown in cooler highlands, where it may be possible to grow multiple crops in a year.
For a few decades, there has also been increasing interest in perennial grain plants. This interest developed due to advantages in erosion control, reduced need of fertiliser, and potential lowered costs to the farmer. Though research is still in early stages, The Land Institute in Salina, Kansas has been able to create a few cultivars that produce a fairly good crop yield.
Planting.
The warm-season cereals are grown in tropical lowlands year-round and in temperate climates during the frost-free season. Rice is commonly grown in flooded fields, though some strains are grown on dry land. Other warm climate cereals, such as sorghum, are adapted to arid conditions.
Cool-season cereals are well-adapted to temperate climates. Most varieties of a particular species are either winter or spring types. Winter varieties are sown in the autumn, germinate and grow vegetatively, then become dormant during winter. They resume growing in the springtime and mature in late spring or early summer. This cultivation system makes optimal use of water and frees the land for another crop early in the growing season.
Winter varieties do not flower until springtime because they require vernalization: exposure to low temperatures for a genetically determined length of time. Where winters are too warm for vernalization or exceed the hardiness of the crop (which varies by species and variety), farmers grow spring varieties. Spring cereals are planted in early springtime and mature later that same summer, without vernalization. Spring cereals typically require more irrigation and yield less than winter cereals.
Period.
Once the cereal plants have grown their seeds, they have completed their life cycle. The plants die and become brown and dry. As soon as the parent plants and their seed kernels are reasonably dry, harvest can begin.
In developed countries, cereal crops are universally machine-harvested, typically using a combine harvester, which cuts, threshes, and winnows the grain during a single pass across the field. In developing countries, a variety of harvesting methods are in use, depending on the cost of labor, from combines to hand tools such as the scythe or cradle.
If a crop is harvested during wet weather, the grain may not dry adequately in the field to prevent spoilage during its storage. In this case, the grain is sent to a dehydrating facility, where artificial heat dries it.
In North America, farmers commonly deliver their newly harvested grain to a grain elevator, a large storage facility that consolidates the crops of many farmers. The farmer may sell the grain at the time of delivery or maintain ownership of a share of grain in the pool for later sale. Storage facilities should be protected from small grain pests, rodents and birds.
Nutritional facts.
Some grains are deficient in the essential amino acid lysine. That is why many vegetarian cultures, in order to get a balanced diet, combine their diet of grains with legumes. Many legumes, on the other hand, are deficient in the essential amino acid methionine, which grains contain. Thus, a combination of legumes with grains forms a well-balanced diet for vegetarians. Common examples of such combinations are dal (lentils) with rice by South Indians and Bengalis, dal with wheat in Pakistan and North India, and beans with corn tortillas, tofu with rice, and peanut butter with wheat bread (as sandwiches) in several other cultures, including Americans. The amount of crude protein found in grain is measured as the grain crude protein concentration.
Standardization.
The ISO has published a series of standards regarding cereal products which are covered by ICS 67.060.

</doc>
<doc id="6704" url="http://en.wikipedia.org/wiki?curid=6704" title="Christendom">
Christendom

Christendom has several meanings. In a cultural sense, it refers to the religion itself, or to the worldwide community of Christians, adherents of Christianity. In its historical sense, the term usually refers to the medieval and early modern period, during which the Christian world represented a geopolitical power juxtaposed with both paganism and especially the military threat of the Muslim world. In the more limited and traditional sense of the word, it refers to the sum total of nations in which the Catholic Church is the established religion of the state, or which have ecclesiastical concordats with the Holy See.
In a contemporary sense, it may simply refer collectively to Christian majority countries or countries in which Christianity dominates or nations in which Christianity is the established religion.
Terminology and usage.
The term "cristendom" existed in Old English, but it had the sense now taken by "Christianity" (as is still the case with the cognate Dutch "christendom" , where it denotes mostly the religion itself, just like the German "Christentum"). 
The current sense of the word of "lands where Christianity is the dominant religion" emerges in Late Middle English (by c. 1400). This semantic development happened independently in the languages of late medieval Europe, which leads to the confusing semantics of English "Christendom" equalling German "Christenheit", French "chrétienté" vs. English "Christianity" equalling German "Christentum", French "christianisme". The reason is the increasing fragmentation of Western Christianity at that time both in theological and in political respect. 
"Christendom" as a geopolitical term is thus meaningful in the context of the Middle Ages, and arguably during the European wars of religion and the Ottoman wars in Europe.
The "Christian world" is also known collectively as the Corpus Christianum, a translated as "the Christian body", meaning the community of all Christians. The Christian polity, embodying a less secular meaning, can be compatible with the idea of both a religious and a temporal body: "Corpus Christianum". The "Corpus Christianum" can be seen as a Christian equivalent of the Muslim "Ummah". 
The word "Christendom" is also used with its other meaning to frame-true Christianity. A more secular meaning can denote that the term "Christendom" refers to Christians considered as a group, the "Political Christian World", as an informal cultural hegemony that Christianity has traditionally enjoyed in the West. 
In its most broad term, it refers to the world's Christian majority countries, which, share little in common aside from the predominance of the faith. Unlike the Muslim world, which has a geo-political and cultural definition that provides a primary identifier for a large swath of the world, Christendom is more complex.
It may be a cultural notion, but has very little weight in international discourse; very few political observers really discuss Christendom, while the Muslim World tends to comprise a civilization in itself.
For example, the Americas and Europe are considered part of Christendom, but this region is further subdivided into the West (representing the North Atlantic) and Latin America. It is also less geographically cohesive than the Muslim world, which stretches almost continuously from North Africa to South Asia.
There is a common and nonliteral sense of the word that is much like the terms "Western world", "known world" or "Free World". When Thomas F. Connolly said, "There isn't enough power in all Christendom to make that airplane what we want!", he was simply using a figure of speech, although it is true that during the Cold War, just as the totalitarianism of the Communist Bloc presented a contrast with the liberty of the Free World, the state atheism of the Communist Bloc contrasted with religious freedom and powerful religious institutions in North America and Western Europe.
History.
Early Christendom.
In the beginning of Christendom, early Christianity was a religion spread in the Greek/Roman world and beyond as a 1st-century Jewish sect, which historians refer to as Jewish Christianity. It may be divided into two distinct phases: the apostolic period, when the first apostles were alive and organizing the Church, and the post-apostolic period, when an early episcopal structure developed, whereby bishoprics were governed by bishops (overseers).
The post-apostolic period concerns the time roughly after the death of the apostles when bishops emerged as overseers of urban Christian populations. The earliest recorded use of the terms "Christianity" (Greek Χριστιανισμός) and "Catholic" (Greek καθολικός), dates to this period, the 2nd century, attributed to Ignatius of Antioch "c." 107. Early Christendom would close at the end of imperial persecution of Christians after the ascension of Constantine the Great and the Edict of Milan in AD 313 and the First Council of Nicaea in 325.
Late Antiquity and Early Middle Ages.
"Christendom" has referred to the medieval and renaissance notion of the "Christian world" as a sociopolitical polity. In essence, the earliest vision of Christendom was a vision of a Christian theocracy, a government founded upon and upholding Christian values, whose institutions are spread through and over with Christian doctrine. In this period, members of the Christian clergy wield political authority. The specific relationship between the political leaders and the clergy varied but, in theory, the national and political divisions were at times subsumed under the leadership of the church as an institution. This model of church-state relations was accepted by various Church leaders and political leaders in European history.
The Church gradually became a defining institution of the Empire. Emperor Constantine issued the Edict of Milan in 313 proclaiming toleration for the Christian religion, and convoked the First Council of Nicaea in 325 whose Nicene Creed included belief in "one holy catholic and apostolic Church". Emperor Theodosius I made Nicene Christianity the state church of the Roman Empire with the Edict of Thessalonica of 380.
As the Western Roman Empire disintegrated into feudal kingdoms and principalities, the concept of Christendom changed as the western church became one of five patriarchal of the Pentarchy and the Christians of the Eastern Roman Empire developed. The Byzantine Empire was the last bastion of Christendom. Christendom would take a turn with the rise of the Franks, a Germanic tribe who converted to the Christian faith and entered into communion with Rome.
On Christmas Day 800 AD, Pope Leo III made crowned Charlemagne resulting in the creation of another Christian king beside the christian emperor in the Byzantine state. The Carolingian Empire created a definition of "Christendom" in juxtaposition with the Byzantine Empire, that of a distributed versus centralized culture respectively.
The classical heritage flourished throughout the Middle Ages in both the Byzantine Greek East and the Latin West. In the Greek philosopher Plato's ideal state there are three major classes, which was representative of the idea of the “tripartite soul”, which is expressive of three functions or capacities of the human soul: “reason”, “the spirited element”, and “appetites” (or “passions”). Will Durant made a convincing case that certain prominent features of Plato's ideal community where discernible in the organization, dogma and effectiveness of "the" Medieval Church in Europe:
"... For a thousand years Europe was ruled by an order of guardians considerably like that which was visioned by our philosopher. During the Middle Ages it was customary to classify the population of Christendom into laboratores (workers), bellatores (soldiers), and oratores (clergy). The last group, though small in number, monopolized the instruments and opportunities of culture, and ruled with almost unlimited sway half of the most powerful continent on the globe. The clergy, like Plato's guardians, were placed in authority... by their talent as shown in ecclesiastical studies and administration, by their disposition to a life of meditation and simplicity, and ... by the influence of their relatives with the powers of state and church. In the latter half of the period in which they ruled [800 AD onwards], the clergy were as free from family cares as even Plato could desire [for such guardians]... [Clerical] Celibacy was part of the psychological structure of the power of the clergy; for on the one hand they were unimpeded by the narrowing egoism of the family, and on the other their apparent superiority to the call of the flesh added to the awe in which lay sinners held them..." "In the latter half of the period in which they ruled, the clergy were as free from family cares as even Plato could desire".
Later Middle Ages and Renaissance.
After the collapse of Charlemagne's empire, the southern remnants of the Holy Roman Empire became a collection of states loosely connected to the Holy See of Rome. Tensions between Pope Innocent III and secular rulers ran high, as the pontiff exerted control over their temporal counterparts in the west and vice versa. The pontificate of Innocent III is considered the height of temporal power of the papacy. The "Corpus Christianum" described the then current notion of the community of all Christians united under the Roman Catholic Church. The community was to be guided by Christian values in its politics, economics and social life. Its legal basis was the "corpus iuris canonica" (body of canon law).
In the East, Christendom became more defined as the Byzantine Empire's gradual loss of territory to an expanding Islam and the muslim conquest of Persia. This caused Christianity to become important to the Byzantine identity. Before the East–West Schism which divided the Church religiously, there had been the notion of a "universal Christendom" that included the East and the West. After the East–West Schism, hopes of regaining religious unity with the West were ended by the Fourth Crusade, when Crusaders conquered the Byzantine capital of Constantinople and hastened the decline of the Byzantine Empire on the path to its destruction. With the breakup of the Byzantine Empire into individual nations with nationalist Orthodox Churches, the term Christendom described Western Europe, Catholicism, Orthodox Byzantines, and other Eastern rites of the Church.
The Catholic Church's peak of authority over all European Christians and their common endeavours of the Christian community — for example, the Crusades, the fight against the Moors in the Iberian Peninsula and against the Ottomans in the Balkans — helped to develop a sense of communal identity against the obstacle of Europe's deep political divisions. But this authority was also sometimes abused, and fostered the Inquisition and anti-Jewish pogroms, to root out divergent elements and create a religiously uniform community. Ultimately, the Inquisition was done away with by order of the Pope Innocent III.
Christendom ultimately was led into specific crisis in the late Middle Ages, when the kings of France managed to establish a French national church during the 14th century and the papacy became ever more aligned with the Holy Roman Empire of the German Nation. Known as the Western Schism, western Christendom was a split between three men, who were driven by politics rather than any real theological disagreement for simultaneously claiming to be the true pope. The Avignon Papacy developed a reputation of corruption that estranged major parts of Western Christendom. The Avignon schism was ended by the Council of Constance.
Before the modern period, Christendom was in a general crisis at the time of the Renaissance Popes because of the moral laxity of these pontiffs and their willingness to seek and rely on temporal power as secular rulers did. Many in the Catholic Church's hierarchy in the Renaissance became increasingly entangled with insatiable greed for material wealth and temporal power, which led to many reform movements, some merely wanting a moral reformation of the Church's clergy, while others repudiated the Church and separated from it in order to form new sects. The Italian Renaissance produced ideas or institutions by which men living in society could be held together in harmony. In the early 16th century, Baldassare Castiglione (The Book of the Courtier) laid out his vision of the ideal gentleman and lady, while Machiavelli cast a jaundiced eye on "la verita effetuale delle cose" — the actual truth of things — in The Prince, composed, humanist style, chiefly of parallel ancient and modern examples of Virtù. Some Protestant movements grew up along lines of mysticism or renaissance humanism (cf. Erasmus). The Catholic Church fell partly into general neglect under the Renaissance Popes, whose inability to govern the Church by showing personal example of high moral standards set the climate for what would ultimately become the Protestant Reformation. During the Renaissance the papacy was mainly run by the wealthy families and also had strong secular interests. To safeguard Rome and the connected Papal States the popes became necessarily involved in temporal matters, even leading armies, as the great patron of arts Pope Julius II did. It during these intermediate times popes strove to make Rome the capital of Christendom while projecting it, through art, architecture, and literature, as the center of a Golden Age of unity, order, and peace.
Professor Frederick J. McGinness described Rome as essential in understanding the legacy the Church and its representatives encapsulated best by The Eternal City: "No other city in Europe matches Rome in its traditions, history, legacies, and influence in the Western world. Rome in the Renaissance under the papacy not only acted as guardian and transmitter of these elements stemming from the Roman Empire but also assumed the role as artificer and interpreter of its myths and meanings for the peoples of Europe from the Middle Ages to modern times... Under the patronage of the popes, whose wealth and income were exceeded only by their ambitions, the city became a cultural center for master architects, sculptors, musicians, painters, and artisans of every kind...In its myth and message, Rome had become the sacred city of the popes, the prime symbol of a triumphant Catholicism, the center of orthodox Christianity, a new Jerusalem."
It is clearly noticeable that the popes of the Italian Renaissance have been subjected by many writers with an overly harsh tone. Pope Julius II for example was not only an effective secular leader in military affairs, a deviously effective politician but foremost one of the greatest patron of the Renaissance period and person who also encouraged open criticism from noted humanists.
The blossoming of renaissance humanism was made very much possible due to the universality of the institutions of Catholic Church and represented by personalities such as Pope Pius II, Nicolaus Copernicus, Leon Battista Alberti, Desiderius Erasmus, sir Thomas More, Bartolomé de Las Casas, Leonardo da Vinci and Teresa of Ávila. George Santayana in his work "The Life of Reason" postulated the tenets of the all encompassing order the Church had brought and as the repository of the legacy of classical antiquity:
"The enterprise of individuals or of small aristocratic bodies has meantime sown the world which we call civilised with some seeds and nuclei of order. There are scattered about a variety of churches, industries, academies, and governments. But the universal order once dreamt of and nominally almost established, the empire of universal peace, all-permeating rational art, and philosophical worship, is mentioned no more. An unformulated conception, the prerational ethics of private privilege and national unity, fills the background of men's minds. It represents feudal traditions rather than the tendency really involved in contemporary industry, science, or philanthropy. Those dark ages, from which our political practice is derived, had a political theory which we should do well to study; for their theory about a universal empire and a catholic church was in turn the echo of a former age of reason, when a few men conscious of ruling the world had for a moment sought to survey it as a whole and to rule it justly."
Reformation and Early Modern era.
Developments in western philosophy and European events brought change to the notion of the "Corpus Christianum". The Hundred Years' War accelerated the process of transforming France from a feudal monarchy to a centralized state. The rise of strong, centralized monarchies denoted the European transition from feudalism to capitalism. By the end of the Hundred Years' War, both France and England were able to raise enough money through taxation to create independent standing armies. In the Wars of the Roses, Henry Tudor took the crown of England. His heir, the absolute king Henry VIII establishing the English church.
In modern history, the Reformation and rise of modernity in the early 16th century entailed a change in the "Corpus Christianum". In the Holy Roman Empire, the Peace of Augsburg of 1555 officially ended the idea among secular leaders that all Christians must be united under one church. The principle of "cuius regio, eius religio" ("whose the region is, his religion") established the religious, political and geographic divisions of Christianity, and this was established with the Treaty of Westphalia in 1648, which legally ended the concept of a single Christian hegemony in the territories of the Holy Roman Empire, despite the Catholic Church's doctrine that it alone is the one true Church founded by Christ.
Subsequently, each government determined the religion of their own state. Christians living in states where their denomination was "not" the established one were guaranteed the right to practice their faith in public during allotted hours and in private at their will.
The European wars of religion are usually taken to have ended with the Treaty of Westphalia (1648), or arguably, including the Nine Years' War and the War of the Spanish Succession in this period, with the Treaty of Utrecht of 1713.
In the 18th century, the focus shifts away from religious conflicts, either between Christian factions or against the external threat of Islam. 
The European Miracle, the Age of Enlightenment and the formation of the great Colonial empire together with the beginning decline of the Ottoman Empire mark the end of the geopolitical "history of Christendom". Instead the focus of western history shifts to the development of the nation-state, accompanied by increasing atheism and secularism, culminating with the French Revolution and the Napoleonic Wars at the turn of the 19th century.
Classical culture.
Art and literature.
Writings and poetry.
Christian literature is writing that deals with Christian themes and incorporates the Christian world view. This constitutes a huge body of extremely varied writing. Christian poetry is any poetry that contains Christian teachings, themes, or references. The influence of Christianity on poetry has been great in any area that Christianity has taken hold. Christian poems often directly reference the Bible, while others provide allegory.
Supplemental arts.
Christian art is art produced in an attempt to illustrate, supplement and portray in tangible form the principles of Christianity. Virtually all Christian groupings use or have used art to some extent. The prominence of art and the media, style, and representations change; however, the unifying theme is ultimately the representation of the life and times of Jesus and in some cases the Old Testament. Depictions of saints are also common, especially in Anglicanism, Roman Catholicism, and Eastern Orthodoxy.
Illumination.
An illuminated manuscript is a manuscript in which the text is supplemented by the addition of decoration. The earliest surviving substantive illuminated manuscripts are from the period AD 400 to 600, primarily produced in Ireland, Constantinople and Italy. The majority of surviving manuscripts are from the Middle Ages, although many illuminated manuscripts survive from the 15th century Renaissance, along with a very limited number from Late Antiquity.
Most illuminated manuscripts were created as codices, which had superseded scrolls; some isolated single sheets survive. A very few illuminated manuscript fragments survive on papyrus. Most medieval manuscripts, illuminated or not, were written on parchment (most commonly of calf, sheep, or goat skin), but most manuscripts important enough to illuminate were written on the best quality of parchment, called vellum, traditionally made of unsplit calfskin, though high quality parchment from other skins was also called "parchment".
Iconography.
Christian art began, about two centuries after Christ, by borrowing motifs from Roman Imperial imagery, classical Greek and Roman religion and popular art. Religious images are used to some extent by the Abrahamic Christian faith, and often contain highly complex iconography, which reflects centuries of accumulated tradition. In the Late Antique period iconography began to be standardised, and to relate more closely to Biblical texts, although many gaps in the canonical Gospel narratives were plugged with matter from the apocryphal gospels. Eventually the Church would succeed in weeding most of these out, but some remain, like the ox and ass in the Nativity of Christ.
An icon is a religious work of art, most commonly a painting, from Orthodox Christianity. Christianity has used symbolism from its very beginnings. In both East and West, numerous iconic types of Christ, Mary and saints and other subjects were developed; the number of named types of icons of Mary, with or without the infant Christ, was especially large in the East, whereas Christ Pantocrator was much the commonest image of Christ.
Christian symbolism invests objects or actions with an inner meaning expressing Christian ideas. Christianity has borrowed from the common stock of significant symbols known to most periods and to all regions of the world. Religious symbolism is effective when it appeals to both the intellect and the emotions. Especially important depictions of Mary include the Hodegetria and Panagia types. Traditional models evolved for narrative paintings, including large cycles covering the events of the Life of Christ, the Life of the Virgin, parts of the Old Testament, and, increasingly, the lives of popular saints. Especially in the West, a system of attributes developed for identifying individual figures of saints by a standard appearance and symbolic objects held by them; in the East they were more likely to identified by text labels.
Each saint has a story and a reason why he or she led an exemplary life. Symbols have been used to tell these stories throughout the history of the Church. A number of Christian saints are traditionally represented by a symbol or iconic motif associated with their life, termed an attribute or emblem, in order to identify them. The study of these forms part of iconography in Art history. They were particularly
Architecture.
Christian architecture encompasses a wide range of both secular and religious styles from the foundation of Christianity to the present day, influencing the design and construction of buildings and structures in Christian culture.
Buildings were at first adapted from those originally intended for other purposes but, with the rise of distinctively ecclesiastical architecture, church buildings came to influence secular ones which have often imitated religious architecture. In the 20th century, the use of new materials, such as concrete, as well as simpler styles has had its effect upon the design of churches and arguably the flow of influence has been reversed. From the birth of Christianity to the present, the most significant period of transformation for Christian architecture in the west was the Gothic cathedral. In the east, Byzantine architecture was a continuation of Roman architecture.
Philosophy.
Christian philosophy is a term to describe the fusion of various fields of philosophy with the theological doctrines of Christianity. Scholasticism, which means "that [which] belongs to the school", and was a method of learning taught by the academics (or "school people") of medieval universities c. 1100–1500. Scholasticism originally started to reconcile the philosophy of the ancient classical philosophers with medieval Christian theology. Scholasticism is not a philosophy or theology in itself but a tool and method for learning which places emphasis on dialectical reasoning.
Christian civilization.
Medieval conditions.
The Byzantine Empire, which was the most sophisticated culture during antiquity, suffered under Muslim conquests limiting its scientific prowess during the Medieval period. Christian Western Europe had suffered a catastrophic loss of knowledge following the fall of the Western Roman Empire. But thanks to the Church scholars such as Aquinas and Buridan, the West carried on at least the spirit of scientific inquiry which would later lead to Europe's taking the lead in science during the Scientific Revolution using translations of medieval works.
Medieval technology refers to the technology used in medieval Europe under Christian rule. After the Renaissance of the 12th century, medieval Europe saw a radical change in the rate of new inventions, innovations in the ways of managing traditional means of production, and economic growth. The period saw major technological advances, including the adoption of gunpowder and the astrolabe, the invention of spectacles, and greatly improved water mills, building techniques, agriculture in general, clocks, and ships. The latter advances made possible the dawn of the Age of Exploration. The development of water mills was impressive, and extended from agriculture to sawmills both for timber and stone, probably derived from Roman technology. By the time of the Domesday Book, most large villages in Britain had mills. They also were widely used in mining, as described by Georg Agricola in De Re Metallica for raising ore from shafts, crushing ore, and even powering bellows.
Significant in this respect were advances within the fields of navigation. The compass and astrolabe along with advances in shipbuilding, enabled the navigation of the World Oceans and thus domination of the worlds economic trade. Gutenberg’s printing press made possible a dissemination of knowledge to a wider population, that would not only lead to a gradually more egalitarian society, but one more able to dominate other cultures, drawing from a vast reserve of knowledge and experience.
Renaissance innovations.
During the Renaissance, great advances occurred in geography, astronomy, chemistry, physics, math, manufacturing, and engineering. The rediscovery of ancient scientific texts was accelerated after the Fall of Constantinople, and the invention of printing which would democratize learning and allow a faster propagation of new ideas. "Renaissance technology" is the set of artifacts and customs, spanning roughly the 14th through the 16th century. The era is marked by such profound technical advancements like the printing press, linear perspectivity, patent law, double shell domes or Bastion fortresses. Draw-books of the Renaissance artist-engineers such as Taccola and Leonardo da Vinci give a deep insight into the mechanical technology then known and applied.
Renaissance science spawned the Scientific Revolution; science and technology began a cycle of mutual advancement. The "Scientific Renaissance" was the early phase of the Scientific Revolution. In the two-phase model of early modern science: a "Scientific Renaissance" of the 15th and 16th centuries, focused on the restoration of the natural knowledge of the ancients; and a "Scientific Revolution" of the 17th century, when scientists shifted from recovery to innovation.
Demographics.
Geographic spread.
Christianity is the predominant religion in Europe, Russia, the Americas, Oceania, the Philippines, Eastern Indonesia, Southern Africa, Central Africa and East Africa. There are also large Christian communities in other parts of the world, such as China, India and Central Asia, where Christianity is the second-largest religion after Islam. The United States is the largest Christian country in the world by population, followed by Brazil and Mexico.
Many Christians not only live under, but also have an official status in, a state religion of the following nations: Armenia (Armenian Apostolic Church), Costa Rica (Roman Catholic Church), Denmark (Church of Denmark), El Salvador (Roman Catholic Church), England (Church of England), Greece (Church of Greece), Iceland (Church of Iceland), Liechtenstein (Roman Catholic Church), Malta (Roman Catholic Church), Monaco (Roman Catholic Church), Romania (Romanian Orthodox Church), Norway (Church of Norway), Vatican City (Roman Catholic Church), Switzerland (Roman Catholic Church, Swiss Reformed Church and Christian Catholic Church of Switzerland).
Number of adherents.
The estimated number of Christians in the world ranges from 1.5 billion to 2.2 billion people. Composed of around 34,000 different denominations, Christianity is the world's largest religion. Christians have composed about 33 percent of the world's population for around 100 years.
Notable Christian organizations.
A religious order is a lineage of communities and organizations of people who live in some way set apart from society in accordance with their specific religious devotion, usually characterized by the principles of its founder's religious practice. In contrast, the term Holy Orders is used by many Christian churches to refer to ordination or to a group of individuals who are set apart for a special role or ministry. Historically, the word "order" designated an established civil body or corporation with a hierarchy, and ordinatio meant legal incorporation into an ordo. The word "holy" refers to the Church. In context, therefore, a holy order is set apart for ministry in the Church. Religious orders are composed of initiates (laity) and, in some traditions, ordained clergies.
Various organizations include:
Christianity law and ethics.
Church and state framing.
Within the framework of Christianity, there are at least three possible definitions for Church law. One is the Torah/Mosaic Law (from what Christians consider to be the Old Testament) also called Divine Law or Biblical law. Another is the instructions of Jesus of Nazareth in the Gospel (sometimes referred to as the Law of Christ or the New Commandment or the New Covenant). A third is canon law which is the internal ecclesiastical law governing the Roman Catholic Church, the Eastern Orthodox churches, and the Anglican Communion of churches. The way that such church law is legislated, interpreted and at times adjudicated varies widely among these three bodies of churches. In all three traditions, a canon was initially a rule adopted by a council (From Greek "kanon" / κανών, Hebrew kaneh / קנה, for rule, standard, or measure); these canons formed the foundation of canon law.
Christian ethics in general has tended to stress the need for grace, mercy, and forgiveness because of human weakness and developed while Early Christians were subjects of the Roman Empire. From the time Nero blamed Christians for setting Rome ablaze (64 AD) until Galarius (311 AD), persecutions against Christians erupted periodically. Consequently, Early Christian ethics included discussions of how believers should relate to Roman authority and to the empire.
Under the Emperor Constantine I (312-337), Christianity became a legal religion. While some scholars debate whether Constantine's conversion to Christianity was authentic or simply matter of political expediency, Constantine's decree made the empire safe for Christian practice and belief. Consequently, issues of Christian doctrine, ethics and church practice were debated openly, see for example the First Council of Nicaea and the First seven Ecumenical Councils. By the time of Theodosius I (379-395), Christianity had become the state religion of the empire. With Christianity in power, ethical concerns broaden and included discussions of the proper role of the state.
Render unto Caesar… is the beginning of a phrase attributed to Jesus in the synoptic gospels which reads in full, "Render unto Caesar the things which are Caesar’s, and unto God the things that are God’s". This phrase has become a widely quoted summary of the relationship between Christianity and secular authority. The gospels say that when Jesus gave his response, his interrogators "marvelled, and left him, and went their way." Time has not resolved an ambiguity in this phrase, and people continue to interpret this passage to support various positions that are poles apart. The traditional division, carefully determined, in Christian thought is the state and church have separate spheres of influence.
Thomas Aquinas thoroughly discussed that "human law" is positive law which means that it is natural law applied by governments to societies. All human laws were to be judged by their conformity to the natural law. An unjust law was in a sense no law at all. At this point, the natural law was not only used to pass judgment on the moral worth of various laws, but also to determine what the law said in the first place. This could result in some tension. Late ecclesiastical writers followed in his footsteps.
Democratic ideology.
Christian democracy is a political ideology that seeks to apply Christian principles to public policy. It emerged in 19th-century Europe, largely under the influence of Catholic social teaching. In a number of countries, the democracy's Christian ethos has been diluted by secularisation. In practice, Christian democracy is often considered conservative on cultural, social and moral issues and progressive on fiscal and economic issues. In places, where their opponents have traditionally been secularist socialists and social democrats, Christian democratic parties are moderately conservative, whereas in other cultural and political environments they can lean to the left.
Women's roles.
Attitudes and beliefs about the roles and responsibilities of women in Christianity vary considerably today as they have throughout the last two millennia — evolving along with or counter to the societies in which Christians have lived. The Bible and Christianity historically have been interpreted as excluding women from church leadership and placing them in submissive roles in marriage. Male leadership has been assumed in the church and within marriage, society and government.
Some contemporary writers describe the role of women in the life of the church as having been downplayed, overlooked, or denied throughout much of Christian history. Paradigm shifts in gender roles in society and also many churches has inspired reevaluation by many Christians of some long-held attitudes to the contrary. Christian egalitarians have increasingly argued for equal roles for men and women in marriage, as well as for the ordination of women to the clergy. Contemporary conservatives meanwhile have reasserted what has been termed a "complementarian" position, promoting the traditional belief that the Bible ordains different roles and responsibilities for women and men in the Church and family.
Major Christian denominations.
A Christian denomination is an identifiable religious body under a common name, structure, and doctrine within Christianity. Worldwide, Christians are divided, often along ethnic and linguistic lines, into separate churches and traditions. Technically, divisions between one group and another are defined by church doctrine and church authority. Centering on language of "professed Christianity" and "true Christianity", issues that separate one group of followers of Jesus from another include:
Christianity is composed of, but not limited to, five major branches of Churches: Catholicism, Eastern Orthodoxy, Oriental Orthodoxy, Anglicanism, and Protestantism. Some listings include Anglicans among Protestants while others list the Eastern Orthodox and Oriental Orthodox together as one group, thus the number of distinct major branches can vary between three and five depending on the listing. The Assyrian Church of the East (Nestorians) and the Old Catholic churches are also distinct Christian bodies of historic importance, but much smaller in adherents and geographic scope. Each of the branches has important subdivisions. Because the Protestant subdivisions do not maintain a common theology or earthly leadership, they are far more distinct than the subdivisions of the other four groupings. "Denomination" typically refers to one of the many Christian groupings including each of the multitude of Protestant subdivisions.
Sizes of denomination.
Catholicism is the largest denomination, comprising just over half of Christians worldwide.
In Christendom, the largest denominations are:
Christendom and other beliefs.
In the interaction between Christendom and other belief systems, men and women when not at war with their neighbors have always made an effort to understand the Other (not least because understanding is a strategy for defense, but also because for as long as there is dialogue wars are delayed). Such interactions have led to various interfaith dialogue events. History records many examples of interfaith initiatives and dialogue throughout the ages. In the field of comparative religion, the interactions connects fundamental ideas in Christianity with similar ones in other religions. Christianity and other religions appear to share some elements. Regarding Christianity's relationship with other world beliefs, Christianity and other beliefs have differences and similarities in connection with each other.
Judaism.
Although Christianity and Judaism share historical roots, these two religions diverge in fundamental ways. Though Judeo-Christian tradition emphasizes continuities and convergences between the two religions, there are many other areas in which the faiths diverge.
Islam.
Christianity and Islam share their origins in the Abrahamic tradition, as well as Judaism. Islam accepts Jesus and his miracles and other aspects of Christianity as part of its faith - with some differences in interpretation, and rejects other aspects.
Buddhism.
There has been much speculation regarding a possible connection between both the Buddha and the Christ, and between Buddhism and Christianity. Buddhism originated in India about 500 years before the Apostolic Age and the origins of Christianity.
Hinduism.
The declaration "Nostra aetate" officially established inter-religious dialogue between Catholics and Hindus. It has promoted common values between religions. There are over 17.3 million Catholics in India, which represents less than 2% of the total population and is the largest Christian Church within India.
Secularism.
Irreligion is an absence, indifference or hostility to religion. Secularism, in one sense, may assert the right to be free to choose religious beliefs and non-beliefs. In its most prominent form, secularism is critical of religious orthodoxy and asserts that reason and the scientific method are better ways to understand reality than religious beliefs. Humanism refers to a philosophy centered on humankind. Much of Humanism's life stance upholds human reason, ethics, and justice, and rejects supernaturalism (Christian mythology).

</doc>
<doc id="6710" url="http://en.wikipedia.org/wiki?curid=6710" title="Coyote">
Coyote

The coyote ( or , , or ; "Canis latrans") is a canid native to North America. It is a smaller, more basal animal than its close relative, the gray wolf, being roughly the North American equivalent to the Old World golden jackal, though it is larger and more predatory in nature. It is listed as "least concern" by the IUCN, on account of its wide distribution and abundance throughout North America, even southwards through Mexico and Central America. It is a highly versatile species, whose range has expanded amidst human environmental modification. This expansion is ongoing, and it may one day reach South America, as shown by the animal's presence beyond the Panama Canal in 2013. s of 2005[ [update]], 19 subspecies are recognized.
The ancestors of the coyote diverged from those of the gray wolf, 1–2 million years ago, with the modern species arising in North America during the Middle Pleistocene. It is highly flexible in social organization, living either in nuclear families or in loosely-knit packs of unrelated individuals. It has a varied diet consisting primarily of animal matter, including ungulates, lagomorphs, rodents, birds, reptiles, amphibians, fish and invertebrates, though it may also eat fruit and vegetable matter on occasion. It is a very vocal animal, whose most iconic sound consists of a howl emitted by solitary individuals. Humans aside, cougars and gray wolves are the coyote's only serious enemies. Nevertheless, coyotes have on occasion mated with the latter species, producing hybrids colloquially called "coywolves".
The coyote is a prominent character in Native American folklore, usually depicted as a trickster who alternately assumes the form of an actual coyote or a man. As with other trickster figures, the coyote acts as a picaresque hero which rebels against social convention through deception and humor. The animal was especially respected in Mesoamerican cosmology as a symbol of military might, with some scholars having traced the origin of the Aztec god Quetzalcoatl to a pre-Aztec coyote deity. After the European colonization of the Americas, it was reviled in Anglo-American culture as a cowardly and untrustworthy animal. Unlike the gray wolf, which has undergone a radical improvement of its public image, cultural attitudes towards the coyote remain largely negative.
Description.
Coyote males average 8 - in weight, while females average 7 -, though size varies geographically. Northern subspecies, which average 18 kg, tend to grow larger than the southern subspecies of Mexico, which average 11.5 kg. Body length ranges on average from 1 to, and tail length 40 cm, with females being shorter in both body length and height. The largest coyote on record was a male killed near Afton, Wyoming on November 19, 1937 which measured 1.6 m from nose to tail, and weighed 33.9 kg. Scent glands are located at the upper side of the base of the tail and are a bluish black color.
The color and texture of the coyote's fur varies somewhat geographically. The hair's predominant color is light gray and red or fulvous, interspersed around the body with black and white. Coyotes living on high elevations tend to have more black and gray shades than their desert-dwelling counterparts, which are more fulvous or whitish-gray. The coyote's fur consists of short, soft underfur and long, coarse guard hairs. The fur of northern subspecies is longer and denser than in southern forms, with the fur of some Mexican and Central American forms being almost hispid. Albinism is extremely rare in coyotes; out of a total of 750,000 coyotes harvested by Federal and cooperative hunters between March 22, 1938 to June 30, 1945, only two were albinos.
The coyote is typically smaller than the gray wolf, but has longer ears and a larger braincase, as well as a thinner frame, face and muzzle. The coyote also carries its tail downwards when running or walking, rather than horizontally as the wolf does. Coyote tracks can be distinguished from those of dogs by their more elongated, less rounded shape. Scent glands are smaller than the gray wolf's, but the same color. Its fur color variation is much less varied than that of a wolf.
History.
By the time of the European colonization of the Americas, coyotes were largely confined to open plains and arid regions of the western half of the continent. It is often difficult in early post-Columbian historical records to distinguish between coyotes and wolves. One record from 1750 in Kaskaskia, Illinois written by a local priest noted that the "wolves" encountered there were smaller and less daring than European wolves. Another account from the early 1800s in Edwards County mentioned wolves howling at night, though these were likely coyotes. The species was encountered several times during the Lewis and Clark Expedition (1804–1806), though it was already well-known to European traders on the upper Missouri. Lewis, writing on May 5, 1805, in northeastern Montana, described the coyote as follows:
the small woolf or burrowing dog of the prairies are the inhabitants almost invariably of the open plains; they usually ascociate in bands of ten or twelve sometimes more and burrow near some pass or place much frequented by game; not being able alone to take deer or goat they are rarely ever found alone but hunt in bands; they frequently watch and seize their prey near their burrows; in these burrows they raise their young and to them they also resort when pursued; when a person approaches them they frequently bark, their note being precisely that of the small dog. they are of an intermediate size between that of the fox and dog, very active fleet and delicately formed; the ears large erect and pointed the head long and pointed more like that of the fox; tale long; . . . the hair and fur also resembles the fox tho' is much coarser and inferior. they are of a pale redish brown colour. the eye of a deep sea green colour small and piercing. their tallons [claws] are reather longer than those of the ordinary wolf or that common to the atlantic states, none of which are to be found in this quarter, nor I believe above the river Plat.
The coyote was first scientifically described by Thomas Say in September 1819 on the site of Lewis and Clark's Council Bluffs, fifteen miles up the Missouri River from the mouth of the Platte during a government-sponsored expedition with Major Stephen Long. He had the first edition of the Lewis and Clark journals in hand, which contained Biddle's edited version of Lewis's observations dated May 5, 1805.
Naming and etymology.
The earliest written reference to the species comes from Francisco Hernández's "Plantas y Animales de la Nueva España" (1651), where it is described as a "Spanish fox" or "jackal". The first published usage of the word "coyote" (the root word of which is the Nahuatl "coyotl") comes from Francisco Javier Clavijero's "Historia de México" in 1780. The first time it was used in English occurred in William Bullock's "Six months' residence and travels in Mexico" (1824), where it is variously transcribed as "cayjotte" and "cocyotie". The word's spelling was standardized as "coyote" by the 1880s. Alternative English names for the coyote include "prairie wolf", "brush wolf", "cased wolf", "little wolf" and "American jackal".
Taxonomy and evolution.
The earliest fossil carnivores that can be linked with some certainty to canids are the Eocene Miacids, which lived some 38 to 56 million years ago. The Miacids later diverged into caniforms and feliforms, with the former line leading to such genera as the coyote-sized "Mesocyon" of the Oligocene (38 to 24 million years ago), the fox-like "Leptocyon" and the wolf-like "Tomarctus" which inhabited North America some 10 million years ago. The coyote represents a more primitive form of "Canis" than the gray wolf, as shown by its relatively small size and its comparatively narrow skull and jaws, which lack the grasping power necessary to hold the large prey wolves specialize in. This is further corroborated by the coyote's sagittal crest, which is low or totally flattened, thus indicating a weaker bite than the wolf's. The coyote is not a specialized carnivore as the wolf is, as shown by the larger chewing surfaces on the molars, reflecting the species' relative dependence on vegetable matter. In these respects, the coyote resembles the fox-like progenitors of the genus more so than the wolf. Modern phylogenetics places the coyote between the gray wolf and golden jackal.
The evolution of the coyote is remarkably well documented, and can be traced back in an unbroken line to the Hemphillian "Eucyon davisi". The coyote likely arose from a certain population of the Blancan species "C. lepophagus" which, although similar in weight to modern coyotes, had shorter limb bones, thus indicating a less cursorial lifestyle. Modern coyotes arose during the Middle Pleistocene, and showed much more variation than they do today. Compared to their modern Holocene counterparts, Pleistocene coyotes ("C. l. orcutti") were larger and more robust, likely in response to larger competitors and prey. Pleistocene coyotes were likely more specialized carnivores than their descendants, as their teeth were more adapted to shearing meat, showing fewer grinding surfaces suited for processing vegetation. Their reduction in size occurred within 1000 years of the Quaternary extinction event, when their large prey died out. Furthermore, Pleistocene coyotes were unable to exploit the big game hunting niche left vacant after the extinction of the dire wolf, as it was rapidly filled by gray wolves, which likely actively killed off the large coyotes, with natural selection favoring the modern gracile morph.
Subspecies.
s of 2005[ [update]], 19 subspecies are recognized. Geographic variation in coyotes is not great, though taken as a whole, the eastern subspecies ("thamnos" and "frustor") are large, dark colored animals, with a gradual paling in color and reduction in size westward and northward ("texensis", "latrans", "lestes" and "incolatus"), a brightening of ochraceous tones towards the Pacific coast ("ochropus", "umpquensis"), a reduction in size in the southwestern United States ("microdon", "mearnsi") and a general trend towards dark reddish colors and short muzzles in Mexican and Central American populations.
Hybridization.
Coyotes have occasionally mated with dogs, sometimes producing crosses capable of work. Such matings are rare in the wild, as the mating cycles of dogs and coyotes do not coincide, and coyotes are usually antagonistic towards dogs, with even captive specimens having shown reluctance to mate with them. Hybridization usually only occurs when coyotes are expanding into areas where conspecifics are few, and dogs are the only alternatives. Even then, pup survival rates are lower than normal, as dogs do not form pair bonds with coyotes, thus making the rearing of pups more difficult. In captivity, F1 hybrids tend to be more mischievous and less manageable as pups than dogs, and are less trustworthy on maturity than wolf-dog hybrids. Hybrids vary in appearance, but generally retain the coyote's adult sable coat color, dark neonatal coat color, bushy tail with an active supracaudal gland, and white facial mask. F1 hybrids tend to be intermediate in form between dogs and coyotes, while F2 hybrids are more varied. Both F1 and F2 hybrids resemble their coyote parents in terms of shyness and intrasexual aggression. Hybrid play behavior includes the coyote "hip-slam". Hybrids of both sexes are fertile, and can be successfully bred through four generations. Melanistic coyotes owe their black pelts to a mutation that first arose in domestic dogs. A population of non-albino white coyotes in Newfoundland owe their coloration to a MC1R mutation inherited from golden retrievers.
Coyotes have hybridized with wolves to varying degrees, particularly in the Eastern United States and Canada. The so-called "eastern coyote" of northeastern North America has been confirmed to be of mixed wolf-coyote parentage, and probably originated in the aftermath of the extermination of wolves in the northeast, thus allowing coyotes to colonize former wolf ranges and mix with remnant wolf populations. This hybrid is smaller than the wolf, and holds smaller territories, but is in turn larger and holds more extensive home ranges than the typical western coyote. As of 2010, the eastern coyote's genetic makeup is fairly uniform, with minimal influence from eastern wolves or western coyotes. Adult eastern coyotes are larger than western coyotes, weighing an average of 30–40 lbs, with female eastern coyotes weighing 21% more than male western coyotes. Eastern coyotes also weigh more at birth; while newborn western coyotes weigh 250–300 grams, eastern coyotes weigh 349–360 grams. By the age of 35 days, eastern coyote pups average 1590 grams, 200 grams more than western coyotes of similar age. By this time, physical differences become more apparent, with eastern coyote pups having longer legs than their western counterparts. Differences in dental development also occur, with tooth eruption being later, and in a different order in the eastern coyote. Aside from its size, the eastern coyote is physically not unlike the western coyote; both have erect ears, a straight and bushy tail, a conspicuous supracaudal gland and a narrow chest. There are four color phases, ranging from dark brown to blond or reddish blond, though the most common phase is gray-brown, with reddish legs, ears and flanks. There are no significant differences between eastern and western coyotes in expressing aggression and fighting, though eastern coyotes tend to fight less, and are more playful. Unlike western coyote pups, in which fighting precedes play behavior, fighting among eastern coyote pups occurs after the onset of play. Eastern coyotes tend to reach sexual maturity when they reach two years of age, much later than in western coyotes. In 2011, an analysis of 48,000 SNP chips in the genomes of various wolf and coyote populations revealed that the eastern wolf (native to Algonquin Provincial Park) and the red wolf (native to North Carolina), both previously labeled as species distinct from the gray wolf, are in fact products of varying degrees of wolf-coyote hybridization. The wolf-coyote admixture resulting in the development of the eastern wolf may have occurred on the order of 600–900 years ago between gray wolves and a now extinct pre-Columbian coyote population. The eastern wolf has since backcrossed extensively with parent gray wolf populations. The red wolf may have originated later, approximately 287–430 years ago, when much of the southeastern U.S. was being converted to agriculture and predators were targeted for extermination. During this period, declining local wolf populations would have been forced to mate with coyotes, with the resulting hybrids backcrossing to coyotes as the wolves disappeared, to the extent that ~75–80% of the modern red wolf's genome is of coyote derivation.
Behavior.
Social and reproductive behaviors.
Like the golden jackal, the coyote is gregarious, but not as dependent on conspecifics as more social canid species like wolves are. This is likely linked to the fact that the coyote is not a specialized hunter of large prey as the latter species is. The basic social unit of a coyote pack is a nuclear family centered on a reproductive female. However, unrelated coyotes may join forces for companionship, or to bring down prey too large to attack singly. Such "non-family" packs are only temporary, and may consist of bachelor males, non-reproductive females and sub-adult young. Families are formed in midwinter, when females enter estrus. Pair bonding can occur 2–3 months before actual copulation takes place. A female entering estrus attracts males by scent marking and howling with increasing frequency. A single female in heat can attract up to seven reproductive males, which can follow her for as much as a month. Although there may be some squabbling among the males, once the female has selected a mate and copulates, the rejected males do not intervene, and move on once they detect other estrous females. Unlike the wolf, which has been known to practice both monogamous and bigamous matings, the coyote is strictly monogamous, even in areas with high coyote densities and abundant food. Females that fail to mate sometimes assist their sisters or mothers in raising their pups, or will join their siblings until the next time they can mate. The newly mated pair then establish a territory and either construct their own den or clean out abandoned badger, marmot or skunk earths. During the pregnancy, the male frequently hunts alone and brings back food for the female. The female may line the den with dried grass or with fur pulled from her belly. The gestation period lasts 63 days, with an average litter size of six, though the number fluctuates depending on coyote population density and the abundance of food.
Coyote pups are born in dens, hollow trees, or under ledges, and weigh 200-50 grams at birth. They are altricial, and are completely dependent on milk for their first 10 days. The incisors erupt at about 12 days, the canines at 16, and the second premolars at 21. Their eyes open after 10 days, by which point the pups become increasingly more mobile, walking by 20 days, and running at the age of six weeks. The parents begin supplementing the pup's diet with regurgitated solid food after 12–15 days. By the age of 4–6 weeks, when their milk teeth are fully functional, the pups are given small food items like mice, rabbits or pieces of ungulate carcasses, with lactation steadily decreasing after two months. Unlike wolf pups, coyote pups begin seriously fighting prior to engaging in play behavior. By 3 weeks of age, coyote pups bite each other with less inhibition than wolf pups. By the age of 4–5 weeks, pups have established dominance hierarchies, and are by then more likely to play rather than fight. The male plays an active role in feeding, grooming and guarding the pups, but will abandon them if the female goes missing before the pups are completely weaned. The den is abandoned by June–July, and the pups follow their parents in patrolling their territory and hunting. Pups may leave their families in August, though can remain for much longer. The pups attain adult dimensions at eight months, and gain adult weight a month later.
Territorial and sheltering behaviors.
Individual feeding territories vary in size from 0.38 to, with the general concentration of coyotes in a given area depending on food abundance, adequate denning sites, and competition with conspecifics and other predators. The coyote generally does not defend its territory outside of the denning season, and is much less aggressive towards intruders than the wolf is, typically chasing and sparring with them, but rarely killing them. Conflicts between coyotes can arise during times of food shortage.
Like wolves, coyotes use a den (usually the deserted holes of other species) when gestating and rearing young, though they may occasionally give birth under sagebrushes in the open. Coyote dens can be located in canyons, washouts, coulees, banks, rock bluffs, or level ground. Some dens have been found under abandoned homestead shacks, grain bins, drainage pipes, railroad tracks, hollow logs, thickets and thistles. The den is continuously dug and cleaned out by the female until the pups are born. Should the den be disturbed or infested with fleas, the pups are moved into another den. A coyote den can have several entrances and passages branching out from the main chamber. A single den can be used year after year.
Hunting and feeding behaviors.
While the popular consensus is that olfaction is very important for hunting, two studies that experimentally investigated the role of olfactory, auditory, and visual cues found that visual cues are the most important ones for hunting in red foxes and coyotes.
When hunting large prey, the coyote often works in pairs or in small groups. Success in killing large ungulates depends on factors such as snow depth and crust density. Younger animals usually avoid participating in such hunts, with the breeding pair typically doing most of the work. Unlike the wolf, which attacks large prey from the rear, the coyote approaches from the front, lacerating its prey's head and throat. Like other canids, the coyote caches excess food. Coyotes catch mouse-like rodents by pouncing, whereas ground squirrels are chased. Although coyotes can live in large groups, small prey is typically caught singly. Coyotes have been observed to kill porcupines in pairs, using their paws to flip the rodents on their backs, then attacking the soft underbelly. Only old and experienced coyotes can successfully prey on porcupines, with many predation attempts by young coyotes resulting in them being injured by their prey's quills. Coyotes sometimes urinate on their food, possibly to claim ownership over it.
Coyotes may occasionally form mutualistic relationships with American badgers, assisting each other in digging up rodent prey. The relationship between the two species may occasionally border on apparent friendship, as some coyotes have been observed laying their heads on their badger companions or licking their faces without protest. The amicable interactions between coyotes and badgers were known to pre-Columbian civilizations, as shown on a Mexican jar dated to 1250–1300 AD depicting the relationship between the two.
Ecology.
Habitat.
Prior to the near extermination of wolves and cougars, the coyote was most numerous in grasslands inhabited by bison, antelope, elk and other deer, doing particularly well in short grass areas with prairie dogs, though it was just as much at home in semiarid areas with sagebrush and jackrabbits or in deserts inhabited by cactus, kangaroo rats and rattlesnakes. As long as it was not in direct competition with the wolf, the coyote ranged from the Sonoran Desert to the alpine regions of adjoining mountains or the plains and mountainous areas of Alberta. With the extermination of the wolf, the coyote's range expanded to encompass broken forests from the tropics of Guatemala and the northern slope of Alaska.
Diet.
The coyote is highly versatile in its choice of food, but is primarily carnivorous, with 90% of its diet consisting of animal matter. Prey species include bison, deer, sheep, rabbits, rodents, birds, amphibians (except toads), lizards, snakes, fish, crustaceans, and insects. Coyotes may be picky over the prey they target, as animals such as shrews, moles and brown rats do not occur in their diet in proportion to their numbers. More unusual prey include fishers, young black bears, harp seals and rattlesnakes. Although coyotes prefer fresh meat, they will scavenge when the opportunity presents itself. It has been estimated that, excluding the insects, fruit and grass eaten, the coyote requires 600 g of food daily, or 250 kg annually. The coyote readily cannibalizes the carcasses of conspecifics, with coyote fat having been successfully used by coyote hunters as a lure or poisoned bait. The coyote's winter diet consists mainly of large ungulate carcasses, with very little vegetable matter. Rodent prey increases in importance during the spring, summer, and fall.
The coyote feeds on a variety of different fruits, including blackberries, blueberries, peaches, pears, apples, prickly pears, chapotes, persimmons, and peanuts. Other vegetable foods include watermelon, cantaloupe and carrots. During the winter and early spring, the coyote eats large quantities of grass, such as green wheat blades. It sometimes eats unusual items like cotton cake, soybean meal, domestic animal droppings, and cultivated grain such as corn, wheat, sorghum, and beans.
Enemies and competitors.
In areas where the ranges of coyotes and gray wolves overlap, interference competition and predation by wolves has been hypothesized to limit local coyote densities. Coyote ranges expanded during the 19th and 20th centuries following the extirpation of wolves, while coyotes were driven to extinction on Isle Royale after wolves colonized the island in the 1940s. One study conducted in Yellowstone National Park, where both species coexist, concluded that the coyote population in the Lamar River Valley declined by 39% following the reintroduction of wolves in the 1990s, while coyote populations in wolf inhabited areas of the Grand Teton National Park are 33% lower than in areas where they are absent. Wolves have been observed to not tolerate coyotes in their vicinity, though coyotes have been known to trail wolves in order to feed on their kills.
Coyotes rarely kill healthy adult red foxes, and have been observed to feed or den alongside them, though they often kill foxes caught in traps. Coyotes may kill fox kits, but this is not a major source of mortality. In southern California, coyotes frequently kill gray foxes, and these smaller canids tend to avoid areas with high coyote densities.
Coyotes may compete with cougars in some areas. In eastern Sierra Nevada, coyotes compete with cougars over mule deer. Cougars usually outcompete coyotes, and may kill them occasionally, thus reducing coyote predation pressure on smaller carnivores like foxes and bobcats.
In some areas, coyotes share their ranges with bobcats. It is rare for these two similarly sized species to physically confront one another, though bobcat populations tend to diminish in areas with high coyote densities. However, several studies have demonstrated interference competition between coyotes and bobcats, and in all cases coyotes dominated the interaction. Multiple researchers all reported instances of coyotes killing bobcats, whereas bobcats killing coyotes is more rare. Coyotes attack bobcats using a bite-and-shake method similar to that used on medium-sized prey. Coyotes (both single individuals and groups) have been known to occasionally kill bobcats – in most cases, the bobcats were relatively small specimens, such as adult females and juveniles. However, coyote attacks (by an unknown number of coyotes) on adult male bobcats have occurred. In California, coyote and bobcat populations are not negatively correlated across different habitat types, but predation by coyotes is an important source of mortality in bobcats. Biologist Stanley Paul Young noted that in his entire trapping career, he had never successfully saved a captured bobcat from being killed by coyotes, and wrote of two incidences wherein coyotes chased bobcats up trees. Coyotes have been documented to directly kill Canadian lynx on occasion, and compete with them for prey, especially snowshoe hares. In some areas, including central Alberta, lynx are more abundant where coyotes are few, thus interactions with coyotes appears to influence lynx populations more than the availability of snowshoe hares.
Communication.
Body language.
Being both a gregarious and solitary animal, the variability of the coyote's visual and vocal repertoire is intermediate between that of the solitary foxes and the highly social wolf. The aggressive behavior of the coyote bears more similarities to that of foxes than it does that of wolves and dogs. An aggressive coyote arches its back and lowers its tail. Unlike dogs, which solicit playful behavior by performing a "play-bow" followed by a "play-leap", play in coyotes consists of a bow, followed by side-to-side head flexions and a series of "spins" and "dives". Although coyotes will sometimes bite their playmates' scruff as dogs do, they typically approach low, and make upward directed bites. Pups will fight each other regardless of sex, while among adults aggression is typically reserved for members of the same sex. Combatants approach each other waving their tails and snarling with their jaws open, though fights are typically silent. Males tend to fight in a vertical stance, while females fight on all four paws. Fights among females tend to be more serious than ones among males, as females seize their opponents' forelegs, throat and shoulders.
Vocalizations.
The coyote has been described as "the most vocal of North American wild mammals...", whose vocal proclivity lead to its being given the binomial name "Canis latrans", meaning "barking dog". At least 11 different vocalizations are known in adult coyotes. These sounds are divided into three categories: 1) Agonistic and alarm, 2) Greeting, and 3) Contact. Vocalizations of the first category include woofs, growls, huffs, barks, bark howls, yelps and high frequency whines. Woofs are used as low intensity threats or alarms, and are usually heard near den sites, prompting the pups to immediately retreat into their burrows. Growls are used as threats in short distances, but have also been heard among pups playing and copulating males. Huffs are high intensity threat vocalizations produced via rapid expiration of air. Barks can be classed as both long distance threat vocalizations and as alarm calls. Bark howls may serve similar functions. Yelps are emitted as a sign of submission, while high frequency whines are produced by dominant animals acknowledging the submission of subordinates. Greeting vocalizations include low frequency whines, "wow-oo-wows" and group yip howls. Low frequency whines are emitted by submissive animals, and are usually accompanied by tail wagging and muzzle nibbling. The sound known as "wow-oo-wow" has been described as a "greeting song". The group yip howl is emitted when two or more pack members reunite, and may be the final act of a complex greeting ceremony. Contact calls include lone howls and group howls, as well as the previously mention group yip howls. The lone howl is the most iconic sound of the coyote, and may serve the purpose of announcing the presence of a lone individual separated from its pack. Group howls are used as both substitute group yip howls and as responses to either lone howls, group howls or group yip howls.
Range.
The coyote's pre-Columbian range was limited to the south-west and plains regions of the U.S. and Canada, and northern and central Mexico. By the 19th century, the species expanded north and west, expanding further after 1900, coinciding with land conversion and the extirpation of wolves. By this time, its range encompassed all of the U.S. and Mexico, southward into Central America, and northward into most of Canada and Alaska. This expansion is ongoing, and the species now occupies the majority of areas between 8°N (Panama) and 70°N (northern Alaska).
In Mexico and Central America.
Although it was once widely believed that coyotes are recent immigrants to southern Mexico and Central America, aided in their expansion by deforestation, Pleistocene-Early Holocene records, as well as records from the Pre-Columbian period and early European colonization show that the animal was present in the area long before modern times. Nevertheless, range expansion did occur south of Costa Rica during the late 1970s and northern Panama in the early 1980s, following the expansion of cattle grazing lands into tropical rainforests. It has been predicted that the coyote should appear in northern Belize in the near future, as the habitat there is favorable to the species. Concerns have been raised of a possible expansion into South America through the Panamanian Isthmus, should the Darién Gap ever be closed by the Pan-American Highway. This fear was partially confirmed in January 2013, when the species was recorded in eastern Panama's Chepo District, beyond the Panama Canal.
Diseases and parasites.
Among large North American carnivores, the coyote probably carries the largest number of diseases and parasites, likely due to its wide range and varied diet. Viral diseases known to infect coyotes include rabies, canine distemper, infectious canine hepatitis, four strains of equine encephalitis, and oral papillomatosis. By the late 1970s, serious rabies outbreaks in coyotes had ceased to be a problem for over 60 years, though sporadic cases every 1–5 years did occur. Distemper causes the deaths of many pups in the wild, though some specimens can survive infection. Tularemia, a bacterial disease, infects coyotes through their rodent and lagomorph prey, and can be deadly for pups.
Coyotes can be infected by both demodectic and sarcoptic mange, the latter being the most common. Mite infestations are rare and incidental in coyotes, while tick infestations are more common, with seasonal peaks depending on locality (May–August in the Northwest, March–November in Arkansas). Coyotes are only rarely infested with lice, while fleas infest coyotes from puphood, though they may be more a source of irritation than serious illness. "Pulex simulans" is the most common species to infest coyotes, while "Ctenocephalides canis" tends to occur only in areas where coyotes and dogs (its primary host) inhabit the same area. Although coyotes are rarely host to flukes, they can nevertheless have serious effects on coyotes, particularly "Nanophyetus salmincola", which can infect them with salmon poisoning disease, a fatal disease with a 90% mortality rate. Tapeworms have been recorded to infest 60–95% of all coyotes examined. The most common species to infest coyotes is "Taenia pisiformis" and "Taenia crassiceps", which uses cottontail rabbits as intermediate hosts. The largest species known in coyotes is "T. hydatigena", which enters coyotes through infected ungulates, and can grow to lengths of 800–4000 mm. Though once largely limited to wolves, "Echinococcus granulosus" has expanded to coyotes since the latter began colonizing former wolf ranges. The most frequent ascaroid roundworm in coyotes is "Toxascaris leonina", which dwells in the coyote's short intestine and has no ill effects, save for causing the host to eat more frequently. Hookworms of the genus "Ancylostoma" infest coyotes throughout their range, being particularly prevalent in humid areas. In areas of high moisture, such as coastal Texas, coyotes can carry up to 250 hookworms each. The blood-drinking "A. caninum" is particularly dangerous, as it damages the coyote through blood-loss and lung congestion. A 10-day-old pup can die from being host to as few as 25 "A. caninum".
Relationships with humans.
In folklore and mythology.
The coyote features prominently as a trickster figure in the folktales of America's indigenous peoples, alternately assuming the form of an actual coyote or a man. As with other trickster figures, the coyote acts as a picaresque hero which rebels against social convention through deception and humor. The coyote was likely given its trickster role in light of the actual animal's intelligence and adaptability; pre-Columbian American people observed its behavior, and their folkloric representations reflected its attributes. It is variously credited for having brought fire to humanity, releasing the bison into the world, and of having slain monsters by petrifying them. The Maidu creation myth has the coyote introducing work, suffering and death to the world. Zuni folklore has the coyote bringing winter into the world by stealing light from the kachinas. Some tribes, such as the Chinook, Maidu, Paiute, Pawnee, Tohono O'odham, and Ute portray the coyote as the companion of the creator. In the Paiute creation myth, the coyote was created by the wolf as a companion, and the two created land by piling dirt on the water-covered world. A Tohono O'odham flood myth has the coyote helping Montezuma survive a global deluge that destroys humanity. After the Great Mystery creates humanity, the coyote and Montezuma teach people how to live. The Crow creation myth portrays Old Man Coyote as the creator. In Navajo mythology, the coyote was present in the First World with First Man and First Woman, though a different version has it being created in the Fourth World. The Navajo coyote brings death into the world, explaining that without death, there would be too many people, and thus no room to plant corn.
Prior to the Spanish conquest of the Aztec Empire, the coyote played a significant role in Mesoamerican cosmology. The coyote symbolized military might in pre-Aztec Teotihuacan, with warriors dressing up in coyote costumes to call upon its predatory power. The species continued to be linked to Central Mexican warrior cults in the centuries leading up to Aztec rule. In Aztec mythology, Huehuecóyotl (meaning "old coyote"), the god of dance, music and carnality, is depicted in several codices as a man with a coyote's head. He is sometimes depicted as a womanizer, responsible for bringing war into the world by seducing Xochiquetzal, the goddess of love. Epigrapher David H. Kelley argued that the god Quetzalcoatl owed its origins to pre-Aztec Uto-Aztecan mythological depictions of the coyote, which is portrayed as mankind's "Elder Brother", a creator, seducer, trickster and culture hero linked to the morning star.
Attacks on humans.
Coyote attacks on humans are uncommon and rarely cause serious injuries, due to the relatively small size of the coyote, but have been increasingly frequent, especially in the state of California. In the 30 years leading up to March 2006, at least 160 attacks occurred in the United States, mostly in the Los Angeles County area. Data from USDA Wildlife Services, the California Department of Fish and Game, and other sources show that while 41 attacks occurred during the period of 1988–1997, 48 attacks were verified from 1998 through 2003. The majority of these incidents occurred in Southern California near the suburban-wildland interface.
In the absence of the harassment of coyotes practiced by rural people, urban coyotes are losing their fear of humans, which is further worsened by people intentionally or unintentionally feeding coyotes. In such situations, some coyotes have begun to act aggressively toward humans, chasing joggers and bicyclists, confronting people walking their dogs, and stalking small children. Non rabid coyotes in these areas will sometimes target small children, mostly under the age of 10, though some adults have been bitten.
Although media reports of such attacks generally identify the animals in question as simply "coyotes," research into the genetics of the eastern coyote indicates those involved in attacks in northeast North America, including Pennsylvania, New York, New England, and eastern Canada, may have actually been coywolves, hybrids of "Canis latrans" and "Canis lupus," not fully coyotes.
Livestock and pet predation.
Coyotes are presently the most abundant livestock predators in western North America, causing the majority of sheep, goat and cattle losses. For example, according to the National Agricultural Statistics Service, coyotes were responsible for 60.5% of the 224,000 sheep deaths attributed to predation in 2004. The total number of sheep deaths in 2004 comprised 2.22% of the total sheep and lamb population in the United States. According to the National Agricultural Statistics Service USDA report, "All sheep and lamb inventory in the United States on July 1, 2005, totaled 7.80 million head, 2% above July 1, 2004. Breeding sheep inventory at 4.66 million head on July 1, 2005 was 2% above July 1, 2004." Because coyote populations are typically many times greater and more widely distributed than those of wolves, coyotes cause more overall predation losses. However, an Idaho census taken in 2005 showed that individual coyotes were one-twentieth as likely to attack livestock than individual wolves.
Livestock guardian dogs are commonly used to aggressively repel predators and have worked well in both fenced pasture and range operations. A 1986 survey of sheep producers in the USA found that 82% reported the use of dogs represented an economic asset.
Coyotes will typically bite the throat just behind the jaw and below the ear when attacking adult sheep or goats, with death commonly resulting from suffocation. Blood loss is usually a secondary cause of death. Calves and heavily fleeced sheep are killed by attacking the flanks or hindquarters, causing shock and blood loss. When attacking smaller prey, such as young lambs, the kill is made by biting the skull and spinal regions, causing massive tissue and bone damage. Small or young prey may be completely carried off, leaving only blood as evidence of a kill. Coyotes will usually leave the hide and most of the skeleton of larger animals relatively intact, unless food is scarce, in which case they may leave only the largest bones. Scattered bits of wool, skin and other parts are characteristic where coyotes feed extensively on larger carcasses.
Coyote predation can usually be distinguished from dog or coydog predation by the fact that coyotes partially consume their victims. Tracks are also an important factor in distinguishing coyote from dog predation. Coyote tracks tend to be more oval-shaped and compact than those of domestic dogs, and their claw marks are less prominent and the tracks tend to follow a straight line more closely than those of dogs. With the exception of sighthounds, most dogs of similar weight to coyotes have a slightly shorter stride. Coyote kills can be distinguished from wolf kills by the fact that there is less damage to the underlying tissues. Also, coyote scats tend to be smaller than wolf scats.
The U.S. government routinely shoots, poisons, traps and kills about 90,000 coyotes each year to protect livestock.
Coyotes are often attracted to dog food and animals that are small enough to appear as prey. Items such as garbage, pet food, and sometimes feeding stations for birds and squirrels will attract coyotes into backyards. About three to five pets attacked by coyotes are brought into the Animal Urgent Care hospital of south Orange County (California) each week, the majority of which are dogs, since cats typically do not survive the attacks. Scat analysis collected near Claremont, California revealed that coyotes relied heavily on pets as a food source in winter and spring. At one location in Southern California, coyotes began relying on a colony of feral cats as a food source. Over time, the coyotes killed most of the cats, and then continued to eat the cat food placed daily at the colony site by people who were maintaining the cat colony.
Coyotes usually attack smaller-sized dogs, but they have been known to attack even large, powerful breeds such as the Rottweiler in exceptional cases. Dogs larger than coyotes are generally able to drive them off, and have been known to kill coyotes. Smaller breeds are more likely to suffer injury or death.
Uses.
Prior to the mid-1800s, coyote fur was considered worthless. This changed with the diminution of beavers, and by 1860, the hunting of coyotes for their fur became a great source of income (75 cents to $1.50 per skin) for wolfers in the Great Plains. Coyote pelts were of significant economic importance during the early 1950s, ranging in price from $5 to $25 per pelt, depending on locality. The coyote's fur is not durable enough to make rugs, but can be used for ladies coats and jackets, scarfs or muffs. The majority of pelts are used for making trimmings, such as coat collars and sleeves for women's clothing. Coyote fur is sometimes dyed black as imitation silver fox.
Coyotes were occasionally eaten by trappers and mountain men during the western expansion. Coyotes sometimes featured in the feasts of the Plains Indians, and coyote pups were eaten by the indigenous people of San Gabriel, California. The taste of coyote meat has been likened to that of the wolf, and is more tender than pork when boiled. Coyote fat, when taken in the fall, has been used on occasion to grease leather or eaten as a spread.
Tameability.
Coyotes were probably semi-domesticated by various pre-Columbian cultures. Some 19th-century writers wrote of coyotes being kept in native villages in the Great Plains. The coyote is easily tamed as a pup, but can become destructive as an adult. Both full-blooded and hybrid coyotes can be playful and confiding with their owners, but are suspicious and shy of strangers, though there are records of coyotes being tractable enough to be used for practical purposes like retrieving and pointing. A tame coyote named "Butch", caught in the summer of 1945, had a short-lived career in cinema, appearing in "Smoky" and "Ramrod" before being shot while raiding a henhouse.

</doc>
<doc id="6711" url="http://en.wikipedia.org/wiki?curid=6711" title="Compressor">
Compressor

Compressor may refer to:

</doc>
<doc id="6713" url="http://en.wikipedia.org/wiki?curid=6713" title="Conan the Barbarian">
Conan the Barbarian

Conan the Barbarian (also known as Conan the Cimmerian) is a fictional sword and sorcery hero who originated in pulp fiction magazines and has since been adapted to books, comics, several films (including "Conan the Barbarian" and "Conan the Destroyer"), television programs (cartoon and live-action), video games, role-playing games and other media. The character was created by writer Robert E. Howard in 1932 via a series of fantasy stories published in "Weird Tales" magazine.
Publication history.
Conan the Barbarian was created by Robert E. Howard in a series of fantasy stories published in "Weird Tales" magazine in 1932. For months, Howard had been in search of a new character to market to the burgeoning pulp outlets of the early 1930s. In October 1931, he submitted the short story "People of the Dark" to Clayton Publications' new magazine, "Strange Tales of Mystery and Terror" (June 1932). "People of the Dark" is a remembrance story of "past lives", and in its first-person narrative the protagonist describes one of his previous incarnations: Conan, a black-haired barbarian hero who swears by a deity called Crom. Some Howard scholars believe this Conan to be a forerunner of the more famous character.
In February 1932, Howard vacationed at a border town on the lower Rio Grande. During this trip, he further conceived the character of Conan and also wrote the poem "Cimmeria", much of which echoes specific passages in Plutarch's "Lives". According to some scholars, Howard's conception of Conan and the Hyborian Age may have originated in Thomas Bulfinch's "The Outline of Mythology" (1913) which inspired Howard to "coalesce into a coherent whole his literary aspirations and the strong physical, autobiographical elements underlying the creation of Conan."
Having digested these prior influences after he returned from his trip, Howard rewrote the rejected story "By This Axe I Rule!" (May 1929), replacing his existing character Kull of Atlantis with his new hero, and retitling it "The Phoenix on the Sword". Howard also wrote "The Frost-Giant's Daughter", inspired by the Greek myth of Daphne, and submitted both stories to "Weird Tales" magazine. Although "The Frost-Giant's Daughter" was rejected, the magazine accepted "The Phoenix on the Sword" after it received the requested polishing.
"The Phoenix on the Sword" appeared in "Weird Tales" cover-dated December 1932. Editor Farnsworth Wright subsequently prompted Howard to write an 8,000 word essay for personal use detailing "the Hyborian Age," the fictional setting for Conan. Using this essay as his guideline, Howard began plotting "The Tower of the Elephant", a new Conan story that would be the first to truly integrate his new conception of the Hyborian world.
The publication and success of "The Tower of the Elephant" would spur Howard to write many more Conan stories for "Weird Tales". By the time of Howard's suicide in 1936, he had written 21 complete stories, 17 of which had been published, as well as a number of unfinished fragments.
Following Howard's death, the copyright of the Conan stories passed through several hands. Eventually, under the guidance of L. Sprague de Camp and Lin Carter, the stories were edited, revised, and sometimes rewritten. For roughly forty years, the original versions of Howard's Conan stories remained out of print. In 1977 the publisher Berkley Books issued three volumes using the earliest published form of the texts from "Weird Tales", but these failed to displace the edited versions. In the 1980s and 1990s, the copyright holders of the Conan franchise permitted Howard's stories to go out of print entirely, while continuing to sell Conan works by other authors.
In 2000, the British publisher Gollancz Science Fiction issued a two-volume, complete edition of Howard's Conan stories as part of its Fantasy Masterworks imprint, which including several stories that had never seen print in their original form. The Gollancz edition mostly used the versions of the stories as published in "Weird Tales".
In 2003, another British publisher, Wandering Star Books, made an effort both to restore Howard's original manuscripts and to provide a more scholarly and historical view of the Conan stories. It published hardcover editions in England, which were republished in the United States by the Del Rey imprint of Ballantine Books. The first book, "Conan of Cimmeria: Volume One (1932–1933)" (2003; published in the US as "The Coming of Conan the Cimmerian") includes Howard's notes on his fictional setting, as well as letters and poems concerning the genesis of his ideas. This was followed by "Conan of Cimmeria: Volume Two (1934)" (2004; published in the US as "The Bloody Crown of Conan") and "Conan of Cimmeria: Volume Three (1935–1936)" (2005; published in the US as "The Conquering Sword of Conan"). These three volumes combined include all of the original, unedited Conan stories.
Setting.
The various stories of Conan the Barbarian occur in the fictional "Hyborian Age", set after the destruction of Atlantis and before the rise of the known ancient civilizations. This is a specific epoch in a fictional timeline created by Howard for many of the low fantasy tales of his artificial legendary.
The reasons behind the invention of the Hyborian Age were perhaps commercial: Howard had an intense love for history and historical dramas; however, at the same time, he recognized the difficulties and the time-consuming research work needed in maintaining historical accuracy. By conceiving a timeless setting – "a "vanished" age" – and by carefully choosing names that resembled human history, Howard shrewdly avoided the problem of historical anachronisms and the need for lengthy exposition.
According to "The Phoenix on the Sword", the adventures of Conan take place "Between the years when the oceans drank Atlantis and the gleaming cities, and the years of the rise of the Sons of Aryas."
Personality and character.
Conan is a Cimmerian. From Robert E. Howard's writings (The Hyborian Age among others) it is known that the Cimmerians were based on the Celts or Gaels. He was born on a battlefield and is the son of a village blacksmith. Conan matured quickly as a youth and, by age fifteen, he was already a respected warrior who had participated in the destruction of the Aquilonian outpost of Venarium. After its demise, he was struck by wanderlust and began the adventures chronicled by Howard, encountering skulking monsters, evil wizards, tavern wenches, and beautiful princesses. He roamed throughout the Hyborian Age nations as a thief, outlaw, mercenary, and pirate. As he grew older, he began commanding larger units of men and escalating his ambitions. In his forties, he seized the crown of the tyrannical king of Aquilonia, the most powerful kingdom of the Hyborian Age, having strangled the previous ruler on the steps of the throne. Conan's adventures often result in him performing heroic feats, though his motivation for doing so is largely to protect his own survival or for personal gain.
Appearance.
Hither came Conan, the Cimmerian, black-haired, sullen-eyed, sword in hand, a thief, a reaver, a slayer, with gigantic melancholies and gigantic mirth, to tread the jeweled thrones of the Earth under his sandalled feet."
 Robert E. Howard, "The Phoenix on the Sword", 1932.
Conan has "sullen", "smoldering" and "volcanic" blue eyes with a black "square-cut mane". Howard once describes him as having a hairy chest and, while comic book interpretations often portray Conan as wearing a loincloth or other minimalist clothing to give him a more barbaric image, Howard describes the character as wearing whatever garb is typical for the land and culture in which Conan finds himself. Howard never gave a strict height or weight for Conan in a story, only describing him in loose terms like "giant" and "massive". In the tales, no human is ever described as being stronger than Conan, although several are mentioned as taller (such as the strangler Baal-pteor) or of larger bulk. In a letter to P. Schuyler Miller and John D. Clark in 1936, only three months before Howard's death, Conan is described as standing 6 ft and weighing 180 lb when he takes part in an attack on Venarium at only 15 years old, though being far from fully grown.
Although Conan is muscular, Howard frequently compares his agility and way of moving to that of a panther (see, for instance, "Jewels of Gwahlur", "Beyond the Black River" or "Rogues in the House"). His skin is frequently characterized as bronzed from constant exposure to the sun. In his younger years, he is often depicted wearing a light chain shirt and a horned helmet, though appearances vary with different stories.
During his reign as king of Aquilonia, Conan was
Howard imagined the Cimmerians as a pre-Celtic people with mostly black hair and blue or grey eyes. Ethnically the Cimmerians to which Conan belongs are descendants of the Atlanteans, though they do not remember their ancestry. In his fictional historical essay "The Hyborian Age", Howard describes how the people of Atlantis – the land where his character King Kull originated – had to move east after a great cataclysm changed the face of the world and sank their island, settling where Ireland and Scotland would eventually be located, Thus they are (in Howard's work) the ancestors of the Irish and Scottish (the Celtic Gaels) and not the Picts, the other ancestor of modern Scots who also appear in Howard's work. In the same work, Howard also described how the Cimmerians eventually moved south and east after the age of Conan (presumably in the vicinity of the Black Sea, where the historical Cimmerians dwelt).
Abilities.
Despite his brutish appearance, Conan uses his brains as well as his brawn. The Cimmerian is a highly skilled warrior, possibly without peer with a sword, but his travels have given him vast experience in other trades, especially as a thief. He is also a talented commander, tactician, and strategist, as well as a born leader. In addition, Conan speaks many languages, including advanced reading and writing abilities: in certain stories, he is able to recognize, or even decipher, certain ancient or secret signs and writings; for example, in "Jewels of Gwahlur" Howard states, "In his roaming about the world the giant adventurer had picked up a wide smattering of knowledge, particularly including the speaking and reading of many alien tongues. Many a sheltered scholar would have been astonished at the Cimmerian's linguistic abilities." He also has incredible stamina, enabling him to go without sleep for a few days. In "A Witch Shall be Born", Conan fights armed men until he is overwhelmed, captured, and crucified, and goes a night and a day without water, but still possesses the strength to pull the nails from his feet, then to hoist himself into a horse's saddle and ride ten miles.
Another noticeable trait is his sense of humor, largely absent in the comics and movies, but very much a part of Howard's original vision of the character (particularly apparent in "Xuthal of the Dusk", also known as "The Slithering Shadow.") His sense of humor can also be rather grimly ironic, as was demonstrated by how he meted out justice to the treacherous - and ill-fated - innkeeper Aram Baksh in "Shadows in Zamboula."
He is a loyal friend to those true to him, with a barbaric code of conduct that often marks him as more honorable than the more sophisticated people he meets in his travels. Indeed, his straightforward nature and barbarism are constants in all the tales.
Conan is a formidable armed and unarmed combatant. With his back to the wall, Conan is capable of engaging and killing opponents by the score. This is seen in several stories, such as "Queen of the Black Coast", "The Scarlet Citadel" and "A Witch Shall Be Born". Conan is not superhuman, though he did need the providential help of Zelata's wolf to defeat four Nemedian soldiers in the story "The Hour of the Dragon". Some of his hardest victories have come from fighting single opponents of inhuman strength: one such as Thak, the ape man from "Rogues in the House", or the strangler Baal-Pteor in "Shadows in Zamboula". Conan is far from untouchable and has been captured and defeated several times (on one occasion knocking himself out drunkenly running into a wall).
Influences.
Howard frequently corresponded with H. P. Lovecraft, and the two would sometimes insert references or elements of each other's settings in their works. Later editors reworked many of the original Conan stories by Howard, thus diluting this connection. Nevertheless, many of Howard's unedited Conan stories are arguably part of the Cthulhu Mythos. Additionally, many of the Conan stories by Howard, de Camp and Carter used geographical place names from Clark Ashton Smith's Hyperborean Cycle.
Original Robert E. Howard Conan stories.
Unfinished Conan stories by Howard.
A number of untitled synopses for Conan stories also exist.
Book editions.
The character of Conan has proven durably popular, resulting in Conan stories by later writers such as Poul Anderson, Leonard Carpenter, Lin Carter, L. Sprague de Camp, Roland J. Green, John C. Hocking, Robert Jordan, Sean A. Moore, Björn Nyberg, Andrew J. Offutt, Steve Perry, John Maddox Roberts, Harry Turtledove, and Karl Edward Wagner. Some of these writers have finished incomplete Conan manuscripts by Howard. Others were created by rewriting Howard stories which originally featured entirely different characters from entirely different milieus. Most, however, are completely original works. In total, more than fifty novels and dozens of short stories featuring the Conan character have been written by authors other than Howard.
The Gnome Press edition (1950–1957) was the first hardcover collection of Howard's Conan stories, including all the original Howard material known to exist at the time, some left unpublished in his lifetime. The later volumes contain some stories rewritten by L. Sprague de Camp (like "The Treasure of Tranicos"), including several non-Conan Howard stories, mostly historical exotica situated in the Levant at the time of the Crusades, which he turned into Conan yarns. The Gnome edition also issued the first Conan story written by an author other than Howard — the final volume published, which is by Björn Nyberg and revised by de Camp.
The Lancer/Ace editions (1966–1977), under the direction of de Camp and Lin Carter, were the first comprehensive paperbacks, compiling the material from the Gnome Press series together in chronological order with all the remaining original Howard material, including that left unpublished in his lifetime and fragments and outlines. These were completed by de Camp and Carter, and new stories written entirely by the two were added as well. Lancer Books went out of business before bringing out the entire series, the publication of which was completed by Ace Books. Eight of the eventual twelve volumes published featured dynamic cover paintings by Frank Frazetta that, for many fans, presented the definitive, iconic impression of Conan and his world. For decades to come, most other portrayals of the Cimmerian and his imitators were heavily influenced by the cover paintings of this series.
Editions after the Lancer/Ace series have been of either the original Howard stories or Conan material by others, but not both. Notable later editions of the Howard stories include the Donald M. Grant editions (1974–1989, incomplete); Berkley editions (1977); Gollancz editions (2000–2006), and Wandering Star/Del Rey editions (2003–2005). Later series of new Conan material include the Bantam editions (1978–1982), Ace Maroto editions (1978–1981), and Tor editions (1982–2004).
Several of the Lancer/Ace Conan versions are rewrites by de Camp and Carter of non Conan stories. For example, at least one El Borak in which the protagonist infiltrates the City of the Assassins was rewritten with Conan replacing Francis Xavier Gordon and a supernatural element added.
Conan chronologies.
In an attempt to provide a coherent timeline which fit the numerous adventures of Conan penned by Robert E. Howard and later writers, various "Conan chronologies" have been prepared by many people from the 1930s onward. Note that no consistent timeline has yet accommodated every single Conan story. The following are the principal theories that have been advanced over the years.
Media.
Films.
"Conan the Barbarian" (1982) and "Conan the Destroyer" (1984).
The very first Conan cinematic project was planned by Edward Summer. Summer envisioned a series of Conan films, much like the James Bond franchise. He outlined six stories for this film series, but none were ever made. An original screenplay by Summer and Roy Thomas was written, but their lore-authentic screen story was never filmed. However, the resulting film, "Conan the Barbarian" (1982), was a combination of director John Milius' ideas and plots from Conan stories (written also by Howard's successors, notably Lin Carter and L. Sprague de Camp). The addition of Nietzschean motto and Conan's life philosophy were crucial for bringing the spirit of Howard's literature to the screen.
The plot of "Conan the Barbarian" (1982) begins with Conan being enslaved by the Vanir raiders of Thulsa Doom, a malevolent warlord who is responsible for the slaying of Conan's parents and the genocide of his people. Later, Thulsa Doom becomes a cult leader of a religion that worships Set, a Snake God. The vengeful Conan, the archer Subotai and the thief Valeria set out on a quest to rescue a princess held captive by Thulsa Doom. The film was directed by John Milius and produced by Dino De Laurentiis. The character of Conan was played by Arnold Schwarzenegger and was his break-through role as an actor.
This film was followed by a less popular sequel, "Conan the Destroyer" in 1984. This sequel was a more typical fantasy-genre film and was even less faithful to Howard's Conan stories.
"Conan the Conqueror" (cancelled).
The third film in the "Conan" trilogy was planned for 1987 to be titled "Conan the Conqueror". The director was to be either Guy Hamilton or John Guillermin. Since Arnold Schwarzenegger was committed to the film "Predator" and De Laurentiis's contract with the star had expired after his obligation to "Red Sonja" and "Raw Deal", he wasn't keen to negotiate a new one; thus the third Conan film sank into development hell. The script was eventually turned into "Kull the Conqueror".
"Conan the Barbarian" (2011).
There were rumours in the late 1990s of another Conan sequel, a story about an older Conan titled "King Conan: Crown of Iron", but Schwarzenegger's election in 2003 as governor of California ended this project. Warner Bros. spent seven years trying to get the project off the ground, with development attempts made by The Wachowskis, John Milius, and Robert Rodriguez, who left the project for "Grindhouse". Boaz Yakin was hired in 2006 to start again. However, in June 2007 the rights reverted to Paradox Entertainment, though all drafts made under Warner remained with them. Paradox's CEO, Fredrik Malmberg, told "Variety" "we have great respect for Warner Bros., but after seven years, we came to the point where we needed to see progress to production." Paradox were auctioning the rights after and various groups took interest in producing, including New Line Cinema, Hollywood Gang, and Millennium Films.
Due to development-time frustrations felt when the rights were with Warner, Malmberg made deal terms where he was asking for $1 million for a one-year option, with another $1 million for each year's renewal. In August 2007, it was announced that Millennium had acquired the right to the project in an unrevealed seven-figure deal, with Malmberg and Millennium's Avi Lerner, Boaz Davidson, Joe Gatta and George Furla set to produce. The deal was brokered by Gatta, who originally made the deal between Paradox and Warner in 2002. Production was aimed for a Spring 2006 start, with the intention of having stories more faithful to the Robert E. Howard creation.
In November 2008, "The Hollywood Reporter", based on a press release from Lerner, reported that Brett Ratner was committed to direct "Conan"; an update changed this to state he was in final negotiations. Days later, however, Ratner announced, "I am not doing 'Conan' now. This is totally premature. For now, 'Conan' is only a development deal. In June 2009, Nu Image/Millennium Films hired Marcus Nispel to direct. In January 2010, Jason Momoa was selected for the role of Conan. In February 2010, on the eve of production, Sean Hood was brought in to retool the screenplay.
"The Legend of Conan".
On October 25, 2012, it was announced that a sequel to the 1982 "Conan the Barbarian" titled "The Legend of Conan" is being planned with Arnold Schwarzenegger reprising his role as Conan. A year later, "Deadline" reported that Andrea Berloff would write the script. The film is being produced by Fredrik Malmberg and Chris Morgan and was originally scheduled for release in 2014, a date which the production team is now "clearly going to miss." Still, production continues, and according to Morgan, Berloff has nearly finished the script, and the project is now looking for a director.
Animated film.
An animated feature, "Conan: Red Nails", based upon the novella of the same name, was partially completed but appears to have stalled.
Television.
There have been three television series related to Conan: A live-action TV series and an animated cartoon series — both entitled "Conan the Adventurer", as well as a second animated series entitled "Conan and the Young Warriors".
Comics.
Conan the Barbarian has appeared in comics nearly non-stop since 1970. The comics are arguably, apart from the books, the vehicle that had the greatest influence on the longevity and popularity of the character. Aside from an earlier and unofficial Conan comic published in Mexico, the two main publishers of Conan comics have been Marvel Comics and Dark Horse Comics. Marvel Comics launched "Conan the Barbarian" (1970–1993) and the classic "Savage Sword of Conan" (1974–1995). Dark Horse launched their "Conan" series in 2003. Dark Horse Comics is currently publishing compilations of the 1970s Marvel Comics series in trade paperback format.
The current president of the United States, Barack Obama, is a collector of Conan the Barbarian comic books and a big fan of the character and appeared as a character in a comic book called "Barack the Barbarian" from Devil's Due.
Marvel Comics.
Marvel Comics introduced a relatively lore-faithful version of Conan the Barbarian in 1970 with "Conan the Barbarian", written by Roy Thomas and illustrated by Barry Windsor-Smith. Smith was succeeded by penciller John Buscema, while Thomas continued to write for many years. Later writers included J.M. DeMatteis, Bruce Jones, Michael Fleisher, Doug Moench, Jim Owsley, Alan Zelenetz, Chuck Dixon and Don Kraar. In 1974, "Conan the Barbarian" series spawned the more adult-oriented, black-and-white comics magazine "Savage Sword of Conan", written by Thomas with art mostly by Buscema or Alfredo Alcala. Marvel also published several graphic novels starring the character.
The Marvel Conan stories were also adapted as a newspaper comic strip which appeared daily and Sunday from 4 September 1978 to 12 April 1981. Originally written by Roy Thomas and illustrated by John Buscema, the strip was continued by several different Marvel artists and writers.
Dark Horse Comics.
Dark Horse Comics began their comic adaptation of the Conan saga in 2003. Entitled simply "Conan", the series was first written by Kurt Busiek and pencilled by Cary Nord. Tim Truman replaced Busiek when Busiek signed an exclusive contract with DC Comics; however, Busiek issues were sometimes used for filler. This series is an interpretation of the original Conan material by Robert E. Howard with no connection whatsoever to the earlier Marvel comics or any Conan story not written or envisioned by Howard supplemented by wholly original material.
A second series, "Conan the Cimmerian" was released in 2008 by Tim Truman (writer) and Tomás Giorello (artist). The series ran for twenty-six issues, including an introductory "zero" issue.
Dark Horse's third series, "", began in December 2010 by Roy Thomas (writer) and Mike Hawthorne (artist) and ran for twelve issues.
A fourth series, "Conan the Barbarian", began in February 2012 by Brian Wood (writer) and Becky Cloonan (artist). It is stated to run for twenty-five issues, and expand on Robert E. Howard's "Queen of the Black Coast".
Games.
See also List of games based on Conan the Barbarian
Video games.
Seven video games have been released based on the Conan mythos.
Role-playing games.
TSR, Inc. signed a license agreement in 1984 to publish Conan-related gaming material:
In 1988 Steve Jackson Games acquired a Conan license and started publishing Conan solo adventures for its "GURPS" generic system of rules as of 1988 and a "GURPS Conan" core rulebook in 1989:
In 2003 the British company Mongoose Publishing bought a license and acquired in turn the rights to make use of the Conan gaming franchise, publishing a Conan role-playing game from 2004 until 2010. The game ran the OGL System of rules that Mongoose established for its "OGL series" of games:
In 2015, British company Modiphius acquired a Conan license, announcing plans to put out a new Conan role-playing game in August of that year.
Copyright and trademark dispute.
The name "Conan" and the names of Robert E. Howard's other principal characters are claimed as trademarked by Paradox Entertainment of Stockholm, Sweden, through its US subsidiary Paradox Entertainment Inc. Paradox copyrights stories written by other authors under license from Conan Properties Inc.
However, since Robert E. Howard published his Conan stories at a time when the date of publication was the marker (1932–1963), and any new owners failed to renew them to maintain the copyrights, the exact copyright status of all of Howard's 'Conan' works is in question. In practice, most of the Conan stories exist in at least two versions subject to different copyright standards: The original "Weird Tales" publications before or shortly after Howard's death, which are generally understood to be public domain, and "restored" versions based on manuscripts that were unpublished in Howard's lifetime.
The Australian site of Project Gutenberg has many Robert E. Howard stories, including several Conan stories.
In the United Kingdom, works fall into the public domain 70 years after the death of an author. Therefore, with Howard having died in 1936, his works have been in the public domain since 2006.

</doc>
<doc id="6715" url="http://en.wikipedia.org/wiki?curid=6715" title="Chris Marker">
Chris Marker

Chris Marker (]; 29 July 1921 – 29 July 2012) was a French writer, photographer, documentary film director, multimedia artist and film essayist. His best known films are "La Jetée" (1962), "A Grin Without a Cat" (1977), "Sans Soleil" (1983) and "AK" (1985), an essay film on the Japanese filmmaker Akira Kurosawa. Marker is often associated with the Left Bank Cinema movement that occurred in the late 1950s and included such other filmmakers as Alain Resnais, Agnès Varda, Henri Colpi and Armand Gatti.
His friend and sometime collaborator Alain Resnais has called him "the prototype of the twenty-first-century man." Film theorist Roy Armes has said of him: "Marker is unclassifiable because he is unique...The French Cinema has its dramatists and its poets, its technicians, and its autobiographers, but only has one true essayist: Chris Marker."
Early life.
Marker was born Christian François Bouche-Villeneuve. Always elusive about his past and known to refuse interviews and not allow photographs to be taken of him, his place of birth is highly disputed. Some sources and Marker himself claim that he was born in Ulan Bator, Mongolia. Other sources say he was born in Belleville, Paris, and others, in Neuilly-sur-Seine. The 1949 edition of "Le Cœur Net" specifies his birthday as 22 July. Film critic David Thomson has stated: "Marker told me himself that Mongolia is correct. I have since concluded that Belleville is correct – but that does not spoil the spiritual truth of Ulan Bator." When asked about his secretive nature, Marker has said "My films are enough for them [the audience]."
Marker was a philosophy student in France prior to World War II. During the German occupation of France he joined the Maquis (FTP), a part of the French Resistance. At some point during the war he left France and joined the United States Air Force as a paratrooper, although some sources claim that this is not true. After the war he began a career as a journalist. He first wrote for the journal "Esprit", a neo-Catholic, Marxist magazine where he met fellow journalist André Bazin. At "Esprit", Marker wrote political commentaries, poems, short stories and (with Bazin) film reviews. He would later become an early contributor to Bazin's "Cahiers du cinéma".
During this time period, Marker began to travel around the world as a journalist and photographer, a vocation he would continue the rest of his life. He was hired by the French publishing company Éditions du Seuil as editor of the series "Petite Planète" ("Small World"). This collection devoted one edition to each country and included information and photographs. In 1949 Marker published his first novel, "Le Coeur net" ("The Forthright Spirit"), which was about aviation. In 1952 Marker published an illustration essay on French writer Jean Giraudoux, "Giraudoux Par Lui-Même".
Early career (1950–1961).
During his early journalism career, Marker became increasingly interested in filmmaking and experimented with photography in the early 1950s. Around this time Marker met and befriended many members of what would be called the Left Bank Film Movement, including Alain Resnais, Agnès Varda, Henri Colpi, Armand Gatti and the novelists Marguerite Duras and Jean Cayrol. This group is often associated with the French New Wave directors who came to prominence during the same time period, and indeed both groups were often friends and journalistic co-workers. The term "Left Bank" was first coined by film critic Richard Roud, who has described them as having "fondness for a kind of Bohemian life and an impatience with the conformity of the Right Bank, a high degree of involvement in literature and the plastic arts, and a consequent interest in experimental filmmaking", as well as an identification with the political left. Many of Marker's earliest films were produced by Anatole Dauman.
In 1952 Marker made his first film, "Olympia 52", a 16mm feature documentary about the 1952 Helsinki Olympic Games. In 1953 Marker collaborated with Resnais on the documentary "Statues Also Die". The film examines traditional African art such as sculptures and masks, and its decline with coming of Western colonialism. The film won the 1954 Prix Jean Vigo, but was banned by French censors for its criticism of French colonialism.
After working as assistant director on Resnais's "Night and Fog" in 1955, Marker made "Sunday in Peking", a short documentary "film essay" that would characterize Marker's unique film style for most of his career. The film was shot in two weeks by Marker while he was traveling through China with Armand Gatti in September 1955. In the film, Marker's commentary overlaps scenes from China, such as tombs which, contrary to Westernized understandings of Chinese legends, do not contain the remains of any Ming Dynasty emperors.
After working on the commentary for Resnais' film "Le mystère de l'atelier quinze" in 1957, Marker continued to form his own cinematic style with the feature documentary "Letter from Siberia". An essay film on the narrativization of Siberia, it contains Marker's signature commentary, which takes the form of a letter from the director, in the long tradition of epistolary treatments by French explorers of the "undeveloped" world. "Letter " looks at the modernization of Siberia with its movement into the twentieth century, but with a look back at some of the tribal cultural practices now receding into the past. It combines footage that Marker shot in Siberia with old newsreel footage, cartoon sequences, stills, and even an illustration of Alfred E. Neuman from "Mad Magazine" as well as a fake TV commercial as part of a humorous attack on Western mass culture. In producing a meta-commentary on narrativity and film, Marker uses the same brief filmic sequence three times but with different commentary—the first one praising the Soviet Union, the second denouncing it, and the third taking an apparently neutral or "objective" stance.
In 1959 Marker made the animated film "Les Astronautes" with Walerian Borowczyk. The film was a combination of traditional drawings with still photography. In 1960 Marker made "Description d'un combat", a documentary on the State of Israel which reflects on the country's past and future. The film won the Golden Bear for Best Documentary at the 1961 Berlin Film Festival.
In January 1961, Marker traveled to Cuba and shot the film "¡Cuba Sí!" The film promotes and defends Fidel Castro and includes two interviews with the Comandante. The film ends with an anti-American epilogue in which the United States is embarrassed by the Bay of Pigs Invasion fiasco, and was subsequently banned. The banned essay was included in Marker's first volume of collected film commentaries, "Commentaires I", published in 1961. The following year Marker published "Coréennes", a collection of photographs and essays on the conditions of Korea.
"La jetée" and "Le joli mai" (1962–1966).
Marker became known internationally for the short film "La jetée" ("The Pier") in 1962. It tells of a post-nuclear war experiment in time travel by using a series of filmed photographs developed as a photomontage of varying pace, with limited narration and sound effects. In the film, a survivor of a futuristic third World War is obsessed with distant and disconnected memories of a pier at the Orly Airport, the image of a mysterious woman, and a man's death. Scientists experimenting in time travel choose him for their studies, and the man travels back in time to contact the mysterious woman, and discovers that the man's death at the Orly Airport was his own. Except for one shot of the woman mentioned above sleeping and suddenly waking up, the film is composed entirely of photographs by Jean Chiabaud and stars Davos Hanich as the man, Hélène Chatelain as the woman and filmmaker William Klein as a man from the future.
"La Jetée" was the inspiration for Mamoru Oshii's 1987 debut live action feature "The Red Spectacles" (and later for parts of Oshii's 2001 film "Avalon" as well) and also inspired Terry Gilliam's "12 Monkeys" (1995). It also inspired many of director Mira Nair's shots for the 2006 film "The Namesake".
While making "La Jetee", Marker was simultaneously making the 150-minute documentary essay-film "Le joli mai", released in 1963. Beginning in the Spring of 1962, Marker and his camera operator Pierre Lhomme shot 55 hours of footage interviewing random people on the streets of Paris. The questions, asked by the unseen Marker, range from their personal lives, as well as social and political issues of relevance at that time. As he had with montages of landscapes and indigenous art, Marker created a film essay that contrasted and juxtaposeed a variety of lives with his signature commentary (spoken by Marker's friends, singer-actor Yves Montand in the French version and Simone Signoret in the English version). The film has been compared to the "Cinéma vérité" films of Jean Rouch, and criticized by its practitioners at the time. The term "Cinéma vérité" was itself anathema to Marker; he would never use it. It was shown in competition at the 1963 Venice Film Festival, where it won the award for Best First Work. It also won the Golden Dove Award at the Leipzig DOK Festival.
After the documentary "Le Mystère Koumiko" in 1965, Marker made "Si j'avais quatre dromadaires", an essay-film that, like "La jetée", is a photomontage of over 800 photographs that Marker had taken over the past 10 years from 26 countries. The commentary takes on a slightly different dimension from his previous commentaries by using a conversation between a fictitious photographer and two friends, who discuss the photos. The film's title is an allusion to a poem by Guillaume Apollinaire. It was the last film in which Marker included "travel footage" for many years.
SLON and ISKRA (1967–1974).
In 1967 Marker published his second volume of collected film essays, "Commentaires II". That same year, Marker organized the omnibus film "Loin du Vietnam", a protest against the Vietnam War with segments contributed by Marker, Jean-Luc Godard, Alain Resnais, Agnès Varda, Claude Lelouch, William Klein, Michele Ray and Joris Ivens. The film includes footage of the war, from both sides, as well as anti-war protests in New York and Paris and other anti-war activities.
From this initial collection of filmmakers with left-wing political agendas, Marker created the group S.L.O.N. (Société pour le lancement des oeuvres nouvelles, but also the Russian word for elephant). SLON was a film collective whose objectives were to make films and to encourage industrial workers to create film collectives of their own. Its members included Valerie Mayoux, Jean-Claude Lerner, Alain Adair and John Tooker. Marker is usually credited as director or co-director of all of the films made by SLON.
After the events of May 1968, Marker felt a moral obligation to abandon his own personal film career and devote himself to SLON and its activities. SLON's first film was about a strike at a Rhodiacéta factory in France, "À bientôt, j'espère" ("Rhodiacéta") in 1968. Later that year SLON made "La Sixième face du pentagone", about an anti-war protest in Washington, D.C. and was a reaction to what SLON considered to be the unfair and censored reportage of such events on mainstream television. The film was shot by François Reichenbach, who received co-director credit. "La Bataille des dix millions" was made in 1970 with Mayoux as co-director and Santiago Álvarez as cameraman and is about the 1970 sugar crop in Cuba and its disastrous effects on the country. In 1971, SLON made "Le Train en marche", a new prologue to Soviet filmmaker Aleksandr Medvedkin's 1935 film "Schastye", which had recently been re-released in France.
In 1974, SLON became I.S.K.R.A. ("Images, Sons, Kinescope, Réalisations, Audiovisuelles", but also the name of Vladimir Lenin's political newspaper "Iskra").
Return to personal work (1974–1986).
In 1974 returned to his personal work and made a film outside of ISKRA. "La Solitude du chanteur de fond" is a one-hour documentary about Marker's friend Yves Montand's benefit concert for Chilean refugees. The concert was Montand's first public performance in four years and the film includes film clips from his long career as a singer and actor.
Marker had been working on a film about Chile with ISKRA since 1973. Marker had collaborated with Belgian sociologist Armand Mattelart and ISKRA members Valérie Mayoux and Jacqueline Meppiel to shoot and collect the visual materials, which Marker then edited together and provided the commentary for. The resulting film was the two and a half-hour documentary "La Spirale", released in 1975. The film chronicles recent events in Chile, beginning with the 1970 election of socialist President Salvador Allende until his murder and the resulting coup in 1973.
Marker then began work on one of his most ambitious films, "A Grin Without a Cat", released in 1977. The film's title refers to the Cheshire Cat from "Alice in Wonderland". The metaphor compares the promise of the global socialist movement before May 1968 (the grin) with its actual presence in the world after May 1968 (the cat). The film's original French title is "Le fond de l'air est rouge", which means "the air is essentially red", or "revolution is in the air", implying that the socialist movement existed only in the air.
The film was intended to be an all-encompassing portrait of political movements since May 1968, a summation of the work which he had taken part in for ten years. The film is divided into two parts: the first half focuses on the hopes and idealism before May 1968, and the second half on the disillusion and disappointments since those events. Marker begins the film with the Odessa Steps sequence from Sergei Eisenstein's film "The Battleship Potemkin", which Marker points out is a fictitious creation of Eisenstein which has still influenced the image of the historical event. Marker used very little commentary in this film, but the film's montage structure and preoccupation with memory make it a Marker film. Upon release, the film was criticized for not addressing many current issues of the New Left such as the woman's movement, sexual liberation and worker self-management. The film was re-released in the US in 2002.
In the late 1970s, Marker traveled extensively throughout the world, including an extended period in Japan. From this inspiration, he first published the photo-essay "Le Dépays" in 1982, and then used the experience for his next film "Sans Soleil", released in 1982.
"Sans Soleil" stretches the limits of what could be called a documentary. It is an essay, a montage, mixing pieces of documentary with fiction and philosophical comments, creating an atmosphere of dream and science fiction. The main themes are Japan, Africa, memory and travel. A sequence in the middle of the film takes place in San Francisco, and heavily references Alfred Hitchcock's "Vertigo". Marker has said that "Vertigo" is the only film "capable of portraying impossible memory, insane memory." The film's commentary are credited to the fictitious cameraman Sandor Krasna, and read in the form of letters by an unnamed woman. Though centered around Japan, the film was also shot in such other countries as Guinea Bissau, Ireland and Iceland. "Sans Soleil" was shown at the 1983 Berlin Film Festival where it won the OCIC Award. It was also awarded the Sutherland Trophy at the 1983 British Film Institute Awards.
In 1984, Marker was invited by producer Serge Silberman to document the making of Akira Kurosawa's film "Ran". From this Marker made "A.K.", released in 1985. The film focuses more on Kurosawa's remote but polite personality than on the making of the film. The film was screened in the Un Certain Regard section at the 1985 Cannes Film Festival, before "Ran" itself had been released.
In 1985, Marker's long-time friend and neighbor Simone Signoret died of cancer. Marker then made the one-hour TV documentary "Mémoires pour Simone" as a tribute to her in 1986.
Multimedia and later career (1987–2012).
Beginning with "Sans Soleil", he developed a deep interest in digital technology, which led to his film "Level Five" (1996) and "Immemory" (1998, 2008), an interactive multimedia CD-ROM, produced for the Centre Pompidou (French language version) and from Exact Change (English version). Marker created a 19-minute multimedia piece in 2005 for the Museum of Modern Art in New York City titled "Owls at Noon Prelude: The Hollow Men" which was influenced by T. S. Eliot's poem.
Marker lived in Paris and very rarely granted interviews. One exception was a lengthy interview with "Libération" in 2003 in which he explained his approach to filmmaking. When asked for a picture of himself, he usually offered a photograph of a cat instead. (Marker was represented in Agnes Varda's 2008 documentary "The Beaches of Agnes" by a cartoon drawing of a cat, speaking in a technologically altered voice.) Marker's own cat is named Guillaume-en-egypte. In 2009 Marker commissioned an to represent him in machinima works. The avatar was created by Exosius Woolley and first appeared in the short film / machinima, "Ouvroir the Movie by Chris Marker."
In the 2007 Criterion Collection release of "La Jetée" and "Sans Soleil", Marker included a short essay, "Working on a shoestring budget". He confessed to shooting all of "Sans Soleil" with a silent film camera and recording all the audio on a primitive audio cassette recorder. Marker also reminds the reader that only one short scene in "La Jetée" is of a moving image, as Marker could only borrow a movie camera for one afternoon while working on the film.
Marker died on 29 July 2012, 91 years to the day From his birth.

</doc>
<doc id="6716" url="http://en.wikipedia.org/wiki?curid=6716" title="Cardinal vowels">
Cardinal vowels

Cardinal vowels are a set of reference vowels used by phoneticians in describing the sounds of languages. For instance, the vowel of the English word "feet" can be described with reference to cardinal vowel 1, [i], which is the cardinal vowel closest to it. It is often stated that to be able to use the cardinal vowel system effectively one must undergo training with an expert phonetician, working both on the recognition and the production of the vowels. Daniel Jones wrote "The values of cardinal vowels cannot be learnt from written descriptions; they should be learnt by oral instruction from a teacher who knows them".
A cardinal vowel is a vowel sound produced when the tongue is in an extreme position, either front or back, high or low. The current system was systematised by Daniel Jones in the early 20th century, though the idea goes back to earlier phoneticians, notably Ellis and Bell.
Cardinal vowels are not vowels of any particular language, but a measuring system. However, some languages contain vowel or vowels that are close to the cardinal vowel(s). An example of such language is Ngwe, which is spoken in West Africa. It has been cited as a language with a vowel system that has 8 vowels which are rather similar to the 8 primary cardinal vowels (Ladefoged 1971:67).
Three of the cardinal vowels—[i], [ɑ] and [u]—have articulatory definitions. The vowel [i] is produced with the tongue as far forward and as high in the mouth as is possible (without producing friction), with spread lips. The vowel [u] is produced with the tongue as far back and as high in the mouth as is possible, with protruded lips. This sound can be approximated by adopting the posture to whistle a very low note, or to blow out a candle. And [ɑ] is produced with the tongue as low and as far back in the mouth as possible.
The other vowels are 'auditorily equidistant' between these three 'corner vowels', at four degrees of aperture or 'height': close (high tongue position), close-mid, open-mid, and open (low tongue position).
These degrees of aperture plus the front-back distinction define 8 reference points on a mixture of articulatory and auditory criteria. These eight vowels are known as the eight 'primary cardinal vowels', and vowels like these are common in the world's languages.
The lip positions can be reversed with the lip position for the corresponding vowel on the opposite side of the front-back dimension, so that e.g. Cardinal 1 can be produced with rounding somewhat similar to that of Cardinal 9 (though normally compressed rather than protruded); these are known as 'secondary cardinal vowels'. Sounds such as these are claimed to be less common in the world's languages. Other vowel sounds are also recognised on the vowel chart of the International Phonetic Alphabet.
Table of cardinal vowels.
In the International Phonetic Association's , the cardinal vowels have the same numbers used above, but added to 300.
Limits on the accuracy of the system.
The usual explanation of the cardinal vowel system implies that the competent user can reliably distinguish between sixteen Primary and Secondary vowels plus a small number of central vowels. The provision of diacritics by the International Phonetic Association further implies that intermediate values may also be reliably recognized, so that a phonetician might be able to produce and recognize not only a close-mid front unrounded vowel [e] and an open-mid front unrounded vowel [ɛ] but also a mid front unrounded vowel [e̞], a centralized mid front unrounded vowel [ë], and so on. This suggests a range of vowels nearer to forty or fifty than to twenty in number. Empirical evidence for this ability in trained phoneticians is hard to come by. Ladefoged, in a series of pioneering experiments published in the 1950's and 60's, studied how trained phoneticians coped with the vowels of a dialect of Scots Gaelic. He asked eighteen phoneticians to listen to a recording of ten words spoken by a native speaker of Gaelic and to place the vowels on a cardinal vowel quadrilateral. He then studied the degree of agreement or disagreement among the phoneticians. Ladefoged himself drew attention to the fact that the phoneticians who were trained in the British tradition established by Daniel Jones were closer to each other in their judgments than those who had not had this training. However, the most striking result is the great divergence of judgments among "all" the listeners regarding vowels that were distant from Cardinal values.

</doc>
<doc id="6719" url="http://en.wikipedia.org/wiki?curid=6719" title="Columbia, Missouri">
Columbia, Missouri

Columbia is a city of 115,276 people in the state of Missouri. Founded in 1820 as the county seat of Boone County and home to the University of Missouri, it is the principal municipality of the Columbia Metropolitan Area, the fourth most populous urban area in Missouri. As a midwestern college town, the city has a reputation for progressive politics, public art, and powerful journalism. The tripartite establishment of Stephens College (1833), "University of Missouri" (1839), and Columbia College (1851) has long made the city a center of education, culture, and athletic competition. These three schools surround Downtown Columbia on the east, south, and north; at the center is the Avenue of the Columns, which connects Francis Quadrangle and Jesse Hall to the Boone County Courthouse and the City Hall. Originally as an agricultural town, the cultivation of the mind today is Columbia's chief economic concern. Never being a major manufacturing center, the city also depends on healthcare, insurance, and technology businesses. Several companies—Shelter Insurance, Carfax, and Slackers CDs and Games among them—were founded in the city. Cultural institutions include the State Historical Society of Missouri, the Museum of Art and Archaeology, and the annual True/False Film Festival. The Missouri Tigers, the state's only major athletic program, play football at Faurot Field and basketball at Mizzou Arena as members of the Southeastern Conference.
The city is built upon the forested hills and rolling prairies of Mid-Missouri, near the Missouri River valley, where the Ozark Mountains begin to transform into plains and savanna; limestone forms bluffs and glades while rain carves caves and springs which water the Hinkson, Roche Perche, and Petite Bonne Femme creeks. Surrounding the city, Rock Bridge State Park, Mark Twain National Forest, and Big Muddy National Fish and Wildlife Refuge form a greenbelt preserving sensitive and rare environments. The first humans were nomadic hunters who entered the area at least twelve-thousand years ago. Later, woodland tribes lived in villages along waterways and built mounds in high places. The Osage and Missouria nations were expelled by the exploration of French traders and the rapid settlement of American pioneers. The latter arrived by the Boone's Lick Trail and hailed from the slave-owning culture of the Upland South, especially Virginia, Kentucky, and Tennessee, giving Boonslick the name Little Dixie during American Civil War. German, Irish, and other European immigrants soon joined. The modern populace is unusually diverse, over eight percent foreign-born. While White and Black remain the largest ethnicities, Asians are now the third-largest group. Today's Columbians are remarkably highly educated and culturally midwestern, though traces of their Southern past remain. The city has been called the "Athens of Missouri" or "Athens of the West," a reference to its classic beauty and educational emphasis, but is more commonly called "CoMo." Residents of Columbia of referred to as "Columbians".
History.
The Columbia area was once part of the Mississippian culture and home to the Mound Builders. When European explorers arrived, the area was populated by the Osage and Missouri Indians. In 1678, La Salle claimed all of Missouri for France. The Lewis and Clark Expedition passed by the area on the Missouri River in early June 1804. In 1806, two sons of Daniel Boone established a salt lick 40 mi northwest of Columbia. giving the area its early name: Boonslick. The Boone's Lick Trail wound from St. Charles to the lick in present day Howard County. In 1818, a group of settlers, incorporated under the Smithton Land Company, purchased over 2000 acre and established the village of Smithton less than a mile from current day downtown Columbia. In 1821, the settlers moved, because of lack of water, across the Flat Branch to the plateau between the Flat Branch and Hinkson creeks in what is now the downtown district. They renamed the settlement Columbia—a poetic personification of the United States.
The roots of Columbia's three economic foundations—education, medicine, and insurance—can be traced back to incorporation in 1821. Original plans for the town set aside land for a state university. In 1833, Columbia Baptist Female College opened, which later became Stephens College. Columbia College (distinct from today's), later to become the University of Missouri, was founded in 1839. When the state legislature decided to establish a state university, Columbia raised three times as much money as any other competing city and James S. Rollins donated the land that is today the Francis Quadrangle. Soon other educational institutions were founded in Columbia such as Christian Female College, the first college for women west of the Mississippi, which later became the current Columbia College. The city benefited from being a stagecoach stop of the Santa Fe and Oregon trails, and later from the Missouri-Kansas-Texas Railroad. In 1822 the first hospital was set up by William Jewell. In 1830, the first newspaper began; in 1832, the first theater in the state was opened; and in 1835, the state's first agricultural fair was held. By 1839, the population (13,000) and wealth of Boone County was exceeded in Missouri only by that of St. Louis County, which at that time included the City of St. Louis.
Columbia's infrastructure was relatively untouched by the Civil War. Missouri, as a slave state, had Southern sympathies, but remained in the union. The majority of the city was pro-union, however, the surrounding agricultural areas of Boone County and the rest of central Missouri were decidedly pro-secession. Because of this, the University of Missouri became a base from which union troops operated. No battles were fought within the city because the presence of union troop dissuaded the confederate guerrillas from attacking, though several major battles occurred nearby at Boonville and Centralia. After the Civil War, race relations in Columbia followed the Southern pattern. In 1889, the city witnessed the lynching of an African American man, George Burke.
In 1963, Columbia become home to the headquarters of both the University of Missouri System, which today serves over 71,000 students, and the Columbia College system, which today serves about 25,000 students. The insurance industry also became important to the local economy as several companies established headquarters in Columbia, including Shelter Insurance, Missouri Employers Mutual, and Columbia Insurance Group. State Farm Insurance has a regional office in Columbia. In addition, the now defunct Silvey Insurance was once a large local employer. Columbia became a transportation crossroads when U.S. Route 63 and U.S. Route 40 (which became present-day Interstate 70) were routed through the city. Soon after the city opened the Columbia Regional Airport. The latter 20th century saw tremendous growth, and by 2000 the population was nearly 85,000 in the city proper.
In early 2006, Columbia embarked on a plan to manage the continued growth as the city passed 100,000 population. The city continues to grow, especially east around the newly opened Battle High School. The downtown district has maintained its status as a cultural center and is undergoing significant development in both residential and commercial sectors. The University of Missouri, which has tremendous economic impact on the city, experienced record enrollment in 2006 and is undertaking significant construction.
Geography.
Columbia, located in northern mid-Missouri, is 120 mi away from both St. Louis and Kansas City, and 29 mi north of the state capital Jefferson City. The city is near the Missouri River between the Ozark Plateau and the Northern Plains. Trees are mainly oak, maple, and hickory; common understory trees include eastern redbud, serviceberry, and flowering dogwood. Riparian areas are forested with mainly American sycamore. Much of the residential area of the city is planted with large native shade trees. In Autumn, the changing color of the trees is notable. Most species here are typical of the Eastern Woodland.
According to the United States Census Bureau, the city has a total area of 63.5 sqmi, of which, 63.08 sqmi is land and 0.42 sqmi is water.
Topography.
The city generally slopes from the highest point in the Northeast to the lowest point in the Southwest towards the Missouri River. Prominent tributaries of the river are Perche Creek, Hinkson Creek, and Flat Branch Creek. Along these and other creeks in the area can be found large valleys, cliffs, and cave systems such as that in Rock Bridge State Park just south of the city. These creeks are largely responsible for numerous stream valleys giving Columbia hilly terrain similar to the Ozarks while also having prairie flatland typical of northern Missouri. Columbia also operates several greenbelts with trails and parks throughout town.
Animal life.
Large mammals found in the city includes urbanized coyotes and numerous whitetail deer. Eastern gray squirrel, and other rodents are abundant, as well as cottontail rabbits and the nocturnal opossum and raccoon. Large bird species are abundant in parks and include the Canada goose, mallard duck, as well as shorebirds, including the great egret and great blue heron. Turkeys are also common in wooded areas and can occasionally be seen on the MKT recreation trail. Populations of bald eagles are found by the Missouri River. The city is on the Mississippi Flyway, used by migrating birds, and has a large variety of small bird species, common to the eastern U.S. The Eurasian tree sparrow, an introduced species, is limited in North America to the counties surrounding St. Louis. Columbia has large areas of forested and open land and many of these areas are home to wildlife.
Frogs are commonly found in the springtime, especially after extensive wet periods. Common species include the American toad and species of chorus frogs, commonly called "spring peepers" that are found in nearly every pond. Some years have outbreaks of cicadas or ladybugs. Mosquitos and houseflies are common insect nuisances; because of this, windows are nearly universally fitted with screens, and "screened-in" porches are common in homes of the area.
Climate.
Columbia has a climate marked by sharp seasonal contrasts in temperature, falling between a humid continental and humid subtropical climate (Köppen "Dfa/Cfa", respectively), and is located in USDA Plant Hardiness Zone 6a. The monthly daily average temperature ranges from 29.7 °F in January to 77.3 °F in July, while the high reaches or exceeds 90 °F on an average 32 days per year, 100 °F on 2.0 days, while 4.0 nights of sub-0 °F lows can be expected. Precipitation tends to be greatest and most frequent in the latter half of spring, when severe weather is also most common. Snow averages 18.0 in per season, mostly from December to March, with occasional November accumulation and falls in April being rarer; historically seasonal snow accumulation has ranged from 3.4 in in 2005–06 to 54.9 in in 1977–78. Extreme temperatures have ranged from −26 °F on February 12, 1899 to 113 °F on July 12 and 14, 1954. Readings of −10 °F or 105 °F are uncommon, the last occurrences being January 7, 2014 and July 31, 2012.
Cityscape.
Columbia's most commonly recognizable architectural attributes reside downtown and within the university campuses. Widely used icons of the city are the University of Missouri's Jesse Hall and the neo-gothic Memorial Union. The David R. Francis Quadrangle is an example of Thomas Jefferson's academic village concept. There are four historic districts listed on the National Register of Historic Places within the city: Downtown Columbia, East Campus Neighborhood, Francis Quadrangle, and North Ninth Street Historic District. The downtown skyline is relatively low and is dominated by the 10-story Tiger Hotel, and the 15-story Paquin Tower.
Downtown Columbia is an area of approximately one square mile surrounded by the University of Missouri on the south, Stephens College to the east and Columbia College on the north. The area serves as Columbia's financial and business district and is the topic of a large initiative to draw tourism, which includes plans to capitalize on the area's historic architecture, and Bohemian characteristics. 
The city's historic residential core lies in a ring around downtown, extending especially to the west along Broadway, and south into the East Campus neighborhoods. Columbia can be divided into roughly 36 neighborhoods and subdivisions. The city's most dense commercial areas are primarily located along Interstate 70, U.S. Route 63, Stadium Boulevard, Grindstone Parkway, and the downtown area.
Demographics.
2010 census.
As of the census of 2010, there were 108,500 people, 43,065 households, and 21,418 families residing in the city. The population density was 1720.0 PD/sqmi. There were 46,758 housing units at an average density of 741.2 /sqmi. The racial makeup of the city was 79.0% White, 11.3% African American, 0.3% Native American, 5.2% Asian, 0.1% Pacific Islander, 1.1% from other races, and 3.1% from two or more races. Hispanic or Latino of any race were 3.4% of the population.
There were 43,065 households of which 26.1% had children under the age of 18 living with them, 35.6% were married couples living together, 10.6% had a female householder with no husband present, 3.5% had a male householder with no wife present, and 50.3% were non-families. 32.0% of all households were made up of individuals and 6.6% had someone living alone who was 65 years of age or older. The average household size was 2.32 and the average family size was 2.94.
In the city the population was spread out with 18.8% of residents under the age of 18; 27.3% between the ages of 18 and 24; 26.7% from 25 to 44; 18.6% from 45 to 64; and 8.5% who were 65 years of age or older. The median age in the city was 26.8 years. The gender makeup of the city was 48.3% male and 51.7% female.
2000 census.
In 2000, the city had a day time population of 106,487. As of the census of 2000, there were 84,531 people, 33,689 households, and 17,282 families residing in the city. The population density was 1,592.8 people per square mile (615.0/km²). There were 35,916 housing units at an average density of 676.8 per square mile (261.3/km²). The racial makeup of the city was 81.5% White, 10.9% Black or African American, 0.4% Native American, 4.3% Asian, <0.1% Pacific Islander, 0.8% from other races, and 2.1% from two or more races. 2.1% of the population were Hispanic or Latino of any race.
There were 33,689 households out of which 26.1% had children under the age of 18 living with them, 38.2% were married couples living together, 10.3% had a female householder with no husband present, and 48.7% were non-families. 33.1% of all households were made up of individuals and 6.5% had someone living alone who was 65 years of age or older. The average household size was 2.26 and the average family size was 2.92.
In the city the population was spread out with 19.7% under the age of 18, 26.7% from 18 to 24, 28.7% from 25 to 44, 16.2% from 45 to 64, and 8.6% who were 65 years of age or older. The median age was 27 years. For every 100 females, there were 91.8 males. For every 100 females age 18 and over, there were 89.1 males.
The median income for a household in the city was $33,729, and the median income for a family was $52,288. Males had a median income of $34,710 versus $26,694 for females. The per capita income for the city was $19,507. About 9.4% of families and 19.2% of the population were below the poverty line, including 14.8% of those under age 18 and 5.2% of those age 65 or over. However, traditional measures of income and poverty can be misleading when applied to cities with high student populations, such as Columbia.
Economy.
Columbia's economy is historically dominated by education, healthcare, and the insurance industry. Jobs in government are also common, either in Columbia or a half-hour south in Jefferson City. Commutes into the city are also common and in 2000, the city had a day time population of 106,487. The Columbia Regional Airport and the Missouri River Port of Rocheport connect the region with trade and transportation. The University of Missouri is by far the city's largest employer.
The economy of Columbia's metro area is slightly larger than that of the Bahamas. With a Gross Metropolitan Product of $5.84 billion in 2004, Columbia's economy makes up 2.9% of the Gross State Product of Missouri. Insurance corporations headquartered in Columbia include Shelter Insurance, and the Columbia Insurance Group. Other organizations include MFA Incorporated, the Missouri State High School Activities Association, and MFA Oil. Companies such as Socket, Datastorm Technologies, Inc. (no longer existent), Slackers CDs and Games, Carfax, and MBS Textbook Exchange were founded in Columbia.
Top employers.
According to Columbia's 2012 Comprehensive Annual Financial Report, the top employers in the city are:
Culture.
The Missouri Theatre Center for the Arts and Jesse Auditorium are Columbia's largest fine arts venues. Ragtag Cinema annually hosts the well-known True/False Film Festival. In 2008, filmmaker Todd Sklar completed Box Elder, which was filmed entirely in and around Columbia and the University of Missouri. The University of Missouri's Museum of Art and Archaeology displays 14,000 works of art and archaeological objects in five galleries for no charge to the public. Libraries include the Columbia Public Library, the University of Missouri Libraries, with over three million volumes in Ellis Library, and the State Historical Society of Missouri. The "We Always Swing" Jazz Series and the Roots N Blues N BBQ Festival bring some of the country's finest Jazz and Blues to Columbia and Central Missouri. One of the last remaining traditional arcades in the country, Gunther's Games, is a popular destination for gamers.
Columbia has a flourishing and progressive music scene thanks in large part to many acts that come out of the University. The indie band White Rabbits was formed while the members were students at the University of Missouri before moving to Brooklyn to record and gain a higher profile. Musical artists from Columbia have been compiled by Painfully Midwestern Records with the ComoMusic Anthology series, and the "Das Kompilation" release. Although the hip genre continues to give Columbia some music recognition, it is their progressive psychedelic-heavy metal music scene that has garnered some attention lately. There are also local punk and hip-hop scenes that are gaining momentum locally. Country music singer-songwriter Brett James is also a native of Columbia. The song "Whiskey Bottle," by Uncle Tupelo, is rumored to be about the city of Columbia as it makes specific reference to a sign which used be displayed on a Columbia tackle shop sign which read, "Liquor, Guns, and Ammo." The sign is now displayed at the downtown location of Shakespeare's Pizza.
Sports.
The University of Missouri's sports teams, the Missouri Tigers, play a significant role in the city's sports culture. Faurot Field at Memorial Stadium capacity 71,168, is host to both home football games and concerts. The Hearnes Center and Mizzou Arena are two other large sport and event venues, the latter being the home arena for Mizzou's basketball team. Taylor Stadium is host to the their baseball team and was the regional host for the 2007 NCAA Baseball Championship. Columbia College has several men and women collegiate sports teams as well. In 2007, Columbia hosted the National Association of Intercollegiate Athletics Volleyball National Championship, which the Lady Cougars participated in.
Columbia also hosts the Show-Me State Games, a non-profit program of the Missouri Governor's Council on Physical Fitness and Health. They are the largest state games in the United States. These games consist of 26—28,000 Missouri amateur athletes (35,000 total) of all ages and ability levels who compete in the Olympic-style sports festival during July and August every year. It recently made ESPN's list of "101 Things All Sports Fans Must Experience Before They Die".
Situated midway between St. Louis and Kansas City, Columbians will often have allegiances to the professional sports teams housed there, such as the St. Louis Cardinals, the Kansas City Royals, the St. Louis Rams, the Kansas City Chiefs, and the St. Louis Blues.
The NRA Bianchi Cup is held in Columbia every year. It is among the most lucrative of all the shooting sports championships.
Media.
The city has two daily newspapers: the "Columbia Missourian" in the morning and the "Columbia Daily Tribune" in the afternoon. The "Missourian" is directed by professional editors and staffed by Missouri School of Journalism students who do reporting, design, copy editing, information graphics, photography and multimedia. The "Missourian" is associated with the Spanish-English bilingual publication "Adelante!" and "Vox" magazine. With a daily circulation of nearly 20,000, the "Daily Tribune" is the most widely read newspaper in central Missouri. The University of Missouri has the independent but official student newspaper called "The Maneater", which is printed bi-weekly. The now-defunct "Prysms Weekly" was also published in Columbia. In Fall 2009, KCOU News launched full operations out of KCOU 88.1FM on the MU Campus. The entirely student-run news organization airs a daily newscast, "The Pulse", weekdays from 4:30 to 5:30 p.m.
The city has 14 radio stations and 4 television channels.
Government and politics.
The City of Columbia's current government was established by a home rule charter adopted by voters on November 11, 1974, which established a Council-manager government that invested power in the City Council. The City Council is made up of seven members – six elected by each of Columbia's six wards, plus an at-large council member, the Mayor, who is elected by all city voters. The mayor currently receives a $9,000 annual stipend and the six remaining council members receive a $6,000 annual stipend. They are elected to staggered three-year terms. The Mayor, in addition to being a voting member of the City Council, is recognized as the head of city government for ceremonial purposes. Chief executive authority is invested in a city manager, who oversees the day-to-day operations of government.
Columbia is the county seat of Boone County, and the county court and government center are located there. The City is located in the fourth U.S. Congressional district. The 19th Missouri State Senate district covers all of Boone County. There are five Missouri House of Representatives districts (9, 21, 23, 24, 25) in the city. Columbia is home to a plethora of attorneys and serves as a legal hub and testing grounds for many new laws and grassroot efforts. The principal law enforcement agency is the Columbia Police Department, with the Columbia Fire Department providing fire protection. The University of Missouri Police Department patrols areas on and around the MU campus and has jurisdiction throughout the city and Boone County. The Public Service Joint Communications Center coordinates efforts between the two organizations as well as the Boone County Fire Protection District which operates Urban Search and Rescue Missouri Task Force 1.
The population generally supports progressive causes such as the extensive city recycling programs and the decriminalization of cannabis both for medical and recreational use at the municipal level (though the scope of latter of the two cannabis ordinances has since been restricted). The city is also one of only four in the state to offer medical benefits to same-sex partners of city employees. The new health plan also extends health benefits to unmarried heterosexual domestic partners of city employees. On October 10, 2006, the City Council approved an ordinance to prohibit smoking in public places, including restaurants and bars. The ordinance was passed with protest, and several amendments to the ordinance reflect this. Today's Columbians are unusually highly educated; over half of citizens possess at least a bachelor's degree, while over a quarter hold a graduate degree, making it the thirteenth most highly educated municipality in the United States.
Education.
Columbia and much of the surrounding area lies within The Columbia Public School District. The district enrolls over 17,000 students and had a revenue of nearly $200 million for the 2007–2008 school year. It is above the state average in attendance percentage and in graduation rate. The city operates four public high schools which cover grades 9–12: David H. Hickman High School, Rock Bridge High School, Muriel Battle High School, and Frederick Douglass High School. Rock Bridge is one of two Missouri high schools to receive a silver medal by U.S. News & World Report, putting it in the top 3% of all high schools in the nation. Hickman has been on Newsweek magazine's list of top 1,300 schools in the country for the past three years and has more named presidential scholars than any other public high school in the United States. There are also several private high schools including: Christian Fellowship School, Columbia Independent School, Heritage Academy, Christian Chapel Academy, and the newly constructed Father Augustine Tolton Regional Catholic High School.
The city has three institutions of higher education: the University of Missouri, Stephens College, and Columbia College all of which surround Downtown Columbia. The city is the headquarters of the University of Missouri System, which also operates campuses in St. Louis, Kansas City, and Rolla. The University of Missouri was founded in 1839 as the first state university west of the Mississippi River. Stephens College prepares students to become leaders and innovators in a rapidly changing world, and engages lifelong learners in an educational experience characterized by intellectual rigor, creative expression and professional practice as well as offering innovative, career-focused programs sound in the liberal arts with focuses on creative arts and sciences. Columbia College offers day and evening classes on its Columbia Campus, extension courses through its 34 nationwide campuses, and ties with U.S. military bases (including Guantanamo Bay, Cuba), and online courses.
Infrastructure.
Transportation.
The Columbia Transit provides public bus and para-transit service, and is owned and operated by the city. In 2008, 1,414,400 passengers boarded along the system's six fixed routes and nine University of Missouri shuttle routes, and 27,000 boarded the Para-transit service. The system is constantly experiencing growth in service and technology. A$3.5 million project to renovate and expand the Wabash Station, a rail depot built in 1910 and converted into the city's transit center in the mid-1980s, was completed in summer of 2007. In 2007, a Transit Master Plan was created to address the future transit needs of the city and county with a comprehensive plan to add infrastructure in three key phases. The five to 15-year plan intends to add service along the southwest, southeast and northeast sections of Columbia and develop alternative transportation models for Boone County.
Fares are $1.50 for adults, and $.75 for children 5–11, for students with valid I.D, for handicapped/Medicare recipients, and for senior citizens age 65 and up. Columbia Transit offers "FASTPass" electronic fare cards and issues electronic transfers for accuracy and convenience. Para-transit fares are $2.00 for a one-way trip, and the service area includes all of Columbia. Buses operate Monday through Saturday, from 6:25am to 6:25pm Monday-Wednesday, 6:25am to 10:25pm Thursday and Friday, and from 1:00am to 7:30pm on Saturday. Buses do not operate on Sunday.
The city's former mayor, Darwin Hindman, is largely in favor of a non-motorized transportation system, and can often be seen riding his bicycle around the city. Columbia is also known for its MKT Trail, a spur of the Katy Trail State Park, which allows foot and bike traffic across the city, and, conceivably, the state. It consists of a soft gravel surface, excellent for running and biking. Columbia also is preparing to embark on construction of several new bike paths and street bike lanes thanks to a $25 million grant from the federal government. The city is also served by American Air Lines at the Columbia Regional Airport, the only commercial airport in mid-Missouri.
I-70 (concurrent with US 40) and US 63 are the two main freeways used for travel to and from Columbia. Within the city, there are also three state highways: Routes 763 (Rangeline St & College Ave), 163 (Providence Rd), and 740 (Stadium Blvd).
Rail service is provided by the city-owned Columbia Terminal (COLT) Railroad, which runs from the north side of Columbia to Centralia and a connection to the Norfolk Southern Railway.
Health systems.
Health care is a big part of Columbia's economy, with nearly one in six people working in a health-care related profession and a physician density that is about three times the United States average. Columbia's hospitals and supporting facilities are a large referral center for the state, and medical related trips to the city are common. There are three hospital systems within the city and five hospitals with a total of 1,105 beds. 
The University of Missouri Health Care operates three hospitals in Columbia: the University of Missouri Hospital, the University of Missouri Women's and Children's Hospital, and the Ellis Fischel Cancer Center. Boone Hospital Center is administered by BJC Healthcare and operates several clinics as well as outpatient locations. The Harry S. Truman Memorial Veterans' Hospital, located next to the University Hospital, is administered by the United States Department of Veterans Affairs.
There are also a large number of medically-related industries in Columbia. The University of Missouri School of Medicine uses university-owned facilities as teaching hospitals. The University of Missouri Research Reactor Center is the largest research reactor in the United States and produces radioisotopes used in nuclear medicine. The center serves as the sole supplier of the active ingredients in two U.S. Food and Drug Administration-approved radiopharmaceuticals and produces Fluorine-18 used in PET imaging with its cyclotron.
Sister cities.
In accordance with the Columbia Sister Cities Program, which operates in conjunction with Sister Cities International Columbia has been paired with five international sister cities in an attempt to foster cross-cultural understanding:

</doc>
<doc id="6720" url="http://en.wikipedia.org/wiki?curid=6720" title="Charlton Athletic F.C.">
Charlton Athletic F.C.

Charlton Athletic Football Club is an English football club based in Charlton in the Royal Borough of Greenwich, London. They currently play in the Football League Championship.
The club was founded on 9 June 1905. This was when a number of youth clubs in the south east London area, including East Street Mission and Blundell Mission combined to form Charlton Athletic. The club play at The Valley in Charlton, where they have played since 1919, apart from one year in Catford, during 1923–24, and seven years at Crystal Palace and West Ham United between 1985 and 1992. Charlton share local South London derbies with rivals Millwall and Crystal Palace.
The club's traditional kit consists of red shirts, white shorts and red socks and their most commonly used nickname is The Addicks. Charlton turned professional in 1920 and first entered the Football League in 1921. Since then they have had four separate periods in the top flight of English football: 1936–1957, 1986–1990, 1998–1999 and 2000–2007. Historically, Charlton's most successful period was the 1930s, when the club's highest league finishes were recorded, including runners-up of the First Division in 1937. After World War II, the club reached the FA Cup Final twice, losing in 1946 and winning in 1947.
History.
Early history.
Charlton Athletic F.C. were formed on 9 June 1905 by a group of 15- to 17-year-old boys in an area of Charlton which is no longer residential, near to the present-day site of the Thames Barrier. Charlton spent the years before the First World War playing in local leagues. They became a senior side by joining the Lewisham League. After the war, they joined the Kent League for one season (1919–20) before becoming professional, appointing Walter Rayner as the first full-time manager. They were accepted by the Southern League and played just a single season (1920–21) before being voted into the Football League. Charlton's first Football League match was against Exeter City in August 1921, which they won 1–0. In 1923 it was proposed that Charlton merge with Catford Southend to create a larger team with bigger support. In the 1923–24 season Charlton played in Catford at The Mount stadium and wore the colours of "The Enders", light and dark blue vertical stripes. However, the move fell through and the Addicks returned to the Charlton area in 1924, returning to the traditional red and white colours in the process. Charlton finished second bottom in the Football League in 1926 and were forced to apply for re-election which was successful. Three years later the Addicks won the Division Three championship in 1929<ref name="England 1928/29"></ref> and they remained at the Division Two level for four years. After relegation into the Third Division south at the end of the 1932/33 season the club appointed Jimmy Seed as manager and he oversaw the most successful period in Charlton's history either side of the Second World War. Seed, an ex-miner who had made a career as a footballer despite suffering the effects of poison gas in the First World War, remains the most successful manager in Charlton's history. He is commemorated in the name of a stand at the Valley. Seed was an innovative thinker about the game at a time when tactical formations were still relatively unsophisticated. He later recalled "a simple scheme that enabled us to pull several matches out of the fire" during the 1934–35 season: when the team was in trouble "the centre-half was to forsake his defensive role and go up into the attack to add weight to the five forwards." The organisation Seed brought to the team proved effective and the Addicks gained successive promotions from the Third Division to the First Division between 1934 and 1936. Charlton finally secured promotion to the First Division by beating local rivals West Ham in front of 41,254 fans at the Valley, with their centre-half John Oakes playing on despite concussion and a broken nose.
In 1937, Charlton finished runners up in the First Division,<ref name="1936/1937 English Division 1 (old) Table"></ref> in 1938 finished fourth<ref name="1937/1938 English Division 1 (old) Table"></ref> and 1939 finished third.<ref name="1938/1939 English Division 1 (old) Table"></ref> They were the most consistent team in the top flight of English football over the three seasons immediately before the Second World War. This continued during the war years and they won the "war" cup and appeared in finals.
Post-war history.
Charlton reached the 1946 FA Cup Final, but lost 4–1 to Derby County at Wembley. Charlton's Bert Turner scored an own goal in the eightieth minute before equalising for the Addicks a minute later to take them into extra time, but they conceded three further goals in the extra period. When the full league programme resumed in 1946–47 Charlton could finish only 19th in the First Division, just above the relegation spots, but they made amends with their performance in the FA Cup, reaching the 1947 FA Cup Final. This time they were successful, beating Burnley 1–0, with Chris Duffy scoring the only goal of the day. In this period of renewed football attendances, Charlton became one of only thirteen English football teams to average over 40,000 as their attendance during a full season. The Valley was the largest football ground in the League, drawing crowds in excess of 70,000. However, in the 1950s little investment was made either for players or to The Valley, hampering the club's growth. In 1956, the then board undermined Jimmy Seed and then sacked, and Charlton were relegated the following year.
From the late 1950s until the early 1970s, Charlton remained a mainstay of the Second Division before relegation to the Third Division in 1972<ref name="England 1971/72"></ref> caused the team's support to drop, and even a promotion in 1975 back to the second division<ref name="England 1974/75"></ref> did little to re-invigorate the team's support and finances. In 1979–80 Charlton were relegated again to the Third Division,<ref name="England 1979/80"></ref> but won immediate promotion back to the Second Division in 1980–81.<ref name="England 1980/81"></ref> Even though it did not feel like it, this was a turning point in the club's history leading to a period of turbulence and change including further promotion and exile. A change in management and shortly after a change in club ownership led to severe problems, such as the reckless signing of former European Footballer of the Year Allan Simonsen, and the club looked like it would go out of business.
The "wilderness" years.
In 1984 financial matters came to a head and the club went into administration, to be reformed as Charlton Athletic (1984) Ltd. But the club's finances were still far from secure, and they were forced to leave the Valley just after the start of the 1985–86 season, in the wake of the Bradford City stadium fire after its safety was criticised by Football League officials.
The club began to groundshare with Crystal Palace at Selhurst Park and this arrangement looked to be for the long-term, as Charlton did not have enough funds to revamp the Valley to meet safety requirements.
Despite the move away from the Valley, Charlton were promoted to the First Division as Second Division runners-up at the end of 1985–86,<ref name="England 1985/86"></ref> and remained at this level for four years (achieving a highest league finish of 14th) often with late escapes, most notably against Leeds in 1987, where the Addicks triumphed in extra-time of the play-off final replay to secure their top flight place. In 1987 Charlton also returned to Wembley for the first time since the 1947 FA Cup final for the Full Members Cup final against Blackburn.
Eventually, Charlton were relegated in 1990 along with Sheffield Wednesday and bottom club Millwall. Manager Lennie Lawrence remained in charge for one more season before he accepted an offer to take charge of Middlesbrough. He was replaced by joint player-managers Alan Curbishley and Steve Gritt. The pair had unexpected success in their first season finishing just outside the play-offs, and 1992–93 began promisingly and Charlton looked good bets for promotion in the new Division One (the new name of the old Second Division following the formation of the Premier League). However, the club was forced to sell players such as Rob Lee to help pay for a return to The Valley, which eventually happened in December 1992.
There was a tragedy at the club late in the 1992–93 season. Defender Tommy Caton, who had been out of action due to injury since January 1991, announced his retirement from playing on medical advice in March 1993 having failed to recover full fitness, and he died suddenly at the end of the following month at the age of 30.
Back to The Valley.
In 1995, new chairman Richard Murray appointed Alan Curbishley as sole manager of Charlton. Under his sole leadership Charlton made an appearance in the playoffs in 1996 but were eliminated by Crystal Palace in the semi-finals and the following season brought a disappointing 15th-place finish. 1997–98 was Charlton's best season for years. They reached the Division One playoff final and battled against Sunderland in a thrilling game which ended with a 4–4 draw after extra time. Charlton won 7–6 on penalties, with the match described as "arguably the most dramatic game of football in Wembley's history", and were promoted to the Premier League.
Charlton's first Premier League campaign began promisingly (they went top after two games) but they were unable to keep up their good form and were soon battling relegation. The battle was lost on the final day of the season but the club's board kept faith in Curbishley, confident that they could bounce back. Curbishley rewarded the chairman's loyalty with the Division One title in 2000 which signalled a return to the Premier League.
After the club's return, Curbishley proved an astute spender and by 2003 he had succeeded in establishing Charlton in the top flight. Charlton spent much of the 2003–04 Premier League season challenging for a Champions League place, but a late-season slump in form and the sale of star player Scott Parker to Chelsea, left Charlton in 7th place, which was still the club's highest finish since the 1950s. Charlton failed to build on this level of achievement and Curbishley departed in 2006, with the club still established as a solid mid-table side.
In May 2006, Iain Dowie was named as Curbishley's successor, but was sacked after twelve league matches in November 2006, with only two wins. Les Reed replaced Dowie as manager, however he too failed to improve Charlton's position in the league table and on Christmas Eve 2006, Reed was replaced by former player Alan Pardew. Although results did improve, Pardew was unable to keep Charlton up and relegation was confirmed in the penultimate match of the season.
Relegation.
Charlton's return to the second tier of English football was a disappointment, with their promotion campaign tailing off to an 11th-place finish. Early in the following season the Addicks were linked with a foreign takeover, but this was swiftly denied by the club. On 10 October 2008 Charlton received an indicative offer for the club from a Dubai-based diversified investment company. However, the deal later fell through. The full significance of this soon became apparent as the club recorded net losses of over £13 million for that financial year.
On 22 November 2008 Charlton suffered a 2–5 loss to Sheffield United at home, which meant that the club had gone eight successive games without a win and had slipped into the relegation zone—particularly disastrous considering they were among the pre-season favourites for promotion. Hours after the game, Alan Pardew left Charlton by mutual consent. Matters did not improve under caretaker manager Phil Parkinson, and a 3–1 defeat at Sheffield United saw the Addicks four points adrift at the bottom of the Championship as 2009 dawned, under threat of their first relegation to English football's third tier for 29 years. Charlton continued their poor run of form to go 18 games without a win, a new club record, before finally achieving a 1–0 away victory over Norwich City in an FA Cup Third Round replay. They then went on to beat Crystal Palace 1–0 at the Valley on 27 January to achieve their first league win under Phil Parkinson, whose contract was made permanent despite the lack of progress in the league. Charlton's relegation from the Championship was all but confirmed on Easter Monday (13 April) when, despite picking up a point in a 0–0 draw at Coventry, they found themselves 12 points from safety with four games remaining. With a vastly inferior goal difference and with the two teams directly above them (Southampton and Nottingham Forest) still having to play each other, it was effectively an impossible task for Charlton to avoid relegation. The following game saw Charlton's relegation to League One become a reality after a 2–2 draw against Blackpool.
League One.
After spending almost the entire 2009/2010 season in the top six of League One, Charlton were defeated in the Football League One play-offs semi-final second leg on penalties to Swindon Town, condemning Charlton to another season in the third tier of English Football.
Parkinson had spent less than any other manager on purchasing players since Lennie Lawrence in the 1980s and was able to maintain a top six status despite only having the opportunity to bring in lower level players on loan. At that time, Charlton went through a change in ownership. The new owners decided to remove both Parkinson and Charlton legend Mark Kinsella after a poor run of results, intending to replace them with an as yet unknown team. Another Charlton legend, Chris Powell was appointed manager of the club in January 2011, winning his first game in charge 2–0 over Plymouth at the Valley, Charlton's first win since November. Powell's bright start continued with a further three victories, before running into a dreadful downturn which saw the club go 11 games in succession without a win. Yet the fans' respect for Powell saw him come under remarkably little criticism. The club's fortunes picked up towards the end of the season, but leaving them far short of the playoffs. In a busy summer, Powell brought in 19 new players and after a successful season, on 14 April 2012, Charlton Athletic won promotion back to the Championship with a 1–0 away win at Carlisle United. A week later, on 21 April 2012, they were confirmed as Champions after a 2–1 home win over Wycombe Wanderers. Charlton then lifted the League One trophy on 5 May 2012, having been in the top position since 15 September 2011, and after recording a 3–2 victory over Hartlepool United, recorded their highest ever league points score of 101, the highest in any professional European league that year.
Return to the Championship.
In the first season back to the Championship since the 2008–09 season, the 2012–13 season of the Championship saw Charlton finish ninth place with 65 points, just three points outside of the play-off places to the Premier League.
In early January 2014 during the 2013–14 season, Belgian businessman, Roland Duchâtelet took over Charlton Athletic football club and immediately after doing so brought in several new players from Belgian Pro League team, Standard Liege – another club he owns – such as Iranian international striker Reza Ghoochannejhad and former Liverpool player, Astrit Ajdarević. Charlton players Yann Kermorgant and Dale Stephens (footballer) left the club soon after. On 11 March 2014 – two days after a disappointing FA Cup quarter-final loss to Sheffield United, and with Charlton sitting bottom of the table, Chris Powell was sacked by Roland Duchâtelet, who was unhappy with Charlton's poor form all season. There was a suggestion Powell was sacked because he wouldn't entertain the owner's guidance in relation to team selection. New manager, Jose Riga, despite having to join Charlton late into the season, was able to help Charlton survive relegation with a 3–1 Home win against Watford and ended the season in 18th place with eight wins, three draws and six losses as Manager and thirteen wins, twelve draws, and twenty-one losses overall combined with Chris Powell's record this season.
The 2014-2015 season meant more upheaval at the club, with significant changes to the playing squad, and two different managers. The club finished in 12th position, after a strong end to the season ensured there was no danger of a relegation battle.
Stadium.
The club's first ground was Siemens Meadow (1905–1907), a patch of rough ground by the River Thames. This was over-shadowed by the now demolished Siemens Telegraph Works. Then followed Woolwich Common (1907–1908), Pound Park (1908–1913), and Angerstein Lane (1913–1915). After the end of the First World War, a chalk quarry known as the Swamps was identified as Charlton's new ground, and in the summer of 1919 work began to create the level playing area and remove debris from the site. The first match at this site, now known as the club's current ground The Valley, was in September 1919. Charlton stayed at The Valley until 1923, when the club moved to The Mount stadium in Catford as part of a proposed merger with Catford Southend Football Club. However, after this move collapsed in 1924 Charlton returned to The Valley.
During the 1930s and 40s, significant improvements were made to the ground, making it one of the largest in the country at that time. In 1938 the highest attendance to date at the ground was recorded at over 75,000 for a FA Cup match against Aston Villa. During the 1940s and 50s the attendance was often above 40,000, and Charlton had one of the largest support bases in the country. However, after the club's relegation little investment was made in The Valley as it fell into decline.
In the 1980s matters came to a head as the ownership of the club and The Valley was divided. The large East Terrace had been closed down by the authorities after the Bradford City stadium fire and the ground's owner wanted to use part of the site for housing. In September 1985, Charlton made the controversial move to ground-share with South London neighbours Crystal Palace at Selhurst Park. This move was unpopular with supporters and in the late 1980s significant steps were taken to bring about the club's return to The Valley.
A single issue political party, the Valley Party, contested the 1990 local Greenwich Borough Council elections on a ticket of reopening the stadium, capturing 11% of the vote, aiding the club's return. The Valley Gold investment scheme was created to help supporters fund the return to The Valley, and several players were also sold to raise funds. For the 1991–92 season and part of the 1992–93 season, the Addicks played at West Ham's Upton Park as Wimbledon had moved into Selhurst Park alongside Crystal Palace. Charlton finally returned to The Valley in December 1992, celebrating with a 1–0 victory against Portsmouth.
Since the return to The Valley, three sides of the ground have been completely redeveloped turning The Valley into a modern, all-seater stadium with a 27,111 capacity. There are plans in place to increase the ground's capacity to approximately 31,000 and even around 40,000 in the future.
The Covered End.
The Valley's North Stand which is known by locals as "The Covered End" to this day and is where the more vocal fans gather. The title comes from the original design of the north stand before it was redeveloped.
The Valley Club (CAFC Supporters Club) was situated in Harvey Gardens behind the North Stand, and was managed by licensee Ray Donn from 1970–1984 the club had a full club licence supplying food and drink to its members and guests during match days and live entertainment, with cabaret and dancing every night of the week. The Valley Club was one of the most popular club venues in South London at this time, featuring named entertainers popular today.
Supporters.
The bulk of the club's support base comes from the London Boroughs of Lewisham, Greenwich, Bexley and Bromley and also north-west Kent, As well as support from numerous countries around the world stretching as far as the U.S and Canada. Charlton benefited from the many former Londoners moving out to the new housing in the suburbs of Welling, Mottingham, Sidcup, and New Bexley in the early '30s, travelling to the ground via the tram from New Eltham to Lewisham and over the heath to Greenwich. Charlton were rare among football clubs, in that they reserved a seat on their directors' board for a supporter. Any season ticket holder could put themselves forward for election, with a certain number of nominations, and votes were cast by all season ticket holders over the age of 18. The last such director, Ben Hayes, was elected in 2006 to serve until 2008, when the role was discontinued as a result of legal issues. Its functions were replaced by a fans forum which met for the first time in December 2008 and is still active to this very day.
Nicknames.
Charlton's most common nickname is The Addicks. Among the theories on the origin of the Addicks name are that it was the south-east London pronunciation of either "haddock > ' addock" or "athletic". However, the most likely origin of name is from a local fishmonger, Arthur "Ikey" Bryan, who rewarded the team with meals of haddock and chips.
The progression of the nickname can be seen in the book "The Addicks Cartoons: An Affectionate Look into the Early History of Charlton Athletic", which covers the pre-First World War history of Charlton through a narrative based on 56 cartoons which appeared in the now defunct Kentish Independent. The very first cartoon, from 31 October 1908, calls the team the Haddocks. By 1910, the name had changed to Addicks although it also appeared as Haddick. The club has had two other nicknames, The Robins, adopted in 1931, and The Valiants, chosen in a fan competition in the 1960s which also led to the adoption of the sword badge which is still in use. The Addicks nickname never went away and was revived by fans after the club lost its Valley home in 1985 and went into exile at Crystal Palace. It is now once again the official nickname of the club.
Charlton fans' chants have included "Valley, Floyd Road", a song noting the stadium's address to the tune of "Mull of Kintyre", and "The Red, Red Robin".
In popular culture.
Charlton Athletic featured in the ITV one-off drama "Albert's Memorial", shown on 12 September 2010 and starring David Jason and David Warner. Jason's character, Harry, is revealed to be a Charlton Athletic fan. Harry later buries his friend Albert, played by Michael Jayston, draped in a Charlton Athletic flag, to which Frank says, "Hear that?". Harry replies, "No... What?" Frank says, "That's the sound of Albert turning in his grave. He hated Charlton Athletic." to which Harry replies, "Well, nobody's perfect."
In the long-running BBC sitcom "Only Fools and Horses", Rodney Trotter is named after the club.
Charlton's ground and the then manager, Alan Curbishley, made appearances in the Sky One TV series, Dream Team.
Charlton Athletic has also featured in a number of book publications, in both the realm of fiction and factual/sports writing. These include works by Charlie Connelly and Paul Breen's work of popular fiction which is entitled The Charlton Men, and published by Thames River Press, an imprint of Wimbledon Press. This book is a work of fiction set against Charlton's highly successful 2011/12 season when they won the League One title with a club record breaking 101 points.
Colours and crest.
Charlton have used a number of crests and badges during their history, although the current design has not been changed since 1968. The first known badge, from the 1930s, consisted of the letters CAF in the shape of a club from a pack of cards. In the 1940s, Charlton used a design featuring a robin sitting in a football within a shield, sometimes with the letters CAFC in the four-quarters of the shield, which was worn for the 1946 FA Cup Final. In the late 1940s and early 1950s, the crest of the former metropolitan borough of Greenwich was used as a symbol for the club but this was not used on the team's shirts.
In 1963, a competition was held to find a new badge for the club, and the winning entry was a hand holding a sword, which complied with Charlton's nickname of the time, the Valiants. Over the next five years modifications were made to this design, such as the addition of a circle surrounding the hand and sword and including the club's name in the badge. By 1968, the design had reached the one known today, and has been used continuously from this year, apart from a period in the 1970s when just the letters CAFC appeared on the team's shirts.
With the exception of one season, Charlton have always played in red and white. The colours had been chosen by the group of boys who had founded Charlton Athletic in 1905 after having to play their first matches in the borrowed kits of their local rivals Woolwich Arsenal, who also played in red and white. The exception came during the 1923–24 season when Charlton wore the colours of Catford Southend as part of the proposed move to Catford, which were light and dark blue stripes. However, after the move fell through, Charlton returned to wearing red and white as their home colours.
Rivalries.
Charlton's main rivals are Millwall and Crystal Palace.
Crystal Palace.
According to a survey conducted in 2012, Crystal Palace are considered to be Charlton's main rivals. The rivalry with Crystal Palace grew substantially in the mid 1980s, when the Addicks left their traditional home at The Valley because of safety concerns and played their home fixtures at The Eagles' Selhurst Park stadium. The ground-sharing arrangement – although seen by Crystal Palace chairman Ron Noades as essential for the future of football – was unpopular with both sets of fans. Indeed, the Charlton fans campaigned for a return to The Valley throughout the club's time at Selhurst Park.
Charlton left Selhurst Park in 1991, and the rivalry between the teams once again returned to a nominal level until two incidents 14 years later:
In 2005, having already lost 1–0 to Charlton at Selhurst Park earlier in the season, Crystal Palace were relegated at The Valley after a 2–2 draw. After the match there was a well publicised altercation between the two chairmen Richard Murray and Simon Jordan, which only served to renew old hostilities between the fans.
Then, in 2006, when Iain Dowie was appointed as Charlton manager just weeks after leaving the same post at Crystal Palace, tensions between the clubs grew still more. A writ was served on behalf of Palace chairman Simon Jordan claiming Dowie had breached their agreement, and that Dowie promised Jordan that he would move to a club in Northern England. Although legally this was a dispute between Jordan and Dowie, the case made headlines and relations between the two teams deteriorated once more.
Millwall.
The rivalry began when Millwall moved south of the river in 1910 to The Den in New Cross, South East London situated less than 4 miles from The Valley. Matches between the two sides are always fiercely contested.
Club officials.
Club officials as of 13 January 2011

</doc>
<doc id="6721" url="http://en.wikipedia.org/wiki?curid=6721" title="Cross-country skiing">
Cross-country skiing

Cross-country skiing is travel on skis over snow-covered terrain, whereby skiers rely on their own locomotion rather than on ski lifts or other forms of assistance. Modern cross-country skiing shares self-locomotion as a core attribute with the original form of skiing from which all skiing disciplines evolved, including alpine skiing, ski jumping and telemark skiing. It is widely practised as a sport and recreational activity, however some still use it as a means of transportation. Variants of cross-country skiing are adapted to a range of terrain which spans unimproved, sometimes mountainous terrain to groomed courses that are specifically designed for the sport. Skiers propel themselves either by striding forward (classic style) or side-to-side in a skating motion (skate skiing), aided by arms pushing on ski poles against the snow. It is practised in regions with snow-covered landscapes, including Northern Europe, Canada, Russia and regions in the United States.
Competitive cross-country skiing is one of the Nordic skiing sports. Cross-country skiing and rifle marksmanship are the two components of biathlon, ski-orienteering is a form of cross-country skiing, which includes map navigation along snow trails and tracks.
History.
The word ski comes from the Old Norse word "skíð" which means stick of wood or ski. Skiing started as a technique for traveling cross-country over snow on skis, starting almost five millennia ago with beginnings in Scandinavia. It may have been practised as early as 600 BCE in Daxing'anling, in what is now China. Early historical evidence includes Procopius' (around CE 550) description of Sami people as "skrithiphinoi" translated as "ski running samis". Birkely argues that the Sami people have practiced skiing for more than 6000 years, evidenced by the very old Sami word "čuoigat" for skiing. Egil Skallagrimsson's 950 CE saga describes King Haakon the Good's practice of sending his tax collectors out on skis. The Gulating law (1274) stated that "No moose shall be disturbed by skiers on private land." Cross-country skiing evolved from a utilitarian means of transportation to being a world-wide recreational activity and sport, which branched out into other forms of skiing starting in the mid-1800s. Early skiers used one long pole or spear in addition to the skis. The first depiction of a skier with two ski poles dates to 1741.
Transportation.
Ski warfare, the use of ski-equipped troops in war, is first recorded by the Danish historian Saxo Grammaticus in the 13th century, who were reportedly able to cover distances comparable to that of light cavalry. The garrison in Trondheim used skis at least from 1675, and the Danish-Norwegian army included specialized skiing battalions from 1747—details of military ski exercises from 1767 are on record. Skis were used in military exercises in 1747. In 1799 French traveller Jacques de la Tocnaye recorded his visit to Norway in his travel diary: Norwegian immigrants used skis ("Norwegian snowshoes") in the US midwest from around 1836. Norwegian immigrant "Snowshoe Thompson" transported mail by skiing across the Sierra Nevada between California and Nevada from 1856. In 1888 Norwegian explorer Fridtjof Nansen and his team crossed the Greenland icecap on skis. Norwegian workers on the Buenos Aires - Valparaiso railway line introduced skiing in South America around 1890. In 1910 Roald Amundsen used skis on his South Pole Expedition. In 1902 the Norwegian consul in Kobe imported ski equipment and introduced skiing to the Japanese, motivated by the death of Japanese soldiers during snow storm.
Sport.
Norwegian skiing regiments organized military skiing contests in the 18th century, divided in four classes: shooting at a target while skiing at "top speed," downhill racing among trees, downhill racing on large slopes without falling, and "long racing" on "flat ground." An early record of a public ski competition occurred in Tromsø, 1843. In Norwegian, "langrenn" refers to "competitive skiing where the goal is to complete a specific distance in groomed tracks in the shortest possible time." In Norway, "ski touring competitions" (Norwegian: "turrenn") are long-distance cross-country competitions open to the public, competition is usually within age intervals.
A new technique, skate skiing, was experimented with early in the 20th Century, but was not widely adapted until the 1980s. Johan Grøttumsbråten used the skating technique at the 1931 World Championship in Oberhof, one of the earliest recorded use of skating in competitive cross-country skiing. This technique was later used in ski orienteering in the 1960s on roads and other firm surfaces. It became widespread during the 1980s after the success of Bill Koch (USA) in 1982 Cross-country Skiing Championships drew more attention to the skating style. Norwegian skier Ove Aunli started using the technique in 1984, when he found it to be much faster than classic style. Finnish skier, Pauli Siitonen, developed a one-sided variant of the style in the 1970s, leaving one ski in the track while skating to the side with the other one during endurance events; this became known as the "marathon skate."
Terminology.
While the noun "ski" originates from the Norwegian language, unlike the English "skiing" there is no corresponding verb in Norwegian. Fridtjov Nansen, for instance, describes the crossing of Greenland as "På ski over Grønland", literally "On skis across Greenland", while the English edition of the report read "The first crossing of Greenland". Nansen referred to the activity of traversing snow on skis as Norwegian: "skilöbning" (he used the term also in the English translation), which may be translated as "ski running". Nansen used "skilöbning" about all forms of skiing, but noted that ski jumping is purely a competitive sport and not for amateurs. He further noted that in some competitions the skier "is also required to show his skill in turning his ski to one side or the other within given marks" at full speed on a steep hill. Nansen regarded these forms (i.e., jumping and slalom) as "special arts", and believed that the most important branch of skiing was travel "in an ordinary way across the country." In Germany, Nansen's Greenland report was published as German: "Auf Schneeschuhen durch Grönland" (literally "On snowshoes through Greenland"). The term German: "Schneeschuhe" was replaced by the Norwegian word "Ski" in late 19th century. The Norwegian encyclopedia of sports also uses the term "ski running" (Norwegian: "skiløping") about all forms of skiing. Around 1900 the word "Skilaufen" was used in German in the same sense as Norwegian: "skiløping". In modern Norwegian, self-propelled skiing is often referred to as Norwegian: "gå på ski" (ski hiking) whereas alpine skiing is referred to as Norwegian: "stå på ski" (literally "ski standing", but also translated as "skiing"). Kirkeby translates "cross-country skiing" as Norwegian: "turgåing på ski" (literally "hiking on skis"), whereas "cross-country (skiing) race" as Norwegian: "langrenn".
Recreation.
Recreational cross-country skiing includes ski touring and groomed-trail skiing, typically at resorts or in parklands. It is an accessible form of recreation for persons with vision and mobility impairments. A related form of recreation is dog skijoring—a winter sport where a cross-country skier is assisted by one or more dogs.
Ski touring.
Ski touring is done off-piste and outside of ski resorts. Tours may extend over multiple days. Typically, skis, bindings, and boots allow for free movement of the heel to enable a walking pace, as with Nordic disciplines and unlike Alpine skiing. Ski touring's subgenre ski mountaineering involves independently navigating and route finding through potential avalanche terrain, and often requires familiarity with meteorology along with skiing skills. Ski touring can also be faster and easier than summer hiking in some terrain allowing for traverses and ascents that would be harder in the summer. Skis can also be used to access backcountry alpine climbing routes when snow is off the technical route, but still covers the hiking trail.In some countries, organizations maintain a network of huts for use by cross-country skiers in wintertime. For example, the Norwegian Trekking Association maintains over 400 huts stretching across thousands of kilometres of trails which are used by hikers in the summer and by skiers in the winter.
Groomed-trail skiing.
Groomed trail skiing occurs at facilities, such as Royal Gorge Cross Country Ski Resort and Gatineau Park, Quebec, where trails are laid out and groomed for both classic and skate-skiing. Such grooming and track setting (for classic technique) requires specialized equipment and techniques that adapt to the condition of the snow. Trail preparation employs snow machines that tow snow compaction, texturing and track-setting devices. Groomers must adapt such equipment to the condition of the snow—crystal structure, temperature, degree of compaction, moisture content, etc. Depending on the initial condition of the snow, grooming may achieve an increase in density for new-fallen snow or a decrease in density for icy or compacted snow. Cross-country ski facilities may incorporate a course design that meets homologation standards for such organizations as the International Olympic Committee, the International Ski Federation or national standards. Standards address course distances, degree of difficulty with maximums in elevation difference and steepness—both up and downhill, plus other factors.
Competition.
Cross-country ski competition encompasses a variety of formats for races over courses of varying lengths according to rules sanctioned by the International Ski Federation (FIS) and by national organizations, such as the U.S. Ski and Snowboard Association and Cross Country Ski Canada. It also encompasses cross-country ski marathon events, sanctioned by the Worldloppet Ski Federation, cross-country ski orienteering events, sanctioned by the International Orienteering Federation, and Paralympic cross-country skiing, sanctioned by the International Paralympic Committee.
FIS-sanctioned competition.
The FIS Nordic World Ski Championships have been held in various numbers and types of events since 1925 for men and since 1954 for women. From 1924 to 1939, the World Championships were held every year, including the Winter Olympic Games. After World War II, the World Championships were held every four years from 1950 to 1982. Since 1985, the World Championships have been held in odd-numbered years. Notable cross-country ski competitions include the Winter Olympics, the FIS Nordic World Ski Championships, and the FIS World Cup events (including the Holmenkollen).
Other sanctioned competition.
Cross-country ski marathons—races with distances greater than 40 kilometers—have their own cup series called Swix Ski Classics, which started in 2011. Skiers race in classic or free-style (skating) events, depending on the rules of the race. Notable ski marathons, include the "Vasaloppet" in Sweden, "Birkebeineren" in Norway, the Engadin Skimarathon in Switzerland, the American Birkebeiner, the Tour of Anchorage in Anchorage, Alaska, and the Boreal Loppet, held in Forestville, Quebec, Canada.
Biathlon combines cross-country skiing and rifle shooting. Depending on the shooting performance, extra distance or time is added to the contestant's total running distance/time. For each shooting round, the biathlete must hit five targets; the skier receives a penalty for each missed target, which varies according to the competition rules.
Ski orienteering is a form of cross-country skiing competition that requires navigation in a landscape, making optimal route choices at racing speeds. Standard orienteering maps are used, but with special green overprinting of trails and tracks to indicate their navigability in snow; other symbols indicate whether any roads are snow-covered or clear. Standard skate-skiing equipment is used, along with a map holder attached to the chest. It is one of the four orienteering disciplines recognized by the International Orienteering Federation. Upper body strength is especially important because of frequent double poling along narrow snow trails.
Paralympic cross-country ski competition is an adaptation of cross-country skiing for athletes with disabilities. Paralympic cross-country skiing includes standing events, sitting events (for wheelchair users), and events for visually impaired athletes under the rules of the International Paralympic Committee. These are divided into several categories for people who are missing limbs, have amputations, are blind, or have any other physical disability, to continue their sport.
Techniques.
Cross-country skiing has two basic propulsion techniques, which apply to different surfaces: classic (undisturbed snow and tracked snow) and skate skiing (firm, smooth snow surfaces). The classic technique relies on a wax or texture on the ski bottom under the foot for traction on the snow to allow the skier to slide the other ski forward in virgin or tracked snow. With the skate skiing technique a skier slides on alternating skis on a firm snow surface at an angle from each other in a manner similar to ice skating. Both techniques employ poles with baskets that allow the arms to participate in the propulsion. Specialized equipment is adapted to each technique and each type of terrain. A variety of turns are used, when descending.
Both poles can be used simultaneously ("double-poling"), or alternating, in classic the alternating technique is most common (the "diagonal stride") while in the skating technique double poles are more common.
Classic.
The classic style is often used on prepared trails (pistes) that have pairs of parallel grooves (tracks) cut into the snow. It is also the most usual technique where no tracks have been prepared. With this technique, each ski is pushed forward from the other stationary ski in a striding and gliding motion, alternating foot to foot. With the "diagonal stride" variant the poles are planted alternately on the opposite side of the forward-striding foot; with the "kick-double-pole" variant the poles are planted simultaneously with every other stride. At times, especially with gentle descents, double poling is the sole means of propulsion. On uphill terrain, techniques include the "side step" for steep slopes, moving the skis perpendicular to the fall line, the "herringbone" for moderate slopes, where the skier takes alternating steps with the skis splayed outwards, and, for gentle slopes, the skier uses the diagonal technique with shorter strides and greater arm force on the poles.
Skate skiing.
With skate skiing, the skier provides propulsion on a smooth, firm snow surface by pushing alternating skis away from one another at an angle, in a manner similar to ice skating. Skate-skiing usually involves a coordinated use of poles and the upper body to add impetus, sometimes with a double pole plant each time the ski is extended on a temporarily "dominant" side ("V1") or with a double pole plant each time the ski is extended on either side ("V2"). Skiers climb hills, using these techniques, by widening the angle of the "V" and by making more frequent, shorter strides and more forceful use of poles. A variant of the technique is the "marathon skate," where the skier leaves one ski in the track while skating outwards to the side with the other ski.
Turns.
Turns, used while descending or for braking, include the snowplough (or "Wedge Turn"), the stem Christie (or "Wedge Christie"), parallel turn, and the Telemark turn. The step turn is used for maintaining speed during descents or out of track on flats.
Equipment.
Equipment comprises skis, poles, boots and bindings; these vary according to:
Skis.
Skis used in cross-country are lighter and narrower than those used in alpine skiing. Ski bottoms are designed to provide a gliding surface and, for classic skis, a traction zone under foot. The base of the gliding surface is a plastic material that is designed both to minimize friction and, in many cases, to accept waxes. Glide wax may be used on the tails and tips of classic skis and across the length of skate skis. Each type of ski is sized and designed differently:
Gliding surface.
Glide waxes enhance the speed of the gliding surface, and are applied by ironing them onto the ski and then polishing the ski bottom. Three classes of glide wax are available, depending on the level of desired performance with higher performance coming at higher cost. Hydrocarbon glide waxes, based on parafin are common for recreational use. Race waxes comprise a combination of fluorinated hydrocarbon waxes and fluorocarbon overlays. Fluorocarbons decrease surface tension and surface area of the water between the ski and the snow, increasing speed and glide of the ski under specific conditions. Either combined with the wax or applied after in a spray, powder, or block form, fluorocarbons significantly improve the glide of the ski and are widely used in cross-country ski races.
Traction surface.
Skis designed for classic technique, both in track and in virgin snow, rely on a traction zone, called the "kick zone," underfoot. This comes either from a texture designed to slide forward, but not backwards, from applied devices, e.g. climbing skins, or from kick waxes. Kick waxes are classified according to their hardness: harder waxes are for colder and newer snow. An incorrect choice of kick wax may cause slipping (too hard for the conditions) or sticking (too soft for the conditions) of the kick zone. Kick waxes generate grip by interacting with snow crystals, which vary with temperature, age and compaction. Hard kick waxes don't work well for snow which has metamorphosed to having coarse grains, whether icy or wet. In these conditions, skiers opt for a stickier substance, called "klister".
Boots and bindings.
Ski boots are attached to the ski only at the toe, leaving the heel free. Depending on application, boots may be lightweight (performance skiing) or heavier and more supportive (back-country skiing).
Bindings connect the boot to the ski. There are three primary groups of binding systems used in cross-country skiing (in descending order of importance):
Poles.
Ski poles are used for balance and propulsion. Modern cross-country ski poles are made from aluminum, fiberglass-reinforced plastic, or carbon fiber, depending on weight, cost and performance parameters. Formerly they were made of bamboo. They feature a foot (called a basket) near the end of the shaft that provides a pushing platform, as it makes contact with the snow. Baskets vary in size, according to the expected softness/firmness of the snow. Racing poles feature smaller, lighter baskets than recreational poles. Poles designed for skating are longer than those designed for classic skiing.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="6724" url="http://en.wikipedia.org/wiki?curid=6724" title="Copacabana, Rio de Janeiro">
Copacabana, Rio de Janeiro

Copacabana (] or ], rarely ] and ] in other Brazilian dialects) is a "bairro" (neighbourhood) located in the South Zone of the city of Rio de Janeiro, Brazil. It is known for its 4 km balneario beach, which is one of the most famous in the world.
History.
The district was originally called "Sacopenapã" (translated from the Tupi language, it means "the way of the socós (a kind of bird)") until the mid-18th century. It was renamed after the construction of a chapel holding a replica of the Virgen de Copacabana, the patron saint of Bolivia.
Characteristics.
Copacabana begins at Princesa Isabel Avenue and ends at Posto Seis (lifeguard watchtower Six). Beyond Copacabana, there are two small beaches: one, inside Fort Copacabana and other, right after it: Diabo ("Devil") Beach. Arpoador beach, where surfers used to go after its perfect waves, comes in the sequence, followed by the famous borough of Ipanema. The area will be one of the four "Olympic Zones" during the 2016 Summer Olympics.
According to Riotur, the Tourism Secretariat of Rio de Janeiro, there are 63 hotels and 10 hostels in Copacabana.
Copacabana Beach.
Copacabana beach, located at the Atlantic shore, stretches from Posto Dois (lifeguard watchtower Two) to Posto Seis (lifeguard watchtower Six). Leme is at Posto Um (lifeguard watchtower One). There are historic forts at both ends of Copacabana beach; Fort Copacabana, built in 1914, is at the south end by Posto Seis and Fort Duque de Caxias, built in 1779, at the north end. One curiosity is that the lifeguard watchtower of Posto Seis never existed.
Hotels, restaurants, bars, night clubs and residential buildings dot the promenade.
Copacabana Beach plays host to millions of revellers during the annual New Year's Eve celebrations and, in most years, has been the official venue of the FIFA Beach Soccer World Cup.
The beach is one of the many areas that suffers from the city's poor waste treatment. In its waters, 'fecal coliform bacteria sometimes spike at 16 times the Brazilian government’s “satisfactory” level.' 
Panorama of the Copacabana beach.
Copacabana promenade.
The Copacabana promenade is a pavement landscape in large scale (4 kilometres long). It was completed in 1970 and has used a black and white Portuguese pavement design since its origin in the 1930s: a geometric wave. The Copacabana promenade was designed by Roberto Burle Marx
Living standard.
Copacabana has the 11th highest Human Development Index in Rio, the 2000 census put the HDI of Copacabana at 0.902.
The neighbourhood.
According to the IBGE, 160,000 people live in Copacabana and 44,000 or 27.5% of them are 60 years old or older. Copacabana covers an area of 7.84 km² which gives the borough a population density of 20,400 people per km².
Residential buildings eleven to thirteen stories high built right next to each other dominate the borough. Houses and two-story buildings are rare.
When Rio was the capital of Brazil this was considered one of the best neighborhoods in the country if not the best. Therefore the sea front apartments are extremely luxurious and classical. In the 1970s the neighborhood descended in the social scales and now is one of the most overcrowded in the planet. 
Transportation.
More than 40 different bus routes serve Copacabana, as do three subway Metro stations: Cantagalo, Siqueira Campos and Cardeal Arcoverde.
Three major arteries parallel to each other cut across the entire borough: Avenida Atlântica (Atlantic Avenue), which is a 6-lane, 4 km avenue by the beachside, Nossa Senhora de Copacabana Avenue and Barata Ribeiro/Raul Pompéia Street both of which are 4 lanes and 3.5 km in length. Barata Ribeiro Street changes its name to Raul Pompéia Street after the Sá Freire Alvim Tunnel. Twenty-four streets intersect all three major arteries, and seven other streets intersect some of the three, but not all.
New Year's Eve in Copacabana.
The fireworks display in Rio de Janeiro to celebrate New Year's Eve is one of the largest in the world, lasting 15 to 20 minutes. It is estimated that 2 million people go to Copacabana Beach to see the spectacle. The festival also includes a concert that extends throughout the night. The celebration has become one of the biggest tourist attractions of Rio de Janeiro, attracting visitors from all over Brazil as well as from different parts of the world, and the city hotels generally stay fully booked.
History.
New Year's Eve has been celebrated on Copacabana beach since the 1950s when cults of African origin such as Candomblé and Umbanda gathered in small groups dressed in white for ritual celebrations. The first firework display occurred in 1976, sponsored by a hotel on the waterfront and this has been repeated ever since. In the 1990s the city saw it a great opportunity to move around the city - organizing and expanding the event.
An assessment made during the New Year's Eve 1992 highlighted the risks associated with the dispersal of increasing crowd numbers on Copacabana beach after the fireworks display and as from the 1993/94 event concerts have been held on the beach to retain the public.
The result was a success with egress spaced out over a period of 2 hours without the previous turmoil although critics claimed that it denied the spirit of the New Year's tradition of a religious festival with fireworks by the sea. The following year Rod Stewart beat attendance records. Finally, the Tribute to Tom Jobim - with Gal Costa, Gilberto Gil, Caetano Veloso, Chico Buarque, Paulinho da Viola... - consolidated the shows at the Copacabana Réveillon. There was a need to transform the fireworks display in a show of the same quality. The fireworks display was created by entrepreneurs Ricardo Amaral and Mariu's. From the previous 8–10 minutes the time was extended to 20 minutes and the quality and diversity of the fireworks was improved. A technical problem in fireworks 2000 required the use of ferries from New Year's Eve 2001/02. New Year's Eve began to compete with the Carnival, since 1992 it has been a tourist attraction in its own right.
Fireworks in Copacabana.

</doc>
<doc id="6725" url="http://en.wikipedia.org/wiki?curid=6725" title="Cy Young Award">
Cy Young Award

The Cy Young Award is given annually to the best pitchers in Major League Baseball (MLB), one each for the American League (AL) and National League (NL). The award was first introduced in 1956 by Baseball Commissioner Ford Frick in honor of Hall of Fame pitcher Cy Young, who died in 1955. The award was originally given to the single best pitcher in the major leagues, but in 1967, after the retirement of Frick, the award was given to one pitcher in each league.
Each league's award is voted on by members of the Baseball Writers Association of America, with one representative from each team. As of the 2010 season, each voter places a vote for first, second, third, fourth and fifth place among the pitchers of each league. The formula used to calculate the final scores is a weighted sum of the votes.[A] The pitcher with the highest score in each league wins the award. If two pitchers receive the same number of votes, the award is shared. The current formula started in the 2010 season. Before that, dating back to 1970, writers voted for three pitchers, with the formula of 5 points for a first place vote, 3 for a second place vote and 1 for a third place vote. Prior to 1970, writers only voted for the best pitcher and used a formula of one point per vote.
History.
The Cy Young Award was first introduced in 1956 by Commissioner of Baseball Ford C. Frick in honor of Hall of Fame pitcher Cy Young, who died in 1955. The award would be given to pitchers only. Originally given to the single best pitcher in the major leagues, the award changed its format over time. From 1956 to 1966, the award was given to one pitcher in Major League Baseball. After Frick retired in 1967, William Eckert became the new Commissioner of Baseball. Due to fan requests, Eckert announced that the Cy Young Award would be given out both in the American League and the National League. From 1956 to 1958, a pitcher was not allowed to win the award on more than one occasion; this rule was eliminated in 1959. After a tie in the 1969 voting for the AL Cy Young Award, the process was changed, in which each writer was to vote for three different pitchers: the first-place vote received five points, the second-place vote received three points, and the third-place vote received one point.
The first recipient of the Cy Young Award was Don Newcombe of the Dodgers, and the most recent winners are Clayton Kershaw, from the National League, and Corey Kluber, from the American League. In 1957, Warren Spahn became the first left-handed pitcher to win the award. In 1963, Sandy Koufax became the first pitcher to win the award in a unanimous vote; two years later he became the first multiple winner. In 1974, Mike Marshall won the award, becoming the first relief pitcher to win the award. In 1978, Gaylord Perry (age 40) became the oldest pitcher to receive the award, only to have the record broken in 2004 by Roger Clemens (age 42). The youngest recipient was Dwight Gooden (age 20 in 1985). In 2012, R.A. Dickey became the first knuckleball pitcher to win. Steve Carlton in 1982 became the first pitcher to win more than three Cy Young Awards, while Greg Maddux in 1994 became the first to win at least three in a row (and received a fourth straight the following year), a feat later repeated by Randy Johnson.
Winners.
Multiple winners.
There have been 17 pitchers who have won the award multiple times. Roger Clemens currently holds the record for the most awards won, with seven. Greg Maddux (1992–1995) and Randy Johnson (1999–2002) share the record for the most consecutive awards won. Clemens, Johnson, Pedro Martínez, Gaylord Perry, and Roy Halladay are the only pitchers to have won the award in both the American League and National League; Sandy Koufax is the only pitcher who won multiple awards during the period when only one award was presented for all of Major League Baseball. Roger Clemens was the youngest pitcher to win a second Cy Young Award, while Tim Lincecum is the youngest pitcher to do so in the National League and Clayton Kershaw is the youngest left-hander to do so. Clayton Kershaw is the youngest pitcher to win a third Cy Young Award.
Wins by teams.
Only four teams have never had a pitcher win the Cy Young Award. The Brooklyn/Los Angeles Dodgers have won more than any other team with 12.
Unanimous winners.
There have been 17 players who unanimously won the Cy Young award, for a total of 23 wins.
Five of these unanimous wins were accompanied with a win of the Most Valuable Player award (marked with * below; ** denotes that the player's unanimous win was accompanied with a unanimous win of the MVP).
In the National League, 11 players have unanimously won the Cy Young award, for a total of 14 wins.
In the American League, six players have unanimously won the Cy Young award, for a total of nine wins.
References.
</dl>

</doc>
<doc id="6728" url="http://en.wikipedia.org/wiki?curid=6728" title="Christianity and antisemitism">
Christianity and antisemitism

Christianity and antisemitism deals with the hostility of Christian Churches, Christian groups, and by Christians in general to Judaism and the Jewish people. Christian rhetoric and antipathy towards Jews developed in the early years of Christianity and was reinforced by ever increasing anti-Jewish measures over the ensuing centuries. The action taken by Christians against Jews included acts of violence, and murder culminating in the Holocaust.:21:169 Christian antisemitism has been attributed to numerous factors including theological differences, competition between Church and Synagogue, the Christian drive for converts, decreed by the Great Commission, misunderstanding of Jewish beliefs and practices, and a perceived Jewish hostility toward Christians. These attitudes were reinforced in Christian preaching, art and popular teaching for two millennia, containing contempt for Jews, as well as statutes which were designed to humiliate and stigmatise Jews.
Modern antisemitism has been described as primarily hatred against Jews as a race with its modern expression rooted in 18th century racial theories, while anti-Judaism is described as hostility to Jewish religion, but in Western Christianity it effectively merged into antisemitism during the 12th century.:16 Scholars have debated how Christian antisemitism played a role in the Nazi Third Reich, World War II and the Holocaust. The Holocaust has driven many within Christianity to reflect on the relationship between Christian theology, practices, and that genocide.
Roman-Jewish tensions.
Although Jewish communities lived and generally flourished in all major cities of the Roman Empire, tension between the Roman authorities and the Jews long preceded Christianity gaining any influence in the government of the Roman Empire. In Rome and throughout the Roman Empire, religion was an integral part of the civil government. The Emperor was from time to time declared to be a god and demanded to be worshiped accordingly. This created religious difficulties for Jews, who were prohibited from worshiping any other god than that of the Hebrew Bible. This, and the financial and political resentment of an occupied nation, created problems in the relations between Rome and its Jewish subjects, as well as for worshipers of Mithras, worshipers of Sabazius, and Christianity. In the case of Jews, this led to several revolts against Rome and severe persecutions by Rome as punishment. Roman political distrust of the Jews centred on suspicions that they preferred Rome's great rival in the Eastern Empire, Persia, to Rome as regional overlord, and these suspicions may have had some justification. Jews revolted against the Romans in the Great Revolt of 66 CE, which culminated with the destruction of the Second Temple in 70 CE. They revolted again under the leadership of the professed messiah Simon Bar Kokhba in 132 CE, which culminated in the expulsion of the Jews from Jerusalem, which Hadrian renamed into Aelia Capitolina in an attempt to wipe out memory of Jews there.
Early differences.
There have been philosophical differences between Christianity and Rabbinical Judaism since the founding of Christianity. Christians acknowledge the roots of Christianity in Judaism. Some claim the entirety of Jewish religious heritage as its own, while interpreting it very differently.
Debates between the Early Christians, who at first saw themselves as a movement within Judaism and not as a separate religion, and other Jews initially revolved around the question whether Jesus was the Messiah, which later encompassed the issue of his divinity. Once gentiles were converted to Christianity, the question arose whether and how far these gentile Christians were obliged to follow Jewish law in order to follow Jesus (see Paul's letter to the Galatians). At the Council of Jerusalem, it was decided that new gentile converts did not need to be circumcised (the "Apostolic Decree" of ), while requiring acceptance to a set of laws similar to Judaism's Noahide Law, (see also Old Testament#Christian view of the Law for the modern debate), but Paul also questioned the validity of Jewish Christians' adherence to the Jewish law in relation to faith in Christ, according to some interpretations. The issue of Paul of Tarsus and Judaism is still debated.
The increase of the numbers of gentile Christians in comparison to Jewish Christians eventually resulted in a rift between Christianity and Judaism, which was further increased by the Jewish-Roman wars (66–73 and 132–135) that drove many more Jews into the diaspora and reduced the influence of the Bishop of Jerusalem, leader of the first Christian church. Early Christians also found in the Old Testament, prophecies which seemed to indicate that God's original covenant with the Jews would be expanded to include also the Gentiles, in other words Proselytes, God-fearers, and Noachides. Thus the Church Fathers tend to emphasise that the Church is the new "spiritual" Israel, completing or replacing the earthly Israel which was but its prototype. In modern times, this view would come to be called "Supersessionism".
Also, the two religions differed in their legal status in the Roman Empire: Judaism, restricted to the Jewish people and Jewish Proselytes, was generally exempt from obligation to the Roman imperial cult and since the reign of Julius Caesar enjoyed the status of a "licit religion", though there were also occasional persecutions, for example in 19 Tiberius expelled the Jews from Rome, as Claudius did again in 49. Christianity however was not restricted to one people, and as Jewish Christians were excluded from the synagogue (see Council of Jamnia), they also lost the protection of the status of Judaism, though said "protection" did have its limits (see for example Titus Flavius Clemens (consul), Akiba ben Joseph, and Ten Martyrs).
From the reign of Nero onwards, who is said by Tacitus to have blamed the Great Fire of Rome on Christians, Christianity was considered to be illegal and Christians were frequently subjected to persecution, differing regionally. Comparably, Judaism suffered the setbacks of the Jewish-Roman wars, remembered in the legacy of the Ten Martyrs. Robin Lane Fox traces the origin of much later hostility to the period of persecution, where the commonest test by the authorities of a suspected Christian was to require homage to be paid to the deified emperor. Jews were exempt from this requirement as long as they paid the Fiscus Judaicus, and Christians (many or mostly of Jewish origins) would say they were Jewish but refused to pay the tax. This had to be confirmed by the local Jewish authorities, who were likely to refuse to accept the Christians as Jewish, often leading to their execution. The Birkat haMinim was often brought forward as support for this charge that the Jews were responsible for the Persecution of Christians in the Roman Empire. In the 3rd century systematic persecution of Christians began and lasted until Constantine's conversion to Christianity. In 390 Theodosius I made Christianity the state church of the Roman Empire. While pagan cults and Manichaeism were suppressed, Judaism retained its legal status as a licit religion, though anti-Jewish violence still occurred. In the 5th century, some legal measures worsened the status of the Jews in the Roman Empire (now more properly called the Byzantine Empire since relocating to Constantinople).
Issues arising from the New Testament.
Jesus as the Messiah.
The Hebrew word "mashiyakh" (משיח) typically signified a man, chosen by God or descended from a man chosen by God, to serve as a civil and military authority. Jesus was considered by Christians to be the Messiah, while for most Jews the death of Jesus would have been sufficient proof that he was not the Jewish Messiah.
Observance of Mosaic law.
Another source of tension between early Christians and Jews was the question of observance of Mosaic Law. Early Christians were divided over the issue: Some Jewish Christians or so-called Judaizers argued that Christians were bound to observe Mosaic law, while Paul perhaps argued that only some of Mosaic Law applied to Christians, though the issue of Paul of Tarsus and Judaism is still hotly debated with some advocating complete abrogation. The issue was argued especially in the context of whether Gentile converts were obligated to undergo circumcision, which was a requirement for Jewish boys. The issue was hotly debated in the 1st century and settled at the Council of Jerusalem, in which Paul and Barnabas participated as representatives of the church at Antioch. The Council decided that Gentile converts were not subject to most Mosaic Law, including circumcision, but required them to stay away from eating meat with blood still on it, eating the meat of strangled animals, eating food offered to idols, and sexual immorality. See also Noahide Law and Proselyte.
Many have interpreted Paul's writings and other parts of the New Testament as ending the requirements of the Jewish law. See Proselyte and New Perspective on Paul for more details. An example of another view is represented by the Catholic Encyclopedia article on "Judaizers":
Criticism of the Pharisees.
Many New Testament passages criticise the Pharisees and it has been argued that these passages have shaped the way that Christians viewed Jews. Like most Bible passages, however, they can and have been interpreted in a variety of ways.
Mainstream Talmudic Rabbinical Judaism today directly descends from the Pharisees who Jesus often criticized. During Jesus' life and at the time of his execution, the Pharisees were only one of several Jewish groups such as the Sadducees, Zealots, and Essenes who mostly died out not long after the period; indeed, some have suggested that Jesus was himself a Pharisee (although there is no biblical basis for that claim). Arguments by Jesus and his disciples against the Pharisees and what he saw as their hypocrisy were most likely examples of disputes among Jews and internal to Judaism that were common at the time, see for example Hillel and Shammai. (Lutheran Pastor John Stendahl has pointed out that "Christianity begins as a kind of Judaism, and we must recognize that words spoken in a family conflict are inappropriately appropriated by those outside the family.")
Recent studies on antisemitism in the New Testament.
Professor Lillian C. Freudmann, author of "Antisemitism in the New Testament" (University Press of America, 1994) has published a detailed study of the description of Jews in the New Testament, and the historical effects that such passages have had in the Christian community throughout history. Similar studies of such verses have been made by both Christian and Jewish scholars, including, Professors Clark Williamsom (Christian Theological Seminary), Hyam Maccoby (The Leo Baeck Institute), Norman A. Beck (Texas Lutheran College), and Michael Berenbaum (Georgetown University). Most rabbis feel that these verses are antisemitic, and many Christian scholars, in America and Europe, have reached the same conclusion. Another example is John Dominic Crossan's 1995 "Who Killed Jesus? Exposing the Roots of Anti-Semitism in the Gospel Story of the Death of Jesus".
Some biblical scholars have also been accused of holding antisemitic beliefs. Bruce J. Malina, founding member of The Context Group, has come under criticism for going as far as to deny the Semitic ancestry of modern Israelis. He then ties this back to his work on first century cultural anthropology.
Church Fathers.
After Paul's death, Christianity emerged as a separate religion, and Pauline Christianity emerged as the dominant form of Christianity, especially after Paul, James and the other apostles agreed on a compromise set of requirements. Some Christians continued to adhere to aspects of Jewish law, but they were few in number and often considered heretics by the Church. One example is the Ebionites, who seem to have denied the virgin birth of Jesus, the physical Resurrection of Jesus, and most of the books that were later canonized as the New Testament. For example, the Ethiopian Orthodox still continue Old Testament practices such as the Sabbath. As late as the 4th century Church Father John Chrysostom complained (see John Chrysostom#Sermons on Jews and Judaizing Christians) that some Christians were still attending Jewish synagogues.
The Church Fathers identified Jews and Judaism with heresy and declared the people of Israel to be "extra Deum" (lat. "outside of God"). Saint Peter of Antioch referred to Christians that refused to worship religious images as having "Jewish minds".
Patristic bishops of the patristic era such as Augustine argued that the Jews should be left alive and suffering as a perpetual reminder of their murder of Christ. Like his anti-Jewish teacher, St. Ambrose of Milan, he defined Jews as a special subset of those damned to hell. As "Witness People", he sanctified collective punishment for the Jewish deicide and enslavement of Jews to Catholics: "Not by bodily death, shall the ungodly race of carnal Jews perish (..) 'Scatter them abroad, take away their strength. And bring them down O Lord". Augustine mentioned to "love" the Jews but as a means to convert them to Christianity. Sometimes he identified all Jews with the evil Judas and developed the doctrine (together with St. Cyprian) that there was "no salvation outside the Church".
Other Church Fathers, such as John Chrysostom went longer in their condemnation. The Catholic editor Paul Harkins wrote that St. John Chrysostom's anti-Jewish theology "is no longer tenable (..) For these objectively unchristian acts he cannot be excused, even if he is the product of his times." John Chrysostom held, as most Church Fathers did, that the sins of all Jews were communal and endless, to him his Jewish neighbours were the collective representation of all alleged crimes of all preexisting Jews. All Church Fathers applied the passages of the New Testament concerning the alleged advocation of the crucifixion of Christ to all Jews of his day, the Jews were the ultimate evil. However, John Chrysostom went so far to say that because Jews rejected the Christian God in human flesh, Christ, they therefore deserved to be killed: "grew fit for slaughter." In citing the New Testament, he claimed that Jesus was speaking about Jews when he said, "as for these enemies of mine who did not want me to reign over them, bring them here and "slay them" before me."
St. Jerome's anti-Judaism identified Jews with Judas Iscariot and the immoral use of money ("Judas is cursed, that in Judas the Jews may be accursed (..) their prayers turn into sins"). Jerome's homiletical assaults, that may have served as the basis for the anti-Jewish Good Friday liturgy, contrasts Jews with the evil, and that "the ceremonies of the Jews are harmful and deadly to Christians", whoever keeps them was doomed to the devil : "My enemies are the Jews; they have conspired in hatred against Me, crucified Me, heaped evils of all kinds upon Me, blasphemed Me."
Ephraim the Syrian wrote polemics against Jews in the 4th century, including the repeated accusation that Satan dwells among them as a partner. These writings were directed at Christians who were being proselytized by Jews and who Ephraim feared were slipping back into the religion of Judaism; thus he portrayed the Jews as enemies of Christianity, like Satan, to emphasize the contrast between the two religions, namely, that Christianity was Godly and true and Judaism was Satanic and false. Like John Chrysostom, his objective was to dissuade Christians from reverting to Judaism by emphasizing what he saw as the wickedness of the Jews and their religion.
However, there are also positive remarks from the Church Fathers on the issue, such as Eusebius of Caesarea (circa. 263-340 A.D) in his Ecclesiastical History, who said, "The race of the Hebrews is not new, but is honoured among all men for its antiquity and is itself well known to all."
Middle Ages.
In the Middle Ages, religion played a major role in driving antisemitism. Though not part of Roman Catholic dogma, many Christians, including members of the clergy, have repeatedly asserted that the Jewish people were collectively responsible for killing Jesus, through the so-called blood curse of Pontius Pilate in the Gospels, among other things.
On the other hand, St.Bernard of Clairvaux (1090-1153), a Doctor of the Catholic Church, said "For us the Jews are Scripture's living words, because they remind us of what Our Lord suffered. They are not to be persecuted, killed, or even put to flight."
Jews were subject to a wide range of legal disabilities and restrictions throughout the Middle Ages, some of which lasted until the end of the 19th century. Jews were excluded from many trades, the occupations varying with place and time, and determined by the influence of various non-Jewish competing interests. Often Jews were barred from all occupations but money-lending and peddling, with even these at times forbidden. The number of Jews permitted to reside in different places was limited; they were concentrated in ghettos, and were not allowed to own land; they were subject to discriminatory taxes on entering cities or districts other than their own, were forced to swear special Jewish Oaths, and suffered a variety of other measures, including restrictions on dress. The Fourth Lateran Council in 1215 was the first to proclaim the requirement for Jews to wear something that distinguished them as Jews (and Muslims the same).
On many occasions, Jews were accused of a blood libel, the supposed drinking of blood of Christian children in mockery of the Christian Eucharist.
Sicut Judaeis.
Sicut Judaeis (the "Constitution for the Jews") was the official position of the papacy regarding Jews throughout the Middle Ages and later. The first bull was issued in about 1120 by Calixtus II, intended to protect Jews who suffered during the First Crusade, and was reaffirmed by many popes, even until the 15th century.
The bull forbade, besides other things, Christians from coercing Jews to convert, or to harm them, or to take their property, or to disturb the celebration of their festivals, or to interfere with their cemeteries, on pain of excommunication.
Popular antisemitism.
Antisemitism in popular European Christian culture escalated beginning in the 13th century. Blood libels and host desecration drew popular attention and led to many cases of persecution against Jews. Antisemitic imagery such as Judensau and Ecclesia et Synagoga recurred in Christian art and architecture.
In Iceland, one of the hymns repeated in the days leading up to Easter include the lines,
Persecutions and expulsions.
In the Middle ages in Europe persecutions and formal expulsions of Jews were liable to occur at intervals, although it should be said that this was also the case for other minority communities, whether religious or ethnic. There were particular outbursts of riotous persecution in the Rhineland massacres of 1096 in Germany accompanying the lead-up to the First Crusade, many involving the crusaders as they travelled to the East. There were many local expulsions from cities by local rulers and city councils. In Germany the Holy Roman Emperor generally tried to restrain persecution, if only for economic reasons, but he was often unable to exert much influence. In the Edict of Expulsion, King Edward I expelled all the Jews from England in 1290 (only after ransoming some 3,000 among the most wealthy of them), on the accusation of usury and undermining loyalty to the dynasty. In 1306 there was a wave of persecution in France, and there were widespread Black Death Jewish persecutions as the Jews were blamed by many Christians for the plague, or spreading it. As late as 1519, the Imperial city of Regensburg took advantage of the recent death of Emperor Maximilian I to expel its 500 Jews.
Expulsion of Jews from Spain.
Much the largest expulsion of Jews followed the Reconquista or reunification of Spain, and preceded the expulsion of the Muslims who would not convert, whose right were protected by the Treaty of Granada (1491). On 31 March 1492 Ferdinand II of Aragon and Isabella I of Castile, the rulers of Spain who financed Christopher Columbus' voyage to the New World just a few months later in 1492, declared that all Jews in their territories should either convert to Christianity or leave the country. While some converted, many others left for Portugal, France, Italy (including the Papal States), Netherlands, Poland, the Ottoman Empire, and North Africa. Many of those who had fled to Portugal were later expelled by King Manuel in 1497 or left to avoid forced conversion and persecution.
Renaissance to the 17th century.
Cum Nimis Absurdum.
On 14 July 1555, Pope Paul IV issued papal bull Cum nimis absurdum which revoked all the rights of the Jewish community and placed religious and economic restrictions on Jews in the Papal States, renewed anti-Jewish legislation and subjected Jews to various degradations and restrictions on their personal freedom.
The bull established the Roman Ghetto and required Jews of Rome, which had existed as a community since before Christian times and which numbered about 2,000 at the time, to live in it. The Ghetto was a walled quarter with three gates that were locked at night. Jews were also restricted to one synagogue per city.
Paul IV's successor, Pope Pius IV, enforced the creation of other ghettos in most Italian towns, and his successor, Pope Pius V, recommended them to other bordering states.
Protestant Reformation.
Martin Luther at first made overtures towards the Jews, believing that the "evils" of Catholicism had prevented their conversion to Christianity. When his call to convert to his version of Christianity was unsuccessful, he became hostile to them.
In his book "On the Jews and their Lies", Luther excoriates them as "venomous beasts, vipers, disgusting scum, canders, devils incarnate." He provided detailed recommendations for a pogrom against them, calling for their permanent oppression and expulsion, writing "Their private houses must be destroyed and devastated, they could be lodged in stables. Let the magistrates burn their synagogues and let whatever escapes be covered with sand and mud. Let them force to work, and if this avails nothing, we will be compelled to expel them like dogs in order not to expose ourselves to incurring divine wrath and eternal damnation from the Jews and their lies." At one point he wrote: "...we are at fault in not slaying them..." a passage that "may be termed the first work of modern antisemitism, and a giant step forward on the road to the Holocaust."
Luther's harsh comments about the Jews are seen by many as a continuation of medieval Christian antisemitism. In his final sermon shortly before his death, however, Luther preached: "We want to treat them with Christian love and to pray for them, so that they might become converted and would receive the Lord."
18th century.
In accordance with the anti-Jewish precepts of the Russian Orthodox Church,:14 Russia's discriminatory policies towards Jews intensified when the partition of Poland in the 18th century resulted, for the first time in Russian history, in the possession of land with a large Jewish population.:28 This land was designated as the Pale of Settlement from which Jews were forbidden to migrate into the interior of Russia.:28 In 1772 Catherine II, the empress of Russia, forced the Jews of the Pale of Settlement to stay in their "shtetls" and forbade them from returning to the towns that they occupied before the partition of Poland.
19th century.
Throughout the 19th century and into the 20th, the Roman Catholic Church still incorporated strong antisemitic elements, despite increasing attempts to separate anti-Judaism (opposition to the Jewish religion on religious grounds) and racial antisemitism. Pope Pius VII (1800–1823) had the walls of the Jewish ghetto in Rome rebuilt after the Jews were emancipated by Napoleon, and Jews were restricted to the ghetto through the end of the Papal States in 1870. Official Catholic organizations, such as the Jesuits, banned candidates "who are descended from the Jewish race unless it is clear that their father, grandfather, and great-grandfather have belonged to the Catholic Church" until 1946.
Brown University historian David Kertzer, working from the Vatican archive, has argued in his book "The Popes Against the Jews" that in the 19th century and early 20th century the Roman Catholic Church adhered to a distinction between "good antisemitism" and "bad antisemitism". The "bad" kind promoted hatred of Jews because of their descent. This was considered un-Christian because the Christian message was intended for all of humanity regardless of ethnicity; anyone could become a Christian. The "good" kind criticized alleged Jewish conspiracies to control newspapers, banks, and other institutions, to care only about accumulation of wealth, etc. Many Catholic bishops wrote articles criticizing Jews on such grounds, and, when accused of promoting hatred of Jews, would remind people that they condemned the "bad" kind of antisemitism. Kertzer's work is not without critics. Scholar of Jewish-Christian relations Rabbi David G. Dalin, for example, criticized Kertzer in the "Weekly Standard" for using evidence selectively.
20th-century.
WWI to the eve of WWII.
In 1916, in the midst of the First World War, American Jews petitioned Pope Benedict XV on behalf of the Polish Jews.
Nazi antisemitism.
On April 26, 1933 Hitler declared during a meeting with Roman Catholic Bishop Wilhelm Berning of Osnabrück:
“I have been attacked because of my handling of the Jewish question. The Catholic Church considered the Jews pestilent for fifteen hundred years, put them in ghettos, etc., because it recognized the Jews for what they were. In the epoch of liberalism the danger was no longer recognized. I am moving back toward the time in which a fifteen-hundred-year-long tradition was implemented. I do not set race over religion, but I recognize the representatives of this race as pestilent for the state and for the Church, and perhaps I am thereby doing Christianity a great service by pushing them out of schools and public functions.”
The transcript of this discussion contains no response by Bishop Berning. Martin Rhonheimer does not consider this unusual since, in his opinion, for a Catholic Bishop in 1933 there was nothing particularly objectionable "in this historically correct reminder".
The Nazis used Martin Luther's book, "On the Jews and Their Lies" (1543), to claim a moral righteousness for their ideology. Luther even went so far as to advocate the murder of those Jews who refused to convert to Christianity, writing that "we are at fault in not slaying them".
Archbishop Robert Runcie has asserted that: "Without centuries of Christian antisemitism, Hitler's passionate hatred would never have been so fervently echoed...because for centuries Christians have held Jews collectively responsible for the death of Jesus. On Good Friday Jews, have in times past, cowered behind locked doors with fear of a Christian mob seeking 'revenge' for deicide. Without the poisoning of Christian minds through the centuries, the Holocaust is unthinkable.":21 The dissident Catholic priest Hans Küng has written that "Nazi anti-Judaism was the work of godless, anti-Christian criminals. But it would not have been possible without the almost two thousand years' pre-history of 'Christian' anti-Judaism...":169
The document Dabru Emet was issued by over 220 rabbis and intellectuals from all branches of Judaism in 2000 as a statement about Jewish-Christian relations. This document states,
"Nazism was not a Christian phenomenon. Without the long history of Christian anti-Judaism and Christian violence against Jews, Nazi ideology could not have taken hold nor could it have been carried out. Too many Christians participated in, or were sympathetic to, Nazi atrocities against Jews. Other Christians did not protest sufficiently against these atrocities. But Nazism itself was not an inevitable outcome of Christianity."
According to American historian Lucy Dawidowicz, antisemitism has a long history within Christianity. The line of "antisemitic descent" from Luther, the author of On the Jews and Their Lies, to Hitler is "easy to draw." In her "The War Against the Jews, 1933-1945", she contends that Luther and Hitler were obsessed by the "demonologized universe" inhabited by Jews. Dawidowicz writes that the similarities between Luther's anti-Jewish writings and modern antisemitism are no coincidence, because they derived from a common history of "Judenhass", which can be traced to Haman's advice to Ahasuerus. Although modern German antisemitism also has its roots in German nationalism and the liberal revolution of 1848, Christian antisemitism she writes is a foundation that was laid by the Roman Catholic Church and "upon which Luther built."
Opposition to the Holocaust.
The Confessing Church was, in 1934, the first Christian opposition group. The Catholic Church officially condemned the Nazi theory of racism in Germany in 1937 with the encyclical "Mit brennender Sorge", signed by Pope Pius XI, and Cardinal Michael von Faulhaber led the Catholic opposition, preaching against racism.
Many individual Christian clergy and laypeople of all denominations had to pay for their opposition with their life, including:
By the 1940s fewer Christians were willing to oppose Nazi policy publicly, but many secretly helped save the lives of Jews. There are many sections of Israel's Holocaust Remembrance Museum, Yad Vashem, dedicated to honoring these "Righteous Among the Nations".
Pope Pius XII.
Before becoming Pope, Cardinal Pacelli addressed the International Eucharistic Congress in Budapest on 25–30 May 1938 during which he made reference to the Jews "whose lips curse [Christ] and whose hearts reject him even today"; at this time antisemitic laws were in the process of being formulated in Hungary.:92
The 1937 encyclical "Mit brennender Sorge" was issued by Pope Pius XI, but drafted by the future Pope Pius XII and read from the pulpits of all German Catholic churches, it condemned Nazi ideology and has been characterized by scholars as the "first great official public document to dare to confront and criticize Nazism" and "one of the greatest such condemnations ever issued by the Vatican."
In the summer of 1942, Pius explained to his college of Cardinals the reasons for the great gulf that existed between Jews and Christians at the theological level: "Jerusalem has responded to His call and to His grace with the same rigid blindness and stubborn ingratitude that has led it along the path of guilt to the murder of God." Historian Guido Knopp describes these comments of Pius as being ""incomprehensible" at a time when "Jerusalem was being murdered by the million"". This traditional adversarial relationship with Judaism would be reversed in "Nostra aetate" issued during the Second Vatican Council.
Prominent members of the Jewish community have contradicted the criticisms of Pius and spoke highly of his efforts to protect Jews. The Israeli historian Pinchas Lapide interviewed war survivors and concluded that Pius XII "was instrumental in saving at least 700,000, but probably as many as 860,000 Jews from certain death at Nazi hands". Some historians dispute this estimate.
"White Power" movement.
The Christian Identity movement, the Ku Klux Klan and other White supremacist groups have expressed antisemitic views. They claim that their antisemitism is based on purported Jewish control of the media, international banks, radical left wing politics, and the Jews' promotion of multiculturalism, anti-Christian groups, liberalism and perverse organizations. They rebuke charges of racism and claim that Jews who share their ideology maintain membership in their organizations. A racial belief common among these groups, but not universal, is an alternative history doctrine, sometimes called British Israelism. In some forms this doctrine absolutely denies that modern Jews have any racial connection to the Israel of the Bible. Instead, according to extreme forms of this doctrine the true racial Israel and true humans are the Adamic (white) race. These groups are often rejected and are not considered to be Christian groups by mainstream Christian denominations as well as by the vast majority of Christians around the world.
Post World War II antisemitism.
Antisemitism in Europe remains a substantial problem. Antisemitism exists to a lesser or greater degree in many other nations as well, including Eastern Europe, the former Soviet Union, and the increasingly frequent tensions between some Muslim immigrants and Jews across Europe. The US State Department reports that antisemitism has increased dramatically in Europe and Eurasia since 2000.
While in a decline since the 1940s, there is still a measurable amount of antisemitism in the United States of America as well, although acts of violence are rare. The 2001 survey by the Anti-Defamation League reported 1432 acts of antisemitism in the United States that year. The figure included 877 acts of harassment, including verbal intimidation, threats and physical assaults. A minority of American churches, such as the Presbyterian Church in America, engage in anti-Israel activism, including support for the controversial BDS movement. While not directly indicative of anti-semitism, this activism often conflates the Israel's treatment of Palestinians with that of Jesus, thereby promoting the anti-semitic doctrine of Jewish guilt. Many Christian Zionists are also accused of anti-semitism, such as John Hagee, who argued that the Jews brought the Holocaust upon themselves by angering God.
Anti-Judaism.
Many Christians do not consider anti-Judaism to be antisemitism. They regard anti-Judaism as a disagreement of religiously sincere people with the tenets of Judaism, while regarding antisemitism as an emotional bias or hatred not specifically targeting the religion of Judaism. Under this approach, anti-Judaism is not regarded as antisemitism as it only rejects the religious ideas of Judaism and does not involve actual hostility to the Jewish people.
Others see anti-Judaism as the rejection of or opposition to beliefs and practices "essentially because" of their source in Judaism or because a belief or practice is associated with the Jewish people. (But see supersessionism)
The position that "Christian theological anti-Judaism is a phenomenon distinct from modern antisemitism, which is rooted in economic and racial thought, so that Christian teachings should not be held responsible for antisemitism" has been articulated, among other places, by Pope John Paul II in 'We Remember: A Reflection on the Shoah,' and the Jewish declaration on Christianity, Dabru Emet. Several scholars, including Susannah Heschel, Gavin I Langmuir and Uriel Tal have challenged this position, arguing that anti-Judaism led directly to modern antisemitism.
Although some Christians in the past did consider anti-Judaism to be contrary to Christian teaching, this view was not widely expressed by leaders and lay people. In many cases, the practical tolerance towards the Jewish religion and Jews prevailed. Some Christian groups, particularly in early years, condemned verbal anti-Judaism.
Jewish converts.
The Southern Baptist Convention (SBC), the largest Protestant Christian denomination in the U.S., has explicitly rejected suggestions that it should back away from seeking to convert Jews, a position that critics have called antisemitic, but that Baptists see as consistent with their view that salvation is found solely through faith in Christ. In 1996 the SBC approved a resolution calling for efforts to seek the conversion of Jews "as well as for the salvation of 'every kindred and tongue and people and nation.'"
Most Evangelicals agree with the SBC position, and some have been supporting efforts specifically seeking Jews' conversion. At the same time these groups are among the most pro-Israeli groups. ("For more, see Christian Zionism".) Among the controversial groups that has found support from some Evangelical churches is Jews for Jesus, which claims that Jews can "complete" their Jewish faith by accepting Jesus as the Messiah.
The Presbyterian Church (USA), the United Methodist Church, and the United Church of Canada have ended their efforts to convert Jews. While Anglicans do not, as a rule, seek converts from other Christian denominations, the General Synod has affirmed that "the good news of salvation in Jesus Christ is for all and must be shared with all including people from other faiths or of no faith and that to do anything else would be to institutionalize discrimination".
The Roman Catholic Church formerly had religious congregations specifically aimed to conversion of Jews. Some of these were founded by Jewish converts themselves, like the Community of Our Lady of Zion, which was composed of nuns and ordained priests. Many Catholic saints were noted specifically because of their missionary zeal in converting Jews, such as Vincent Ferrer. After the Second Vatican Council many missionary orders aimed at converting Jews to Christianity no longer actively sought to missionize (or proselytize) among Jews. Traditionalist Roman Catholic groups, congregations and clergymen, however, continue to support missionizing Jews according to traditional patterns, sometimes with success ("e.g.", the Society of St. Pius X which has notable Jewish converts among its faithful, many of whom have become traditionalist priests).
Some Jewish organizations have described evangelism and missionary activity directed specifically at Jews as antisemitic.
Reconciliation between Judaism and Christian groups.
In recent years there has been much to note in the way of reconciliation between some Christian groups and the Jews. Most of this reconciliation has occurred between the Jewish community and the Catholic Church, and evangelical Christian organizations.
Further reading.
</dl>

</doc>
<doc id="6731" url="http://en.wikipedia.org/wiki?curid=6731" title="Boeing C-17 Globemaster III">
Boeing C-17 Globemaster III

The Boeing C-17 Globemaster III is a large military transport aircraft. It was developed for the United States Air Force (USAF) from the 1980s to the early 1990s by McDonnell Douglas. The C-17 carries forward the name of two previous piston-engined military cargo aircraft, the Douglas C-74 Globemaster and the Douglas C-124 Globemaster II. The C-17 commonly performs strategic airlift missions, transporting troops and cargo throughout the world; additional roles include tactical airlift, medical evacuation and airdrop duties.
Boeing, which merged with McDonnell Douglas in the 1990s, continued to manufacture C-17s for export customers following the end of deliveries to the U.S. Air Force. Aside from the United States, the C-17 is in service with the United Kingdom, Australia, Canada, Qatar, United Arab Emirates, NATO Heavy Airlift Wing, India, and Kuwait. The final C-17 was completed in May 2015.
Development.
Background and design phase.
In the 1970s, the U.S. Air Force began looking for a replacement for its Lockheed C-130 Hercules tactical cargo aircraft. The Advanced Medium STOL Transport (AMST) competition was held, with Boeing proposing the YC-14, and McDonnell Douglas proposing the YC-15. Though both entrants exceeded specified requirements, the AMST competition was canceled before a winner was selected. The Air Force started the C-X program in November 1979 to develop a larger AMST with longer range to augment its strategic airlift.
By 1980, the USAF found itself with a large fleet of aging C-141 Starlifter cargo aircraft. Compounding matters, USAF needed increased strategic airlift capabilities to fulfill its rapid-deployment airlift requirements. The USAF set mission requirements and released a request for proposals (RFP) for C-X in October 1980. McDonnell Douglas elected to develop a new aircraft based on the YC-15; Boeing bid an enlarged three-engine version of its AMST YC-14. Lockheed submitted two designs, a C-5-based design and an enlarged C-141 design. On 28 August 1981, McDonnell Douglas was chosen to build its proposed aircraft, then designated "C-17". Compared to the YC-15, the new aircraft differed in having swept wings, increased size, and more powerful engines. This would allow it to perform the work done by the C-141, and also fulfill some of the duties of the Lockheed C-5 Galaxy, freeing the C-5 fleet for outsize cargo.
Alternate proposals were pursued to fill airlift needs after the C-X contest. These were lengthening of C-141As into C-141Bs, ordering more C-5s, continued purchases of KC-10s, and expansion of the Civil Reserve Air Fleet. Limited budgets reduced program funding, requiring a delay of four years. During this time contracts were awarded for preliminary design work and for the completion of engine certification. In December 1985, a full-scale development contract was awarded. At this time, first flight was planned for 1990. The Air Force had formed a requirement for 210 aircraft.
Development problems and limited funding caused delays in the late 1980s. Criticisms were made of the developing aircraft and questions were raised about more cost-effective alternatives during this time. In April 1990, Secretary of Defense Dick Cheney reduced the order from 210 to 120 aircraft. The maiden flight of the C-17 took place on 15 September 1991 from the McDonnell Douglas's plant in Long Beach, California, about a year behind schedule. The first aircraft (T-1) and five more production models (P1-P5) participated in extensive flight testing and evaluation at Edwards Air Force Base. Two complete airframes were built for static and repeated load testing.
Development difficulties.
A static test of the C-17 wing in October 1992 resulted in the wing failing at 128% of design limit load, which was below the 150% requirement. Both wings buckled rear to the front and failures occurred in stringers, spars and ribs. Some $100 million was spent to redesign the wing structure; the wing failed at 145% during a second test in September 1993. A careful review of the test data however, showed that the wing was not loaded correctly and did indeed meet the requirement. The C-17 received the "Globemaster III" name in early 1993. In late 1993, the Department of Defense gave the contractor two years to solve production and cost overrun problems or face termination of the contract after the delivery of the 40th aircraft. By accepting the 1993 terms, McDonnell Douglas incurred a loss of nearly US$1.5 billion on the development phase of the program.
In April 1994, the C-17 program remained over budget, and did not meet weight, fuel burn, payload and range specifications. It failed several key criteria during airworthiness evaluation tests. Technical problems were found with the mission software, landing gear, and other areas. In May 1994, it was proposed to cut production to as few as 32 aircraft; these cuts were later rescinded. A July 1994 GAO document revealed that Air Force and DoD studies from 1986 and 1991 stated the C-17 could use 6,400 more runways outside the U.S. than the C-5; it was discovered that these studies only considered runway dimensions, but not runway strength or Load Classification Numbers (LCN). The C-5 has a lower LCN, but the USAF classifies both in the same broad Load Classification Group (LCG). When considering runway dimensions and load ratings, the C-17's worldwide runway advantage over the C-5 shrank from 6,400 to 911 airfields. The C-17's ability to use low quality, austere airfields was not considered.
A January 1995 GAO report revealed that, over the original cost of $41.8 billion for 210 C17s, the 120 aircraft on order were costing $39.5 billion. In March 1994, the U.S. Army decided it did not need the 60000 lb Low Altitude Parachute Extraction System (LAPES) delivery with the C-17 and that the C-130's 42000 lb capability was sufficient; C-17 testing was limited to this lower weight. Airflow issues prevented the C-17 from meeting airdrop requirements. A February 1997 GAO report revealed that a C-17 with a full payload could not land on 3000 ft wet runways; simulations suggested 5000 ft was required. The YC-15 was transferred to AMARC to be made flightworthy again for further flight tests for the C-17 program in March 1997. In 1995, most of the problems had been reportedly resolved. The first C-17 squadron was declared operational by the USAF in January 1995.
Production and deliveries.
In 1996, DoD ordered another 80 aircraft for a total of 120. In 1997 McDonnell Douglas merged with its former competitor, Boeing. In April 1999, Boeing proposed to cut the price of the C-17 if the Air Force bought 60 more, and in August 2002, the order was increased to 180 aircraft. In 2007, 190 C-17s were on order for the USAF. On 6 February 2009, Boeing was awarded a $2.95 billion contract for 15 additional aircraft, increasing the total USAF C-17 fleet to 205 and extending production from August 2009 to August 2010. On 6 April 2009, U.S. Secretary of Defense Robert Gates stated that there would be no more C-17s ordered beyond the 205 planned. However, on 12 June 2009, the House Armed Services Air and Land Forces Subcommittee added a further 17 C-17s.
In 2010, Boeing transitioned to a production rate of 10 C-17s per year from a high of 16 per year, this was due to dwindling orders and to extend the life of the production line while additional international orders were sought. The workforce was reduced by approximately 1,100 through 2012, and a second shift at the Long Beach assembly plant was also eliminated. By April 2011, 230 production C-17s had been delivered, including 210 to the USAF. The C-17 prototype "T-1" was retired in 2012 after being used by the USAF for testing and development. In January 2010, the USAF announced the end of Boeing's performance-based logistics contracts to maintain the aircraft. On 19 June 2012, the USAF ordered its 224th and final C-17, as a replacement for an aircraft that crashed in Alaska in July 2010.
In September 2013, Boeing announced that C-17 production was starting to close down. In October 2014, the main wing spar of the 279th and last aircraft was completed, this C-17 shall be delivered in 2015, after which Boeing will close the Long Beach plant. Production of spare components shall continue until at least 2017. The C-17 is projected to be in service for several decades. In February 2014, Boeing was engaged in sales talks with "five or six" countries for the remaining 15 C-17s, "two to four" of which are not current operators, and Boeing decided to build 10 aircraft without confirmed buyers in anticipation of future purchases. s of 2015[ [update]], five aircraft found buyers, including two for the Middle East, two for Australia and one for Canada.
In May 2015, the Wall Street Journal reported that Boeing expected to book a charge of less than $100 million and eliminate 3,000 positions associated with the C-17 program. According to Teal Group analyst Richard Aboulafia, Airbus' introduction of the cheaper A400M Atlas undercut international sales of the C-17.
Sources: C-17 Globemaster III Pocket Guide, Boeing IDS Major Deliveries
Design.
The C-17 is 174 ft long and has a wingspan of about 170 ft. It can airlift cargo fairly close to a battle area. The size and weight of U.S. mechanized firepower and equipment have grown in recent decades from increased air mobility requirements, particularly for large or heavy non-palletized outsize cargo.
The C-17 is powered by four Pratt & Whitney F117-PW-100 turbofan engines, which are based on the commercial Pratt and Whitney PW2040 used on the Boeing 757. Each engine is rated at 40,400 lbf (180 kN) of thrust. The engine's thrust reversers direct engine exhaust air upwards and forward, reducing the chances of foreign object damage by ingestion of runway debris, and providing enough reverse thrust to back the aircraft up on the ground while taxiing. The thrust reversers can also be used in flight at idle-reverse for added drag in maximum-rate descents. In vortex surfing tests performed by C-17s, up to 10% fuel savings were reported.
The aircraft requires a crew of three (pilot, copilot, and loadmaster) for cargo operations. Cargo is loaded through a large aft ramp that accommodates rolling stock, such as a 69-ton (63-metric ton) M1 Abrams main battle tank, other armored vehicles, trucks, and trailers, along with palletized cargo. The cargo compartment is 88 feet (26.82 m) long by 18 feet (5.49 m) wide by 12 feet 4 inches (3.76 m) high. The cargo floor has rollers for palletized cargo that can be flipped to provide a flat floor suitable for vehicles and other rolling stock.
Maximum payload of the C-17 is 170,900 lb (77,500 kg), and its Maximum takeoff weight is 585,000 lb (265,350 kg). With a payload of 160,000 lb (72,600 kg) and an initial cruise altitude of 28,000 ft (8,500 m), the C-17 has an unrefueled range of about 2,400 nautical miles (4,400 km) on the first 71 aircraft, and 2,800 nautical miles (5,200 km) on all subsequent extended-range models that include a sealed center wing bay as a fuel tank. Boeing informally calls these aircraft the "C-17 ER". The C-17's cruise speed is about 450 knots (833 km/h) (Mach 0.74). It is designed to airdrop 102 paratroopers and their equipment. The U.S. Army's Ground Combat Vehicle is to be transported by the C-17.
The C-17 is designed to operate from runways as short as 3,500 ft (1,064 m) and as narrow as 90 ft (27 m). In addition, the C-17 can operate from unpaved, unimproved runways (although with greater chance of damage to the aircraft). The thrust reversers can be used to back the aircraft and reverse direction on narrow taxiways using a three- (or more) point turn. The plane is designed for 20 man-hours of maintenance per flight hour, and a 74% mission availability rate.
Operational history.
United States Air Force.
The first production model was delivered to Charleston Air Force Base, South Carolina on 14 July 1993. The first C-17 squadron, the 17th Airlift Squadron, became operationally ready on 17 January 1995. The C-17 has broken 22 records for oversized payloads. The C-17 was awarded U.S. aviation's most prestigious award, the Collier Trophy in 1994. A Congressional report on operations in Kosovo and Operation Allied Force noted "One of the great success stories...was the performance of the Air Force's C-17A" The C-17 flew half of the strategic airlift missions in the operation, the type could use small airfields, easing operations; rapid turnaround times also led to efficient utilization.
The U.S. Air Force originally planned to buy a total of 120 C-17s, with the last one being scheduled for delivery in November 2004. The fiscal 2000 budget funded another 14 aircraft, primarily for Air Mobility Command (AMC) support of the United States Special Operations Command (USSOCOM). Basing of the original 120 C-17s was with the 437th Airlift Wing and 315th Airlift Wing at Charleston AFB, South Carolina, the 62nd Airlift Wing and 446th Airlift Wing at McChord Air Force Base, Washington, the Air Education and Training Command's (AETC) 97th Air Mobility Wing at Altus AFB, Oklahoma, and the Air Mobility Command-gained 172nd Airlift Wing of the Mississippi Air National Guard at Jackson-Evers International Airport/ANGB, Mississippi.
In FY 2006, eight C-17s were delivered to March Joint Air Reserve Base, California; controlled by the Air Force Reserve Command (AFRC), assigned to the 452d Air Mobility Wing; and subsequently assigned to AMC's 436th Airlift Wing and its AFRC "associate" unit, the 512th Airlift Wing, at Dover Air Force Base, Delaware, supplementing the Lockheed C-5 Galaxy. In 2011, the New York Air National Guard's 105th Airlift Wing at Stewart Air National Guard Base, New York, transitioned from the C-5 to the C-17.
The C-17 have been used to deliver military goods and humanitarian aid during Operation Enduring Freedom in Afghanistan as well as Operation Iraqi Freedom in Iraq. On 26 March 2003, 15 USAF C-17s participated in the biggest combat airdrop since the United States invasion of Panama in December 1989: the night-time airdrop of 1,000 paratroopers from the 173rd Airborne Brigade occurred over Bashur, Iraq. The airdrop of paratroopers were followed by C-17s ferrying M1 Abrams, M2 Bradleys, M113s and artillery. USAF C-17s have also been used to assist allies in their airlift requirements, including Canadian vehicles to Afghanistan in 2003 and Australian forces during the Australian-led military deployment to East Timor in 2006. In 2006, USAF C-17s flew 15 Canadian Leopard C2 tanks from Kyrgyzstan into Kandahar in support of NATO's Afghanistan mission. In 2013, five USAF C-17s supported French operations in Mali, operating with other nation's C-17s (RAF, NATO and RCAF deployed a single C-17 each).
A C-17 accompanies the President of the United States on his visits to both domestic and foreign arrangements, consultations, and meetings. The C-17 is used to transport the Presidential Limousine and security detachments. There have been several occasions when a C-17 has been used to transport the President himself, temporarily gaining the Air Force One call sign while doing so.
There was debate over follow-on C-17 orders, Air Force having requested line shutdown while Congress attempted to reinstate production. In FY2007, the Air Force requested $1.6 billion in response to "excessive combat use" on the C-17 fleet. In 2008, USAF General Arthur Lichte, Commander of Air Mobility Command, indicated before a House of Representatives subcommittee on air and land forces a need to extend production to another 15 aircraft to increase the total to 205. Pending the delivery of the results of two studies in 2009, Lichte observed that the production line may remain open for further C-17s to satisfy airlift requirements. The USAF eventually decided to cap its C-17 fleet at 223 aircraft, the final delivery was accepted on 12 September 2013.
Royal Air Force.
Boeing has marketed the C-17 to many European nations including Belgium, Germany, France, Italy, Spain and the United Kingdom. The Royal Air Force (RAF) has established an aim of having interoperability and some weapons and capabilities commonality with the USAF. The 1998 Strategic Defence Review identified a requirement for a strategic airlifter. The Short-Term Strategic Airlift (STSA) competition commenced in September of that year, however tendering was canceled in August 1999 with some bids identified by ministers as too expensive, including the Boeing/BAe C-17 bid, and others unsuitable. The project continued, with the C-17 seen as the favorite. In the light of Airbus A400M delays, the UK Secretary of State for Defence, Geoff Hoon, announced in May 2000 that the RAF would lease four C-17s at an annual cost of £100 million from Boeing for an initial seven years with an optional two-year extension. The RAF had the option to buy or return the aircraft to Boeing. The UK committed to upgrading its C-17s in line with the USAF so that if they were returned, the USAF could adopt them.
The first C-17 was delivered to the RAF at Boeing's Long Beach facility on 17 May 2001 and flown to RAF Brize Norton by a crew from No. 99 Squadron. The RAF's fourth C-17 was delivered on 24 August 2001. The RAF aircraft were some of the first to take advantage of the new center wing fuel tank found in Block 13 aircraft. In RAF service, the C-17 has not been given an official service name and designation (for example, C-130J referred to as Hercules C4 or C5), but is referred to simply as the C-17 or "C-17A Globemaster".
The RAF declared itself delighted with the C-17. Although the Globemaster fleet was to be a fallback for the A400M, the Ministry of Defence (MoD) announced on 21 July 2004 that they had elected to buy their four C-17s at the end of the lease, even though the A400M appeared to be closer to production. The C-17 gives the RAF strategic capabilities that it would not wish to lose, for example a maximum payload of 169,500 lb (77,000 kg) compared to the A400M's 82,000 lb (37,000 kg). The C-17's capabilities allow the RAF to use it as an airborne hospital for medical evacuation missions.
Another C-17 was ordered in August 2006, and delivered on 22 February 2008. The four leased C-17s were to be purchased later in 2008. Because of fears that the A400M may suffer further delays, the MoD announced in 2006 that it planned to acquire three more C-17s, for a total of eight, with delivery in 2009–2010. On 26 July 2007, Defence Secretary Des Browne announced that the MoD intended to order a sixth C-17 to boost operations in Iraq and Afghanistan. On 3 December 2007, the MoD announced a contract for a sixth C-17, which was received on 11 June 2008.
On 18 December 2009, Boeing confirmed that the RAF had ordered a seventh C-17, which was delivered on 16 November 2010. The UK announced the purchase of its eighth C-17 in February 2012. The RAF showed interest in buying a ninth C-17 in November 2013.
On 13 January 2013, the RAF deployed two C-17s of No. 99 Squadron from RAF Brize Norton to the French Évreux Air Base. The aircraft transported French armored vehicles to the Malian capital of Bamako during the French Intervention in Mali.
Royal Australian Air Force.
The Royal Australian Air Force (RAAF) began investigating an acquisition of heavy lift aircraft for strategic transport in 2005. In late 2005 the then Minister for Defence Robert Hill stated that such aircraft were being considered due to the limited availability of strategic airlift aircraft from partner nations and air freight companies. The C-17 was considered to be favored over the A400M as it was a "proven aircraft" and in production. One major RAAF requirement was the ability to airlift the Army's M1 Abrams tanks; another requirement was immediate delivery. Though unstated, commonality with the USAF and the United Kingdom's RAF was also considered advantageous. RAAF aircraft were ordered directly from the USAF production run and are identical to American C-17 even in paint scheme, the only difference being the national markings. This allowed delivery to commence within nine months of commitment to the program.
On 2 March 2006, the Australian government announced the purchase of three aircraft and one option with an entry into service date of 2006. In July 2006 a fixed price contract was awarded to Boeing to deliver four C-17s for US$ (A$). Australia also signed a US$80.7M contract to join the global 'virtual fleet' C-17 sustainment program and the RAAF's C-17s will receive the same upgrades as the USAF's fleet.
The Royal Australian Air Force took delivery of its first C-17 in a ceremony at Boeing's plant at Long Beach, California on 28 November 2006. Several days later the aircraft flew from Hickam Air Force Base, Hawaii to Defence Establishment Fairbairn, Canberra, arriving on 4 December 2006. The aircraft was formally accepted in a ceremony at Fairbairn shortly after arrival. The second aircraft was delivered to the RAAF on 11 May 2007 and the third was delivered on 18 December 2007. The fourth Australian C-17 was delivered on 19 January 2008. All the Australian C-17s are operated by No. 36 Squadron and are based at RAAF Base Amberley in Queensland.
On 18 April 2011, Boeing announced that Australia had signed an agreement with the U.S. government to acquire a fifth C-17 due to an increased demand for humanitarian and disaster relief missions. The aircraft was delivered to the RAAF on 14 September 2011. On 23 September 2011, Australian Minister for Defence Materiel Jason Clare announced that the government was seeking information from the U.S. about the price and delivery schedule for a sixth Globemaster. In November 2011, Australia requested a sixth C-17 through the U.S. FMS program; it was ordered in June 2012, and was delivered on 1 November 2012.
Australia's C-17s have supported ADF operations around the world, including supporting Air Combat Group training deployments to the U.S., transporting Royal Australian Navy Sea Hawk helicopters and making fortnightly supply missions to Australian forces in Iraq and Afghanistan. The C-17s have also carried humanitarian supplies to Papua New Guinea during Operation Papua New Guinea Assist in 2007, supplies and South African Puma helicopters to Burma in 2008 following Cyclone Nargis, relief supplies to Samoa following the 2009 earthquake, aid packages around Queensland following the 2010–2011 floods and Cyclone Yasi, and rescue teams and equipment to New Zealand following the February 2011 Christchurch earthquake, and equipment after the 2011 Tōhoku earthquake and tsunami from Western Australia to Japan. In July 2014, an Australian C-17 transported several bodies of victims of Malaysia Airlines Flight 17 from Ukraine to the Netherlands.
In August 2014, Defence Minister David Johnston announced the intention to purchase one or two additional C-17s. On 3 October 2014, Johnston announced the government's approval to buy two C-17s at a total cost of US$770 million. The United States Congress approved the sale under the Foreign Military Sales program. Prime Minister Tony Abbott confirmed in April 2015 that two additional aircraft are to be ordered, with delivery in late 2015; these are to add to the six C-17s it has as of 2015[ [update]].
Royal Canadian Air Force.
Canada's air arm has had a long-standing need for strategic airlift for humanitarian and military operations around the world. It had followed a pattern similar to the German Air Force in leasing Antonovs and Ilyushins for many of its needs, including deploying the Disaster Assistance Response Team (DART) to tsunami-stricken Sri Lanka in 2005. The air service was forced to rely entirely on leased An-124 "Ruslan" for a Canadian Army deployment to Haiti in 2003. The service has also used a combination of leased "Ruslans", Ilyushins and USAF C-17s for moving heavy equipment into Afghanistan. In 2002, the Canadian Forces Future Strategic Airlifter Project began to study alternatives, including long-term leasing arrangements.
On 5 July 2006, the Canadian government issued a notice that it intended to negotiate directly with Boeing to procure four airlifters for the Canadian Forces Air Command (renamed Royal Canadian Air Force in August 2011). On 1 February 2007, Canada awarded a contract for four C-17s with delivery beginning in August 2007. Like Australia, Canada was granted airframes originally slated for the U.S. Air Force, to accelerate delivery.
On 16 June 2007, the first Canadian C-17 rolled off the assembly line at Long Beach, California and into the paint hangar for painting and addition of Canadian markings including the national logo and air force roundel. The first Canadian C-17 made its initial flight on 23 July. It was turned over to Canada on 8 August, and participated at the Abbotsford International Airshow on 11 August prior to arriving at its new home base at 8 Wing, CFB Trenton, Ontario on 12 August. Its first operational mission was delivery of disaster relief to Jamaica in the aftermath of Hurricane Dean. The second C-17 arrived at 8 Wing, CFB Trenton on 18 October 2007. The last of four aircraft was delivered in April 2008. The official Canadian designation is "CC-177 Globemaster III". The aircraft are assigned to 429 Transport Squadron based at CFB Trenton.
On 14 April 2010, a Canadian C-17 landed for the first time at CFS Alert, the world's most northerly airport. Canadian Globemasters have been deployed in support of numerous humanitarian and military missions worldwide, including Operation Hestia after the earthquake in Haiti, providing airlift as part of Operation Mobile and support to the Canadian mission in Afghanistan. After Typhoon Haiyan hit the Philippines in 2013, Canadian C-17s established an air bridge between the two nations, deploying Canada's DART Team and delivering humanitarian supplies and equipment. In 2014, they supported Operation Reassurance and Operation Impact.
On 19 December 2014, it was reported that Canada's Defence Department intended to purchase one more C-17. On 30 March 2015, Canada's fifth C-17 landed at Canada’s largest air base, CFB Trenton. Lt. Gen. Yvan Blondin, commander of the Royal Canadian Air Force (RCAF), noted the new military plane will improve the Canadian Armed Forces’ response capability to both domestic and international emergencies and provide support to a variety of missions, including humanitarian assistance, peace support and combat.
NATO (Strategic Airlift Capability Program).
At the 2006 Farnborough Airshow, a number of NATO member nations signed a letter of intent to jointly purchase and operate several C-17s within the NATO Strategic Airlift Capability. Strategic Airlift Capability members are Bulgaria, Estonia, Hungary, Lithuania, the Netherlands, Norway, Poland, Romania, Slovenia, the United States, as well as two Partnership for Peace countries Finland and Sweden as of 2010. The purchase was for two C-17s, and a third was contributed by the U.S. On 14 July 2009, Boeing delivered the first C-17 under NATO's Strategic Airlift Capability (SAC) program. The second and third C-17s were delivered in September and October 2009.
The SAC C-17s are based at Pápa Air Base, Hungary. The Heavy Airlift Wing is hosted by Hungary, which acts as the flag nation. The aircraft are manned in similar fashion as the NATO E-3 AWACS aircraft. The C-17 flight crew are multi-national, but each mission is assigned to an individual member nation based on the SAC's annual flight hour share agreement. The NATO Airlift Management Programme Office (NAMPO) provides management and support for the Heavy Airlift Wing. NAMPO is a part of the NATO Support Agency (NSPA). In September 2014, Boeing revealed that the three C-17s supporting NATO SAC missions had achieved a readiness rate of nearly 94 percent over the last five years and supported over 1,000 missions.
Indian Air Force.
In June 2009, the Indian Air Force (IAF) selected the C-17 for its "Very Heavy Lift Transport Aircraft" requirement, it is to replace several types of transport aircraft. In January 2010, India requested 10 C-17s through the U.S.'s Foreign Military Sales program, the sale was approved by Congress in June 2010. On 23 June 2010, the Indian Air Force successfully test-landed a USAF C-17 at the Gaggal Airport, India to complete the IAF's C-17 trials. In February 2011, the IAF and Boeing agreed terms for the order of 10 C-17s with an option for six more; the US$4.1 billion order was approved by the Indian Cabinet Committee on Security on 6 June 2011. Deliveries began in June 2013 and are to continue until 2014. In 2012, the IAF reportedly finalized plans to buy six more C-17s in the 13th five-year plan (2017–2022).
The aircraft provides strategic airlift and the ability to deploy special forces, such as during national emergencies. They are operated in diverse terrain – from Himalayan air bases in North India at 13000 ft to Indian Ocean bases in South India. The C-17s are based at Hindon Air Force Station and are operated by the No. 81 Squadron "Skylords". The first C-17 was delivered in January 2013 for testing and training; it was officially accepted on 11 June 2013. The second C-17 was delivered on 23 July 2013 and put into service immediately. IAF Chief of Air Staff Norman AK Browne called the Globemaster III "a major component in the IAF's modernization drive" while taking delivery of the aircraft at Boeing's Long Beach factory. On 2 September 2013, the "Skylords" squadron with three C-17s officially entered IAF service.
The "Skylords" regularly fly missions within India, such as to high-altitude bases at Leh and Thoise. The IAF first used the C-17 to transport an infantry battalion's equipment to Port Blair on Andaman Islands on 1 July 2013. Foreign deployments to date include Tajikistan in August 2013, and Rwanda to support Indian peacekeepers. One C-17 was used for transporting relief materials during Cyclone Phailin. The fifth aircraft was received in November 2013. The sixth aircraft was received in July 2014.
The C-17 played a crucial role in Operation Raahat, in which over 4,500 Indian nationals and 960 foreign nationals from 41 countries including US, UK, France, Canada and Egypt were evacuated from war-torn Yemen in April 2015. C-17 also played a major role in supplying and rescuing during devastating 2015 Nepal earthquake, which killed nearly 6,000. C-17s were used to transport relief material and rescue Indian citizens.
Qatar.
Boeing delivered Qatar's first C-17 on 11 August 2009 and the second on 10 September 2009 for the Qatar Emiri Air Force. Qatar received its third C-17 in 2012, and fourth C-17 was received on 10 December 2012. In June 2013, the New York Times reported that Qatar was allegedly using its C-17s to ship weapons from Libya to the Syrian opposition during the civil war via Turkey.
United Arab Emirates.
In February 2009, the United Arab Emirates Air Force agreed to purchase four C-17s. In January 2010, a contract was signed for six C-17s. In May 2011, the first C-17 was handed over and the last of the six was received in June 2012.
Kuwait.
Kuwait requested the purchase of one C-17 in September 2010 and a second in April 2013 through the U.S.'s Foreign Military Sales (FMS) program. The nation ordered two C-17s; the first was delivered on 13 February 2014.
References.
Bibliography.
</dl>

</doc>
<doc id="6734" url="http://en.wikipedia.org/wiki?curid=6734" title="Garbage collection (computer science)">
Garbage collection (computer science)

In computer science, garbage collection (GC) is a form of automatic memory management. The "garbage collector", or just "collector", attempts to reclaim "garbage", or memory occupied by objects that are no longer in use by the program. Garbage collection was invented by John McCarthy around 1959 to solve problems in Lisp.
Garbage collection is often portrayed as the opposite of manual memory management, which requires the programmer to specify which objects to deallocate and return to the memory system. However, many systems use a combination of approaches, including other techniques such as stack allocation and region inference. Like other memory management techniques, garbage collection may take a significant proportion of total processing time in a program and can thus have significant influence on performance.
Resources other than memory, such as network sockets, database handles, user interaction windows, and file and device descriptors, are not typically handled by garbage collection. Methods used to manage such resources, particularly destructors, may suffice to manage memory as well, leaving no need for GC. Some GC systems allow such other resources to be associated with a region of memory that, when collected, causes the other resource to be reclaimed; this is called "finalization". Finalization may introduce complications limiting its usability, such as intolerable latency between disuse and reclaim of especially limited resources, or a lack of control over which thread performs the work of reclaiming.
Principles.
The basic principles of garbage collection are to find data objects in a program that cannot be accessed in the future, and to reclaim the resources used by those objects.
Many programming languages require garbage collection, either as part of the language specification (for example, Java, C#, D language, Go and most scripting languages) or effectively for practical implementation (for example, formal languages like lambda calculus); these are said to be "garbage collected languages". Other languages were designed for use with manual memory management, but have garbage-collected implementations available (for example, C and C++). Some languages, like Ada, Modula-3, and C++/CLI, allow both garbage collection and manual memory management to co-exist in the same application by using separate heaps for collected and manually managed objects; others, like D, are garbage-collected but allow the user to manually delete objects and also entirely disable garbage collection when speed is required.
While integrating garbage collection into the language's compiler and runtime system enables a much wider choice of methods, "post-hoc" GC systems exist, such as ARC, including some that do not require recompilation. ("Post-hoc" GC is sometimes distinguished as "litter collection".) The garbage collector will almost always be closely integrated with the memory allocator.
Advantages.
Garbage collection frees the programmer from manually dealing with memory deallocation. As a result, certain categories of bugs are eliminated or substantially reduced:
Some of the bugs addressed by garbage collection can have security implications.
Disadvantages.
Typically, garbage collection has certain disadvantages, including consuming additional resources, performance impacts, possible stalls in program execution, and incompatibility with manual resource management.
Garbage collection consumes computing resources in deciding which memory to free, even though the programmer may have already known this information. The penalty for the convenience of not annotating object lifetime manually in the source code is overhead, which can lead to decreased or uneven performance. A peer-reviewed paper came to the conclusion that GC needs five times the memory to compensate for this overhead and to perform as fast as explicit memory management. Interaction with memory hierarchy effects can make this overhead intolerable in circumstances that are hard to predict or to detect in routine testing. The impact on performance was also given by Apple as a reason for not adopting garbage collection in iOS despite being the most desired feature.
The moment when the garbage is actually collected can be unpredictable, resulting in stalls scattered throughout a session. Unpredictable stalls can be unacceptable in real-time environments, in transaction processing, or in interactive programs. Incremental, concurrent, and real-time garbage collectors address these problems, with varying trade-offs.
Non-deterministic GC is incompatible with RAII based management of non-GCed resources. As a result, the need for explicit manual resource management (release/close) for non-GCed resources becomes transitive to composition. That is: in a non-deterministic GC system, if a resource or a resource-like object requires manual resource management (release/close), and this object is used as "part of" another object, then the composed object will also become a resource-like object that itself requires manual resource management (release/close).
Tracing garbage collectors.
Tracing garbage collection is the most common type of garbage collection, so much so that "garbage collection" often refers to tracing garbage collection, rather than other methods such as reference counting. The overall strategy consists of determining which objects should be garbage collected by tracing which objects are "reachable" by a chain of references from certain root objects, and considering the rest as garbage and collecting them. However, there are a large number of algorithms used in implementation, with widely varying complexity and performance characteristics.
Reference counting.
Reference counting is a form of garbage collection whereby each object has a count of the number of references to it. Garbage is identified by having a reference count of zero. An object's reference count is incremented when a reference to it is created, and decremented when a reference is destroyed. The object's memory is reclaimed when the count reaches zero.
As with manual memory management, and unlike tracing garbage collection, reference counting guarantees that objects are destroyed as soon as their last reference is destroyed, and usually only accesses memory which is either in CPU caches, in objects to be freed, or directly pointed by those, and thus tends to not have significant negative side effects on CPU cache and virtual memory operation.
There are a number of disadvantages to reference counting; this can generally be solved or mitigated by more sophisticated algorithms:
Escape analysis.
Escape analysis can be used to convert heap allocations to stack allocations, thus reducing the amount of work needed to be done by the garbage collector. This is done using a compile-time analysis to determine whether an object allocated within a function is not accessible outside of it (i.e. escape) to other functions or threads. In such a case the object may be allocated directly on the thread stack and released when the function returns, reducing its potential garbage collection overhead.
Compile-time.
Compile-time garbage collection is a form of static analysis allowing memory to be reused and reclaimed based on invariants known during compilation. This form of garbage collection has been studied in the Mercury programming language, and it saw greater usage with the introduction of LLVM's automatic reference counter (ARC) into Apple's ecosystem (iOS and OS X) in 2011.
Availability.
Generally speaking, higher-level programming languages are more likely to have garbage collection as a standard feature. In languages that do not have built in garbage collection, it can often be added through a library, as with the Boehm garbage collector for C (for "nearly all programs") and C++. This approach is not without drawbacks, such as changing object creation and destruction mechanisms.
Most functional programming languages, such as ML, Haskell, and APL, have garbage collection built in. Lisp is especially notable as both the first functional programming language and the first language to introduce garbage collection.
Other dynamic languages, such as Ruby (but not Perl 5 or PHP before version 5.3, which both use reference counting), also tend to use GC. Object-oriented programming languages such as Smalltalk, Java and ECMAScript usually provide integrated garbage collection. Notable exceptions are C++ and Delphi which have destructors.
BASIC.
Historically, languages intended for beginners, such as BASIC and Logo, have often used garbage collection for heap-allocated variable-length data types, such as strings and lists, so as not to burden programmers with manual memory management. On early microcomputers, with their limited memory and slow processors, BASIC garbage collection could often cause apparently random, inexplicable pauses in the midst of program operation.
Some BASIC interpreters, such as Applesoft BASIC on the Apple II family, repeatedly scanned the string descriptors for the string having the highest address in order to compact it toward high memory, resulting in O(N*N) performance, which could introduce minutes-long pauses in the execution of string-intensive programs. A replacement garbage collector for Applesoft BASIC published in Call-A.P.P.L.E. (January 1981, pages 40–45, Randy Wigginton) identified a group of strings in every pass over the heap, which cut collection time dramatically. BASIC.System, released with ProDOS in 1983, provided a windowing garbage collector for BASIC that reduced most collections to a fraction of a second.
Objective-C.
While the Objective-C traditionally had no garbage collection, with the release of OS X 10.5 in 2007 Apple introduced garbage collection for Objective-C 2.0, using an in-house developed runtime collector. 
However, with the 2012 release of OS X 10.8, garbage collection was deprecated in favor of LLVM's automatic reference counter (ARC) that was introduced with OS X 10.7. Furthermore, since May 2015 Apple even forbids the usage of garbage collection for new OS X applications in the App Store. For the iOS, garbage collection has never been introduced due to problems in application responsivity and performance; instead, iOS uses ARC.
Limited environments.
Garbage collection is rarely used on embedded or real-time systems because of the perceived need for very tight control over the use of limited resources. However, garbage collectors compatible with such limited environments have been developed. The Microsoft .NET Micro Framework and Java Platform, Micro Edition are embedded software platforms that, like their larger cousins, include garbage collection.

</doc>
<doc id="6736" url="http://en.wikipedia.org/wiki?curid=6736" title="Canidae">
Canidae

The biological family Canidae 
 is a lineage of carnivorans that includes dogs, wolves, foxes, jackals, and many other extant and extinct dog-like mammals. A member of this family is called a canid (, ). The Canidae family is divided into two tribes: the Canini (dogs, wolves, jackals, and some south American "foxes") and the Vulpini (true foxes).
Canids have a long evolutionary history. In the Eocene, about 50 million years ago, the carnivorans split into two lineages, the caniforms (dog-like) and feliforms (cat-like). By the Oligocene, some ten million years later, the first proper canids had appeared and the family had split into three subfamilies, Hesperocyoninae, Borophaginae, and Caninae. Only the last of these has survived until the present day.
Canids are found on all continents except Antarctica and vary in size from the 2-m-long (6 ft 7 in) gray wolf to the 24-cm-long (9.4 in) fennec fox. The body forms of canids are similar, typically having long muzzles, upright ears, teeth adapted for cracking bones and slicing flesh, long legs, and bushy tails. They are mostly social animals, living together in family units or small groups and behaving cooperatively. Typically, only the dominant pair in a group breeds, and a litter of young is reared annually in an underground den. Canids communicate by scent signals and by vocalizations. One canid, the domestic dog (including the dingo), long ago entered into a partnership with humans and today remains one of the most widely kept domestic animals.
Phylogenetic relationships.
Within the Canidae, the results of allozyme and chromosome analyses have previously suggested several phylogenetic divisions:
Recent DNA analysis shows that there are three monophyletic clades. Two of these, the South American canine group, which includes a number of "foxes", and the wolf group, together form the tribe Canini. The third clade is the "true fox" group, tribe Vulpini. Molecular data imply a North American origin of living Canidae some ten million years ago and an African origin of wolf-like canines ("Canis", "Cuon", and "Lycaon"), with the jackals being the most basal of this group. The South American clade is rooted by the maned wolf and bush dog, and the fox-like canids by the fennec fox and Blanford's fox. The grey fox and island fox are basal to the other clades, however this topological difference is not strongly supported.
Evolution.
The Canidae includes a diverse group of some 34 species ranging in size from the maned wolf with its long limbs to the short-legged bush dog. Modern canids inhabit forests, tundra, savannahs and deserts throughout tropical and temperate parts of the world. The evolutionary relationships between the species have been studied in the past using morphological approaches but more recently, molecular studies have enabled the investigation of phylogenetic relationships. In some species, genetic divergence has been suppressed by the high level of gene flow between different populations and where the species have hybridized, large hybrid zones exist.
Eocene epoch.
Carnivorans evolved from miacoids about 55 million years ago (Mya) during the late Paleocene. Some five million years later, the carnivorans split into two main divisions: caniforms (dog-like) and feliforms (cat-like). By 40 Mya, the first member of the dog family proper had arisen. Called "Prohesperocyon wilsoni", its fossilized remains have been found in what is now the southwestern part of Texas. The chief features which identify it as a canid include the loss of the upper third molar (part of a trend toward a more shearing bite), and the structure of the middle ear which has an enlarged bulla (the hollow bony structure protecting the delicate parts of the ear). "Prohesperocyon" probably had slightly longer limbs than its predecessors, and also had parallel and closely touching toes which differ markedly from the splayed arrangements of the digits in bears.
The canid family soon subdivided into three subfamilies, each of which diverged during the Eocene: Hesperocyoninae (about 39.74-15 Mya), Borophaginae (about 34-2 Mya), and Caninae (about 34-0 Mya). Caninae is the only surviving subfamily and all present-day canids including wolves, foxes, coyotes, jackals, and domestic dogs belong to it. Members of each subfamily showed an increase in body mass with time, and some exhibited specialised hypercarnivorous diets that made them prone to extinction.:Fig. 1
Evolution of the Canids</div Scale><div id=ScaleBar style="width:1px; float:left; height:40em; padding:0; background-color:#242020" />em;
 height:1.47058823529em;
 margin-left:0.0em;
 width:0.7em;
">em; 
"> em;
 height:38.5294117647em;
 margin-left:0em;
 width:0.7em;
">em; 
"> em;
 height:24.8223529412em;
 margin-left:0.7em;
 width:6.3em;
">em; 
"> em;
 height:1.47058823529em;
 margin-left:0.7em;
 width:6.3em;
">em; 
">Cretaceous
em;
 height:11.8647058824em;
 margin-left:0.7em;
 width:6.3em;
">em; 
"> em;
 height:1.36235294118em;
 margin-left:0.7em;
 width:6.3em;
">em; 
">Quaternary
em;
 height:5.54588235294em;
 margin-left:1.82em;
 width:5.18em;
">em; 
">Palæocene
em;
 height:12.7823529412em;
 margin-left:1.82em;
 width:5.18em;
">em; 
">Eocene
em;
 height:6.29411764706em;
 margin-left:1.82em;
 width:5.18em;
">em; 
">Oligocene
em;
 height:2.07176470588em;
 margin-left:1.82em;
 width:5.18em;
">em; 
">Pliocene
em;
 height:10.3105882353em;
 margin-left:1.82em;
 width:5.18em;
">em; 
">Miocene
em;
 height:37.4670588235em;
 margin-left:1.764em;
 width:0.056em;
">em; 
"> em;
 height:40em;
 margin-left:6.93em;
 width:0.07em;
">em; 
"> em;
 height:40em;
 margin-left:0.63em;
 width:0.07em;
">em; 
"> </div Timeline>em;
">←K-P mass extinction
em;
">←First Hesperocyoninae 
em;
">←First Borophaginae
em;
">←Caninae
em;
">←Modern-looking dogsem;
">←Canine<br>radiationem;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">P<br>a<br>l<br>æ<br>o<br>g<br>e<br>n<br>e
em;
">em;
">N<br>e<br>o<br>g<br>e<br>n<br>e
em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
"></div containsTSN> Cenozoic
 Mesozoic
</div Legend>An approximate timescale of key events in canid evolution.<br>For precise dates, see text.<br>Axis scale: millions of years ago.<br>
px; padding-left:5px; text-align:left; background-color:rgb(254,214,123); background-image: -moz-linear-gradient(left, rgba(255,255,255,1), rgba(254,217,106,1) 15%, rgba(254,217,106,1)); background-image: -o-linear-gradient(left, rgba(255,255,255,1), rgba(254,217,106,1) 15%, rgba(254,217,106,1)); background-image: -webkit-linear-gradient(left, rgba(255,255,255,1), rgba(254,217,106,1) 15%, rgba(254,217,106,1)); background-image: linear-gradient(to right, rgba(255,255,255,1), rgba(254,217,106,1) 15%, rgba(254,217,106,1));">PreꞒ
px; width:px;">Ꞓ
px; width:px;">O
px; width:px;">S
px; width:px;">D
px; width:px;">C
px; width:px;">P
px; width:px;">T
px; width:px;">J
px; width:px;">K
px; width:px;">
px; width:px;">
↓</div caption></div Container>
Oligocene epoch.
By the Oligocene, all three subfamilies of canids (Hesperocyoninae, Borophaginae, and Caninae) had appeared in the fossil records of North America. The earliest and most primitive branch of the Canidae was the Hesperocyoninae lineage, which included the coyote-sized "Mesocyon" of the Oligocene (38-24 Mya). These early canids probably evolved for the fast pursuit of prey in a grassland habitat; they resembled modern civets in appearance. Hesperocyonines eventually became extinct in the middle Miocene. One of the early members of the Hesperocyonines, the genus "Hesperocyon", gave rise to "Archaeocyon" and "Leptocyon". These branches led to the borophagine and canine radiations.
Miocene epoch.
Around 9–10 Mya during the Late Miocene, "Canis", "Urocyon", and "Vulpes" genera expanded from southwestern North America, where the canine radiation began. The success of these canines was related to the development of lower carnassials that were capable of both mastication and shearing. Around 8 Mya, the Beringian land bridge allowed members of the genus "Eucyon" a means to enter Asia and they continued on to colonise Europe.
Pliocene epoch.
During the Pliocene, around 4–5 Mya, "Canis lepophagus" appeared in North America. This was small and sometimes coyote-like. Others were wolf-like in characteristics."Canis latrans" (the coyote) is theorized to have descended from "Canis lepophagus".
The formation of the Isthmus of Panama, about 3 Mya, joined South America to North America, allowing canids to invade South America, where they diversified. However the most recent common ancestor of the South American canids lived in North America some 4 Mya and the likelihood is that there were more than one incursion across the new land bridge. One of the resulting lineages consisted of the gray fox ("Urocyon cinereoargentus") and the now extinct dire wolf ("Canis dirus"). The other lineage consisted of the so-called South American endemic species, the maned wolf ("Chrysocyon brachyurus"), the short-eared dog ("Atelocynus microtis"), the bush dog ("Speothos venaticus"), the crab-eating fox ("Cerdocyon thous") and the South American foxes ("Lycalopex" spp.). The monophyly of this group has been established by molecular means.
Pleistocene epoch.
During the Pleistocene, the North American wolf line appeared, with "Canis edwardii", clearly identifiable as a wolf, and "Canis rufus" appeared, possibly a direct descendent of "Canis edwardii". Around 0.8 Mya, "Canis ambrusteri" emerged in North America. A large wolf, it was found all over North and Central America, and was eventually supplanted by its descendant, the dire wolf, which then spread into South America during the late Pleistocene.
By 0.3 Mya, a number of subspecies of the gray wolf ("Canis lupus") had developed and had spread throughout Europe and northern Asia. The gray wolf colonized North America during the late Rancholabrean era across the Bering land bridge, there being at least three separate invasions, with each one consisting of one or more different Eurasian gray wolf clades. MtDNA studies have shown that there are at least four extant "C. lupus" lineages. The dire wolf shared its habitat with the gray wolf but became extinct in a large-scale extinction event that occurred around 11,500 years ago. It may have been more of a scavenger than a hunter; its molars appear to be adapted for crushing bones and it may have died out as a result of the extinction of the large herbivorous animals on whose carcases it relied.
Characteristics.
Wild canids are found on every continent except Antarctica, and inhabit a wide range of different habitats, including deserts, mountains, forests, and grasslands. They vary in size from the fennec fox, which may be as little as 24 cm in length and weigh 0.6 kg, to the gray wolf, which may be up to 160 cm long, and can weigh up to 39 kg. Only a few species are arboreal – the North American gray fox, the closely related Channel Island fox, and the raccoon dog habitually climb trees.
All canids have a similar basic form, as exemplified by the grey wolf, although the relative length of muzzle, limbs, ears and tail vary considerably between species. With the exceptions of the bush dog, raccoon dog, and some domestic breeds of "Canis lupus", canids have relatively long legs and lithe bodies, adapted for chasing prey. The tails are bushy and the length and quality of the pelage varies with the season. The muzzle portion of the skull is much more elongated than that of the cat family. The zygomatic arches are wide, there is a transverse lambdoidal ridge at the rear of the cranium and in some species, a sagittal crest running from front to back. The bony orbits around the eye never form a complete ring and the auditory bullae are smooth and rounded.
All canids are digitigrade, meaning they walk on their toes. The tip of the nose is always naked, as are the cushioned pads on the soles of the feet. These latter consist of a single pad behind the tip of each toe and a more-or-less three-lobed central pad under the roots of the digits. Hairs grow between the pads and in the Arctic fox, the sole of the foot is densely covered with hair at some times of year. With the exception of the four-toed African hunting dog ("Lycaon pictus"), there are five toes on the forefeet but the pollex (thumb) is reduced and does not reach the ground. On the hind feet, there are four toes, but in some domestic dogs, a fifth vestigial toe, known as a dewclaw, is sometimes present but has no anatomical connection to the rest of the foot. The slightly curved nails are non-retractile and more or less blunt.
The penis in male canids is supported by a bone called the baculum. It also contains a structure at the base called the bulbus glandis which helps to create a copulatory tie during mating, locking the animals together for up to an hour. Young canids are born blind, with their eyes opening a few weeks after birth. All living canids (Caninae) have a ligament analogous to the nuchal ligament of ungulates used to maintain the posture of the head and neck with little active muscle exertion; this ligament allows them to conserve energy while running long distances following scent trails with their nose to the ground. However, based on skeletal details of the neck, at least some Borophaginae (such as "Aelurodon") are believed to have lacked this ligament.
Dentition.
 Most canids have 42 teeth, with a dental formula of: 3.1.4.23.1.4.3. The bush dog has only one upper molar with two below, the dhole has two above and two below, and the bat-eared fox has three or four upper molars and four lower ones. As in other members of Carnivora, the upper fourth premolar and lower first molar are adapted as carnassial teeth for slicing flesh, although the bat-eared fox differs in this respect, being largely insectivorous. The molar teeth are strong in most species, allowing the animals to crack open bone to reach the marrow. The deciduous, or baby teeth, formula in canids is 3.1.33.1.3, molars being completely absent.
Social behavior.
Almost all canids are social animals and live together in groups. In general, they are territorial or have a home range and sleep in the open, using their dens only for breeding and sometimes in bad weather. In most foxes, and in many of the true dogs, a male and female pair work together to hunt and to raise their young. Gray wolves and some of the other larger canids live in larger groups called packs. African wild dogs have packs which may consist of twenty to forty animals, and packs of fewer than about seven individuals may be incapable of successful reproduction. Hunting in packs has the advantage that larger prey items can be tackled. Some species form packs or live in small family groups depending on the circumstances, including the type of available food. In most species, some individuals live on their own. Within a canid pack, there is a system of dominance so that the strongest, most experienced animals lead the pack. In most cases, the dominant male and female are the only pack members to breed.
Canids communicate with each other by scent signals, by visual clues and gestures, and by vocalizations such as growls, barks, and howls. In most cases, groups have a home territory from which they drive out other conspecifics. The territory is marked by leaving urine scent marks, which warn trespassing individuals. Social behaviour is also mediated by secretions from glands on the upper surface of the tail near its root and from the anal glands.
Reproduction.
Canids as a group exhibit several reproductive traits that are uncommon among mammals as a whole. They are typically monogamous, provide paternal care to their offspring, have reproductive cycles with lengthy proestral and dioestral phases and have a copulatory tie during mating. They also retain adult offspring in the social group, suppressing the ability of these to breed while making use of the alloparental care they can provide to help raise the next generation of offspring.
During the proestral period, increased levels of oestradiol make the female attractive to the male. There is a rise in progesterone during the oestral phase and the female is now receptive. Following this, the level of oestradiol fluctuates and there is a lengthy dioestrous phase during which the female is pregnant. Pseudo-pregnancy frequently occurs in canids that have ovulated but failed to conceive. A period of anoestrus follows pregnancy or pseudo-pregnancy, there being only one oestral period during each breeding season. Small and medium-sized canids mostly have a gestation period of fifty to sixty days while larger species average sixty to sixty-five days. The time of year in which the breeding season occurs is related to the length of day, as has been demonstrated in the case of several species that have been translocated across the equator to the other hemisphere and experiences a six-month shift of phase. Domestic dogs and certain small canids in captivity may come into oestrus more frequently, perhaps because the photoperiod stimulus breaks down under conditions of artificial lighting.
The size of a litter varies,with from one to sixteen or more pups being born. The young are born small, blind and helpless and require a long period of parental care. They are kept in a den, most often dug into the ground, for warmth and protection. When the young begin eating solid food, both parents, and often other pack members, bring food back for them from the hunt. This is most often vomited up from the adult's stomach. Where such pack involvement in the feeding of the litter occurs, the breeding success rate is higher than is the case where females split from the group and rear their pups in isolation. Young canids may take a year to mature and learn the skills they need to survive. In some species, such as the African wild dog, male offspring usually remain in the natal pack, while females disperse as a group, and join another small group of the opposite sex to form a new pack.
Canids and humans.
One canid, the domestic dog, entered into a partnership with humans a long time ago. This partnership is documented as far back as 26,000 years ago, when the footprints of a young boy aged about eight to ten was found in Chauvet Cave in southern France, walking alongside what was identified as a large dog or wolf. The earliest recorded fossil of a dog was found to be around 36,000 years ago in Goyet Cave in Belgium. Even earlier, wolves were found fossilized in the same locations as humans at sites that date back 300,000 years, showing how far back humans and wolves had interactions with one another. The fact that wolves are pack animals with cooperative social structures may have been the reason that the relationship developed. Humans benefited from the canid's loyalty, cooperation, teamwork, alertness and tracking abilities while the wolf may have benefited from the use of weapons to tackle larger prey and the sharing of food. Humans and dogs may have evolved together. The bond between humans and dogs can be seen in the burial of dogs with their owners as early as 11,000 years ago in the Americas and 8,500 years ago in Europe.
Among canids, only the gray wolf has widely been known to prey on humans. Nonetheless, at least two records have coyotes killing humans, and two have golden jackals killing children. Human beings have trapped and hunted some canid species for their fur and, especially the gray wolf, coyote and the red fox, for sport. Canids such as the dhole are now endangered in the wild because of persecution, habitat loss, a depletion of ungulate prey species and transmission of diseases from domestic dogs.
Extant and recently extinct species.
All extant species of family Canidae are in subfamily Caninae.
Prehistoric Canidae.
Except where otherwise stated, the following classification is based on a 1994 paper by Xiaoming Wang, curator of terrestrial mammals at the Natural History Museum of Los Angeles County on the systematics of the subfamily Hesperocyoninae, a 1999 paper by Wang, together with the zoologists Richard H. Tedford and Beryl E. Taylor on the subfamily Borophaginae, and a 2009 paper by Tedford, Wang and Taylor on the North American fossil Caninae.
Subfamily Borophaginae.
† (Mya = million years ago) (million years = in existence)
Subfamily Hesperocyoninae.
† (Mya = million years ago)

</doc>
<doc id="6739" url="http://en.wikipedia.org/wiki?curid=6739" title="Subspecies of Canis lupus">
Subspecies of Canis lupus

"Canis lupus", the gray wolf, has 40 subspecies currently described, including the dingo, "Canis lupus dingo", and the domestic dog, "Canis lupus familiaris", and many subspecies of wolf throughout the Northern hemisphere. The nominate subspecies is "Canis lupus lupus".
"Canis lupus" is assessed as Least Concern by the IUCN, as its relatively widespread range and stable population trend mean that the species, at global level, does not meet, or nearly meet, any of the criteria for the threatened categories. However, some local populations are classified as Endangered, and some subspecies are endangered or extinct.
Biological taxonomy is not fixed, and placement of taxa is reviewed as a result of new research. The current categorization of subspecies of "Canis lupus" is shown below. Also included are synonyms, which are now discarded duplicate or incorrect namings, or in the case of the domestic dog synonyms, old taxa referring to subspecies of domestic dog which, when the dog was declared a subspecies itself, had nowhere else to go. Common names are given but may vary, as they have no set meaning.
Geographical variations.
Wolves show a great deal of polymorphism geographically, though they can interbreed. The Zoological Gardens of London for example once successfully managed to mate a male European wolf to an Indian female, resulting in a pup bearing an almost exact likeness to its sire.
Europe.
European wolves tend to have fur with less soft wool intermixed than American wolves. Their heads are narrower, their ears longer, higher placed and somewhat closer to each other. Their loins are more slender, their legs longer, their feet narrower, and their tails more thinly clothed with fur. Pelt color in European wolves ranges from white, cream, red, grey and black, sometimes with all colors combined. Wolves in central Europe tend to be more richly colored than those in Northern Europe. Eastern European wolves tend to be shorter and more heavily built than Northern Russian ones.
North America.
North American wolves are generally the same size as European wolves, but have shorter legs, larger, rounder heads, broader, more obtuse muzzles, and a sensible depression at the union of nose and forehead, which is more arched and broad. Their ears are shorter and have a more conical form. They typically lack the black mark on the forelegs, as is the case in European races. They have long and comparatively fine fur, mixed with a shorter wooly hair, and are more robust. Fur color in American wolves ranges from white, black, red, yellow, brown, grey, and grizzled skins, and others representing every shade between, although usually each locality has its prevailing tint. There are pronounced differences in North American wolves of different localities; wolves from Texas and New Mexico are comparatively slim animals with small teeth. Mexican wolves in particular resemble some European wolves in stature, though their heads are usually broader, their necks thicker, their ears longer and their tails shorter. Wolves of the central and northern chains of the Rocky Mountains and coastal ranges are more formidable animals than the more southern plains wolves, and resemble Russian and Scandinavian wolves in size and proportions.
List of subspecies.
"Canis lupus" subspecies.
Subspecies as of 2005[ [update]]:
Disputed subspecies and species.
Two subspecies not mentioned in the list above are the Italian Wolf ("Canis lupus italicus") and the Iberian Wolf ("Canis lupus signatus"). The wolves of the Italian and Iberian peninsulas have morphologically distinct features from other Eurasian wolves and each are considered by their researchers to represent their own subspecies.
The genetic distinction of the Italian wolf subspecies was recently supported by analysis which consistently assigned all the wolf genotypes of a sample in Italy to a single group. This population also showed a unique mitochondrial DNA control-region haplotype, the absence of private alleles and lower heterozygosity at microsatellite loci, as compared to other wolf populations.
Recent genetic research suggests that the Indian Wolf populations in the Indian subcontinent may represent a distinct species from their conspecifics. Similar results were obtained for the Himalayan wolf, which is traditionally placed under the Tibetan wolf ("Canis lupus chanco").

</doc>
<doc id="6742" url="http://en.wikipedia.org/wiki?curid=6742" title="Central Asia">
Central Asia

Central Asia is the core region of the Asian continent and stretches from the Caspian Sea in the west to China in the east and from Afghanistan in the south to Russia in the north. It is also sometimes referred to as Middle Asia, and, colloquially, "the 'stans" (as the six countries generally considered to be within the region all have names ending with the Persian suffix "-stan", meaning "land of") and is within the scope of the wider Eurasian continent. The region, along with Russia, is also part of "the great pivot" as per the Heartland Theory of Halford Mackinder, which says that the power which controls Central Asia—richly endowed with natural resources—shall ultimately be the "empire of the world".
In modern contexts, all definitions of Central Asia include these five republics of the former Soviet Union: Kazakhstan (pop. 17 million), Kyrgyzstan (5.7 million), Tajikistan (8.0 million), Turkmenistan (5.2 million), and Uzbekistan (30 million), for a total population of about 66 million as of 2013–2014. Afghanistan (pop. 31.1 million) is also sometimes included.
Various definitions of Central Asia's exact composition exist, and not one definition is universally accepted. Despite this uncertainty in defining borders, the region does have some important overall characteristics. For one, Central Asia has historically been closely tied to its nomadic peoples and the Silk Road. As a result, it has acted as a crossroads for the movement of people, goods, and ideas between Europe, Western Asia, South Asia, and East Asia.
During pre-Islamic and early Islamic times, Central Asia was a predominantly Iranian region that included the sedentary Eastern Iranian-speaking Bactrians, Sogdians and Chorasmians, and the semi-nomadic Scythians and Parthians. The ancient sedentary population played an important role in the history of Central Asia. After expansion by Turkic peoples, Central Asia also became the homeland for many Turkic peoples, including the Kazakhs, Uzbeks, Turkmen, Kyrgyz, Uyghurs and other extinct Turkic nations. Central Asia is sometimes referred to as Turkestan.
Since the earliest of times, Central Asia has been a crossroads between different civilizations. The Silk Road connected Muslim lands with the people of Europe, India, and China. This crossroads position has intensified the conflict between what Andrew Phillips and Paul James call continuing formations of tribalism and traditionalism and intensifying processes of modernization. They argue that:
In Central Asia the collision of modernity and tradition led all but the most deracinated of the intellectuals-clerics to seek salvation in reconstituted variants of traditional identities rather than succumb to the modern European idea of nationalism. The inability of the elites to form a united front, as demonstrated in the numerous declarations of autonomy by different authorities during the Russian civil war, paved the way for the Soviet re-conquest of Central Asia in the early 1920s.
From the mid-19th century, up to the end of the 20th century, most of Central Asia was part of the Russian Empire and later the Soviet Union, both being Slavic-majority countries. As of 2011, the 5 "'stans'" are still home to about 7 million Russians and 500,000 Ukrainians.
Definitions.
The idea of Central Asia as a distinct region of the world was introduced in 1843 by the geographer Alexander von Humboldt. The borders of Central Asia are subject to multiple definitions.
The most limited definition was the official one of the Soviet Union, which defined Middle Asia as consisting solely of Uzbekistan, Turkmenistan, Tajikistan and Kyrgyzstan. This definition was also often used outside the USSR during this period.
However, the Russian culture has two distinct terms: "Средняя Азия" ("Srednjaja Azija" or "Middle Asia", the narrower definition, which includes only those traditionally non-Slavic, Central Asian lands that were incorporated within those borders of historical Russia) and "Центральная Азия" ("Central'naja Azija" or "Central Asia", the wider definition, which includes Central Asian lands that have never been part of historical Russia).
Soon after independence, the leaders of the four former Soviet Central Asian Republics met in Tashkent and declared that the definition of Central Asia should include Kazakhstan as well as the original four included by the Soviets. Since then, this has become the most common definition of Central Asia.
The UNESCO general history of Central Asia, written just before the collapse of the USSR, defines the region based on climate and uses far larger borders. According to it, Central Asia includes Mongolia, Tibet, northeast Iran (Golestan, North Khorasan and Razavi provinces), central-east Russia south of the Taiga, Afghanistan, Pakistan, northern part of India, and the former Central Asian Soviet republics (the five "Stans" of the former Soviet Union).
An alternative method is to define the region based on ethnicity, and in particular, areas populated by Eastern Turkic, Eastern Iranian, or Mongolian peoples. These areas include Xinjiang Uyghur Autonomous Region, the Turkic regions of southern Siberia, the five republics, and Afghan Turkestan. Afghanistan as a whole, the northern and western areas of Pakistan and the Kashmir Valley of India may also be included. The Tibetans and Ladakhi are also included. Insofar, most of the mentioned peoples are considered the "indigenous" peoples of the vast region.
There are several places that claim to be the geographic center of Asia, for example Kyzyl, the capital of Tuva in the Russian Federation, and a village 200 mi north of Ürümqi, the capital of the Xinjiang region of China.
Geography.
Central Asia is an extremely large region of varied geography, including high passes and mountains (Tian Shan), vast deserts (Kara Kum, Kyzyl Kum, Taklamakan), and especially treeless, grassy steppes.
The vast steppe areas of Central Asia are considered together with the steppes of Eastern Europe as a homogeneous geographical zone known as the Eurasian Steppe.
Much of the land of Central Asia is too dry or too rugged for farming. The Gobi desert extends from the foot of the Pamirs, 77° E, to the Great Khingan (Da Hinggan) Mountains, 116°–118° E.
Central Asia has the following geographic extremes:
A majority of the people earn a living by herding livestock. Industrial activity centers in the region's cities.
Major rivers of the region include the Amu Darya, the Syr Darya, Irtysh, the Hari River and the Murghab River. Major bodies of water include the Aral Sea and Lake Balkhash, both of which are part of the huge west-central Asian endorheic basin that also includes the Caspian Sea.
Both of these bodies of water have shrunk significantly in recent decades due to diversion of water from rivers that feed them for irrigation and industrial purposes. Water is an extremely valuable resource in arid Central Asia and can lead to rather significant international disputes.
Divisions.
The northern belt is part of the Eurasian Steppe. In the northwest, north of the Caspian Sea, Central Asia merges into the Russian Steppe. To the northeast, Dzungaria and the Tarim Basin may sometimes be included in Central Asia. Just west of Dzungaria, Zhetysu, or Semirechye, is south of Lake Balkhash and north of the Tian Shan Mountains. Khorezm is south of the Aral Sea along the Amu Darya. Southeast of the Aral Sea, Maveranahr is between the Amu Darya and Syr Darya. Transoxiana is the land north of the middle and upper Amu Darya (Oxus). Bactria included northern Afghanistan and the upper Amu Darya. Sogdiana was north of Bactria and included the trading cities of Bukhara and Samarkhand. Khorasan and Margiana approximate northeastern Iran. The Kyzyl Kum Desert is northeast of the Amu Darya, and the Karakum Desert southwest of it.
Climate.
Because Central Asia is not buffered by a large body of water, temperature fluctuations are more severe. In most of the places the climate is moderate.
According to the WWF Ecozones system, Central Asia is part of the Palearctic ecozone. The largest biomes in Central Asia are the temperate grasslands, savannas, and shrublands biome. Central Asia also contains the montane grasslands and shrublands, deserts and xeric shrublands as well as temperate coniferous forests biomes.
History.
The history of Central Asia is defined by the area's climate and geography. The aridness of the region made agriculture difficult, and its distance from the sea cut it off from much trade. Thus, few major cities developed in the region; instead, the area was for millennia dominated by the nomadic horse peoples of the steppe.
Relations between the steppe nomads and the settled people in and around Central Asia were long marked by conflict. The nomadic lifestyle was well suited to warfare, and the steppe horse riders became some of the most militarily potent people in the world, limited only by their lack of internal unity. Any internal unity that was achieved was most probably due to the influence of the Silk Road, which traveled along Central Asia. Periodically, great leaders or changing conditions would organize several tribes into one force and create an almost unstoppable power. These included the Hun invasion of Europe, the Wu Hu attacks on China and most notably the Mongol conquest of much of Eurasia.
During pre-Islamic and early Islamic times, southern Central Asia was inhabited predominantly by speakers of Iranian languages. Among the ancient sedentary Iranian peoples, the Sogdians and Chorasmians played an important role, while Iranian peoples such as Scythians and the later on Alans lived a nomadic or semi-nomadic lifestyle. The well-preserved Tarim mummies with Caucasoid features have been found in the Tarim Basin.
The main migration of Turkic peoples occurred between the 5th and 10th centuries, when they spread across most of Central Asia. The Tang Chinese were defeated by the Arabs at the battle of Talas in 751, marking the end of the Tang Dynasty's western expansion. The Tibetan Empire would take the chance to rule portion of Central Asia along with South Asia. During the 13th and 14th centuries, the Mongols conquered and ruled the largest contiguous empire in recorded history. Most of Central Asia fell under the control of the Chagatai Khanate. 
The dominance of the nomads ended in the 16th century, as firearms allowed settled peoples to gain control of the region. Russia, China, and other powers expanded into the region and had captured the bulk of Central Asia by the end of the 19th century. After the Russian Revolution, the western Central Asian regions were incorporated into the Soviet Union. The eastern part Central Asia, known as East Turkistan or Xinjiang, was incorporated into the People's Republic of China. Mongolia remained independent but became a Soviet satellite state. Afghanistan remained relatively independent of major influence by the USSR until the Soviet invasion of 1979.
The Soviet areas of Central Asia saw much industrialization and construction of infrastructure, but also the suppression of local cultures, hundreds of thousands of deaths from failed collectivization programs, and a lasting legacy of ethnic tensions and environmental problems. Soviet authorities deported millions of people, including entire nationalities, from western areas of the USSR to Central Asia and Siberia. According to Touraj Atabaki and Sanjyot Mehendale, "From 1959 to 1970, about two million people from various parts of the Soviet Union migrated to Central Asia, of which about one million moved to Kazakhstan."
With the collapse of the Soviet Union, five countries gained independence. In nearly all the new states, former Communist Party officials retained power as local strongmen. None of the new republics could be considered functional democracies in the early days of independence, although in recent years Kyrgyzstan, Kazakhstan and Mongolia have made further progress towards more open societies, unlike Uzbekistan, Tajikistan, and Turkmenistan, which have maintained many Soviet-style repressive tactics.
Culture.
Religions.
Islam is the religion most common in the Central Asian Republics, Afghanistan, Xinjiang and the peripheral western regions, such as Bashkortostan. Most Central Asian Muslims are Sunni, although there are sizable Shia minorities in Afghanistan and Tajikistan.
Buddhism and Zoroastrianism were the major faiths in Central Asia prior to the arrival of Islam. Zoroastrian influence is still felt today in such celebrations as Nowruz, held in all five of the Central Asian states.
Buddhism was a prominent religion in Central Asia prior to the arrival of Islam, and the transmission of Buddhism along the Silk Road eventually brought the religion to China. Amongst the Turkic peoples, Tengrianism was the popular religion before arrival of Islam. Tibetan Buddhism is most common in Tibet, Mongolia, Ladakh and the southern Russian regions of Siberia.
The form of Christianity most practiced in the region in previous centuries was Nestorianism, but now the largest denomination is the Russian Orthodox Church, with many members in Kazakhstan.
The Bukharan Jews were once a sizable community in Uzbekistan and Tajikistan, but nearly all have emigrated since the dissolution of the Soviet Union.
In Siberia, Shamanism is practiced, including forms of divination, such as Kumalak.
Contact and migration with Han people from China has brought Confucianism, Daoism, Mahayana Buddhism, and other Chinese folk beliefs into the region.
Arts.
At the crossroads of Asia, shamanistic practices live alongside Buddhism. Thus, Yama, Lord of Death, was revered in Tibet as a spiritual guardian and judge. Mongolian Buddhism, in particular, was influenced by Tibetan Buddhism. The Qianlong Emperor of China in the 18th century was Tibetan Buddhist and would sometimes travel from Beijing to other cities for personal religious worship.
Central Asia also has an indigenous form of improvisational oral poetry that is over 1000 years old. It is principally practiced in Kyrgyzstan and Kazakhstan by "akyns", lyrical improvisationists. They engage in lyrical battles, the "aitysh" or the "alym sabak". The tradition arose out of early bardic oral historians. They are usually accompanied by a stringed instrument—in Kyrgyzstan, a three-stringed "komuz", and in Kazakhstan, a similar two-stringed instrument, the dombra.
Photography in Central Asia began to develop after 1882, when a Russian Mennonite photographer named Wilhelm Penner moved to the Khanate of Khiva during the Mennonite migration to Central Asia led by Claas Epp, Jr.. Upon his arrival to Khanate of Khiva, Penner shared his photography skills with a local student Khudaybergen Divanov, who later became the founder of the Uzbek photography.
Some also learn to sing the "Manas", Kyrgyzstan's epic poem (those who learn the "Manas" exclusively but do not improvise are called "manaschis"). During Soviet rule, "akyn" performance was co-opted by the authorities and subsequently declined in popularity. With the fall of the Soviet Union, it has enjoyed a resurgence, although "akyns" still do use their art to campaign for political candidates. A 2005 "Washington Post" article proposed a similarity between the improvisational art of "akyns" and modern freestyle rap performed in the West.
As a consequence of Russian colonization, European fine arts – painting, sculpture and graphics – have developed in Central Asia. The first years of the Soviet regime saw the appearance of modernism, which took inspiration from the Russian avant-garde movement. Until the 80's Central Asian arts had developed along with general tendencies of Soviet arts. In the 90's, arts of the region underwent some significant changes. Institutionally speaking, some fields of arts were regulated by the birth of the art market, some stayed as representatives of official views, while many were sponsored by international organizations. The years of 1990 – 2000 were times for the establishment of contemporary arts. In the region, many important international exhibitions are taking place, Central Asian art is represented in European and American museums, and the Central Asian Pavilion at the Venice Biennale has been organized since 2005.
Demographics.
By a broad definition including Mongolia and Afghanistan, more than 90 million people live in Central Asia, about 2% of Asia's total population. Of the regions of Asia, only North Asia has fewer people. It has a population density of 9 people per km2, vastly less than the 80.5 people per km2 of the continent as a whole.
Languages.
Russian, as well as being spoken by around six million ethnic Russians and Ukrainians of Central Asia, is the de facto lingua franca throughout the former Soviet Central Asian Republics. Mandarin Chinese has an equally dominant presence in Inner Mongolia, Qinghai and Xinjiang.
The languages of the majority of the inhabitants of the former Soviet Central Asian Republics come from the Turkic language group. Turkmen, is mainly spoken in Turkmenistan, and as a minority language in Afghanistan, Russia, Iran and Turkey. Kazakh and Kyrgyz are related languages of the Kypchak group of Turkic languages and are spoken throughout Kazakhstan, Kyrgyzstan, and as a minority language in Tajikistan, Afghanistan and Xinjiang. Uzbek and Uyghur are spoken in Uzbekistan, Tajikistan, Kyrgyzstan, Afghanistan and Xinjiang.
The Turkic languages may belong to a larger, but controversial, Altaic language family, which includes Mongolian. Mongolian is spoken throughout Mongolia and into Buryatia, Kalmyk, Tuva, Inner Mongolia, and Xinjiang.
Middle Iranian languages were once spoken throughout Central Asia, such as the once prominent Sogdian, Khwarezmian, Bactrian and Scythian, which are now extinct and belonged to the Eastern Iranian family. The Eastern Iranian Pashto language is still spoken in Afghanistan and northwestern Pakistan. Other minor Eastern Iranian languages such as Shughni, Munji, Ishkashimi, Sarikoli, Wakhi, Yaghnobi and Ossetic are also spoken at various places in Central Asia. Varieties of Persian are also spoken as a major language in the region, locally known as Dari (in Afghanistan), Tajik (in Tajikistan and Uzbekistan), and Bukhori (by the Bukharan Jews of Central Asia).
Tocharian, another Indo-European language group, which was once predominant in oases on the northern edge of the Tarim Basin of Xinjiang, is now extinct.
Other language groups include the Tibetic languages, spoken by around six million people across the Tibetan Plateau and into Qinghai, Sichuan, Ladakh and Baltistan, and the Nuristani languages of northeastern Afghanistan. Dardic languages, such as Shina, Kashmiri, Pashayi and Khowar, are also spoken in eastern Afghanistan, the Gilgit-Baltistan and Khyber Pakhtunkhwa of Pakistan and the Kashmir state of India.
Geostrategy.
Central Asia has long been a strategic location merely because of its proximity to several great powers on the Eurasian landmass. The region itself never held a dominant stationary population nor was able to make use of natural resources. Thus, it has rarely throughout history become the seat of power for an empire or influential state. Central Asia has been divided, redivided, conquered out of existence, and fragmented time and time again. Central Asia has served more as the battleground for outside powers than as a power in its own right.
Central Asia had both the advantage and disadvantage of a central location between four historical seats of power. From its central location, it has access to trade routes to and from all the regional powers. On the other hand, it has been continuously vulnerable to attack from all sides throughout its history, resulting in political fragmentation or outright power vacuum, as it is successively dominated.
In the post–Cold War era, Central Asia is an ethnic cauldron, prone to instability and conflicts, without a sense of national identity, but rather a mess of historical cultural influences, tribal and clan loyalties, and religious fervor. Projecting influence into the area is no longer just Russia, but also Turkey, Iran, China, Pakistan, India and the United States:
Russian historian Lev Gumilev wrote that Xiongnu, Mongols (Mongol Empire, Zunghar Khanate) and Turkic peoples (Turkic Khaganate, Uyghur Khaganate) played a role to stop Chinese aggression to the north. The Turkic Khaganate had special policy against Chinese assimilation policy.
War on Terror.
In the context of the United States' War on Terror, Central Asia has once again become the center of geostrategic calculations. Pakistan's status has been upgraded by the U.S. government to Major non-NATO ally because of its central role in serving as a staging point for the invasion of Afghanistan, providing intelligence on Al-Qaeda operations in the region, and leading the hunt on Osama bin Laden.
Afghanistan, which had served as a haven and source of support for Al-Qaeda under the protection of Mullah Omar and the Taliban, was the target of a U.S. invasion in 2001 and ongoing reconstruction and drug-eradication efforts. U.S. military bases have also been established in Uzbekistan and Kyrgyzstan, causing both Russia and the People's Republic of China to voice their concern over a permanent U.S. military presence in the region.
Western governments have accused Russia, China and the former Soviet republics of justifying the suppression of separatist movements, and the associated ethnics and religion with the War on Terror.
Major cultural and economic centers.
      Cities within the regular definition of Central Asia and Afghanistan

</doc>
<doc id="6744" url="http://en.wikipedia.org/wiki?curid=6744" title="Constantine II">
Constantine II

Constantine II may refer to:

</doc>
<doc id="6745" url="http://en.wikipedia.org/wiki?curid=6745" title="Couscous">
Couscous

Couscous ( or ; Berber: ⵙⴽⵙⵓ, "seksu", Arabic: كسكس, "kuskus" or كسكسو "kseksou") is a traditional Berber dish of semolina (granules of durum wheat) which is cooked by steaming. It is traditionally served with a meat or vegetable stew spooned over it. Couscous is a staple food throughout the North African cuisines of Tunisia, Algeria, Morocco, Mauritania and Libya and to a lesser extent in the Middle East and Sicily.
Couscous was voted as the third-favourite dish of French people in 2011 in a study by TNS Sofres for magazine "Vie Pratique Gourmand", and the first in the east of France.
Etymology.
The original name is derived from the Berber "seksu" or "kesksu", meaning "well rolled", "well formed", or "rounded".
Numerous different names and pronunciations for couscous exist around the world. Couscous is or in the United Kingdom and only the latter in the United States. It is in Arabic: كسكسي‎, pronounced "kuskusi", while it is also known as "seksu" or "kesksu" in Morocco; "ṭa`ām" (طعام, literally meaning "food") in Algeria; "kosksi" or "kuseksi" in Tunisia, Libya; "kuskusi" (كسكسي) in Egypt; and "keskes" in Tuareg.
History.
One of the first written references is from an anonymous 13th-century North African cookbook, "Kitāb al-tabǐkh fǐ al-Maghrib wa'l-Andalus" "The cookbook of the Maghreb and Al-Andalus", with a recipe for couscous that was 'known all over the world'. To this day, couscous is known as 'the North Africa national dish'. Couscous was known to the Nasrid royalty in Granada as well. And in the 13th century a Syrian historian from Aleppo includes four references for couscous. These early mentions show that couscous spread rapidly, but generally that couscous was common from Tripolitania to the west, while from Cyrenaica to the east the main cuisine was Egyptian, with couscous as an occasional dish. Today, in Egypt and the Middle East, couscous is known, but in Tunisia, Algeria, Morocco and Libya couscous is a staple. It is the national dish of the Maghreb countries. Couscous reached Turkey from Syria to in the 16th century and is eaten in most of the Turkish southern provinces.
Couscous is a traditional meal of the cuisine from Trapani. In Rome Bartolomeo Scappi's culinary guide of 1570 describes a Moorish dish, "succussu"; in Tuscany.
One of the earliest references to couscous in France is in Brittany, in a letter dated January 12, 1699. But it made an earlier appearance in Provence, where the traveler Jean-Jacques Bouchard wrote of eating it in Toulon in 1630.
Couscous was originally made from millet. Historians have different opinions as to when wheat began to replace the use of millet. The conversion seems to have occurred sometime in the 20th century, although many regions continue to use the traditional millet. Couscous seems to have a North African origin. Archaeological evidence dating back to the 10th century, consisting of kitchen utensils needed to prepare this dish, has been found in this part of the world.
In some regions couscous is made from Farina or coarsely ground barley or pearl millet. In Brazil, the traditional couscous is made from cornmeal.
Preparation.
The semolina is sprinkled with water and rolled with the hands to form small pellets, sprinkled with dry flour to keep them separate, and then sieved. Any pellets which are too small to be finished granules of couscous and fall through the sieve are again rolled and sprinkled with dry semolina and rolled into pellets. This process continues until all the semolina has been formed into tiny granules of couscous. This process is labour-intensive. In the traditional method of preparing couscous, groups of women came together to make large batches over several days, which were then dried in the sun and used for several months. Couscous was traditionally made from the hard part of the durum, the part of the grain that resisted the grinding of the millstone. In modern times, couscous production is largely mechanized, and the product is sold in markets around the world.
In the Sahelian countries of West Africa, such as Mali and Senegal, pearl millet is pounded or milled to the size and consistency necessary for the couscous.
Properly cooked couscous is light and fluffy, not gummy or gritty. Traditionally, North Africans use a food steamer (called a"Taseksut" in Berber, a كِسْكَاس "kiskas" in Arabic or a "couscoussier" in French). The base is a tall metal pot shaped rather like an oil jar in which the meat and vegetables are cooked as a stew. On top of the base, a steamer sits where the couscous is cooked, absorbing the flavours from the stew. The lid to the steamer has holes around its edge so steam can escape. It is also possible to use a pot with a steamer insert. If the holes are too big, the steamer can be lined with damp cheesecloth. There is little archaeological evidence of early diets including couscous, possibly because the original "couscoussier" was probably made from organic materials which could not survive extended exposure to the elements.
Instant couscous.
The couscous that is sold in most Western supermarkets has been pre-steamed and dried; the package directions usually instruct to add 1.5 measures of "boiling" water or stock and butter to each measure of couscous and to cover tightly for 5 minutes. The couscous swells and within a few minutes it is ready to fluff with a fork and serve. Pre-steamed couscous takes less time to prepare than regular couscous, most dried pasta, or dried grains such as rice.
Local variations.
In Tunisia, Algeria, Morocco, and Libya, couscous is generally served with vegetables (carrots, potatoes, turnips, "etc.") cooked in a spicy or mild broth or stew, and some meat (generally, chicken, lamb or mutton).
In Algeria and Morocco it is also served, sometimes at the end of a meal or just by itself, as a delicacy called "seffa". The couscous is usually steamed several times until it is very fluffy and pale in color. It is then sprinkled with almonds, cinnamon and sugar. Traditionally, this dessert is served with milk perfumed with orange flower water, or it can be served plain with buttermilk in a bowl as a cold light soup for supper.
In Tunisia, it is made mostly spicy with harissa sauce and served with almost everything, including lamb, fish, seafood, beef and sometimes in southern regions camel. Fish couscous is a Tunisian specialty and can also be made with octopus or other seafood in hot, red, spicy sauce. Couscous in Tunisia is served on every occasion; it is also served in some regions (mostly during Ramadan), sweetened as a dessert called masfouf.
In Libya, it is mostly served with meat, specifically mostly lamb, but also camel, and very rarely beef, in Tripoli and the western parts of Libya, but not during official ceremonies or weddings. Another way to eat couscous is as a dessert; it is prepared with dates, sesame, and pure honey, and locally referred to as "maghrood".
Israelis typically serve it on occasions and holidays. It was brought by Maghrebi migrants from Tunisia, Algeria, Morocco and Libya to Israel.
In Egypt, couscous is eaten more as a dessert. It is prepared with butter, sugar, cinnamon, raisins, and nuts and topped with cream.
Couscous is also very popular in France, where it is now considered a traditional dish, and has also become popular in Spain, Portugal, Italy, and Greece. Indeed, many polls have indicated that it is often a favorite dish. Study conducted on January 11 and 12, 2006, for the magazine "Notre Temps" based on face-to-face interviews with a sample of people representative of the adult French population, stratified by age, sex, profession of the head of household, region and type of municipality. Couscous is served in many Maghrebi restaurants all over the world. In France, Spain, Italy, and Portugal, the word "couscous" ("cuscús" in Spanish, Portuguese, and Italian) usually refers to couscous together with the stew. Packaged sets containing a box of quick-preparation couscous and a can of vegetables and, generally, meat are sold in French, Spanish, Italian, and Portuguese grocery stores and supermarkets. In France, it is generally served with harissa sauce, a style inherited from the Tunisian cuisine.
In North America, Australia, New Zealand, and the United Kingdom, couscous is available most commonly either plain or pre-flavoured in quick-preparation boxes. In the United States, it is widely available, normally found in the ethnic or health-food section of larger grocery stores.
There are related recipes in Latin America, where a corn meal mix is boiled and moulded into a timbale with other ingredients. Among them, "cuscuz" (]), a popular recipe usually associated with Northeastern Brazil and its diaspora, a steamed cake of corn meal served with sugar and milk, varied meats, cheese and eggs or other ingredients.
Maftoul is considered as a special type of couscous but made from different ingredients and a different shape. It is larger than North African couscous. Maftoul is an Arabic word derived from the root “fa-ta-la”, which means to roll or to twist, which is exactly describing the method used to make maftoul by hand rolling bulgur with wheat flour.
In Israel there is a dish similar to maftoul (by look, not taste) called Ptitim, which has many variations and could appear as a mini triangles, or looking like a rice (that is the original type and it is sometimes called (Ben Gurion rice)) and many other variations.
The most famous variation of ptitim looks like couscous and as Americans started to call it "Israeli couscous" then Osem company started to product it outside Israel under this name.
Ptitim is not the same as Mograbia (a.k.a. maftoul, pearl couscous etc.), though the two can be used similarly. Mograbia is a coated couscous; Ptitim is an extruded paste. The word "maftoul" is sometimes incorrectly used in America to refer to Israeli couscous.
Nutrition.
Couscous has 3.6 g of protein for every 100 grams. Furthermore, couscous contains a 1% fat-to-calorie ratio.

</doc>
<doc id="6746" url="http://en.wikipedia.org/wiki?curid=6746" title="Constantius II">
Constantius II

Constantius II (Latin: "Flavius Julius Constantius Augustus"; 7 August 317 – 3 November 361) was Roman Emperor from 337 to 361. The second son of Constantine I and Fausta, he ascended to the throne with his brothers Constantine II and Constans upon their father's death.
In 340, Constantius' brothers clashed over the western provinces of the empire. The resulting conflict left Constantine II dead and Constans as ruler of the west until he was overthrown and assassinated in 350 by the usurper Magnentius. Unwilling to accept Magnentius as co-ruler, Constantius defeated him at the battles of Mursa Major and Mons Seleucus. Magnentius committed suicide after the latter, leaving Constantius as sole ruler of the empire.
His subsequent military campaigns against Germanic tribes were successful: he defeated the Alamanni in 354 and campaigned across the Danube against the Quadi and Sarmatians in 357. In contrast, the war in the east against the Sassanids continued with mixed results.
In 351, due to the difficulty of managing the empire alone, Constantius elevated his cousin Constantius Gallus to the subordinate rank of "Caesar", but had him executed three years later after receiving scathing reports of his violent and corrupt nature. Shortly thereafter, in 355, Constantius promoted his last surviving cousin, Gallus' younger half-brother, Julian, to the rank of "Caesar".
However, Julian claimed the rank of Augustus in 360, leading to war between the two. Ultimately, no battle was fought as Constantius became ill and died late in 361, though not before naming his opponent as his successor.
Life.
Constantius was born in 317 at Sirmium, Pannonia. He was the third son of Constantine the Great, and second by his second wife Fausta, the daughter of Maximian. Constantius was made Caesar by his father on 13 November 324. 
In 336, religious unrest in Armenia and tense relations between Constantine and king Shapur II caused war to break out between Rome and Sassanid Persia. Though he made initial preparations for the war, Constantine fell ill and sent Constantius east to take command of the eastern frontier. Before Constantius arrived, the Persian general Narses, who was possibly the king's brother, overran Mesopotamia and captured Amida. Constantius promptly attacked Narses, and after suffering minor setbacks defeated and killed Narses at the Battle of Narasara. Constantius captured Amida and initiated a major refortification the city, enhancing the city's circuit walls and constructing large towers. He also built a new stronghold in the hinterland nearby, naming it "Antinopolis".
Augustus in the East.
In early 337, Constantius hurried to Constantinople after receiving news that his father was near death. After Constantine died, Constantius buried him with lavish ceremony in the Church of the Holy Apostles. Soon after his father's death Constantius supposedly ordered a massacre of his relatives descended from the second marriage of his paternal grandfather Constantius Chlorus, though the details are unclear. Eutropius, writing between 350 and 370, states that Constantius merely sanctioned “"the act, rather than commanding it"”. The massacre killed two of Constantius' uncles and six of his cousins, including Hannibalianus and Dalmatius, rulers of Pontus and Moesia respectively. The massacre left Constantius, his older brother Constantine II, his younger brother Constans, and three cousins Gallus, Julian and Nepotianus as the only surviving male relatives of Constantine the Great.
Soon after, Constantius met his brothers in Pannonia at Sirmium to formalize the partition of the empire. Constantius received the eastern provinces, including Constantinople, Thrace, Asia Minor, Syria, Egypt, and Cyrenaica; Constantine received Britannia, Gaul, Hispania, and Mauretania; and Constans, initially under the supervision of Constantine II, received Italy, Africa, Illyricum, Pannonia, Macedonia, and Achaea.
Constantius then hurried east to Antioch to resume the war with Persia. While Constantius was away from the eastern frontier in early 337, Shapur assembled a large army, which included war elephants, and launched an attack on Roman territory, laying waste to Mesopotamia and putting the city of Nisibis under siege. Despite initial success, Shapur lifted his siege after his army missed an opportunity to exploit a collapsed wall. When Constantius learned of Shapur's withdrawal from Roman territory, he prepared his army for a counter-attack, drilling them.
Constantius repeatedly defended the eastern border against invasions by the aggressive Sassanid Empire under king Shapur II. These conflicts were mainly limited to Sassanid sieges of the major fortresses of Roman Mesopotamia, including Nisibis (Nusaybin), Singara, and Amida (Diyarbakir). Although Shapur seems to have been victorious in most of these confrontations, the Sassanids were able to achieve little. However, the Romans won a decisive victory at the Battle of Narasara, killing Shapur's brother, Narses. Ultimately, Constantius was able to push back the invasion, and Shapur failed to make any significant gains.
Meanwhile, Constantine II desired to retain control of Constans' realm, leading the brothers into open conflict. Constantine was killed in 340 near Aquileia during an ambush. As a result, Constans took control of his deceased brother’s realms and became sole ruler of the Western two-thirds of the empire. This division lasted until 350, when Constans was assassinated by forces loyal to the usurper Magnentius.
War against Magnentius.
As the only surviving son of Constantine the Great, Constantius felt that the position of emperor was his alone, and he determined to march west to fight the usurper, Magnentius. However, feeling that the east still required some sort of imperial presence, he elevated his cousin Constantius Gallus to Caesar of the eastern provinces. As an extra measure to ensure the loyalty of his cousin, he married the elder of his two sisters, Constantina, to him.
Before facing Magnentius, Constantius first came to terms with Vetranio, a loyal general in Illyricum who had recently been acclaimed emperor by his soldiers. Vetranio immediately sent letters to Constantius pledging his loyalty, which Constantius may have accepted simply in order to stop Magnentius from gaining more support. These events may have been spurred by the action of Constantina, who had since traveled east to marry Gallus. Constantius subsequently sent Vetranio the imperial diadem and acknowledged the general‘s new position as "Augustus". However, when Constantius arrived, Vetranio willingly resigned his position and accepted Constantius’ offer of a comfortable retirement in Bithynia.
In 351, Constantius clashed with Magnentius in Pannonia with a large army. The ensuing Battle of Mursa Major was one of the largest and bloodiest battles ever between two Roman armies. The result was a victory for Constantius, but a costly one. Magnentius survived the battle and, determined to fight on, withdrew into northern Italy. Rather than pursuing his opponent, however, Constantius turned his attention to securing the Danubian border, where he spent the early months of 352 campaigning against the Sarmatians along the middle Danube. After achieving his aims, Constantius advanced on Magnentius in Italy. This action led the cities of Italy to switch their allegiance to him and eject the usurper's garrisons. Again, Magnentius withdrew, this time to southern Gaul.
In 353, Constantius and Magnentius met for the final time at the Battle of Mons Seleucus in southern Gaul, and again Constantius emerged the victor. Magnentius, realizing the futility of continuing his position, committed suicide on 10 August 353.
Sole ruler of the empire.
Constantius spent much of the rest of 353 and early 354 on campaign against the Alamanni on the Danube frontier. The campaign was successful and raiding by the Alamanni ceased temporarily. In the meantime, Constantius had been receiving disturbing reports regarding the actions of his cousin Gallus. Possibly as a result of these reports, Constantius concluded a peace with the Alamanni and traveled to Mediolanum (Milan).
In Mediolanum, Constantius first summoned Ursicinus, Gallus’ "magister equitum", for reasons that remain unclear. Constantius then summoned Gallus and Constantina. Although Gallus and Constantina complied with the order at first, when Constantina died in Bithynia, Gallus began to hesitate. However, after some convincing by one of Constantius’ agents, Gallus continued his journey west, passing through Constantinople and Thrace to Poetovio (Ptuj) in Pannonia.
In Poetovio, Gallus was arrested by the soldiers of Constantius under the command of Barbatio. Gallus was then moved to Pola and interrogated. Gallus claimed that it was Constantina who was to blame for all the trouble while he was in charge of the eastern provinces. This angered Constantius so greatly that he immediately ordered Gallus' execution. He soon changed his mind, however, and recanted the order. Unfortunately for Gallus, this second order was delayed by Eusebius, one of Constantius' eunuchs, and Gallus was executed.
More usurpers and Julian.
On 11 August 355, the magister militum Claudius Silvanus revolted in Gaul. Silvanus had surrendered to Constantius after the Battle of Mursa Major. Constantius had made him magister militum in 353 with the purpose of blocking the German threats, a feat that Silvanus achieved by bribing the German tribes with the money he had collected. A plot organized by members of Constantius' court led the emperor to recall Silvanus. After Silvanus revolted, he received a letter from Constantius recalling him to Milan, but which made no reference to the revolt. Ursicinus, who was meant to replace Silvanus, bribed some troops, and Silvanus was killed.
Constantius realised that too many threats still faced the Empire, however, and he could not possibly handle all of them by himself. So on 6 November 355, he elevated his last remaining male relative, Julian, to the rank of Caesar. A few days later, Julian was married to Helena, the last surviving sister of Constantius. Constantius soon sent Julian off to Gaul.
Constantius spent the next few years overseeing affairs in the western part of the empire primarily from his base at Mediolanum. In 357 he visited Rome for the only time in his life. The same year, he forced Sarmatian and Quadi invaders out of Pannonia and Moesia Inferior, then led a successful counter-attack across the Danube.
In the winter of 357–58, Constantius received ambassadors from Shapur II who demanded that Rome restore the lands surrendered by Narseh. Despite rejecting these terms, Constantius tried to avert war with the Sassanid Empire by sending two embassies to Shapur II. Shapur II nevertheless launched another invasion of Roman Mesopotamia. In 360, when news reached Constantius that Shapur II had destroyed Singara, and taken Kiphas (Hasankeyf), Amida, and Ad Tigris (Cizre), he decided to travel east to face the re-emergent threat.
Usurpation of Julian and crises in the east.
In the meantime, Julian had won some victories against the Alamanni, who had once again invaded Roman Gaul. However, when Constantius requested reinforcements from Julian’s army for the eastern campaign, the Gallic legions revolted and proclaimed Julian "Augustus".
However, on account of the immediate Sassanid threat, Constantius was unable to directly respond to his cousin’s usurpation, other than by sending missives in which he tried to convince Julian to resign the title of "Augustus" and be satisfied with that of "Caesar". By 361, Constantius saw no alternative but to face the usurper with force; and yet the threat of the Sassanids remained. Constantius had already spent part of early 361 unsuccessfully attempting to re-take the fortress of Ad Tigris. After a time he had withdrawn to Antioch to regroup and prepare for a confrontation with Shapur II. The campaigns of the previous year had inflicted heavy losses on the Sassanids, however, and they did not attempt another round of campaigns that year. This temporary respite in hostilities allowed Constantius to turn his full attention to facing Julian.
Death.
Constantius immediately gathered his forces and set off west. However, by the time he reached Mopsuestia in Cilicia, it was clear that he was fatally ill and would not survive to face Julian. Apparently, realising his death was near, Constantius had himself baptised by Euzoius, the Semi-Arian bishop of Antioch, and then declared that Julian was his rightful successor. Constantius II died of fever on 3 November 361.
Marriages and children.
Constantius II was married three times:
First to a daughter of his half-uncle Julius Constantius, whose name is unknown. She was a full-sister of Gallus and a half-sister of Julian. She died c. 352/3.
Second, to Eusebia, a woman of Macedonian originally from the city of Thessaloniki, whom Constantius married before his defeat of Magnentius in 353. She died in 360.
Third and lastly, in 360, to Faustina, who gave birth to Constantius' only child, a posthumous daughter named Flavia Maxima Constantia, who later married Emperor Gratian.
Religious issues.
Constantius seems to have had a particular interest in the religious state of the Roman Empire. As a Christian Roman Emperor, Constantius made a concerted effort to promote Christianity at the expense of Roman polytheism (‘paganism’). He issued a number of edicts designed to carry out this agenda (see below). Constantius also took an active part in attempting to shape the Christian church.
Paganism.
In spite of the some of the edicts issued by Constantius, he was not fanatically anti-pagan – he never made any attempt to disband the various Roman priestly colleges or the Vestal Virgins, he never acted against the various pagan schools, and, at times, he actually made some effort to protect paganism. In fact, he even ordered the election of a priest for Africa. Also, he remained pontifex maximus and was deified by the Roman Senate after his death. His relative moderation toward paganism is reflected by the fact that it was over twenty years after his death, during the reign of Gratian, that any pagan senator protested his treatment of their religion.
Pagan-related edicts issued by Constantius (by himself or with others) included:
Christianity.
Although often considered an Arian, Constantius ultimately preferred a third, compromise version that lay somewhere in between Arianism and the Nicene Creed, retrospectively called Semi-Arianism. During his reign he attempted to mold the Christian church to follow this compromise position, convening several Christian councils. The most notable of these were the Council of Rimini and its twin at Seleuca, which met in 359 and 360 respectively. "Unfortunately for his memory the theologians whose advice he took were ultimately discredited and the malcontents whom he pressed to conform emerged victorious," writes the historian A.H.M. Jones. "The great councils of 359–60 are therefore not reckoned ecumenical in the tradition of the church, and Constantius II is not remembered as a restorer of unity, but as a heretic who arbitrarily imposed his will on the church."
Christian-related edicts issued by Constantius (by himself or with others) included:
Judaism.
Judaism faced some severe restrictions under Constantius, who seems to have followed an anti-Jewish policy in line with that of his father. Early in his reign, Constantius issued a double edict in concert with his brothers limiting the ownership of slaves by Jewish people and banning marriages between Jews and Christian women. A later edict issued by Constantius after becoming sole emperor decreed that a person who was proven to have converted from Christianity to Judaism would have all of his property confiscated by the state. However, Constantius' actions in this regard may not have been so much to do with Jewish religion as with Jewish business—apparently, privately owned Jewish businesses were often in competition with state-owned businesses. As a result, Constantius may have sought to provide an advantage to state-owned businesses by limiting the skilled workers and slaves available to Jewish businesses.
Jew-related edicts issued by Constantius (by himself or with others) included:
Reputation.
Constantius II is a particularly difficult figure to judge properly due to the hostility of most sources toward him. A.H.M Jones writes that Constantius "appears in the pages of Ammianus as a conscientious emperor but a vain and stupid man, an easy prey to flatterers. He was timid and suspicious, and interested persons could easily play on his fears for their own advantage." However, Kent & M. and A. Hirmer suggest that Constantius "has suffered at the hands of unsympathetic authors, ecclesiastical and civil alike. To orthodox churchmen he was a bigoted supporter of the Arian heresy, to Julian the Apostate and the many who have subsequently taken his part he was a murderer, a tyrant and inept as a ruler". They go on to add, "Most contemporaries seem in fact to have held him in high esteem, and he certainly inspired loyalty in a way his brother could not". In the military sphere, the campaigns of Constantius and his subordinates on the Rhine and Danube frontiers in the late 350s restored stability to those regions after the troubles caused by Magnentius' revolt.
Ancestry.
Ancestors of Constantius II
References.
Ancient sources.
</dl>
Modern sources.
</dl>

</doc>
<doc id="6747" url="http://en.wikipedia.org/wiki?curid=6747" title="Constans">
Constans

Constans (Latin: "Flavius Julius Constans Augustus"; c. 323 – 350) was Roman Emperor from 337 to 350. He defeated his brother Constantine II in 340, but anger in the army over his personal life and preference for his barbarian bodyguards led the general Magnentius to rebel, resulting in the assassination of Constans in 350.
Career.
Constans was the third and youngest son of Constantine the Great and Fausta, his father's second wife. He was educated at the court of his father at Constantinople under the tutelage of the poet Aemilius Magnus Arborius. 
On 25 December 333, Constantine I elevated Constans to the rank of "Caesar" at Constantinople. Constans became engaged to Olympias, the daughter of the Praetorian Prefect Ablabius, but the marriage never came to pass. With Constantine’s death in 337, Constans and his two brothers, Constantine II and Constantius II, divided the Roman world between themselves and disposed of virtually all relatives who could possibly have a claim to the throne. The army proclaimed them "Augusti" on September 9, 337. Almost immediately, Constans was required to deal with a Sarmatian invasion in late 337, over whom he won a resounding victory.
Constans was initially under the guardianship of Constantine II. The original settlement assigned Constans the praetorian prefectures of Italy and Africa. Constans was unhappy with this division, so the brothers met at Viminacium in 338 to revise the boundaries. Constans managed to extract the prefecture of Illyricum and the diocese of Thrace, provinces that were originally to be ruled by his cousin Dalmatius, as per Constantine I’s proposed division after his death. Constantine II soon complained that he had not received the amount of territory that was his due as the eldest son. 
Annoyed that Constans had received Thrace and Macedonia after the death of Dalmatius, Constantine demanded that Constans hand over the African provinces, which he agreed to do in order to maintain a fragile peace. Soon, however, they began quarreling over which parts of the African provinces belonged to Carthage, and thus Constantine, and which belonged to Italy, and therefore Constans. This led to growing tensions between the two brothers, which were only heightened by Constans finally coming of age and Constantine refusing to give up his guardianship. In 340 Constantine II invaded Italy. Constans, at that time in Dacia, detached and sent a select and disciplined body of his Illyrian troops, stating that he would follow them in person with the remainder of his forces. Constantine was eventually trapped at Aquileia, where he died, leaving Constans to inherit all of his brother’s former territories – Hispania, Britannia and Gaul.
Constans began his reign in an energetic fashion. In 341-42, he led a successful campaign against the Franks, and in the early months of 343 he visited Britain. The source for this visit, Julius Firmicus Maternus, does not provide a reason, but the quick movement and the danger involved in crossing the channel in the dangerous winter months suggests it was in response to a military emergency, possibly to repel the Picts and Scots.
Regarding religion, Constans was tolerant of Judaism and promulgated an edict banning pagan sacrifices in 341. He suppressed Donatism in Africa and supported Nicene orthodoxy against Arianism, which was championed by his brother Constantius. Although Constans called the Council of Sardica in 343 to settle the conflict, it was a complete failure, and by 346 the two emperors were on the point of open warfare over the dispute. The conflict was only resolved by an interim agreement which allowed each emperor to support their preferred clergy within their own spheres of influence.
The Roman historian Eutropius says he "indulged in great vices" in reference to his homosexuality; and Aurelius Victor that he had a reputation for scandalous behaviour with "handsome barbarian hostages". Nevertheless, Constans did sponsor a decree alongside Constantius II that ruled that marriage based on unnatural sex should be punished meticulously. Boswell believed the decree outlawed homosexual marriages only It may also be that Constans was not expressing his own feeling when promulgating the legislation, but rather trying to placate public outrage at his own indecencies.
Death.
In the final years of his reign, Constans developed a reputation for cruelty and misrule. Dominated by favourites and openly preferring his select bodyguard, he lost the support of the legions. In 350, the general Magnentius declared himself emperor at Augustodunum with the support of the troops on the Rhine frontier, and later the western provinces of the Empire. Constans was enjoying himself nearby when he was notified of the elevation of Magnentius. Lacking any support beyond his immediate household, he was forced to flee for his life. As he was trying to reach either Italy or Spain, supporters of Magnentius cornered him in a fortification in Vicus Helena (now Elne) in the Pyrenees, southwestern Gaul, where he was killed after seeking sanctuary in a temple.

</doc>
<doc id="6749" url="http://en.wikipedia.org/wiki?curid=6749" title="Cheerleading">
Cheerleading

Cheerleading ranges from yelling to intense physical activity for sports team motivation, audience entertainment or competition based upon organized routines. The routines usually range anywhere from one to three minutes, which may contain many components of tumbling, dance, jumps, cheers and stunting in order to direct spectators of events to cheer for sports teams at games or to participate in cheerleading competitions. The yellers, dancers and athletes involved in cheerleading are called cheerleaders. Cheerleading originated in the United States, and remains predominantly American, with an estimated 1.5 million participants in all-star cheerleading. The presentation of cheerleading as a sport to a global audience was led by the 1997 start of broadcasts of cheerleading competition by ESPN International and the worldwide release of the 2000 film "Bring It On". Due in part to this recent exposure, there are now an estimated 100,000 participants scattered around the rest of the world in countries including Australia, Canada, China, Colombia, Finland, France, Germany, Japan, the Netherlands, New Zealand and the United Kingdom.
History.
Before organized cheerleading.
The roots of cheerleading began during the late 18th century with the rebellion of male students. After the American Revolutionary War, students experienced harsh treatment from teachers. In response to faculty's abuse, college students violently acted out. The undergraduates began to riot, burn down buildings located on their college campuses, and assault faculty members. As a more subtle way to gain independence however, students invented and organized their own extracurricular activities outside their professors' control. This brought America sports and cheerleading to participate in, beginning first with collegiate teams.
In the 1860s, students from Great Britain began to cheer and chant in unison for their favorite athletes at sporting events. Soon that gesture of support crossed overseas to America.
On November 6, 1869, the United States witnessed its first intercollegiate football game. It took place between Princeton and Rutgers University, and marked the day the original "Sis Boom Rah!" cheer was shouted out by student fans.
The beginning of organized cheer.
Organized cheerleading started as an all-male activity. As early as 1877, Princeton University had a "Princeton Cheer", documented in the February 22, 1877, March 12, 1880, and November 4, 1881, issues of the "Daily Princetonian". This cheer was yelled from the stands by students at games, as well as by the baseball and football athletes themselves. The cheer, "Hurrah! Hurrah! Hurrah! Tiger! S-s-s-t! Boom! A-h-h-h!" remains in use with slight modifications today and is now referred to as the "Locomotive".
Princeton class of 1882 graduate Thomas Peebles moved to Minnesota in 1884. He transplanted the idea of organized crowds cheering at football games to the University of Minnesota. The term "Cheer Leader" had been used as early as 1897, with Princeton's football officials having named three students as "Cheer Leaders:" Thomas, Easton and Guerin from Princeton's classes of 1897, 1898 and 1899, respectively, on October 26, 1897. These students would cheer for the team also at football practices, and special cheering sections were designated in the stands for the games themselves for both the home and visiting teams.
It was not until 1898 that University of Minnesota student Johnny Campbell directed a crowd in cheering "Rah, Rah, Rah! Ski-u-mah, Hoo-Rah! Hoo-Rah! Varsity! Varsity! Varsity, Minn-e-So-Tah!", making Campbell the very first cheerleader and November 2, 1898 the official birth date of organized cheerleading. Soon after, the University of Minnesota organized a "yell leader" squad of six male students, who still use Campbell's original cheer today. In 1903 the first cheerleading fraternity, Gamma Sigma, was founded.
Female participation.
In 1923, at the University of Minnesota, women were admitted into cheerleading. However it took time for other schools to follow. In the late 1920s, many school manuals and newspapers that were published still referred to cheerleaders as "chap," "fellow," and "man". Women as cheerleaders seemed to be overlooked until the 1940s. In the 1940s collegiate men were drafted for World War II, creating the opportunity for more women to make their way onto sporting event sidelines. As noted by Kieran Scott in "Ultimate Cheerleading": "Girls really took over for the first time." An overview written on behalf of cheerleading in 1955 explained that in larger schools "occasionally boys as well as girls are included," and in smaller schools "boys can usually find their place in the athletic program, and cheerleading is likely to remain solely a feminine occupation." During the 1950s, cheerleading in America also increased in popularity. By the 1960s, some began to consider cheerleading a feminine extracurricular for boys and by the 1970s, girls primarily cheered at public school games. However this did not stop its growth, cheer could be found in almost every school ranging from adolescent to grade school level across the country, and pee wee and youth leagues began to appear.
In 1975, it was estimated by a man named Randy Neil that over 500,000 students actively participated in American cheerleading from grade school to the collegiate level. He also approximated that ninety-five percent of cheerleaders within America were female. Since 1973, cheerleaders have started to attend female basketball and other all-female sports as well.
As of 2005, overall statistics show around 97% of all modern cheerleading participants are female, although at the collegiate level, cheerleading is co-ed with about 50% of participants being male.
Cheerleading firsts.
In 1948, Lawrence "Herkie" Herkimer, of Dallas, Texas, a former cheerleader at Southern Methodist University, formed the National Cheerleaders Association (NCA) in order to hold clinics for cheerleading. In 1949, The National Cheerleaders Association held its first clinic in Huntsville, Texas, with 52 girls in attendance. Herkimer contributed many firsts to cheerleading: the founding of the Cheerleader & Danz Team cheerleading uniform supply company, inventing the herkie (where one leg is bent towards the ground as if kneeling and the other is out to the side as high as it will stretch in the toe-touch position), and creating the "Spirit Stick". By the 1960s, college cheerleaders began hosting workshops across the nation, teaching fundamental cheer skills to high-school-age girls. In 1965, Fred Gastoff invented the vinyl pom-pon, which was introduced into competitions by the International Cheerleading Foundation (now the World Cheerleading Association or WCA). Organized cheerleading competitions began to pop up with the first ranking of the "Top Ten College Cheerleading Squads" and "Cheerleader All America" awards given out by the International Cheerleading Foundation in 1967. In 1978, America was introduced to competitive cheerleading by the first broadcast of Collegiate Cheerleading Championships on CBS.
Professional cheerleading.
In the 1950s the formation of professional cheerleading started. The first recorded cheer squad in National Football League (NFL) history was for the Baltimore Colts. Professional cheerleaders gave a new perception of American cheerleading. They began to be selected for two reasons: visual sex appeal, and the ability to dance. Women were exclusively chosen because men were the targeted marketing group. The Dallas Cowboys Cheerleaders soon gained the spotlight with their revealing outfits and sophisticated dance moves, debuting in the 1972–1973 season, but were first widely seen in Super Bowl X (1976). These pro squads of the 1970s established cheerleaders as "American icons of wholesome sex appeal." By 1981, a total of seventeen Nation Football League teams had their own cheerleaders and within these squads there were 580 participants. The only teams without NFL cheerleaders at this time were New Orleans, New York, Detroit, Cleveland, Denver, Minnesota, Pittsburg, San Francisco, and San Diego. Professional cheerleading eventually spread to soccer and basketball teams as well.
The advancements and traditions of cheerleading.
The 1980s saw the beginning of modern cheerleading with more difficult stunt sequences and gymnastics incorporated into routines. All-star teams popped up, and with them the creation of the United States All-Star Federation (USASF). ESPN first broadcast the National High School Cheerleading Competition nationwide in 1983. Cheerleading organizations such as the American Association of Cheerleading Coaches and Advisors (AACCA), founded in 1987, started applying universal safety standards to decrease the number of injuries and prevent dangerous stunts, pyramids and tumbling passes from being included in the cheerleading routines. In 2003, the National Council for Spirit Safety and Education (NCSSE) was formed to offer safety training for youth, school, all star and college coaches. The NCAA requires college cheer coaches to successfully complete a nationally recognized safety-training program. The NCSSE or AACCA certification programs are both recognized by the NCAA.
Even with its athletic and competitive development, cheerleading at the school level has retained its ties to the spirit leading traditions started back in the 1890s. Cheerleaders are quite often seen as ambassadors for their schools, and leaders among the student body. At the college level, cheerleaders are often invited to help at university fundraisers.
Cheerleading is very closely associated with American football and basketball. Sports such as association football (soccer), ice hockey, volleyball, baseball and wrestling will sometimes sponsor cheerleading squads. The ICC Twenty20 Cricket World Cup in South Africa in 2007 was the first international cricket event to have cheerleaders. The Florida Marlins were the first Major League Baseball team to have a cheerleading team. Debuting in 2003, the "Marlin Mermaids" gained national exposure and have influenced other MLB teams to develop their own cheer/dance squads.
Types of teams in the United States today.
School-sponsored.
Most American middle schools, high schools, and colleges have organized cheerleading squads made up solely of students. Several colleges that compete at cheerleading competitions offer cheerleading scholarships. School-sponsored cheerleading promotes school spirit, motivates the players and fans as well as gives enjoyment for the participants. A cheerleading team may compete (local, regional, and national competitions), as well as cheer for sporting events and encourage audience participation. Cheerleading is quickly becoming year-round, starting with tryouts during the spring of the preceding school year, organized camp as a team, practices, attendance at various events and ending with National competition season, typically from winter through spring.
School cheerleaders also compete with recreational-style routines at many competitions year-round. They practice intensely for them and come up with a no greater than 2 minute 30 second routine to show off at the competitions. Like other school-level athletes they compete to win their league title and move on to bigger competitions eventually reaching nationals, the ultimate title for a school squad. The advantages to a school squad versus an all-star squad is cheering at various games. For some squads the level of competition on the weekends can equal that of an all-star squad.
The tryout process sometimes takes place over many days. The cheerleading coach usually will arrange for a cheerleading clinic, during which basic materials are taught or reviewed before the final day of tryouts. The clinic gives returning cheerleaders and new cheerleaders an equal chance of becoming familiar with the material. Skills that coaches look for include jumps, tumbling, motions, and dance ability. Tryouts often take place during the spring, so that the coach has the squad chosen in time to attend summer camp as a team.
Middle school.
Middle school cheerleading evolved shortly after high school squads started. In middle school, the squads serve mostly the same functions as high school squads and usually follow the same rules and regulations. Depending on how advanced the squad is, they usually do similar stunts to high school cheer squads. The cheerleaders cheer for basketball teams, football teams, and other sports teams in their school. They also perform at pep rallies and compete against other schools from local competitions all the way to nationals. Cheerleading in middle school sometimes can be a two-season activity: fall and winter. However, many middle school cheer squads will go year-round like high school squads. Middle school cheerleaders use the same cheerleading movements as their older counterparts, yet they perform less extreme stunts. These stunts range from simple elevators, knee stands, and extensions to harder stunts such as the heel stretch and scorpion.
High school.
In high school, there are usually two squads per school: varsity and a junior varsity. Some schools also include a freshman level of cheering in order to develop skills as the athletes continue to mature. High school cheerleading contains aspects of school spirit as well as competition. These squads have become part of a year-round cycle. Starting with tryouts in the spring, year-round practice, cheering on teams in the fall and winter, and participating in cheerleading competitions. Most squads practice at least three days a week for about two hours each practice during the summer. Many teams also attend separate tumbling sessions outside of practice. During the school year, cheerleading is usually practiced five- to six-days-a-week. During competition season it often becomes seven days with practice twice a day sometimes. The school spirit aspect of cheerleading involves cheering, supporting, and "pumping up" the crowd at football games, basketball games, and even at wrestling meets. Along with this they make posters, perform at pep rallies, and bring school spirit to the other students. In May 2009, the National Federation of State High School Associations released the results of their first true high school participation study. They estimated that the number of high school cheerleaders from public high schools is around 394,700.
There are different cheerleading organizations that put on competitions, some of the major ones include state and regional competitions. Many high schools will often host cheerleading competitions, bringing in IHSA judges. The regional competitions are the qualifiers for the national competitions, such as the in Orlando, Florida every year. The competition aspect of cheerleading can be very enduring; styles and rules change every year making it important and difficult to find the newest and hottest routines. Most teams have a professional choreographer choreograph their routine in order to ensure they are not breaking rules and will be up to par with the other teams. For a list of rules visit . All high school coaches are required to attend an IHSA rules meeting at the beginning of the season. This ensures their knowledge of changed rules and their compliance with these rules. Routines usually last around 2 minutes and 30 seconds and always require cheer, dance, jumps, tumbling, and stunting portions.
Not all high school cheerleading squads compete in competitions. Cheerleaders dress in matching uniforms, to look like a team while they are performing.
Youth league/athletic association.
Organizations that sponsor youth cheer teams usually sponsor either youth league football or basketball teams as well. This allows for the two, under the same sponsor, to be intermingled. Both teams have the same mascot name and the cheerleaders perform at the football or basketball team's game they share a sponsorship with. Examples of such sponsors include Pop Warner and Pasco Police Athletic League (PPAL). The YMCA (Young Men's Christian Association) is also a well-known sponsor for youth cheerleading leagues.
All-star.
During the early 1980s, cheerleading squads not associated with a school or sports leagues, whose main objective was competition, began to emerge. The first organization to call themselves all stars and go to competitions were the Q94 Rockers from Richmond, Virginia, founded in 1982. All-star teams competing prior to 1987 were placed into the same divisions as teams that represented schools and sports leagues. In 1986, the National Cheerleaders Association (NCA) addressed this situation by creating a separate division for teams lacking a sponsoring school or athletic association, calling it the All-Star Division and debuting it at their 1987 competitions. As the popularity of this type of team grew, more and more of them were formed, attending competitions sponsored by many different organizations and companies, each using its own set of rules, regulations and divisions. This situation became a concern to gym owners because the inconsistencies caused coaches to keep their routines in a constant state of flux, detracting from time that could be better utilized for developing skills and providing personal attention to their athletes. More importantly, because the various companies were constantly vying for a competitive edge, safety standards had become more and more lax. In some cases, unqualified coaches and inexperienced squads were attempting dangerous stunts as a result of these expanded sets of rules.
The USASF was formed in 2003 by the competition companies to act as the national governing body for all star cheerleading and to create a standard set of rules and judging standards to be followed by all competitions sanctioned by the Federation, ultimately leading to the Cheerleading Worlds. The USASF hosted the first Cheerleading Worlds on April 24, 2004. In 2009, the first All-Level Worlds was held. It included teams from all levels, with each winner continuing to the online championships, where teams from across the nation competed to win the Worlds Title. At the same time, cheerleading coaches from all over the country organized themselves for the same rule making purpose, calling themselves the National All Star Cheerleading Coaches Congress (NACCC). In 2005, the NACCC was absorbed by the USASF to become their rule making body. In late 2006, the USASF facilitated the creation of the International All-Star Federation (IASF).
s of 2012[ [update]] all-star cheerleading as sanctioned by the USASF involves a squad of 6–36 females and/or males. The squad prepares year-round for many different competition appearances, but they only actually perform for up to 2½ minutes during their team's routines. The numbers of competitions a team participates in varies from team to team, but generally, most teams tend to participate in eight to twelve competitions a year. These competitions include locals, which are normally taken place in school gymnasiums or local venues, nationals, hosted in big venues all around the U.S. with national champions, and the Cheerleading Worlds, taken place at Disney World in Orlando, Florida. During a competition routine, a squad performs carefully choreographed stunting, tumbling, jumping, and dancing to their own custom music. Teams create their routines to an eight-count system and apply that to the music so that the team members execute the elements with precise timing and synchronization.
There are many different organizations that host their own state and national competitions. Some major companies include: Universal Spirit, AmeriCheer, Cheersport, Planet Spirit, Eastern Cheer and Dance Association, and The JAM Brands. This means that many gyms within the same area could be state and national champions for the same year and never have competed against each other. Currently, there is no system in place that awards only one state or national title.
Judges at the competition watch closely for illegal moves from the group or any individual member. Here, an illegal move is something that is not allowed in that division due to difficulty and/or safety restrictions. They look out for deductions, or things that go wrong, such as a dropped stunt. They also look for touch downs in tumbling for deductions. More generally, judges look at the difficulty and execution of jumps, stunts and tumbling, synchronization, creativity, the sharpness of the motions, showmanship, and overall routine execution.
All-star cheerleaders are placed into divisions, which are grouped based upon age, size of the team, gender of participants, and ability level. The age levels vary from under 4 year of age to 18 years and over. The divisions used by the USASF/IASF are currently Tiny, Mini, Youth, Junior, Junior International, Junior Coed, Senior, Senior Coed, Special Needs, and Open International. It originally began with "all girl" teams and later co-ed teams began to gain popularity. That being said, the all-girl squad remains the most prevalent.
If a team places high enough at selected USASF/IASF sanctioned national competitions, they could be included in the Cheerleading Worlds and compete against teams from all over the world, as well as receive money for placing. Each team receives a bid from another cheerleading company and goes in the name of that company. One must get a bid from a company in order to compete at the Cheerleading Worlds. For example, a team could get a bid from Cheersport, and they compete as a team representing that company. Cheerleading companies give out three types of bids to go to Cheerleading Worlds, Full Paid Bid, Partial Bid, or an Un-paid bid. The Cheerleading Worlds are only for teams that are level 5 and up.
Professional.
Professional cheerleaders and dancers cheer for sports such as football, basketball, baseball, wrestling, or hockey. There are only a small handful of professional cheerleading leagues around the world; some professional leagues include the NBA Cheerleading League, the NFL Cheerleading League, the CFL Cheerleading League, the MLS Cheerleading League, the MLB Cheerleading League, and the NHL Ice Dancers. Although professional cheerleading leagues exist in multiple countries, there are no Olympic Teams.
In addition to cheering at games and competing, professional cheerleaders also, as teams, can often do a lot of philanthropy and charity work, modeling, motivational speaking, television performances, and advertising.
Associations, federations and organizations.
Americheer: Americheer was founded in 1987 by Elizabeth Rossetti. It is the parent company to Ameridance and Eastern Cheer and Dance Association. In 2005, Americheer became one of the founding members of the NLCC. This means that Americheer events offer bids to The U.S. Finals: The Final Destination. AmeriCheer InterNational Championship competition is held every March at the Walt Disney World Resort in Orlando, Florida.
International Cheer Union (ICU): Established on April 26, 2004, the ICU is recognized by the SportAccord as the world governing body of cheerleading and the authority on all matters with relation to it. Including participation from its 105-member national federations reaching 3.5 million athletes globally, the ICU continues to serve as the unified voice for those dedicated to cheerleading's positive development around the world.
Following a positive vote by the SportAccord General Assembly on May 31, 2013, in Saint Petersburg, the International Cheer Union (ICU) became SportAccord's 109th member, and SportAccord's 93rd international sports federation to join the international sports family. In accordance with the SportAccord statutes, the ICU is recognized as the world governing body of cheerleading and the authority on all matters related to it.
The ICU holds training seminars for judges and coaches, global events and the World Cheerleading Championships. The ICU is also fully applied to the International Olympic Committee (IOC) and is compliant under the code set by the World Anti-Doping Agency (WADA).
International Federation of Cheerleading (IFC): Established on July 5, 1998, the International Federation of Cheerleading (IFC) is a non-profit federation based in Tokyo, Japan, and is the world governing body of cheerleading. The IFC objectives are to promote cheerleading worldwide, to spread knowledge of cheerleading and to develop friendly relations among the member associations and federations.
National Cheerleaders Association: The NCA was founded in 1948 by Lawrence Herkimer. Every year, the NCA hosts the NCA High School Cheerleading Nationals and the NCA All-Star Cheerleading Nationals in Dallas, Texas. They also host the NCA/NDA Collegiate Cheer & Dance Championship in Daytona Beach, Florida.
United Spirit Association: In 1950 Robert Olmstead directed his first summer training camp, and USA later sprouted from this. USA's focus is on the game day experience as a way to enhance audience entertainment. This focus led to the first American football half-time shows to reach adolescences from around the world and expose them to American style cheerleading. USA has choreographed material for professional and competitive cheerleaders alike. USA provides competitions for cheerleading squads without prior qualifications needed in order to participate. The organization also allows the opportunity for cheerleaders to become an All-American, participate in the Macy's Thanksgiving Parade, and partake in both the London New Year's Day Parade and other special events much like UCA and NCA allow participants to do.
Universal Cheerleaders Association: Universal Cheerleaders Association was founded in 1974 by Jeff Webb. Since 1980, UCA has hosted the National High School Cheerleading Championship in Walt Disney World Resort. They also host the National All-Star Cheerleading Championship, and the College Cheerleading National Championship at Walt Disney World Resort. To qualify for these events, all teams must submit a video. All of these events air on ESPN.
Competitions and companies.
Asian Thailand Cheerleading Invitational (ATCI): Organised by the Cheerleading Association of Thailand (CAT) in accordance with the rules and regulations of the International Federation of Cheerleading (IFC). The ATCI is held every year since 2009. At the ATCI many teams from all over Thailand compete, joining them are many invited neighbouring nations who also send cheer squads.
Cheerleading Asia International Open Championships (CAIOC): Hosted by the Foundation of Japan Cheerleading Association (FJCA) in accordance with the rules and regulations of the IFC. The CAIOC has been a yearly event since 2007. Every year many teams from all over Asia converge in Tokyo to compete.
Cheerleading World Championships (CWC): Organised by the IFC. The IFC is a non-profit organisation founded in 1998 and based in Tokyo, Japan.
The CWC has been held every two years since 2001, and to date the competition has been held in Japan, the United Kingdom, Finland, Germany and Hong Kong. The 6th CWC was held at the Hong Kong Coliseum on November 26–27, 2011.
ICU World Championships: The International Cheer Union currently encompasses 105 National Federations from countries across the globe. Every year the ICU host the World Cheerleading Championship. Unlike the USASF Worlds, this competition uses Level 6/ Collegiate style rules. Countries assemble and send only one team to represent them.
National Cheerleading Championships (NCC): The NCC is the annual IFC-sanctioned national cheerleading competition in Indonesia organised by the Indonesian Cheerleading Community (ICC). Since the NCC2010 the event is now open to international competition, representing a significant step forward for the ICC. Teams from many countries such as Japan, Thailand, the Philippines, Singapore, participated in the ground breaking event.
NLCC Final Destination: Nation's Leading Cheer Companies is a multi brand company, partnered with other companies such as: Americheer/Ameridance, American Cheer & Dance Academy, Eastern Cheer & Dance Association, and Spirit Unlimited. Every year, starting in 2006, the NLCC hosts The US Finals: The Final Destination of Cheerleading and Dance. Every team that attends must qualify and receive a bid at a partner company's competition. In May 2008, the NLCC and The JAM Brands announced a partnership to produce The U.S. Finals - Final Destination. There are nine Final Destination locations across the country. After the regional events, videos of all the teams that competed are sent to a new panel of judges and rescored to rank teams against those against whom they may never have had a chance to compete.
Pan-American Cheerleading Championships (PCC): The PCC was held for the first time in 2009 in the city of Latacunga, Ecuador and is the continental championship organised by the Pan-American Federation of Cheerleading (PFC). The PFC, operating under the umbrella of the IFC, is the non-profit continental body of cheerleading whose aim it is to promote and develop cheerleading in the Americas. The PCC is a biennial event, and was held for the second time in Lima, Peru, in November 2010.
The JAM Brands: The JAM Brands, headquartered in Louisville, Kentucky, provides products and services for the cheerleading and dance industry. It is made up of approximately 12 different brands that produce everything from competitions to camps to uniforms to merchandise and apparel. JAMfest, the original brand of The JAM Brands, has been around since 1996 and was founded by Aaron Flaker and Emmitt Tyler. Dan Kessler has since become a co-owner of The JAM Brands along with Flaker and Tyler.
USASF/IASF Worlds: Many United States cheerleading organizations form and register the not-for-profit entity the United States All Star Federation (USASF) and also the International All Star Federation (IASF) to support international club cheerleading and the World Cheerleading Club Championships. The first World Cheerleading Championships, or Cheerleading Worlds, were hosted by the USASF/IASF at the Walt Disney World Resort and taped for an ESPN global broadcast in 2004. This competition is only for All-Star/Club cheer. Only levels Junior 5, Senior 5, Senior Open 5, International 5, International Open 5, International 6, and International Open 6 may attend. Teams must receive a bid from a partner company to attend.
Varsity: Partnered with the UCA, Varsity created the National High School Cheerleading Championship in 1980. Varsity All-Star owns or partners with many of the largest cheerleading events in the country.
Title IX sports status.
There is a large debate on whether or not cheerleading should be considered a sport for Title IX (a portion of the United States Education Amendments of 1972 forbidding discrimination under any education program on the basis of sex) purposes. Supporters consider cheerleading, as a whole, a sport, citing the heavy use of athletic talents while critics see it as a physical activity because a "sport" implies a competition among all squads and not all squads compete, along with subjectivity of competitions where—as with gymnastics, diving, and figure skating—scores are assessed based on human judgment and not an objective goal or measurement of time.
On January 27, 2009, in a lawsuit involving an accidental injury sustained during a cheerleading practice, the Wisconsin Supreme Court ruled that cheerleading is a full-contact sport in that state, not allowing any participants to be sued for accidental injury. In contrast, on July 21, 2010, in a lawsuit involving whether college cheerleading qualified as a sport for purposes of Title IX, a federal court, citing a current lack of program development and organization, ruled that it is not a sport at all.
Dangers of cheerleading.
The risks of cheerleading were highlighted when Kristi Yamaoka, a cheerleader for Southern Illinois University, suffered a fractured vertebra when she hit her head after falling from a human pyramid. She also suffered from a concussion, and a bruised lung. The fall occurred when Yamaoka lost her balance during a basketball game between Southern Illinois University and Bradley University at the Savvis Center in St. Louis on March 5, 2006. The fall gained "national attention", because Yamaoka continued to perform from a stretcher as she was moved away from the game. Yamaoka has since made a full recovery.
The accident caused the Missouri Valley Conference to ban its member schools from allowing cheerleaders to be "launched or tossed and from taking part in formations higher than two levels" for one week during a women's basketball conference tournament, and also resulted in a recommendation by the NCAA that conferences and tournaments do not allow pyramids two and one half levels high or higher, and a stunt known as basket tosses, during the rest of the men's and women's basketball season. On July 11, 2006, the bans were made permanent by the AACCA rules committee:
The committee unanimously voted for sweeping revisions to cheerleading safety rules, the most major of which restricts specific upper-level skills during basketball games. Basket tosses, 2½ high pyramids, one-arm stunts, stunts that involve twisting or flipping, and twisting tumbling skills may only be performed during halftime and post-game on a matted surface and are prohibited during game play or time-outs.
Of the United States' 2.9 million female high school athletes, only 3% are cheerleaders, yet cheerleading accounts for nearly 65% of all catastrophic injuries in girls' high school athletics. The NCAA does not recognize cheerleading as a collegiate sport; there are no solid numbers on college cheerleading, yet when it comes to injuries, 67% of female athlete injuries at the college level are due to cheerleading mishaps. Another study found that between 1982 and 2007, there were 103 fatal, disabling or serious injuries recorded among female high school athletes, with the vast majority (67) occurring in cheerleading.
In the early 2000s, cheerleading was considered one of the most dangerous school activities. The main source of injuries comes from stunting, also known as pyramids. These stunts are performed at games and pep rallies, as well as competitions. Sometimes competition routines are focused solely around the use of difficult and risky stunts. These stunts usually include a flyer (the person on top), along with one or two bases (the people on the bottom) and, one or two spotters in the front and back on the bottom. The most common cheerleading related injuries are: sprained ankles, sprained wrists, back injuries, head injuries (sometimes concussions), broken arms, elbow injuries, knee injuries, broken noses, and broken collarbones. Sometimes, however, injuries can be as serious as whiplash, broken necks, broken vertebrae, and death.
The journal "Pediatrics" has reportedly said that the number of cheerleaders suffering from broken bones, concussions, and sprains has increased by over 100 percent between the years of 1990 and 2002, and that in 2001 there were 25,000 hospital visits reported for cheerleading injuries dealing with the shoulder, ankle, head, and neck. Meanwhile, in the USA, cheerleading accounted for 65.1% of all major physical injuries to high school females, and to 66.7% of major injuries to college students due to physical activity from 1982 to 2007, with 22,900 minors being admitted to hospital with cheerleading-related injuries in 2002.
In October 2009, the American Association of Cheerleading Coaches and Advisors (AACCA), a subsidiary of Varsity Brands, released a study that analyzed the data from Emergency Room visits of all high school athletes. The study asserted that contrary to many perceptions, cheerleading injuries are in line with female sports.
Cheerleading (for both girls and boys) was one of the sports studied in the Pediatric Injury Prevention, Education and Research Program of the Colorado School of Public Health in 2009/10–2012/13. Data on cheerleading injuries is included in the report for 2012–13.
In popular culture.
The revamped and provocative Dallas Cowboys Cheerleaders of the 1970s—and the many imitators that followed—firmly established the cheerleader as an American icon of wholesome sex appeal. In response, a new subgenre of exploitation films suddenly sprang up with titles such as "The Cheerleaders" (1972), "The Swinging Cheerleaders" (1974), "Revenge of the Cheerleaders" (1975), "The Pom Pom Girls" (1976), "Satan's Cheerleaders" (1977), and "Cheerleaders's Wild Weekend" (1979). In addition to R-rated sex comedies and horror films, cheerleaders became a staple of the adult film industry, starting with "Debbie Does Dallas" (1978) and its four sequels.
On television, the made-for-TV movie "The Dallas Cowboys Cheerleaders" (which aired January 14, 1979) starring Jane Seymour was a highly rated success, spawning the 1980 sequel "The Dallas Cowboys Cheerleaders II".
The Dallas squad was in high demand during the late 1970s with frequent appearances on network specials, awards shows, variety programs, commercials, the game show "Family Feud" and sitcoms such as "The Love Boat". The sci-fi sitcom "Mork & Mindy" also based a 1979 episode around the Denver Broncos cheerleaders with Mork (Robin Williams) trying out for the squad.
Cheerleading's increasing popularity in recent decades has made it a prominent feature in high-school themed movies and television shows. The 2000 film "Bring It On", about a San Diego high school cheerleading squad called "The Toros", starred real-life former cheerleader Kirsten Dunst. "Bring It On" was a surprise hit and earned nearly $70 million domestically. It spawned five direct-to-video sequels: "Bring It On Again" in 2003, ' in 2006, ' in 2007, and . "Bring It On" was followed in 2001 by another teen cheerleading comedy, "Sugar & Spice". In 1993, "The Positively True Adventures of the Alleged Texas Cheerleader-Murdering Mom" was a TV movie which told the true story of Wanda Holloway, the Texas mother whose obsession with her daughter's cheerleading career made headline news.
In 2006, Hayden Panettiere, star of "Bring It On: All or Nothing", took another cheerleading role as Claire Bennet, the cheerleader with an accelerated healing factor on NBC's hit sci-fi TV series "Heroes", launching cheerleading back into the limelight of pop culture. Claire was the main focus of the show's first story arc, featuring the popular catchphrase, "Save the cheerleader, save the world". Her prominent, protagonist role in "Heroes" was supported by a strong fan-base and provided a positive image for high school cheerleading.
In 2009, Panettiere starred again as a cheerleader, this time as Beth Cooper in the film adaptation of the novel "I Love You, Beth Cooper".
In 2006, the reality show "Cheerleader Nation" was featured on the Lifetime television channel. "Cheerleader Nation" is a 60-minute television series based on the Paul Laurence Dunbar High School cheerleading team's ups and downs on the way to nationals, of which they are the three time champions. The show also believes that cheerleading is tough. The show takes place in Lexington, Kentucky.
The 2007 series "" shows the process of getting on the pro squad of the Dallas Cowboy cheerleaders. Everything from initial tryouts to workout routines and the difficulties involved is shown. The series was extended a year to show the process of getting the 2008 Cheerleaders ready.
In 2009, Universal Pictures signed music video and film director Billie Woodruff ("Barbershop", "Honey") to direct the fifth film in the "Bring It On" series titled "". The film stars Christina Milian (who previously played cheerleaders in "Love Don't Cost a Thing" and "Man of the House") and Rachelle Brook Smith, and was released directly to DVD and Blu-ray on September 1, 2009.
The series "Glee", which began in 2009, features Dianna Agron as Quinn Fabray, the captain of her high school cheerleading squad, the Cheerios. Quinn becomes pregnant, leading to her expulsion from the squad, but two of the other Cheerios, Santana Lopez and Brittany Pierce also feature heavily in the show. In "The Power of Madonna" Kurt Hummel joins the Cheerios along with Mercedes Jones.
The CW Television Network created the short-lived "Hellcats" series (2010–11). This drama was about the ups and downs of being a college cheerleader. It starred Alyson Michalka as Marty (a former gymnast forced to become a cheerleader after her academic scholarship is canceled) and Ashley Tisdale from "High School Musical".
Nintendo has released a pair of video games in Japan for the Nintendo DS, "Osu! Tatakae! Ouendan" and its sequel "Moero! Nekketsu Rhythm Damashii" that star teams of male cheer squads, or Ouendan that practice a form of cheerleading. Each of the games' most difficult modes replaces the male characters with female cheer squads that dress in western cheerleading uniforms. The games task the cheer squads with assisting people in desperate need of help by cheering them on and giving them the motivation to succeed. There are also a "All Star Cheerleader" and "We Cheer" for the Wii in which one does routines at competitions with the Wiimote & Nunchuck. All Star Cheerleader is also available for Nintendo DS.
Cheerleading in Canada.
Cheerleading in Canada is rising in popularity among the youth in co-curricular programs. Cheerleading has grown from the sidelines to a competitive activity throughout the world and in particular Canada. Cheerleading has a few streams in Canadian sports culture. It is available at the middle-school, high-school, collegiate and best known for all-star. There are multiple regional, provincial and national championship opportunities for all athletes participating in cheerleading. Canada does not have a provincial teams, just a national program referred to as CCU or Team Canada. Their first year as a National team was in 2009 when they represented Canada at the International Cheer Union World Cheerleading Championships International Cheer Union (ICU).
Canada as a competition.
There is no official governing body for Canadian Cheerleading. The rules and guidelines for cheerleading used in Canada are the ones set out by the USASF. However, there are many organizations in Canada that put on competitions and have separate and individual rules and scoresheets for each competition. Cheer Evolution is the largest cheerleading and dance organization for Canada. They hold many competitions as well as provide a competition for bids to Worlds. There are other organizations such as the Ontario Cheerleading Federation (Ontario), Power Cheerleading Association (Ontario), Kicks Athletics (Quebec) and the International Cheer Alliance (Vancouver). There are over forty recognized competitive gym clubs with numerous teams that compete at competitions across Canada.
Canada at the World Championships of Cheerleading (USASF/ICU).
There are two world championship competitions that Canada participates in. There is the ICU World Championships where the National Teams compete against each other and then there are the Club team World Championships. These club teams are referred to as "All-star" teams who compete at the USASF World Championships of Cheerleading. This is where teams must have earned a bid from their own country to attend. National team members who compete at the ICU Worlds can also compete with their "All-Star Club" teams. Although athletes can compete in both International Cheer Union (ICU) and USASF, crossovers between teams at each individual competition are not permitted. Teams compete against the other teams from their countries on the first day of competition and the top three teams from each country in each division continue to Finals. At the end of finals, the top team scoring the highest for their country earns the "Nations Cup". Canada has multiple teams across their country that compete in the USASF Cheerleading Worlds Championship.
The International Cheer Union (ICU) is built of 103 countries that compete against each other in four divisions; Coed Premier, All-girl Premier, Coed Elite and All-girl Elite. Canada has a national team ran by the Canadian Cheer Union (CCU). Their Coed Elite Level 5 Team and their All-girl Elite Level 5 team are 4-time world champions. They are found from all over the country. In 2013, they have added two more teams to their roster. A new division that will compete head-to-head with the United States: in both the All-girl and Coed Premier Level 6 divisions. Members tryout and are selected on the basis of their skills and potential to succeed. Athletes are selected from all over. Canada's national program has grown to be one of the most successful programs.
References.
Notes

</doc>
<doc id="6751" url="http://en.wikipedia.org/wiki?curid=6751" title="Cottingley Fairies">
Cottingley Fairies

The Cottingley Fairies appear in a series of five photographs taken by Elsie Wright (1900–88) and Frances Griffiths (1907–86), two young cousins who lived in Cottingley, near Bradford in England. In 1917, when the first two photographs were taken, Elsie was 16 years old and Frances was 9. The pictures came to the attention of writer Sir Arthur Conan Doyle, who used them to illustrate an article on fairies he had been commissioned to write for the Christmas 1920 edition of "The Strand Magazine". Doyle, as a spiritualist, was enthusiastic about the photographs, and interpreted them as clear and visible evidence of psychic phenomena. Public reaction was mixed; some accepted the images as genuine, but others believed they had been faked.
Interest in the Cottingley Fairies gradually declined after 1921. Both girls married and lived abroad for a time after they grew up, yet the photographs continued to hold the public imagination. In 1966 a reporter from the "Daily Express" newspaper traced Elsie, who had by then returned to the UK. Elsie left open the possibility that she believed she had photographed her thoughts, and the media once again became interested in the story.
In the early 1980s Elsie and Frances admitted that the photographs were faked, using cardboard cutouts of fairies copied from a popular children's book of the time, but Frances maintained that the fifth and final photograph was genuine. The photographs and two of the cameras used are on display in the National Media Museum in Bradford.
1917 photographs.
In mid-1917 nine-year-old Frances Griffiths and her mother—both newly arrived in the UK from South Africa—were staying with Frances' aunt, Elsie Wright's mother, in the village of Cottingley in West Yorkshire; Elsie was then 16 years old. The two girls often played together beside the beck (stream) at the bottom of the garden, much to their mothers' annoyance, because they frequently came back with wet feet and clothes. Frances and Elsie said they only went to the beck to see the fairies, and to prove it, Elsie borrowed her father's camera, a Midg quarter-plate. The girls returned about 30 minutes later, "triumphant".
Elsie's father, Arthur, was a keen amateur photographer, and had set up his own darkroom. The picture on the photographic plate he developed showed Frances behind a bush in the foreground, on which four fairies appeared to be dancing. Knowing his daughter's artistic ability, and that she had spent some time working in a photographer's studio, he dismissed the figures as cardboard cutouts. Two months later the girls borrowed his camera again, and this time returned with a photograph of Elsie sitting on the lawn holding out her hand to a 1 ft gnome. Exasperated by what he believed to be "nothing but a prank", and convinced that the girls must have tampered with his camera in some way, Arthur Wright refused to lend it to them again. His wife Polly, however, believed the photographs to be authentic.
I am learning French, Geometry, Cookery and Algebra at school now. Dad came home from France the other week after being there ten months, and we all think the war will be over in a few days ... I am sending two photos, both of me, one of me in a bathing costume in our back yard, while the other is me with some fairies. Elsie took that one.
”
Letter from Frances Griffiths to a friend in South Africa
Towards the end of 1918, Frances sent a letter to Johanna Parvin, a friend in Cape Town, South Africa, where Frances had lived for most of her life, enclosing the photograph of herself with the fairies. On the back she wrote "It is funny, I never used to see them in Africa. It must be too hot for them there."
The photographs became public in mid-1919, after Elsie's mother attended a meeting of the Theosophical Society in Bradford. The lecture that evening was on "Fairy Life", and at the end of the meeting Polly Wright showed the two fairy photographs taken by her daughter and niece to the speaker. As a result, the photographs were displayed at the Society's annual conference in Harrogate, held a few months later. There they came to the attention of a leading member of the Society, Edward Gardner. One of the central beliefs of Theosophy is that humanity is undergoing a cycle of evolution, towards increasing "perfection", and Gardner recognised the potential significance of the photographs for the movement:
... the fact that two young girls had not only been able to see fairies, which others had done, but had actually for the first time ever been able to materialise them at a density sufficient for their images to be recorded on a photographic plate, meant that it was possible that the next cycle of evolution was underway.
Initial examinations.
Gardner sent the prints along with the original glass-plate negatives to Harold Snelling, a photography expert. Snelling's opinion was that "the two negatives are entirely genuine, unfaked photographs ... [with] no trace whatsoever of studio work involving card or paper models". He did not go so far as to say that the photographs showed fairies, stating only that "these are straight forward photographs of whatever was in front of the camera at the time". Gardner had the prints "clarified" by Snelling, and new negatives produced, "more conducive to printing", for use in the illustrated lectures he gave around the UK. Snelling supplied the photographic prints which were available for sale at Gardner's lectures.
Author and prominent Spiritualist Sir Arthur Conan Doyle learned of the photographs from the editor of the Spiritualists' publication "Light". He had been commissioned by "The Strand Magazine" to write an article on fairies for their Christmas issue, and the fairy photographs "must have seemed like a godsend" according to broadcaster and historian Magnus Magnusson. Doyle contacted Gardner in June 1920 to determine the background to the photographs, and wrote to Elsie and her father to request permission from the latter to use the prints in his article. Arthur Wright was "obviously impressed" that Doyle was involved, and gave his permission for publication, but he refused payment on the grounds that, if genuine, the images should not be "soiled" by money.
Gardner and Doyle sought a second expert opinion from the photographic company Kodak. Several of the company's technicians examined the enhanced prints, and although they agreed with Snelling that the pictures "showed no signs of being faked", they concluded that "this could not be taken as conclusive evidence ... that they were authentic photographs of fairies". Kodak declined to issue a certificate of authenticity. Gardner believed that the Kodak technicians might not have examined the photographs entirely objectively, observing that one had commented "after all, as fairies couldn't be true, the photographs must have been faked somehow". The prints were also examined by another photographic company, Ilford, who reported unequivocally that there was "some evidence of faking". Gardner and Doyle, perhaps rather optimistically, interpreted the results of the three expert evaluations as two in favour of the photographs' authenticity and one against.
Doyle also showed the photographs to the physicist and pioneering psychical researcher Sir Oliver Lodge, who believed the photographs to be fake. He suggested that a troupe of dancers had masqueraded as fairies, and expressed doubt as to their "distinctly 'Parisienne'" hairstyles.
1920 photographs.
Doyle was preoccupied with organising an imminent lecture tour of Australia, and in July 1920, sent Gardner to meet the Wright family. Frances was by then living with her parents in Scarborough, but Elsie's father told Gardner that he had been so certain the photographs were fakes that while the girls were away he searched their bedroom and the area around the beck (stream), looking for scraps of pictures or cutouts, but found nothing "incriminating".
Gardner believed the Wright family to be honest and respectable. To place the matter of the photographs' authenticity beyond doubt, he returned to Cottingley at the end of July with two Kodak Cameo cameras and 24 secretly marked photographic plates. Frances was invited to stay with the Wright family during the school summer holiday so that she and Elsie could take more pictures of the fairies. Gardner described his briefing in his 1945 "Fairies: A Book of Real Fairies":
Until 19 August the weather was unsuitable for photography. Because Frances and Elsie insisted that the fairies would not show themselves if others were watching, Elsie's mother was persuaded to visit her sister's for tea, leaving the girls alone. In her absence the girls took several photographs, two of which appeared to show fairies. In the first, "Frances and the Leaping Fairy", Frances is shown in profile with a winged fairy close by her nose. The second, "Fairy offering Posy of Harebells to Elsie", shows a fairy either hovering or tiptoeing on a branch, and offering Elsie a flower. Two days later the girls took the last picture, "Fairies and Their Sun-Bath".
The plates were packed in cotton wool and returned to Gardner in London, who sent an "ecstatic" telegram to Doyle, by then in Melbourne. Doyle wrote back:
Publication and reaction.
Doyle's article in the December 1920 issue of "The Strand" contained two higher-resolution prints of the 1917 photographs, and sold out within days of publication. To protect the girls' anonymity, Frances and Elsie were called Alice and Iris respectively, and the Wright family was referred to as the Carpenters. An enthusiastic and committed Spiritualist, Doyle hoped that if the photographs convinced the public of the existence of fairies then they might more readily accept other psychic phenomena. He ended his article with the words:
Early press coverage was "mixed", generally a combination of "embarrassment and puzzlement". The historical novelist and poet Maurice Hewlett published a series of articles in the literary journal "John O' London's Weekly", in which he concluded: "And knowing children, and knowing that Sir Arthur Conan Doyle has legs, I decide that the Miss Carpenters have pulled one of them." The Sydney newspaper "Truth" on 5 January 1921 expressed a similar view; "For the true explanation of these fairy photographs what is wanted is not a knowledge of occult phenomena but a knowledge of children." Some public figures were more sympathetic. Margaret McMillan, the educational and social reformer, wrote: "How wonderful that to these dear children such a wonderful gift has been vouchsafed." The novelist Henry De Vere Stacpoole decided to take the fairy photographs and the girls at face value. In a letter to Gardner he wrote: "Look at Alice's [Frances'] face. Look at Iris's [Elsie's] face. There is an extraordinary thing called TRUTH which has 10 million faces and forms – it is God's currency and the cleverest coiner or forger can't imitate it."
Major John Hall-Edwards, a keen photographer and pioneer of medical X-ray treatments in Britain, was a particularly vigorous critic:
On the evidence I have no hesitation in saying that these photographs could have been "faked". I criticize the attitude of those who declared there is something supernatural in the circumstances attending to the taking of these pictures because, as a medical man, I believe that the inculcation of such absurd ideas into the minds of children will result in later life in manifestations and nervous disorder and mental disturbances.
Doyle used the later photographs in 1921 to illustrate a second article in "The Strand", in which he described other accounts of fairy sightings. The article formed the foundation for his 1922 book "The Coming of the Fairies". As before, the photographs were received with mixed credulity. Sceptics noted that the fairies "looked suspiciously like the traditional fairies of nursery tales" and that they had "very fashionable hairstyles".
Gardner's final visit.
Gardner made a final visit to Cottingley in August 1921. He again brought cameras and photographic plates for Frances and Elsie, but was accompanied by the clairvoyant Geoffrey Hodson. Although neither of the girls claimed to see any fairies, and there were no more photographs, "on the contrary, he [Hodson] saw them [fairies] everywhere" and wrote voluminous notes on his observations.
By now Elsie and Frances were tired of the whole fairy business. Years later Elsie looked at a photograph of herself and Frances taken with Hodson and said: "Look at that, fed up with fairies!" Both Elsie and Frances later admitted that they "played along" with Hodson "out of mischief", and that they considered him "a fake".
Later investigations.
Public interest in the Cottingley Fairies gradually subsided after 1921. Elsie and Frances eventually married and lived abroad for many years. In 1966, a reporter from the "Daily Express" newspaper traced Elsie, who was by then back in England. She admitted in an interview given that year that the fairies might have been "figments of my imagination", but left open the possibility she believed that she had somehow managed to photograph her thoughts. The media subsequently became interested in Frances and Elsie's photographs once again. BBC television's "Nationwide" programme investigated the case in 1971, but Elsie stuck to her story: "I've told you that they're photographs of figments of our imagination, and that's what I'm sticking to".
Elsie and Frances were interviewed by journalist Austin Mitchell in September 1976, for a programme broadcast on Yorkshire Television. When pressed, both women agreed that "a rational person doesn't see fairies", but they denied having fabricated the photographs. In 1978 the magician and scientific sceptic James Randi and a team from the Committee for the Scientific Investigation of Claims of the Paranormal examined the photographs, using a "computer enhancement process". They concluded that the photographs were fakes, and that strings could be seen supporting the fairies. Geoffrey Crawley, editor of the "British Journal of Photography", undertook a "major scientific investigation of the photographs and the events surrounding them", published between 1982 and 1983, "the first major postwar analysis of the affair". He also concluded that the pictures were fakes.
Confession.
In 1983, the cousins admitted in an article published in the magazine "The Unexplained" that the photographs had been faked, although both maintained that they really had seen fairies. Elsie had copied illustrations of dancing girls from a popular children's book of the time, "Princess Mary's Gift Book", published in 1914, and drew wings on them. They said they had then cut out the cardboard figures and supported them with hatpins, disposing of their props in the beck once the photograph had been taken. But the cousins disagreed about the fifth and final photograph, which Doyle in his "The Coming of the Fairies" described in this way:
Seated on the upper left hand edge with wing well displayed is an undraped fairy apparently considering whether it is time to get up. An earlier riser of more mature age is seen on the right possessing abundant hair and wonderful wings. Her slightly denser body can be glimpsed within her fairy dress.
Elsie maintained it was a fake, just like all the others, but Frances insisted that it was genuine. In an interview given in the early 1980s Frances said:
Both Frances and Elsie claimed to have taken the fifth photograph. In a letter published in "The Times" newspaper on 9 April 1983, Geoffrey Crawley explained the discrepancy by suggesting that the photograph was "an unintended double exposure of fairy cutouts in the grass", and thus "both ladies can be quite sincere in believing that they each took it".
In a 1985 interview on Yorkshire Television's "Arthur C. Clarke's World of Strange Powers", Elsie said that she and Frances were too embarrassed to admit the truth after fooling Doyle, the author of Sherlock Holmes: "Two village kids and a brilliant man like Conan Doyle – well, we could only keep quiet." In the same interview Frances said: "I never even thought of it as being a fraud – it was just Elsie and I having a bit of fun and I can't understand to this day why they were taken in – they wanted to be taken in."
Subsequent history.
Frances died in 1986, and Elsie in 1988. Prints of their photographs of the fairies, along with a few other items including a first edition of Doyle's book "The Coming of the Fairies", were sold at auction in London for £21,620 in 1998. That same year, Geoffrey Crawley sold his Cottingley Fairy material to the National Museum of Film, Photography and Television in Bradford (now the National Media Museum), where it is on display. The collection included prints of the photographs, two of the cameras used by the girls, watercolours of fairies painted by Elsie, and a nine-page letter from Elsie admitting to the hoax.
The glass photographic plates were bought for £6,000 by an unnamed buyer at a London auction held in 2001.
Frances' daughter, Christine Lynch, appeared in an episode of the television programme "Antiques Roadshow" in Belfast, broadcast on BBC One in January 2009, with the photographs and one of the cameras given to the girls by Doyle. Christine told the expert, Paul Atterbury, that she believed, as her mother had done, that the fairies in the fifth photograph were genuine. Atterbury estimated the value of the items at between £25,000 and £30,000. The first edition of Frances' memoirs was published a few months later, under the title "Reflections on the Cottingley Fairies". The book contains correspondence, sometimes "bitter", between Elsie and Frances. In one letter, dated 1983, Frances wrote:
I hated those photographs from the age of 16 when Mr Gardner presented me with a bunch of flowers and wanted me to sit on the platform [at a Theosophical Society meeting] with him. I realised what I was in for if I did not keep myself hidden.
The 1997 films "" and "Photographing Fairies" were inspired by the events surrounding the Cottingley Fairies. The photographs were parodied in a 1994 book written by Terry Jones and Brian Froud, "Lady Cottington's Pressed Fairy Book".
References.
Notes
Bibliography
</dl>

</doc>
<doc id="6752" url="http://en.wikipedia.org/wiki?curid=6752" title="Cheka">
Cheka

Cheka (ЧК – чрезвыча́йная коми́ссия "chrezvychaynaya komissiya", Emergency Committee, ]) was the first of a succession of Soviet state security organizations. It was created on December 20, 1917, after a decree issued by Vladimir Lenin, and was subsequently led by Felix Dzerzhinsky, a Polish aristocrat turned communist. By late 1918, hundreds of Cheka committees had been created in various cities, at multiple levels including: oblast, guberniya ("Gubcheks"), raion, uyezd, and volost Chekas, with Raion and Volost Extraordinary Commissioners. Many thousands of dissidents, deserters, or other people were arrested, tortured or executed by various Cheka groups. After 1922, Cheka groups underwent a series of reorganizations, with the NKVD, into bodies whose members continued to be referred to as "Chekisty" (Chekists) into the late 1980s.
From its founding, being the military and security arm of the Bolshevik communist party, the Cheka was instrumental in the Red Terror. In 1921 the "Troops for the Internal Defense of the Republic" (a branch of the Cheka) numbered at least 200,000. These troops policed labor camps; ran the Gulag system; conducted requisitions of food; subjected political opponents to torture and summary execution; and put down rebellions and riots by workers or peasants, and mutinies in the desertion-plagued Red Army.
Name.
The name of the agency was originally "The All-Russian Emergency Commission for Combating Counter-Revolution and Sabotage"
(Russian: Всеросси́йская чрезвычайная коми́ссия по борьбе́ с контрреволюцией и саботажем; "Vserossiyskaya chrezvychaynaya komissiya po bor'bye s kontrrevolyutsiyei i sabotazhem"), but was often shortened to "VCheka" or "Cheka," after its Russian initials. In 1918 its name was changed, becoming "All-Russian Extraordinary Commission for Combating Counter-Revolution, Profiteering and Corruption".
A member of Cheka was called a "chekist". Also, the term "chekist" often referred to Soviet secret police throughout the Soviet period, despite official name changes over time. In "The Gulag Archipelago", Alexander Solzhenitsyn recalls that zeks in the labor camps used "old 'Chekist'" as "a mark of special esteem" for particularly experienced camp administrators. The term is still found in use in Russia today (for example, President Vladimir Putin has been referred to in the Russian media as a "chekist" due to his career in the KGB).
The Chekists commonly dressed in black leather, including long flowing coats, reportedly after being issued such distinctive coats early in their existence. Western communists adopted this clothing fashion. The Chekists also often carried with them Greek-style worry beads made of amber, which had become "fashionable among high officials during the time of the 'cleansing'".
History.
Creation.
In the first month and half after the October Revolution (1917), the duty of "extinguishing the resistance of exploiters" was assigned to the Petrograd Military Revolutionary Committee (or VRK). It represented a temporary body working under directives of the Council of People's Commissars (Sovnarkom) and Central Committee of RDSRP(b). The VRK created new bodies of government, organized food delivery to cities and the Army, requisitioned products from "bourgeoisie", and sent its emissaries and agitators into provinces. One of its most important functions was the security of "revolutionary order", and the fight against "counterrevolutionary" activity (see: Anti-Soviet agitation).
On December 1, 1917, the All-Russian Central Executive Committee (VTsIK or TsIK) reviewed a proposed reorganization of the VRK, and possible replacement of it. On December 5, the Petrograd VRK published an announcement of dissolution and transferred the functions to the department of TsIK to the fight against "counterrevolutionaries". On December 6, the Council of People's Commissars (Sovnarkom) strategized how to persuade government workers to strike across Russia. They decided that a special commission was needed to implement the "most energetically revolutionary" measures. Felix Dzerzhinsky (the Iron Felix) was appointed as Director and invited the participation of the following individuals: V. K. Averin, V. N. Vasilevsky, D. G. Yevseyev, N. A. Zhydelev, I. K. Ksenofontov, G. K. Ordjonikidze, Ya. Kh. Peters, K. A. Peterson, V. A. Trifonov.
On December 7, 1917, all invited except Zhydelev and Vasilevsky gathered in the Smolny Institute to discuss the competence and structure of the commission to combat counterrevolution and sabotage. The obligations of the commission were:
The commission should also observe the press and counterrevolutionary parties, sabotaging officials and other criminals. It was decided to create three sections: informational, organizational, and a unit to combat counter-revolution and sabotage. Upon the end of the meeting, Dzerzhinsky reported to the Sovnarkom with the requested information. The commission was allowed to apply such measures of repression as 'confiscation, deprivation of ration cards, publication of lists of enemies of the people etc.'". That day, Sovnarkom officially confirmed the creation of VCheKa. The commission was created not under the VTsIK as was previously anticipated, but rather under the Council of the People's Commissars.
On December 8, 1917, some of the original members of the VCheka were replaced. Averin, Ordzhonikidze, and Trifonov were replaced by V. V. Fomin, S. E. Shchukin, Ilyin, and Chernov. On the meeting of December 8, the presidium of VChK was elected of five members, and chaired by Dzerzhinsky. The issue of "speculation" was raised at the same meeting, which was assigned to Peters to address and report with results to one of the next meetings of the commission. A circular, published on December 28 [O.S. December 15] 1917, gave the address of VCheka's first headquarters as "Petrograd, Gorokhovaya 2, 4th floor". On December 11, Fomin was ordered to organize a section to suppress "speculation." And in the same day, VCheKa offered Shchukin to conduct arrests of counterfeiters.
In January 1918, a subsection of the anti-counterrevolutionary effort was created to police bank officials. The structure of VCheKa was changing repeatedly. By March 1918, when the organization came to Moscow, it contained the following sections: against counterrevolution, speculation, non-residents, and information gathering. By the end of 1918–1919, some units had been created: secretly operative, investigatory, of transportation, military (special), operative, and instructional. By 1921, it changed once again, forming the following sections: directory of affairs, administrative-organizational, secretly operative, economical, and foreign affairs.
First months.
In the first months of its existence, VCheKa consisted of only 40 officials. It commanded a team of soldiers, the Sveaborgesky regiment, as well as a group of Red Guardsmen. On January 14, 1918, Sovnarkom ordered Dzerzhinsky to organize teams of "energetic and ideological" sailors to combat speculation. By the spring of 1918, the commission had several teams. In addition to the Sveaborge team, it had an intelligence team, a team of sailors, and a strike team. Through the winter of 1917–1918, all the activities of VCheKa were centralized mainly in the city of Petrograd. It was one of the several other commissions in the country that fought against counterrevolution, speculation, banditry, and other activities perceived as crimes. Other organizations included: the Bureau of Military Commissars, and an Army-Navy investigatory commission to attack the counterrevolutionary element in the Red Army, plus the Central Requisite and Unloading Commission to fight speculation. The investigation of counterrevolutionary or major criminal offenses was conducting by the Investigatory Commission of Revtribunal. The functions of VCheKa were closely intertwined with the Commission of V. D. Bonch-Bruyevich, which beside the fight against wine pogroms was engaged in the investigation of most major political offenses (see: Bonch-Bruyevich Commission).
All results of its activities, VCheKa had either to transfer to the Investigatory Commission of Revtribunal or to dismiss a case. The control of the commission's activity was provided by the People's Commissariat for Justice (Narkomjust, at that time headed by Isidor Steinberg) and Internal Affairs (NKVD, at that time headed by Hryhoriy Petrovsky). Although the VCheKa was officially an independent organization from the NKVD, its main members such as Dzerzhinsky, Latsis, Unszlicht, and Uritsky (all main chekists), since November 1917 composed the collegiate of NKVD headed by Petrovsky. In November 1918, Petrovsky was appointed as the head of the All-Ukrainian Central Military Revolutionary Committee during VCheKa's expansion to provinces and front-lines. At the time of political competition between Bolsheviks and SRs (January 1918), Left SRs attempted to curb the rights of VCheKa and establish through the Narkomiust its control over its work. Having failed in attempts to subordinate the VCheKa to Narkomiust, the Left SRs were to seek control of the Extraordinary Commission in a different way. They requested that the Central Committee of the party was granted the right to directly enter their representatives into the VCheKa. Sovnarkom recognized the desirability of including five representatives of the Left Socialist-Revolutionary faction of VTsIK. Left SRs were granted the post of a companion (deputy) chairman of VCheKa. However, Sovnarkom, in which the majority belonged to the representatives of RSDLP(b) retained the right to approve members of the collegium of the VCheKa.
Originally, the members of the Cheka were exclusively Bolshevik; however, in January 1918, the Left SRs also joined the organization The Left SRs were expelled or arrested later in 1918, following the attempted assassination of Lenin by an SR, Fanni Kaplan.
Consolidation of VCheKa and National Establishment.
By the end of January 1918, the Investigatory Commission of Petrograd Soviet (probably same as of Revtribunal) petitioned Sovnarkom to delineate the role of detection and judicial-investigatory organs. It offered to leave, for the VCheKa and the Commission of Bonch-Bruyevich, only the functions of detection and suppression, while investigative functions entirely transferred to it. The Investigatory Commission prevailed. On January 31, 1918, Sovnarkom ordered to relieve VCheKa of the investigative functions, leaving for the commission only the functions of detection, suppression, and prevention of so-called crimes. At the meeting of the Council of People's Commissars on January 31, 1918, a merger of VCheKa and the Commission of Bonch-Bruyevich was proposed. The existence of both commissions, VCheKa of Sovnarkom and the Commission of Bonch-Bruyevich of VTsIK, with almost the same functions and equal rights, became impractical. A decision followed two weeks later.
On February 23, 1918, VCheKa sent a radio telegram to all Soviets with a petition to immediately organize emergency commissions to combat counter-revolution, sabotage and speculation, if such commissions had not been yet organized. February 1918 saw the creation of local Extraordinary Commissions. One of the first founded was the Moscow Cheka. Sections and commissariats to combat counterrevolution were established in other cities. The Extraordinary Commissions arose, usually in the areas during the moments of the greatest aggravation of political situation. On February 25, 1918, as the counterrevolutionary organization "Union of Front-liners" was making advances, the executive committee of the Saratov Soviet formed a counter-revolutionary section. On March 7, 1918, because of the move from Petrograd to Moscow, the Petrograd Cheka was created. On March 9, a section for combating counterrevolution was created under the Omsk Soviet. Extraordinary commissions were also created in Penza, Perm, Novgorod, Cherepovets, Rostov, Taganrog. On March 18, VCheKa adopted a resolution, "The Work of VCheKa on the All-Russian Scale", foreseeing the formation everywhere of Extraordinary Commissions after the same model, and sent a letter that called for the widespread establishment of the Cheka in combating counterrevolution, speculation, and sabotage. Establishment of provincial Extraordinary Commissions was largely completed by August 1918. In the Soviet Republic, there were 38 gubernatorial Chekas (Gubcheks) by this time.
On June 12, 1918, the All-Russian Conference of Cheka adopted the "Basic Provisions on the Organization of Extraordinary Commissions". They set out to form Extraordinary Commissions not only at Oblast and Guberniya levels, but also at the large Uyezd Soviets. In August 1918, in the Soviet Republic had accounted for some 75 Uyezd-level Extraordinary Commissions. By the end of the year, 365 Uyezd-level Chekas were established. In 1918, the All-Russia Extraordinary Commission and the Soviets managed to establish a local Cheka apparatus. It included Oblast, Guberniya, Raion, Uyezd, and Volost Chekas, with Raion and Volost Extraordinary Commissioners. In addition, border security Chekas were included in the system of local Cheka bodies.
In the autumn of 1918, as consolidation of the political situation of the republic continued, a move toward elimination of Uyezd-, Raion-, and Volost-level Chekas, as well as the institution of Extraordinary Commissions was considered. On January 20, 1919, VTsIK adopted a resolution prepared by VCheKa, "On the abolition of Uyezd Extraordinary Commissions". On January 16 the presidium of VCheKa approved the draft on the establishment of the Politburo at Uyezd militsiya. This decision was approved by the Conference of the Extraordinary Commission IV, held in early February 1920.
Other types of Cheka.
On August 3, a VCheKa section for combating counterrevolution, speculation and sabotage on railways was created. On August 7, 1918, Sovnarkom adopted a decree on the organization of the railway section at VCheKa. Combating counterrevolution, speculation, and malfeasance on railroads was passed under the jurisdiction of the railway section of VCheKa and local Cheka. In August 1918, railway sections were formed under the Gubcheks. Formally, they were part of the non-resident sections, but in fact constituted a separate division, largely autonomous in their activities. The gubernatorial and oblast-type Chekas retained in relationship to the transportation sections only control and investigative functions.
The beginning of a systematic work of organs of VCheKa in RKKA refers to July 1918, the period of extreme tension of the civil war and class struggle in the country. On July 16, 1918, the Council of People's Commissars formed the Extraordinary Commission for combating counterrevolution at the Czechoslovak (Eastern) Front, led by M. I. Latsis. In the fall of 1918, Extraordinary Commissions to combat counterrevolution on the Southern (Ukraine) Front were formed. In late November, the Second All-Russian Conference of the Extraordinary Commissions accepted a decision after the report of I. N. Polukarov to establish at all frontlines and army sections of the Cheka and granted them the right to appoint their commissioners in military units. On December 9, 1918, the collegiate (or presidium) of VCheKa had decided to form a military section, headed by M. S. Kedrov, to combat counterrevolution in the Army. In early 1919, the military control and the military section of VCheKa were merged into one body, the Special Section of the Republic. Kedrov was appointed as head. On January 1, he issued an order to establish the Special Section. The order instructed agencies everywhere to unite the Military control and the military sections of Chekas and to form special sections of frontlines, armies, military districts, and guberniyas.
In November 1920 the Soviet of Labor and Defense created a Special Section of VCheKa for the security of the state border.
On February 6, 1922, after the Ninth All-Russian Soviet Congress, the Cheka was dissolved by VTsIK, "with expressions of gratitude for heroic work." It was replaced by the State Political Administration or GPU, a section of the NKVD of the Russian Soviet Federative Socialist Republic (RSFSR). Dzerzhinsky remained as chief of the new organization.
Operations.
Suppression of political opposition.
Initially formed to fight against counter-revolutionaries and saboteurs, as well as financial speculators, Cheka had its own classifications. Those counter-revolutionaries fell under these categories:
As its name implied, the Extraordinary Commission had virtually unlimited powers and could interpret them in any way it wished. No standard procedures were ever set up, except that the Commission was supposed to send the arrested to the Military-Revolutionary tribunals if outside of a war zone. This left an opportunity for a wide range of interpretations, as the whole country was in total chaos. At the direction of Lenin, the Cheka performed mass arrests, imprisonments, and executions of "enemies of the people". In this, the Cheka said that they targeted "class enemies" such as the bourgeoisie, and members of the clergy; the first organized mass repression began against the libertarians and socialists of Petrograd in April 1918. Over the next few months, 800 were arrested and shot without trial.
However, within a month, the Cheka had extended its repression to all political opponents of the communist government, including anarchists and others on the left. On April 11/12, 1918, some 26 anarchist political centres in Moscow were attacked. There 40 anarchists were killed by Cheka forces, and about 500 were arrested and jailed after a pitched battle took place between the two groups. (P. Avrich. G. Maximoff) In response to the anarchists' resistance, the Cheka orchestrated a massive retaliatory campaign of repression, executions, and arrests against all opponents of the Bolshevik government, in what came to be known as "Red Terror". The "Red Terror", implemented by Dzerzhinsky on September 5, 1918, was vividly described by the Red Army journal "Krasnaya Gazeta":
An early Bolshevik, Victor Serge described in his book "Memoirs of a Revolutionary":Since the first massacres of Red prisoners by the Whites, the murders of Volodarsky and Uritsky and the attempt against Lenin (in the summer of 1918), the custom of arresting and, often, executing hostages had become generalized and legal. Already the Cheka, which made mass arrests of suspects, was tending to settle their fate independently, under formal control of the Party, but in reality without anybody's knowledge. The Party endeavoured to head it with incorruptible men like the former convict Dzerzhinsky, a sincere idealist, ruthless but chivalrous, with the emaciated profile of an Inquisitor: tall forehead, bony nose, untidy goatee, and an expression of weariness and austerity. But the Party had few men of this stamp and many Chekas. I believe that the formation of the Chekas was one of the gravest and most impermissible errors that the Bolshevik leaders committed in 1918 when plots, blockades, and interventions made them lose their heads. All evidence indicates that revolutionary tribunals, functioning in the light of day and admitting the right of defense, would have attained the same efficiency with far less abuse and depravity. Was it necessary to revert to the procedures of the Inquisition?"
The Cheka was also used against the armed anarchist Black Army of Nestor Makhno in the Ukraine. After the Black Army had served its purpose in aiding the Red Army to stop the Whites under Denikin, the Soviet communist government decided to eliminate the anarchist forces. In May 1919, two Cheka agents sent to assassinate Makhno were caught and executed.
Many victims of Cheka repression were 'bourgeois hostages' rounded up and held in readiness for summary execution in reprisal for any alleged counter-revolutionary act. Lenin's dictum was: that it was better to arrest 100 innocent people rather than to risk one enemy going free. That ensured that wholesale, indiscriminate arrests became an integral part of the system.
It was during the Red Terror that the Cheka, hoping to avoid the bloody aftermath of having half-dead victims writhing on the floor, developed a technique for execution known later by the German words ""Nackenschuss" or "Genickschuss"", a shot to the nape of the neck, which caused minimal blood loss and instant death. The victim's head was bent forward, and the executioner fired slightly downward at point blank range. This had become the standard method used later by the NKVD to liquidate Joseph Stalin's purge victims and others.
Persecution of deserters.
It is believed that there were more than three million deserters from the Red Army in 1919 and 1920. Approximately 500,000 deserters were arrested in 1919 and close to 800,000 in 1920, by troops of the 'Special Punitive Department' of the Cheka, created to punish desertions. These troops were used to forcibly repatriate deserters, taking and shooting hostages to force compliance or to set an example. Throughout the course of the civil war, several thousand deserters were shot – a number comparable to that of belligerents during World War I.
In September 1918, according to "The Black Book of Communism", in only twelve provinces of Russia, 48,735 deserters and 7,325 "bandits" were arrested, 1,826 were killed and 2,230 were executed. The exact identity of these individuals is confused by the fact that the Soviet Bolshevik government used the term 'bandit' to cover ordinary criminals as well as armed and unarmed political opponents, such as the anarchists.
Number of victims.
Estimates on Cheka executions vary widely. The lowest figures ("disputed below") are provided by Dzerzhinsky's lieutenant Martyn Latsis, limited to RSFSR over the period 1918–1920:
Experts generally agree these semi-official figures are vastly understated. Pioneering historian of the Red Terror Sergei Melgunov claims that this was done deliberately in an attempt to demonstrate the government's humanity. For example, he refutes the claim made by Latsis that only 22 executions were carried out in the first six months of the Cheka's existence by providing evidence that the true number was 884 executions. W. H. Chamberlin claims, "It is simply impossible to believe that the Cheka only put to death 12,733 people in all of Russia up to the end of the civil war." Donald Rayfield concurs, noting that, "Plausible evidence reveals that the actual numbers . . . vastly exceeded the official figures." Chamberlin provides the "reasonable and probably moderate" estimate of 50,000, while others provide estimates ranging up to 500,000. Several scholars put the number of executions at about 250,000. Some believe it is possible more people were murdered by the Cheka than died in battle.
Lenin himself seemed unfazed by the killings. On 12 January 1920, while addressing trade union leaders, he said: "We did not hesitate to shoot thousands of people, and we shall not hesitate, and we shall save the country.". On 14 May 1921, the Politburo, chaired by Lenin, passed a motion "broadening the rights of the [Cheka] in relation to the use of the [death penalty]."
Atrocities.
The Cheka is reported to have practiced torture. Depending on Cheka committees in various cities, the methods included: being skinned alive, scalped, "crowned" with barbed wire, impaled, crucified, hanged, stoned to death, tied to planks and pushed slowly into furnaces or tanks of boiling water, or rolled around naked in internally nail-studded barrels. Chekists reportedly poured water on naked prisoners in the winter-bound streets until they became living ice statues. Others reportedly beheaded their victims by twisting their necks until their heads could be torn off. The Chinese Cheka detachments stationed in Kiev reportedly would attach an iron tube to the torso of a bound victim and insert a rat in the tube closed off with wire netting, while the tube was held over a flame until the rat began gnawing through the victim's guts in an effort to escape. Anton Denikin's investigation discovered corpses whose lungs, throats, and mouths had been packed with earth.
Women and children were also victims of Cheka terror. Women would sometimes be tortured and raped before being shot. Children between the ages of 8 and 13 were imprisoned and occasionally executed.
All of these atrocities were published on numerous occasions in Pravda and Izvestiya: January 26, 1919 Izvestiya #18 article "Is it really a medieval imprisonment?" («Неужели средневековый застенок?»); February 22, 1919 Pravda #12 publishes details of the Vladimir Cheka's tortures, September 21, 1922 "Socialist Herald" publishes details of series of tortures conducted by the Stavropol Cheka (hot basement, cold basement, skull measuring etc.).
The Chekists were also supplemented by the militarized Units of Special Purpose (the Party's Spetsnaz or Russian: ЧОН).
Cheka was actively and openly utilizing kidnapping methods. With kidnapping methods Cheka was able to extinguish numerous cases of discontent especially among the rural population. Among the notorious ones was the Tambov rebellion.
Villages were bombarded to complete annihilation like in the case of Tretyaki, Novokhopersk uyezd, Voronezh Governorate. 
As a result of this relentless violence more than a few Chekists ended up with psychopathic disorders, which Nikolai Bukharin said were "an occupational hazard of the Chekist profession." Many hardened themselves to the executions by heavy drinking and drug use. Some developed a gangster-like slang for the verb to kill in an attempt to distance themselves from the killings, such as 'shooting partridges', of 'sealing' a victim, or giving him a "natsokal" (onomatopoeia of the trigger action).
On November 30, 1992, by the initiative of the President of the Russian Federation the Constitutional Court of the Russian Federation recognized the Red Terror as unlawful, which in turn led to suspension of the Communist Party of the RSFSR.
Regional Chekas.
Cheka departments were organized not only in big cities and guberniya seats, but also in each uyezd, at any front-lines and military formations. Nothing is known on what resources they were created. Many who were hired to head those departments were so-called "nestlings of Alexander Keren".
Legacy.
Konstantin Preobrazhenskiy criticised the continuing celebration of the professional holiday of the old and the modern Russian security services on the anniversary of the creation of the Cheka, with the assent of the Presidents of Russia. (Vladimir Putin, former KGB officer, chose not to change the date to another): "The successors of the KGB still haven't renounced anything; they even celebrate their professional holiday the same day, as during repression, on the 20th of December. It is as if the present intelligence and counterespionage services of Germany celebrated Gestapo Day. I can imagine how indignant our press would be!"

</doc>
<doc id="6753" url="http://en.wikipedia.org/wiki?curid=6753" title="Clitic">
Clitic

In morphology and syntax, a clitic (from Greek κλιτικός "klitikos", "inflexional") is a morpheme that has syntactic characteristics of a word, but depends phonologically on another word or phrase. In this sense, it is syntactically independent but phonologically dependent, always attached to a host. The term is derived from the Greek for "leaning". It is pronounced like an affix, but plays a syntactic role at the phrase level. In other words, clitics have the "form" of affixes, but the distribution of function words. For example, the English possessive "’s" is a clitic in the phrase "the king of England's horse": It "looks" like a suffix, but its position at the end of "the king of England" rather than on "king" is like that of a separate word.
Clitics can belong to any grammatical category, although they are commonly pronouns, determiners, or adpositions. Note that orthography is not always a good guide for distinguishing clitics from affixes: clitics may be written as separate words, but sometimes they are joined to the word on which they depend (like the Latin clitic "que", meaning "and"), or separated by special characters such as hyphens or apostrophes (like the English clitic "’s"). The word "clitic" is often used loosely for what may be better described as an affix or word.
Classification.
Clitics fall into various categories depending on their position in relation to the word to which they are connected.
Proclitic.
A proclitic appears before its host. It is common in Romance languages. For example, in French, there is "il s'est réveillé", "je t'aime" 'I love you'.
Enclitic.
An enclitic appears after its host.
Mesoclitic.
A mesoclitic appears between the stem of the host and other affixes. For example, in Portuguese it is possible to hear (but actually much more possible to read) "conquistar-se-á" "it will be conquered", "dá-lo-ei" "I will give it", "matá-la-ia" "he/she/it would kill her", and it is even possible to use two pronouns inside the verb, as in "dar-no-lo-á" "he/she/it will give it to us", or "dar-ta-ei" ("ta" < "te" + "a") "I will give it/her to you". As in other Romance languages, the Portuguese synthetic future tense comes from the merging of the infinitive and the corresponding finite forms of the verb "haver" (< Latin "habere"), which explains the possibility of separating it from the infinitive.
Endoclitic.
The endoclitic splits apart the root and is inserted between the two pieces. Endoclitics defy the Lexical Integrity Hypothesis (Lexicalist Hypothesis) and so were long claimed to be impossible, but evidence from the Udi language suggests that they do exist. Endoclitics are also found in Pashto and are reported to exist in Degema.
Distinction.
One important distinction divides the broad term 'clitics' into two categories, simple clitics and special clitics. This distinction is, however, disputed. 
Simple clitics.
Simple clitics are free morphemes, meaning they can stand alone in a phrase or sentence. They are unaccented and thus phonologically dependent upon a nearby word. They only derive meaning from this “host."
Special clitics.
Special clitics are morphemes that are bound to the word they are dependent upon, meaning they exist as a part of their host. This form, which is unaccented, represents a variant of a free form that does carry stress. While the two variants carry similar meaning and phonological makeup, the special clitic is bound to a host word and unaccented.
Properties.
Some clitics can be understood as elements undergoing a historical process of grammaticalization:
According to this model from Judith Klavans, an autonomous lexical item in a particular context loses the properties of a fully independent word over time and acquires the properties of a morphological affix (prefix, suffix, infix, etc.). At any intermediate stage of this evolutionary process, the element in question can be described as a "clitic". As a result, this term ends up being applied to a highly heterogeneous class of elements, presenting different combinations of word-like and affix-like properties.
Prosody.
One characteristic shared by many clitics is a lack of prosodic independence. A clitic attaches to an adjacent word, known as its "host". Orthographic conventions treat clitics in different ways: Some are written as separate words, some are written as one word with their hosts, and some are attached to their hosts, but set off by punctuation (a hyphen or an apostrophe, for example).
Comparison with affixes.
Given this basic definition, further criteria are needed to establish a dividing line between postlexical clitics and morphological affixes, since both are characterized by a lack of prosodic autonomy. There is no natural, clear-cut boundary between the two categories (since from a historical point of view, a given form can move gradually from one to the other by morphologization). However, by identifying clusters of observable properties that are associated with core examples of clitics on the one hand, and core examples of affixes on the other, one can pick out a battery of tests that provide an empirical foundation for a clitic/affix distinction.
An affix syntactically and phonologically attaches to a base morpheme of a limited part of speech, such as a verb, to form a new word. A clitic syntactically functions above the word level, on the phrase or clause level, and attaches only phonetically to the first, last, or only word in the phrase or clause, whichever part of speech the word belongs to.
The results of applying these criteria sometimes reveal that elements that have traditionally been called "clitics" actually have the status of affixes (e.g., the Romance pronominal clitics discussed below).
Zwicky and Pullum postulated five characteristics that distinguish clitics from affixes:
Comparison with words.
Similar to the discussion above, clitics must be able to be distinguished from words. There have been a number of linguistic tests proposed to differentiate between the two categories. Some tests, specifically, are based upon the understanding that when comparing the two, clitics resemble affixes, while words resemble syntactic phrases. Clitics and words resemble different categories in the sense that they share certain properties with them. Six such tests are described below. These, of course, are not the only ways to differentiate between words and clitics.
If a morpheme is bound to a word and can never occur in complete isolation, then it is likely a clitic. In contrast, a word is not bound and can appear on its own.
If the addition of a morpheme to a word prevents further affixation, then it is likely a clitic.
If a morpheme combines with single words to convey a further degree of meaning, then it is likely a clitic. A word will combine with a group of words or phrases to denote further meaning.
If a morpheme is required to be in a certain order with respect to other morphemes within the construction, then it is likely a clitic. Independent words enjoy free ordering with respect to other words, within the confines of the word order of the language.
If a morpheme’s allowable behavior is determined by one principle, it is likely an clitic. For example, "a" proceeds indefinite nouns in English. Words can rarely be described with one such description.
In general, words are more morphologically complex than clitics. Clitics are rarely composed of more than one morpheme.
Word order.
Clitics do not always appear next to the word or phrase that they are associated with grammatically. They may be subject to global word order constraints that act on the entire sentence. Many Indo-European languages, for example, obey "Wackernagel's Law", which requires clitics to appear in "second position", after the first syntactic phrase or the first stressed word in a clause:
Indo-European languages.
Germanic languages.
English.
English enclitics include the possessive marker, which attaches to the entire noun phrase, not to the noun that is the possessor:
The negative marker "n’t" as in "couldn’t" etc. is often thought to be a clitic developed from the lexical item "not". Linguists Arnold Zwicky and Geoffrey Pullum argue, however, that the form has the properties of an affix rather than a syntactically independent clitic.
Romance languages.
In the Romance languages, the object personal pronoun forms are sometimes claimed to be clitics; others claim that they are actually affixes, as they only attach to the verb they are the object of. There is no general agreement on the issue. In Spanish, for example:
Colloquial European Portuguese allows object suffixes before the conditional and future suffixes of the verbs:
Colloquial Portuguese of Brazil and Portugal and Spanish of the former Gran Colombia allow ser to be conjugated as a verbal clitic adverbial adjunct to emphasize the importance of the phrase compared to its context or with the meaning of "really" or "in truth":
Note that this clitic form is only for the verb ser and is restricted to only third-person singular conjugations. It is not used as a verb in the grammar of the sentence but introduces prepositional phrases and adds emphasis. It does not need to concord with the tense of the main verb, as in the second example, and can be usually removed from the sentence without affecting the simple meaning.
Proto-Indo-European.
In the Indo-European languages, some clitics can be traced back to Proto-Indo-European: for example, *"-kʷe" is the original form of Sanskrit "च" ("-ca"), Greek "τε" ("-te"), and Latin "-que".

</doc>
<doc id="6759" url="http://en.wikipedia.org/wiki?curid=6759" title="Context-free grammar">
Context-free grammar

In formal language theory, a context-free grammar (CFG)
is a formal grammar in which every production rule is of the form
where "V" is a "single" nonterminal symbol, and "w" is a string of terminals and/or nonterminals ("w" can be empty). A formal grammar is considered "context free" when its production rules can be applied regardless of the context of a nonterminal. No matter which symbols surround it, the single nonterminal on the left hand side can always be replaced by the right hand side. This is what distinguishes it from a context-sensitive grammar.
Languages generated by context-free grammars are known as context-free languages (CFL). Different context-free grammars can generate the same context-free language. It is important to distinguish properties of the language (intrinsic properties) from properties of a particular grammar (extrinsic properties). The language equality question (do two given context-free grammars generate the same language?) is undecidable.
Context-free grammars arise in linguistics where they are used to describe the structure of sentences and words in natural language, and they were in fact invented by the linguist Noam Chomsky for this purpose, but have not really lived up to their original expectation. By contrast, in computer science, as the use of recursively defined concepts increased, they were used more and more. In an early application, grammars are used to describe the structure of programming languages. In a newer application, they are used in an essential part of the Extensible Markup Language (XML) called the "Document Type Definition".
In linguistics, some authors use the term phrase structure grammar to refer to context-free grammars, whereby phrase structure grammars are distinct from dependency grammars. In computer science, a popular notation for context-free grammars is Backus–Naur Form, or "BNF".
Background.
Since the time of Pāṇini, at least, linguists have described the grammars of languages in terms of their block structure, and described how sentences are recursively built up from smaller phrases, and eventually individual words or word elements. An essential property of these block structures is that logical units never overlap. For example, the sentence:
can be logically parenthesized as follows:
A context-free grammar provides a simple and mathematically precise mechanism for describing the methods by which phrases in some natural language are built from smaller blocks, capturing the "block structure" of sentences in a natural way. Its simplicity makes the formalism amenable to rigorous mathematical study. Important features of natural language syntax such as agreement and reference are not part of the context-free grammar, but the basic recursive structure of sentences, the way in which clauses nest inside other clauses, and the way in which lists of adjectives and adverbs are swallowed by nouns and verbs, is described exactly.
The formalism of context-free grammars was developed in the mid-1950s by Noam Chomsky, and also their classification as a special type of formal grammar (which he called phrase-structure grammars). What Chomsky called a phrase structure grammar is also known now as a constituency grammar, whereby constituency grammars stand in contrast to dependency grammars. In Chomsky's generative grammar framework, the syntax of natural language was described by context-free rules combined with transformation rules.
Block structure was introduced into computer programming languages by the Algol project (1957–1960), which, as a consequence, also featured a context-free grammar to describe the resulting Algol syntax. This became a standard feature of computer languages, and the notation for grammars used in concrete descriptions of computer languages came to be known as Backus-Naur Form, after two members of the Algol language design committee. The "block structure" aspect that context-free grammars capture is so fundamental to grammar that the terms syntax and grammar are often identified with context-free grammar rules, especially in computer science. Formal constraints not captured by the grammar are then considered to be part of the "semantics" of the language.
Context-free grammars are simple enough to allow the construction of efficient parsing algorithms which, for a given string, determine whether and how it can be generated from the grammar. An Earley parser is an example of such an algorithm, while the widely used LR and LL parsers are simpler algorithms that deal only with more restrictive subsets of context-free grammars.
Formal definitions.
A context-free grammar "G" is defined by the 4-tuple:
formula_1
where
Production rule notation.
A production rule in formula_8 is formalized mathematically as a pair formula_16, where formula_17 is a non-terminal and formula_18 is a string of variables and/or terminals; rather than using ordered pair notation, production rules are usually written using an arrow operator with formula_19 as its left hand side and formula_20 as its right hand side:
formula_21.
It is allowed for formula_20 to be the empty string, and in this case it is customary to denote it by ε. The form formula_23 is called an ε-production.
It is common to list all right-hand sides for the same left-hand side on the same line, using | (the pipe symbol) to separate them. Rules formula_24 and formula_25 can hence be written as formula_26. In this case, formula_27 and formula_28 is called the first and second alternative, respectively.
Rule application.
For any strings formula_29, we say formula_30 directly yields formula_31, written as formula_32, if formula_33 with formula_17 and formula_35 such that formula_36 and formula_37. Thus, formula_38 is a result of applying the rule formula_39 to formula_40.
Repetitive rule application.
For any strings formula_41 we say formula_42 yields formula_43, written as formula_44 (or formula_45 in some textbooks), if formula_46 such that formula_47. In this case, if formula_48 (i.e., formula_49), the relation formula_50 holds. In other words, formula_51 and formula_52 are the reflexive transitive closure (allowing a word to yield itself) and the transitive closure (requiring at least one step) of formula_53, respectively.
Context-free language.
The language of a grammar formula_1 is the set
A language formula_56 is said to be a context-free language (CFL), if there exists a CFG formula_57, such that formula_58.
Proper CFGs.
A context-free grammar is said to be "proper", if it has
Every context-free grammar can be effectively transformed into a weakly equivalent one without unreachable symbols, a weakly equivalent one without unproductive symbols, and a weakly equivalent one without cycles.
Every context-free grammar not producing ε can be effectively transformed into a weakly equivalent one without ε-productions; altogether, every such grammar can be effectively transformed into a weakly equivalent proper CFG.
Example.
The grammar formula_63, with productions
is context-free. It is not proper since it includes an ε-production. A typical derivation in this grammar is
This makes it clear that 
formula_64. 
The language is context-free, however it can be proved that it is not regular.
Examples.
Well-formed parentheses.
The canonical example of a context free grammar is parenthesis matching, which is representative of the general case. There are two terminal symbols "(" and ")" and one nonterminal symbol S. The production rules are
The first rule allows Ss to multiply; the second rule allows Ss to become enclosed by matching parentheses; and the third rule terminates the recursion.
Well-formed nested parentheses and square brackets.
A second canonical example is two different kinds of matching nested parentheses, described by the productions:
with terminal symbols [ ] ( ) and nonterminal S.
The following sequence can be derived in that grammar:
However, there is no context-free grammar for generating all sequences of two different types of parentheses, each separately balanced disregarding the other, but where the two types need not nest inside one another, for example:
or
A regular grammar.
Every regular grammar is context-free, but not all context-free grammars are regular. The following context-free grammar, however, is also regular.
The terminals here are "a" and "b", while the only non-terminal is S.
The language described is all nonempty strings of formula_65s and formula_66s that end in formula_65.
This grammar is regular: no rule has more than one nonterminal in its right-hand side, and each of these nonterminals is at the same end of the right-hand side.
Every regular grammar corresponds directly to a nondeterministic finite automaton, so we know that this is a regular language.
Using pipe symbols, the grammar above can be described more tersely as follows:
Matching pairs.
In a context-free grammar, we can pair up characters the way we do with brackets. The simplest example:
This grammar generates the language formula_68, which is not regular (according to the pumping lemma for regular languages).
The special character ε stands for the empty string. By changing the above grammar to
we obtain a grammar generating the language formula_69 instead. This differs only in that it contains the empty string while the original grammar did not.
Algebraic expressions.
Here is a context-free grammar for syntactically correct infix algebraic expressions in the variables x, y and z:
This grammar can, for example, generate the string
as follows:
Note that many choices were made underway as to which rewrite was going to be performed next.
These choices look quite arbitrary. As a matter of fact, they are, in the sense that the string finally generated is always the same. For example, the second and third rewrites
could be done in the opposite order:
Also, many choices were made on which rule to apply to each selected codice_1.
Changing the choices made and not only the order they were made in usually affects which terminal string comes out at the end.
Let's look at this in more detail. Consider the parse tree of this derivation:
 S
 /|\
 S - S
 /|\ /|\
 S * S S / S
 /|\ x /|\ /|\
 ( S ) S * S ( S )
 /|\ z y /|\
 S + S S + S
 x y x x
Starting at the top, step by step, an S in the tree is expanded, until no more unexpanded codice_1es (non-terminals) remain.
Picking a different order of expansion will produce a different derivation, but the same parse tree.
The parse tree will only change if we pick a different rule to apply at some position in the tree.
But can a different parse tree still produce the same terminal string,
which is codice_3 in this case?
Yes, for this particular grammar, this is possible.
Grammars with this property are called ambiguous.
For example, codice_4 can be produced with these two different parse trees:
 S S
 /|\ /|\
 S * S S + S 
 /|\ z x /|\
 S + S S * S 
 x y y z
However, the "language" described by this grammar is not inherently ambiguous:
an alternative, unambiguous grammar can be given for the language, for example:
(once again picking codice_1 as the start symbol). This alternative grammar will produce codice_4 with a parse tree similar to the left one above, i.e. implicitly assuming the association codice_7, which is not according to standard operator precedence. More elaborate, unambiguous and context-free grammars can be constructed that produce parse trees that obey all desired operator precedence and associativity rules.
Further examples.
Example 1.
A context-free grammar for the language consisting of all strings over {a,b} containing an unequal number of a's and b's:
Here, the nonterminal T can generate all strings with the same number of a's as b's, the nonterminal U generates all strings with more a's than b's and the nonterminal V generates all strings with fewer a's than b's. Omitting the third alternative in the rule for U and V doesn't restrict the grammar's language.
Example 2.
Another example of a non-regular language is formula_70. It is context-free as it can be generated by the following context-free grammar:
Other examples.
The formation rules for the terms and formulas of formal logic fit the definition of context-free grammar, except that the set of symbols may be infinite and there may be more than one start symbol.
Derivations and syntax trees.
A "derivation" of a string for a grammar is a sequence of grammar rule applications that transforms the start symbol into the string.
A derivation proves that the string belongs to the grammar's language.
A derivation is fully determined by giving, for each step:
For clarity, the intermediate string is usually given as well.
For instance, with the grammar:
 (1) S → S + S
 (2) S → 1
 (3) S → a
the string
 1 + 1 + a
can be derived with the derivation:
 S
 → (rule 1 on first S)
 S+S
 → (rule 1 on second S)
 S+S+S
 → (rule 2 on second S)
 S+1+S
 → (rule 3 on third S)
 S+1+a
 → (rule 2 on first S)
 1+1+a
Often, a strategy is followed that deterministically determines the next nonterminal to rewrite:
Given such a strategy, a derivation is completely determined by the sequence of rules applied. For instance, the leftmost derivation
 S
 → (rule 1 on first S)
 S+S
 → (rule 2 on first S)
 1+S
 → (rule 1 on first S)
 1+S+S
 → (rule 2 on first S)
 1+1+S
 → (rule 3 on first S)
 1+1+a
can be summarized as
 rule 1, rule 2, rule 1, rule 2, rule 3
The distinction between leftmost derivation and rightmost derivation is important because in most parsers the transformation of the input is defined by giving a piece of code for every grammar rule that is executed whenever the rule is applied. Therefore it is important to know whether the parser determines a leftmost or a rightmost derivation because this determines the order in which the pieces of code will be executed. See for an example LL parsers and LR parsers.
A derivation also imposes in some sense a hierarchical structure on the string that is derived. For example, if the string "1 + 1 + a" is derived according to the leftmost derivation:
the structure of the string would be:
where { ... }S indicates a substring recognized as belonging to S. This hierarchy can also be seen as a tree:
 S
 / | \
 S '+' S
 | / | \
 '1' S '+' S
 '1' 'a'
This tree is called a "parse tree" or "concrete syntax tree" of the string, by contrast with the abstract syntax tree. In this case the presented leftmost and the rightmost derivations define the same parse tree; however, there is another (rightmost) derivation of the same string
and this defines the following parse tree:
 S 
 / | \
 S '+' S
 / | \ |
 S '+' S 'a'
 '1' '1'
If, for certain strings in the language of the grammar, there is more than one parsing tree, then the grammar is said to be an "ambiguous grammar". Such grammars are usually hard to parse because the parser cannot always decide which grammar rule it has to apply. Usually, ambiguity is a feature of the grammar, not the language, and an unambiguous grammar can be found that generates the same context-free language. However, there are certain languages that can only be generated by ambiguous grammars; such languages are called "inherently ambiguous languages".
Normal forms.
Every context-free grammar that does not generate the empty string can be transformed into one in which there is no ε-production (that is, a rule that has the empty string as a product). If a grammar does generate the empty string, it will be necessary to include the rule formula_71, but there need be no other ε-rule. Every context-free grammar with no ε-production has an equivalent grammar in Chomsky normal form or Greibach normal form. "Equivalent" here means that the two grammars generate the same language.
The especially simple form of production rules in Chomsky Normal Form grammars has both theoretical and practical implications. For instance, given a context-free grammar, one can use the Chomsky Normal Form to construct a polynomial-time algorithm that decides whether a given string is in the language represented by that grammar or not (the CYK algorithm).
Closure properties.
Context-free languages are closed under union, concatenation, Kleene star,
substitution (in particular homomorphism), inverse homomorphism,
and intersection with a regular language.
They are not closed under general intersection (hence neither under complementation) and set difference.
Decidable problems.
There are algorithms to decide whether a context-free language is empty, and whether it is finite.
Undecidable problems.
Some questions that are undecidable for wider classes of grammars become decidable for context-free grammars; e.g. the emptiness problem (whether the grammar generates any terminal strings at all), is undecidable for context-sensitive grammars, but decidable for context-free grammars.
However, many problems are undecidable even for context-free grammars. Examples are:
Universality.
Given a CFG, does it generate the language of all strings over the alphabet of terminal symbols used in its rules?
A reduction can be demonstrated to this problem from the well-known undecidable problem of determining whether a Turing machine accepts a particular input (the halting problem). The reduction uses the concept of a "computation history", a string describing an entire computation of a Turing machine. A CFG can be constructed that generates all strings that are not accepting computation histories for a particular Turing machine on a particular input, and thus it will accept all strings only if the machine doesn't accept that input.
Language equality.
Given two CFGs, do they generate the same language?
The undecidability of this problem is a direct consequence of the previous: it is impossible to even decide whether a CFG is equivalent to the trivial CFG defining the language of all strings.
Language inclusion.
Given two CFGs, can the first one generate all strings that the second one can generate?
If this problem was decidable, then language equality could be decided too: two CFGs G1 and G2 generate the same language if L(G1) is a subset of L(G2) and L(G2) is a subset of L(G1).
Being in a lower or higher level of the Chomsky hierarchy.
Using Greibach's theorem, it can be shown that the two following problems are undecidable:
Grammar ambiguity.
Given a CFG, is it ambiguous?
The undecidability of this problem follows from the fact that if an algorithm to determine ambiguity existed, the Post correspondence problem could be decided, which is known to be undecidable.
Language disjointness.
Given two CFGs, is there any string derivable from both grammars?
If this problem was decidable, the undecidable Post correspondence problem could be decided, too: given strings formula_72 over some alphabet formula_73, let the grammar G1 consist of the rule
where formula_79 denotes the reversed string formula_80 and formula_66 doesn't occur among the formula_82; and let grammar G2 consist of the rule
Then the Post problem given by formula_72 has a solution if and only if L(G1) and L(G2) share a derivable string.
Extensions.
An obvious way to extend the context-free grammar formalism is to allow nonterminals to have arguments, the values of which are passed along within the rules. This allows natural language features such as agreement and reference, and programming language analogs such as the correct use and definition of identifiers, to be expressed in a natural way. E.g. we can now easily express that in English sentences, the subject and verb must agree in number. In computer science, examples of this approach include affix grammars, attribute grammars, indexed grammars, and Van Wijngaarden two-level grammars. Similar extensions exist in linguistics.
An extended context-free grammar (or regular right part grammar) is one in which the right-hand side of the production rules is allowed to be a regular expression over the grammar's terminals and nonterminals. Extended context-free grammars describe exactly the context-free languages.
Another extension is to allow additional terminal symbols to appear at the left hand side of rules, constraining their application. This produces the formalism of context-sensitive grammars.
Subclasses.
There are a number of important subclasses of the context-free grammars:
LR parsing extends LL parsing to support a larger range of grammars; in turn, generalized LR parsing extends LR parsing to support arbitrary context-free grammars. On LL grammars and LR grammars, it essentially performs LL parsing and LR parsing, respectively, while on nondeterministic grammars, it is as efficient as can be expected. Although GLR parsing was developed in the 1980s, many new language definitions and parser generators continue to be based on LL, LALR or LR parsing up to the present day.
Linguistic applications.
Chomsky initially hoped to overcome the limitations of context-free grammars by adding transformation rules.
Such rules are another standard device in traditional linguistics; e.g. passivization in English. Much of generative grammar has been devoted to finding ways of refining the descriptive mechanisms of phrase-structure grammar and transformation rules such that exactly the kinds of things can be expressed that natural language actually allows. Allowing arbitrary transformations doesn't meet that goal: they are much too powerful, being Turing complete unless significant restrictions are added (e.g. no transformations that introduce and then rewrite symbols in a context-free fashion).
Chomsky's general position regarding the non-context-freeness of natural language has held up since then, although his specific examples regarding the inadequacy of context-free grammars in terms of their weak generative capacity were later disproved.
Gerald Gazdar and Geoffrey Pullum have argued that despite a few non-context-free constructions in natural language (such as cross-serial dependencies in Swiss German and reduplication in Bambara), the vast majority of forms in natural language are indeed context-free.

</doc>
<doc id="6760" url="http://en.wikipedia.org/wiki?curid=6760" title="Cryonics">
Cryonics

Cryonics (from Greek κρύος 'kryos-' meaning 'icy cold') is the low-temperature preservation of animals and humans who cannot be sustained by contemporary medicine, with the hope that healing and resuscitation may be possible in the future.
Cryopreservation of people or large animals is not reversible with current technology. The stated rationale for cryonics is that people who are considered dead by current legal or medical definitions may not necessarily be dead according to the more stringent information-theoretic definition of death. It is proposed that cryopreserved people might someday be recovered by using highly advanced technology.
Some scientific literature supports the feasibility of cryonics. An open letter supporting the idea of cryonics has been signed by 63 scientists, including Aubrey de Grey and Marvin Minsky. However, many other scientists regard cryonics with skepticism. As of 2013, approximately 270 people have undergone cryopreservation procedures since cryonics was first proposed in 1962. In the United States, cryonics can only be legally performed on humans after they have been pronounced legally dead, as otherwise it would be considered murder or assisted suicide.
Cryonics procedures ideally begin within minutes of cardiac arrest, and use cryoprotectants to prevent ice formation during cryopreservation. However, the idea of cryonics also includes preservation of people long after legal death because of the possibility that brain structures that encode memory and personality may still persist and be inferable in the future. Whether sufficient brain information still exists for cryonics to successfully preserve may be intrinsically unprovable by present knowledge. Therefore, most proponents of cryonics see it as an intervention with prospects for success that vary widely depending on circumstances.
Premises.
A central premise of cryonics is that long-term memory, personality, and identity are stored in durable cell structures and patterns within the brain that do not require continuous brain activity to survive. This premise is generally accepted in medicine; it is known that under certain conditions the brain can stop functioning and still later recover with retention of long-term memory. Additional scientific premises of cryonics are that (1) brain structures encoding personality and long-term memory persist for some time after legal death, (2) these structures are preserved by cryopreservation, and (3) future technologies that could restore encoded memories to functional expression in a healed person are theoretically possible.
At present only cells, tissues, and some small organs can be reversibly cryopreserved.
Cryonics advocates say it is possible to preserve the fine cell structures of the brain in which memory and identity reside with present technology. They say that demonstrably reversible cryopreservation is not necessary to achieve the present-day goal of cryonics, which is preservation of brain information that encodes memory and personal identity. They say current cryonics procedures can preserve the anatomical basis of mind, and that this should be sufficient to prevent information-theoretic death until future repairs might be possible.
A moral premise of cryonics is that all terminally ill patients should have the right, if they so choose, to be cryopreserved. Some cryonicists believe as a matter of principle that anyone who would ordinarily be regarded as dead should instead be made a "permanent patient" subject to whatever future advances might bring.
Obstacles to success.
Preservation injury.
Long-term cryopreservation can be achieved by cooling to near 77.15 Kelvin (approximately -196.01°C), the boiling point of liquid nitrogen. It is a common mistaken belief that cells will lyse (burst) due to the formation of ice crystals within the cell, since this only occurs if the freezing rate exceeds the osmotic loss of water to the extracellular space. However, damage from freezing can still be serious; ice may still form between cells, causing mechanical and chemical damage. Cryonics organizations use cryoprotectants to reduce this damage. Cryoprotectant solutions are circulated through blood vessels to remove and replace water inside cells with chemicals that prevent freezing. This can reduce damage greatly, but freezing of the entire body still causes injuries that are not reversible with present technology. The difficulties of recovering complex organisms from a frozen state have been long known. Attempts to recover large frozen mammals by simply rewarming were abandoned by 1957.
When used at high concentrations, cryoprotectants stop ice formation completely. Cooling and solidification without crystal formation is called vitrification. The first cryoprotectant solutions able to vitrify at very slow cooling rates while still being compatible with tissue survival were developed in the late 1990s by cryobiologists Gregory Fahy and Brian Wowk for the purpose of banking transplantable organs. These solutions were adopted for use in cryonics by the Alcor Life Extension Foundation, for which they are believed to permit vitrification of some parts of the human body, especially the brain. This has allowed animal brains to be vitrified, warmed back up, and examined for ice damage using light and electron microscopy. No ice crystal damage was found. The Cryonics Institute also uses a vitrification solution developed by their staff cryobiologist, Yuri Pichugin, applying it principally to the brain.
Vitrification in cryonics is different from vitrification in mainstream cryobiology because vitrification in cryonics is not reversible with current technology. It is only structural vitrification. When successful, it can prevent freezing injury in some body parts, but at the price of toxicity caused by cryoprotectant chemicals. The nature of this toxicity is still poorly understood. Cryonicists assume that toxicity is more subtle and repairable than obvious structural damage that would otherwise be caused by freezing. If, for example, toxicity is due to denatured proteins, those proteins could be repaired or replaced.
Ischemic injury.
Ischemia means inadequate or absent blood circulation that deprives tissue of oxygen and nutrients. At least several minutes of ischemia is a typical part of cryonics because of the common legal requirement that cryonics procedures do not begin until after blood circulation stops. The heart must stop beating so that legal death can be declared. When there is advance notice of impending legal death, it is sometimes possible to deploy a team of technicians to perform a “standby procedure”. The team artificially restores blood circulation and breathing using techniques similar to cardiopulmonary resuscitation as soon as possible after the heart stops. The aim is to keep tissues alive after legal death by analogy to conventional medical procedures in which viable organs and tissues are obtained for transplant from legally deceased donors. Legal death does not mean that all the cells of the body have died.
Often in cryonics the brain is without oxygen for many minutes at warm temperatures, or even hours if the heart stops unexpectedly. This causes ischemic injury to the brain and other tissues that makes resuscitation impossible by present medical technology. Cryonicists justify preservation under such conditions by noting recent advances that allow brain resuscitation after longer periods of ischemia than the traditional 4-to-6-minute limit, and persistence of brain structure and even some brain cell function after long periods of clinical death. They argue that definitions of death change as technology advances, and the early stages of what is called “death” today is actually a form of ischemic injury that will be reversible in the future. They claim that personal survival during long periods of clinical death is determined by information-theoretic criteria.
Revival.
Those who believe that revival may someday be possible generally look toward advanced bioengineering, molecular nanotechnology, nanomedicine, or mind uploading as key technologies. Revival requires repairing damage from lack of oxygen, cryoprotectant toxicity, thermal stress (fracturing), freezing in tissues that do not successfully vitrify, and reversing the effects that caused the patient's death. In many cases extensive tissue regeneration will be necessary. Hypothetical revival scenarios generally envision repairs being performed by vast numbers of microscopic organisms or devices. These devices would restore healthy cell structure and chemistry at the molecular level, ideally before warming. More radically, mind transfer has also been suggested as a possible revival approach if and when technology is ever developed to scan the memory contents of a preserved brain.
It has sometimes been written that cryonics revival will be a last in, first out process. People cryopreserved in the future, with better technology, may require less advanced technology to be revived because they will have been cryopreserved with better technology that caused less damage to tissue. In this view, preservation methods will get progressively better until eventually they are demonstrably reversible, after which medicine will begin to reach back and revive people cryopreserved by more primitive methods. Revival of people cryopreserved by early cryonics technology may require centuries, if it is possible at all. The "last in, first out" view of cryonics has been criticized because the quality of cryopreservation depends on many factors other than the era in which cryopreservation takes place.
It has been claimed that if technologies for general molecular analysis and repair are ever developed, then theoretically any damaged body could be “revived”.
Survival would then depend on whether preserved brain information was sufficient to permit restoration of all or part of the personal identity of the original person, with amnesia being the final dividing line between success and failure.
Neuropreservation.
Neuropreservation is cryopreservation of the brain, often within the head, with surgical removal and disposal (usually cremation) of the rest of the body. Neuropreservation, sometimes called “neuro,” is one of two distinct preservation options in cryonics, the other being "whole body" preservation.
Neuropreservation is motivated by the brain's role as the primary repository of memory and personal identity. (For instance, spinal cord injury victims, organ transplant patients, and amputees retain their personal identity.) It is also motivated by the belief that reversing any type of cryonic preservation is so difficult and complex that any future technology capable of it must by its nature be capable of generalized tissue regeneration, including growth of a new body around a repaired brain. Some suggested revival scenarios for whole body patients even involve discarding the original body and regenerating a new body because tissues are so badly damaged by the preservation process. These considerations, along with lower costs, easier transportation in emergencies, and the specific focus on brain preservation quality, have motivated many cryonicists to choose neuropreservation.
The advantages and disadvantages of neuropreservation are often debated among cryonics advocates. Critics of neuropreservation note that the body is a record of much life experience. While few cryonicists doubt that a revived neuro patient would be the same person, there are wider questions about how a regenerated body might feel different from the original. Partly for these reasons (as well as for better public relations), the Cryonics Institute preserves only whole bodies. Some proponents of neuropreservation agree with these concerns, but still feel that lower costs and better brain preservation justify concentrating preservation efforts on the brain. About two-thirds of the patients stored at Alcor are neuropreservation patients. Although the American Cryonics Society no longer offers the neuropreservation option, about half of their patients are "neuros".
Financial difficulties.
Financing storage of a cryonics patient at a cryonics organization by an ongoing payment system was done in the early days of cryonics, but this system proved to be unworkable. Cryonics patients are to be stored for many decades, if not a century or two or longer, and a reliable source of outside funding is highly unlikely. Pay-as-you go funding was part of the reason for the CSC Chatsworth financial failure described in the history section. All modern cryonics organizations require full payment for all future costs associated with storage "in perpetuity" before patient cryostorage will be accepted.
Costs of cryonics vary greatly, ranging from the basic fee of $14,000 for brain-only at Oregon Cryonics and $12,000 for neuro (head or brain only) cryopreservation at the European cryonics company KrioRus, to more than $250,000 for whole body cryopreservation by Alcor with overseas and last-minute fees. Alcor's neuropreservation is priced at $80,000 while a full body preservation is priced at $200,000. There is an extra $500 annual membership fee during life by Alcor. After payment of an initiation fee, ACS full members pay an annual fee of $300 currently. To some extent these cost differences reflect differences in how fees are quoted. The Cryonics Institute fee of $28,000 or $35,000 does not include transportation costs (around $4000 by plane in North America), or funeral director expenses outside of Michigan, which must be purchased as extras. CI Members wanting Standby and Transport can use local paramedics, private bedside service, volunteers team or can contract for additional payment to the Florida-based company Suspended Animation, Inc.
While cryonics is sometimes suspected of being greatly profitable, the high expenses of doing cryonics are well documented. The expenses are comparable to major transplant surgeries. The two most expensive things are standby expenses (a team of 5+ people needs to be hired for up to several weeks) and the money that must be set aside to generate interest to pay for storage of the patient in liquid nitrogen in perpetuity (especially for whole body patients).
The most common method of paying for cryonics is life insurance, which spreads the cost over many years. Cryonics advocates are quick to point out that such insurance is especially affordable for young people. Cryonics providers claim that even the most expensive cryonics plans are “affordable for the vast majority” of people in the industrialized world who really want it and plan for it in advance. With the advent of low-cost cryonics provided by companies such as KrioRus (so far in Europe only) cryonics becomes feasible even for last-minute cases. The Cryonics Institute accepts pre-payments.
Legal issues.
Legally, cryonics patients are treated as deceased persons. A long established legal tradition, the concept of "lost persons," permits a person who has been declared legally dead to later be declared legally alive. Cryonics providers tend to be treated as medical research institutes.
In France, cryonics is not considered a legal mode of body disposal; only burial, cremation, and formal donation to science are allowed. However, bodies may legally be shipped to other, less restrictive countries for cryonic freezing.
Standby services and transportation.
Standby and transportation is a critical phase in a cryopreservation process. Cryonics patients need a professional response team to stand ready for suspended animation, when the patients are legally declared as dead. Standby services include stabilization, cooling, and other procedure to ensure that the damage to the patient during transportation are minimal. Some cryonics services provider may provide standby as well as transportation services to their client.
Philosophical and ethical considerations.
Cryonics is based on a view of dying as a process that can be stopped in the minutes, and perhaps hours, following legal death. If death is not an event that happens suddenly when the heart stops (and "legal death" is often pronounced) this raises philosophical questions about what exactly death is. In 2005 an ethics debate in the medical journal, Critical Care, noted “…few if any patients pronounced dead by today’s physicians are in fact truly dead by any scientifically rigorous criteria.” Cryonics proponent Thomas Donaldson has argued that “death” based on cardiac arrest or resuscitation failure is a purely social construction used to justify terminating care of dying patients. In this view, legal death and its aftermath are a form of euthanasia in which sick people are abandoned. Philosopher Max More suggested a distinction between death associated with circumstances and intention versus death that is absolutely irreversible. Absolutely irreversible death has also been called information-theoretic death, which implies destruction of the brain to such an extent that the original information content can no longer be recovered. Bioethicist James Hughes has written that increasing rights will accrue to cryonics patients as prospects for revival become clearer, noting that recovery of legally dead persons has precedent in the discovery of missing persons.
Ethical and theological opinions of cryonics tend to pivot on the issue of whether cryonics is regarded as interment or medicine. If cryonics is interment, then religious beliefs about death and afterlife may come into consideration. Resuscitation may be deemed impossible by those with religious beliefs because the soul is gone, and according to most religions only a deity can resurrect the dead. Cryonics advocates say theological dismissal of cryonics because it is interment is a circular argument because calling cryonics "interment" presumes "a priori" that cryonics cannot work. They believe future technical advances will validate their view that cryonics patients are recoverable, and therefore never really dead.
If cryonics is regarded as medicine, with legal death as a mere enabling mechanism, then cryonics is a long-term coma with uncertain prognosis.
Alcor has published a vigorous Christian defense of cryonics, including excerpts of a sermon by Lutheran Reverend Kay Glaesner. Noted Christian commentator John Warwick Montgomery has defended cryonics. In 1969, a Roman Catholic priest consecrated the cryonics capsule of Ann DeBlasio, one of the first cryonics patients. Many followers of Nikolai Fyodorovich Fyodorov see cryonics as an important step in the Common Cause project.
At the request of the American Cryonics Society, in 1995, Philosopher Charles Tandy, Ph.D.
authored a paper entitled “Cryonic-Hibernation in Light of the Bioethical Principles of Beauchamp and Childress.” Tandy considered the four bioethical factors or principles articulated by philosophers Beauchamp and Childress as they apply to cryonics. These four principles are 1) respect for autonomy; 2) nonmaleficence; 3) beneficence; and 4) justice. Tandy concluded that in respect to all four principles “biomedical professionals have a strong (not weak) and actual (not prima facie, but binding) obligation to help insure cryonic-hibernation of the cryonics patient.”
History.
Early history.
Benjamin Franklin, in a 1773 letter, expressed regret that he lived "in a century too little advanced, and too near the infancy of science" that he could not be preserved and revived to fulfil his "very ardent desire to see and observe the state of America a hundred years hence."
20th century.
In 1922 Alexander Yaroslavsky, member of Russian immortalists-biocosmists movement, wrote "Anabiosys Poem". 
However, the modern era of cryonics began in 1962 when Michigan college physics teacher Robert Ettinger proposed in a privately published book, "The Prospect of Immortality", that freezing people may be a way to reach future medical technology. (The book was republished in 2005 and remains in print.) Even though freezing a person is apparently fatal, Ettinger argued that what appears to be fatal today may be reversible in the future. He applied the same argument to the process of dying itself, saying that the early stages of clinical death may be reversible in the future. Combining these two ideas, he suggested that freezing recently deceased people may be a way to save lives. In 1955 James Lovelock was able to reanimate rats frozen at 0 Celsius using microwave diathermy.
Slightly before Ettinger’s book was complete, Evan Cooper (writing as Nathan Duhring) privately published a book called "Immortality: Physically, Scientifically, Now" that independently suggested the same idea. Cooper founded the Life Extension Society (LES) in 1964 to promote freezing people. Ettinger came to be credited as the originator of cryonics, perhaps because his book was republished by Doubleday in 1964 on recommendation of Isaac Asimov and Fred Pohl, and received more publicity. Ettinger also stayed with the movement longer. Nevertheless, Alcor's in-house historian, R. Michael Perry, has written “Evan Cooper deserves the principal credit for forming an organized cryonics movement.” The first LES newsletter credits Lawrence Neil Jensen, an art professor at Castleton State College, as "one of the original formulators of the 'freeze and wait' theory." Jensen helped raise awareness of the concept through such means as a letter to President Kennedy, a presentation at Green Mountain College, and an appearance with Ettinger on the Mike Douglas Show in 1965.
Cooper’s Life Extension Society became the seed tree for cryonics societies throughout the United States where local cryonics advocates would get together as a result of contact through the LES mailing list. The actual word “cryonics” was invented by Karl Werner, then a student in the studio of William Katavolos at Pratt Institute in Brooklyn, NY, in 1965 in conjunction with the founding of the Cryonics Society of New York (CSNY) by Curtis Henderson and Saul Kent that same year. This was followed by the founding of the Cryonics Society of Michigan (CSM) and Cryonics Society of California (CSC) in 1966, and Bay Area Cryonics Society (BACS) in 1969 (renamed the American Cryonics Society, or ACS, in 1985). Neither CSNY nor CSC are currently in operation. CSM eventually became the Immortalist Society, a non-profit affiliate of the Cryonics Institute (CI), a cryonics service organization founded by Ettinger in 1976. Alcor now has more current cryonics patients than any other organization, 110 as of December 2011.
Although there was at least one earlier aborted case, it is generally accepted that the first person frozen with intent of future resuscitation was James Bedford, a 73-year-old psychology professor frozen under crude conditions by CSC on January 12, 1967. The case made the cover of a limited print run of "Life" magazine before the presses were stopped to report the death of three astronauts in the Apollo 1 fire instead. Bedford is still frozen today at Alcor.
Cryonics suffered a major setback in 1979 when it was discovered that nine bodies stored by the head of the CSC, Robert Nelson, in a cemetery in Chatsworth, California, had thawed due to depletion of funds by relatives, after being maintained for a year and a half at the personal expense of Nelson. Some of the bodies had apparently thawed years earlier without notification. Nelson was sued, and negative publicity slowed cryonics growth for years afterward. Of 17 documented cryonics cases between 1967 and 1973, only James Bedford remains cryopreserved today. Strict financial controls and requirements adopted in response to the Chatsworth scandal have resulted in the successful maintenance of almost all cryonics cases since that era.
The cryonics organization who was established as a nonprofit organization by Fred and Linda Chamberlain in 1972 as the Alcor Society for Solid State Hypothermia (ALCOR). In 1977, the name was changed to the Alcor Life Extension Foundation. In 1982, the Institute for Advanced Biological Studies (IABS), founded by Mike Darwin and Steve Bridge in Indiana, merged with Alcor. During the 1980s, Darwin worked with UCLA cardiothoracic surgery researcher Jerry Leaf at Alcor to develop a medical model for cryonics procedures. They pioneered the first consistent use of a cryonics procedure now known as a “standby”, in which a team waits to begin life support procedures at the bedside of a cryonics patient as soon as possible after the heart stops.
The oldest incorporated cryonics society still in existence is the American Cryonics Society (ACS). This tax-exempt 501(c)(3) membership organization was incorporated in 1969 as the Bay Area Cryonics Society (BACS) by a group of cryonics advocates that included two prominent Bay Area physicians, M. Coleman Harris and Grace Talbot. The first suspensions under BACS auspices were performed in 1974 by Trans Time, Inc., a for-profit company started by BACS members. BACS researcher Paul Segall, working with Jerry Leaf of CryoVita, developed a medical model to induce hypothermia shortly after pronouncement of death. Segall later went on to pioneer blood substitutes for use in both cryonic suspension and in mainstream medicine.
Cryonics received new support in the 1980s when MIT engineer Eric Drexler started publishing papers and books foreseeing the new field of molecular nanotechnology. His 1986 book, "Engines of Creation", included an entire chapter on cryonics applications. Cryonics advocates saw the nascent field of nanotechnology as vindication of their long held view that molecular repair of injured tissue was theoretically possible. In the late 1980s Alcor member Dick Clair (who was dying of AIDS) sued for, and ultimately won for everyone, the right to be cryonically preserved in the State of California. Alcor’s membership expanded tenfold within a decade, with a 30% annual growth rate between 1988 and 1992. But since 2013, membership began to fall after growing discontent and economic reality. (According the Cryonics Magazine of Alcor, July 2013)
On July 24, 1988, a Ph.D. in computer science named Kevin Brown started an electronic mailing list called "CryoNet" that became a powerful tool of communication for the cryonics community. Numerous other mailing lists and web forums for discussing cryonics and the affairs of particular organizations have since appeared, but CryoNet remained a central point of contact for cryonicists until it was shut down on March 17, 2011.
Alcor was disrupted by political turmoil in 1993 when a group of activists left to start the CryoCare Foundation, and associated for-profit companies CryoSpan, Inc. (headed by Paul Wakfer) and BioPreservation, Inc. (headed by Mike Darwin). Darwin and collaborators made many technical advances during this time period, including a landmark study documenting high quality brain preservation by freezing with high concentrations of glycerol. CryoCare ceased operations in 1999 when they were unable to renew their service contract with BioPreservation. CryoCare’s two patients stored at CryoSpan were transferred to Alcor. Several ACS patients stored at CryoSpan were transferred to CI.
There have been numerous, often transient, for-profit companies involved in cryonics. For-profit companies were often paired or affiliated with non-profit groups they served. Some of these companies, with non-profits they served in parentheses, were Cryonic Interment, Inc. (CSC), Cryo-Span Corporation (CSNY), Cryo-Care Equipment Corporation (CSC and CSNY), Manrise Corporation (Alcor), CryoVita, Inc. (Alcor), BioTransport, Inc. (Alcor), Trans Time, Inc. (BACS), Soma, Inc. (IABS), CryoSpan, Inc. (CryoCare and ACS), BioPreservation, Inc. (CryoCare and ACS), Kryos, Inc. (ACS), Suspended Animation, Inc. (CI, ACS, and Alcor). Trans Time and Suspended Animation are the only for-profit cryonics organizations that still exist.
The cryonics field seems to have largely consolidated around three non-profit groups, Alcor Life Extension Foundation, Cryonics Institute (CI), and the American Cryonics Society (ACS), all deriving significant income from bequests and donations. In 2006, a non-profit called the Cryonics Society was formally incorporated but the group is devoted solely to promotion and public education of the cryonics concept.
21st century.
As research in the 1990s revealed in greater detail the damaging effects of freezing, there was a trend to use higher concentrations of glycerol cryoprotectant to prevent freezing injury. In 2001 Alcor began using vitrification, a technology borrowed from mainstream organ preservation research, in an attempt to completely prevent ice formation during cooling. Initially the technology could only be applied to the head when separated from the body. In 2005 Alcor began treating the whole body with their vitrification solution in a procedure called "neurovitrification with whole body cryoprotection". In the same year, the Cryonics Institute began treating the head of their whole body patients with their own vitrification solution.
In June 2005 scientists at the University of Pittsburgh's Safar Center for Resuscitation Research announced they had managed to place dogs in suspended animation and bring them back to life, most of them without brain damage, by draining the blood out of the dogs' bodies and injecting a low temperature solution into their circulatory systems, which in turn keeps the bodies alive in stasis. After three hours of being clinically dead, the dogs' blood was returned to their circulatory systems, and the animals were revived by delivering an electric shock to their hearts. The heart started pumping the blood around the frozen body, and the dogs were brought back to life.
On 20 January 2006, doctors from the Massachusetts General Hospital in Boston announced they had placed pigs in suspended animation with a similar technique. The pigs were anaesthetized and major blood loss was induced, along with simulated - via scalpel - severe injuries (e.g. a punctured aorta as might happen in a car accident or shooting). After the pigs lost about half their blood the remaining blood was replaced with a chilled saline solution. As the body temperature reached 10 °C the damaged blood vessel was repaired and the blood was returned. The method was tested 200 times with a 90% success rate.
As of November 2014, the Cryonics Institute has 1301 members. At its Clinton Township, Michigan facility, about a fifth of the cryopreserved humans and a smaller portion of the pets came to the CI facility through contract with the American Cryonics Society (which has no storage facilities of its own). As of May 2014, Alcor maintains 124 cryonics patients and about 45 pets in Scottsdale, Arizona. Cryonics Institute and Alcor have support groups in Canada, Europe and Australia. There is also a smaller cryonics company in Russia called KrioRus, which maintains 20 human patients and 10 pets, and the new not-for-profit company Stasis Systems Australia plans to build the first facility in the southern hemisphere.
There are also plans being developed by renowned architect Stephen Valentine for a multi-acre futuristic high security facility called Timeship to be built in an undisclosed location in the United States, as well as for an underground facility in Switzerland. Trans Time, a small company, currently maintains 3 cryonics patients.
DARPA currently funds several research projects aimed on sending the human body into a state of suspended animation, essentially “shutting down” the heart and brain until proper care can be administered that can be regarded as a step to cryopreservation of humans.
In popular culture.
A survey in Germany found that about half of the respondents were familiar with cryonics, and about half of those familiar with cryonics had learned of the subject from television or film. Procedures similar to cryonics have been featured in innumerable science fiction stories as a means to transport a character from the past into the future, or sometimes to aid space travel (in particular interstellar travel). Often (especially in the former use case), in addition to accomplishing whatever the character's primary task is in the future, he or she must cope with the strangeness of a new world, which may contain only traces of their previous surroundings. This prospect of alienation is often cited as a major reason for the unpopularity of cryonics.
Literature.
Notable early science fiction short stories featuring human cryopreservation, deliberate or accidental, include Lydia Maria Child's short story "Hilda Silfverling, A Fantasy" (1845), Jack London's first published work "A Thousand Deaths" (1899), V. Mayakovsky's "Klop" (1928), H.P. Lovecraft's "Cool Air" (1928), and Edgar Rice Burroughs' "The Resurrection of Jimber-Jaw" (1937). The comic book super-hero Captain America, popular in the 1940s and discontinued in the 1950s, returned to publication in 1964 with the explanation that he had been accidentally frozen in Arctic ice. Many of the subjects in these stories are unwilling ones, although a 1931 short story by Neil R. Jones called "The Jameson Satellite", in which the subject has himself deliberately preserved in space after death, has been credited with giving Robert Ettinger the seed of the idea of cryonics, when he was a teenager. Ettinger would later write a science fiction story called "The Penultimate Trump", published in 1948, in which the explicit idea of cryopreservation of legally dead persons for future repair of medical causes of death is promulgated.
Relatively few stories have been published concerning the primary objective and definition of cryonics, which is medical time travel. The most in-depth novel based on contemporary cryonics is national best-seller "The First Immortal" by James L. Halperin (1998). Giles Milton's 2014 thriller, "The Perfect Corpse" is set in a fictional cryonics laboratory in Nevada; the narrative revolves around the resurrection of a perfectly frozen body discovered in the Greenland ice sheet.
Fictional application of cryonics as rescue after freezing in space has continued since "The Jameson Satellite" in 1931. Arthur C. Clarke's ' reveals that Frank Poole, murdered by HAL 9000 in ' was cryopreserved by his exposure to space, and found and revived a thousand years later.
During the fall of 2009, Larry Johnson released a book called, "FROZEN: My Journey into the World of Cryonics, Deception, and Death". This book has received heavy criticism from those working in the field of cryonics.
Film.
Movies featuring cryonics include "Forever Young" (1992), "Demolition Man" (1993), "Sexmission" (1984), and the Woody Allen comedy "Sleeper" (1973) and "Open Your Eyes" ("Abre los Ojos" 1997, remade as "Vanilla Sky", 2001). Cryopreservation is used during space travel in the James Cameron films "Aliens" (1986) and "Avatar" (2009). Cryosleep was also used in Christopher Nolan's "Interstellar (film)" 2014. "" (1980) involves the test freezing of Han Solo as proof of concept for suspension, which caused temporary blindness upon his successful revival. "Austin Powers" (1997) and its sequels (1999, 2002) use cryonics as a plot device to insert a 1960s spy character and arch villain into a world decades later in which their behavior and expectations are often jarringly out of place.
Television.
On television, cryonics has appeared occasionally since the 1960s. It was prominently featured in the opening episode of the space adventure series "Lost in Space" (1965), in which a family of space travelers was placed in suspended animation for a five and a half year interstellar journey to a planet of star Alpha Centauri. 
In the original series of "," cryonics was used as a plot device in the episode "Space Seed" (1967), in which 72 humans are found adrift in space in a state of suspended animation. Their leader, Khan Noonien Singh, is played by Ricardo Montalbán who reprised the role in the film "" (1982). Many elements of the "Space Seed" plot, including the cryogenic preservation of Kahn and his followers, were used in the updated "Star Trek Into Darkness" (2013) with Khan played by Benedict Cumberbatch. Producer David E. Kelley wrote well-researched portrayals of cryonics for the TV show "L.A. Law" (1990). "Picket Fences" (1994), and "Boston Legal"
(2005); In each case, there is a dying plaintiff petitioning a court for the right to elective cryopreservation. Cryonics also features as a plot element in the "Castle" episode "Head Case", where the episode's murder victim is recovered by a cryonics company before the team can discover the body, with the subsequent investigation being complicated by the legal battle to claim and analyse the body without jeopardising the client's potential for future reanimation. In the " episode " (1988), the 24th-century protagonists criticize cryonics despite its in-universe success, regarding it as "a fad" of primitive 20th-century people who were "afraid of death". In two separate comedy series, "Red Dwarf" (1988) and "Futurama" (1999), accidental long-term cryonic suspension is used as an initial plot device to permanently thrust a hapless contemporary protagonist into the far future. In 2010 a Spanish soap opera titled "Aurora" premiered in the television network Telemundo. The theme of this soap is cryonics and everything centers around it. It tells the story of Aurora Ponce De Leon a 20 year old who is frozen by her father after her death from a rare and mysterious disease. She comes back to life 20 years later and finds out how everything changed after her death. She has to adjust to life 20 years later; to being chronologically 40 years old but looking like her 20 year old self.
Video games.
Cryonics is used as a plot device in numerous video games, including games such as Halo that use cryonics as a means of preventing aging during lengthy interstellar travel.
Famous people.
The best known cryopreserved patient is baseball player Ted Williams. The urban legend suggesting Walt Disney was cryopreserved is false; he was cremated and interred at Forest Lawn Memorial Park Cemetery. Robert A. Heinlein, who wrote enthusiastically of the concept in "The Door into Summer", was cremated and had his ashes distributed over the Pacific Ocean. Timothy Leary was a long-time cryonics advocate, and signed up with a major cryonics provider. He changed his mind, however, shortly before his death, and so was not cryopreserved.
Hal Finney and L. Stephen Coles were cryopreserved in 2014.
Among cryopreserved are also James Bedford, Dick Clair, Thomas K. Donaldson, Fereidoun Esfandiary, Jerry Leaf, John-Henry Williams.

</doc>
