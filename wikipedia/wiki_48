<doc id="6761" url="http://en.wikipedia.org/wiki?curid=6761" title="Unitary patent">
Unitary patent

The European patent with unitary effect (EPUE), more commonly known as the unitary patent, is a proposed new type of European patent that would be valid in participating member states of the European Union. Unitary effect can be registered for a European patent upon grant, replacing validation of the European patent in the individual countries concerned. The unitary effect means a single renewal fee, a single ownership, a single object of property, a single court (the Unified Patent Court) and uniform protection - which means that revocation as well as infringement proceedings are to be decided for the unitary patent as a whole rather than for each country individually. Licensing is however to remain possible for part of the unitary territory.
Formal agreement on the two EU regulations, which made the unitary patent possible through enhanced cooperation at EU level, was reached between the European Council and European Parliament on 17 December 2012. The legality of the two regulations was however challenged by Spain and Italy, who filed in total four actions for annulment, two of which were rejected, and two of which are currently pending before the European Court of Justice. All EU member states, except Spain, Croatia and Italy, participate in the enhanced cooperation. Unitary effect of newly granted European patents can be requested, from the date the related Unified Patent Court Agreement enters into force for the first group of ratifiers, and will extend to those participating member states for which the UPC Agreement had entered into force upon the registration of unitary effect.
The negotiations which resulted in the unitary patent can be traced back to various initiatives dating to the 1970s. At different times, the project, or very similar projects, have been referred to as the "European Union patent" (the name used in the EU treaties, which serve as the legal basis for EU competency), "EU patent", "Community patent", "European Community Patent", "EC patent" and "COMPAT".
By not requiring translations into a language of each contracting state, and by requiring the payment of only a single renewal fee for the group of contracting states, the unitary patent aims to be cheaper than European patents. Instead, unitary patents will be accepted in English, French, or German with no further translation required after grant. Machine translations will be provided, but will be, in the words of the regulation, "for information purposes only and should not have any legal effect". The maintenance fees, with a single fee for the whole area, are also expected to be lower compared to renewal fees for the whole area but the fees have yet to be announced.
The proposed unitary patent will be a particular type of European patent, granted under the European Patent Convention. A European patent, once granted, becomes a "bundle of nationally enforceable patents", in the states which are designated by the applicant, and the unitary effect would effectively create a single enforceable region in a subgroup of those 38 states, which may coexist with nationally enforceable patents ("classical" patents) in the remaining states. "Classical", non-unitary European patents hold exclusively for single countries and require the filing of a translation in some contracting states, in accordance with EPC.
Background.
Legislative history.
In 2009, three draft documents were published regarding a community patent: a European patent in which the European Community was designated: 
Based on those documents, the European Council requested on 6 July 2009 an opinion from the Court of Justice of the European Union, regarding the compatibility of the envisioned Agreement with EU law: "‘Is the envisaged agreement creating a Unified Patent Litigation System (currently named European and Community Patents Court) compatible with the provisions of the Treaty establishing the European Community?’"
In December 2010, the use of the enhanced co-operation procedure, under which of the Treaty on the Functioning of the European Union provides that a group of member states of the European Union can choose to co-operate on a specific topic, was proposed by twelve Member States to set up a unitary patent applicable in all participating European Union Member States. The use of this procedure had only been used once in the past, for harmonising rules regarding the applicable law in divorce across several EU Member States.
In early 2011, the procedure leading to the enhanced co-operation was reported to be progressing. Twenty-five Member States had written to the European Commission requesting to participate, with Spain and Italy remaining outside, primarily on the basis of ongoing concerns over translation issues. On 15 February, the European Parliament approved the use of the enhanced co-operation procedure for unitary patent protection by a vote of 471 to 160. and on 10 March 2011 the Council gave their authorisation. Two days earlier, on 8 March 2011, the Court of Justice of the European Union had issued its opinion, stating that the draft Agreement creating the European and Community Patent Court would be incompatible with EU law. The same day, the Hungarian Presidency of the Council insisted that this opinion would not affect the enhanced co-operation procedure.
In November 2011, negotiations on the enhanced co-operation system were reportedly advancing rapidly—too fast, in some views. It was announced that implementation required an enabling European Regulation, and a Court agreement between the states that elect to take part. The European Parliament approved the continuation of negotiations in September. A draft of the agreement was issued on 11 November 2011 and was open to all member states of the European Union, but not to other European Patent Convention states. However, serious criticisms of the proposal remained mostly unresolved. A meeting of the Competitiveness Council on 5 December failed to agree on the final text. In particular, there was no agreement on where the Central Division of a Unified Patent Court should be located, "with London, Munich and Paris the candidate cities."
The Polish Presidency, acknowledged on 16 December 2011 the failure to reach an agreement "on the question of the location of the seat of the central division." The Danish Presidency therefore inherited the issue. According to the President of the European Commission in January 2012, the only question remaining to be settled was the location of the Central Division of the Court. However, evidence presented to the UK House of Commons European Scrutiny Committee in February suggested that the position was more complicated. At an EU summit at the end of January 2012, participants agreed to press on and finalise the system by June. On 26 April, Herman Van Rompuy, President of the European Council, wrote to members of the Council, saying "This important file has been discussed for many years and we are now very close to a final deal... This deal is needed now, because this is an issue of crucial importance for innovation and growth. I very much hope that the last outstanding issue will be sorted out at the May Competitiveness Council. If not, I will take it up at the June European Council." The Competitiveness Council met on 30 May and failed to reach agreement.
A compromise agreement on the seat(s) of the unified court was eventually reached at the June European Council (28–29 June 2012), splitting the central division according to technology between Paris (the main seat), London and Munich. However, on 2 July 2012, the European Parliament decided to postpone the vote following a move by the European Council to modify the arrangements previously approved by MEPs in negotiations with the European Council. The modification was considered controversial and included the deletion of three key articles (6–8) of the legislation, seeking to reduce the competence of the European Union Court of Justice in unitary patent litigation. On 9 July 2012, the Committee on Legal Affairs of the European Parliament debated the patent package following the decisions adopted by the General Council on 28–29 June 2012 in camera in the presence of MEP Bernhard Rapkay. A later press release by Rapkay quoted from a legal opinion submitted by the Legal Service of the European Parliament, which affirmed the concerns of MEPs to approve the decision of a recent EU summit to delete said articles as it "nullifies central aspects of a substantive patent protection". A Europe-wide uniform protection of intellectual property would thus not exist with the consequence that the requirements of the corresponding EU treaty would not be met and that the European Court of Justice could therefore invalidate the legislation. By the end of 2012 a new compromise was reached between the European Parliament and the European Council, including a limited role for the European Court of Justice. The Unified Court will apply national patent laws, which the Court agreement makes the same in each country. The legislation for the enhanced co-operation mechanism was approved by the European Parliament on 11 December 2012 and the regulations were signed by the European Council and European Parliament officials on 17 December 2012.
On 30 May 2011, Italy and Spain challenged the Council's authorisation of the use of enhanced co-operation to introduce the trilingual (English, French, German) system for the unitary patent, which they viewed as discriminatory to their languages, with the CJEU on the grounds that it did not comply with the EU treaties. In January 2013, Advocate General Yves Bot delivered his recommendation that the court reject the complaint. Suggestions by the Advocate General are advisory only, but are generally followed by the court. The case was dismissed by the court in April 2013, however Spain launched two new challenges with the EUCJ in March 2013 against the regulations implementing the unitary patent package which have yet to be resolved. The court hearing for both cases has been scheduled to take place on 1 July 2014. Advocate-General Yves Bot published his opinion on 18 November 2014, suggesting that both actions be dismissed (ECLI:EU:C:2014:2380 and ECLI:EU:C:2014:2381). The court handed down its decisions on 5 May 2015 as ECLI:EU:C:2015:298 and ECLI:EU:C:2015:299 fully dismissing the Spanish claims.
European patents.
European patents are granted in accordance with the provisions of the European Patent Convention, via a unified procedure before the European Patent Office. A single patent application, in one language, may be filed at the European Patent Office or at a national patent office of certain Contracting States. Patentable inventions, according to the EPC, are "any inventions, in all fields of technology, providing that they are new, involve an inventive step, and are susceptible of industrial application."
In contrast to the unified character of a European patent application, a granted European patent has, in effect, no unitary character, except for the centralized opposition procedure (which can be initiated within 9 months from grant, by somebody else than the patent proprietor), and the centralized limitation and revocation procedures (which can only be instituted by the patent proprietor). In other words, a European patent in one Contracting State, i.e. a "national" European patent, is effectively independent of the same European patent in each other Contracting State, except for the opposition, limitation and revocation procedures. The enforcement of a European patent is dealt with by national law. The abandonment, revocation or limitation of the European patent in one state does not affect the European patent in other states.
The EPC provides however the possibility for a group of member states to allow European patents to have a unitary character also after grant. Until now, only Liechtenstein and Switzerland have opted to create a unified protection area (see Unitary patent (Switzerland and Liechtenstein)).
Upon filing of a European patent application, all 38 Contracting States are automatically designated, unless, when filing the application, the applicant withdraws one or more designations. This may be important to avoid conflicts with national (non European patent) applications. Designations may also be withdrawn after filing, at any time before grant. Upon grant, a European patent has immediate effect in all designated States, but to remain effective, yearly renewal fees have to be paid, in each State, and in certain countries translation requirements have to be met.
Legal basis and implementation.
Three instruments were proposed for the implementation of the unitary patent:
The system is based on EU law as well as the European Patent Convention (EPC). EPC provides for establishing a common system for Parties to the EPC. Until now, only Liechtenstein and Switzerland have used this possibility to create a unified protection area (see Unitary patent (Switzerland and Liechtenstein)).
Regulations regarding the unitary patent.
The first two regulations were approved by the European Parliament on 11 December 2012, with a future appliance set for the 25 member states participating in the enhanced cooperation for a unitary patent (all current EU member states except Croatia, Italy and Spain). The instruments were adopted as regulations EU 1257/2012 and 1260/2012 on 17 December 2012, and entered into force in January 2013. Although formally the Regulations will apply to all 25 states from the moment the UPC Agreement enters into force for the first group of ratifiers, the unitary effect of newly granted unitary patents will only extend to those of the 25 participating states where the UPC Agreement has entered into force.
 EPC provides the legal basis for European patents with unitary effect within the European Patent Convention.
As of August 2014, none of the 3 non-participants in the Unitary Patent had requested the European Commission to participate. In July 2013 the Italian Senate endorsed joining the unitary patent regulations, and Enzo Moavero Milanesi, Italy's Minister of European Affairs, informed the Italian Chamber of Deputies's EU Policy Committee that the government was now in favour of signing up. Michael Bordo, the Chairman of the European Committee of the Chamber of Deputies, stated that he hoped the lower house would express its position on the EU patent by June 2014. In July 2014, the opposition Five Star Movement submitted a proposed resolution to the lower house which described the unitary patent as an opportunity to reduce the patent application costs for Italian business, and called for the government to simplify the country's patent laws. The Civic Choice party representing 4.3% of the seats in the chamber, reiterated their requirement in September 2014, for the Ministry of Development to publish an impact assessment for the unitary patent.
Role of the European Patent Office.
Some administrative tasks relating to the European patents with unitary effect will be performed by the European Patent Office. These tasks include the collection of renewal fees and registration of unitary effect upon grant, exclusive licenses and statements that licenses are available to any person. Decisions of the European Patent Office regarding the unitary patent are open to appeal to the Unified Patent Court, rather than to the EPO Boards of Appeal.
Translation requirements.
For a unitary patent ultimately no translation will be required, which significantly reduces the cost for protection in the whole area. However, during a transition period of maximum 12 years one translation needs to be provided, either into English if the application is in French or German, or into any EU official language if the application is in English. In addition, machine translations will be provided, which will be, in the words of the regulation, "for information purposes only and should not have any legal effect".
In several contracting states, for "national" European patents a translation has to be filed within a three-month time limit after the publication of grant in the European Patent Bulletin under EPC, otherwise the patent is considered never to have existed (void ab initio) in that state. For the 20 parties to the London Agreement, this requirement has already been abolished or reduced (e.g. by dispensing with the requirement if the patent is available in English, and/or only requiring translation of the claims). Translation requirements for the participating states in the enhanced cooperation for a unitary patent are shown below:
Unitary patent as an object of property.
Article 7 of Regulation 1257/2012 provides that, as an object of property, a European patent with unitary effect will be treated "in its entirety and in all participating Member States as a national patent of the participating Member State in which that patent has unitary effect and in which the applicant had her/his residence or principal place of business or, by default, had a place of business on the date of filing the application for the European patent." When the applicant had no domicile in a participating Member State, German law will apply. These rules have been criticized as being "in conflict with both the purpose of the creation of unitary patent protection and with primary EU law."
Implementation of the regulations at the EPO.
In January 2013, after the two regulations about the unitary patent had entered into force, but before the regulations applied, the participating member states in the unitary patent established (as member states of the European Patent Convention) a Select Committee of the Administrative Council of the European Patent Organisation in order to prepare the work for implementation of the provisions. The committee held its inaugural meeting on 20 March 2013. The work of the Select Committee has to proceed in parallel to the work of the Preparatory Committee for the creation of the Unified Patent Court. Implementation of the Unitary Patent - including the legal, administrative and financial measures - shall be completed in due time before the entry into operation of the Unified Patent Court. In May 2014 an updated roadmap was published, indicating a completion of the work of the Select Committee during the first semester 2015.
Agreement on a Unified Patent Court.
The Agreement on a Unified Patent Court provides the legal basis for the Unified Patent Court: a patent court for European patents (with and without unitary effect), with jurisdiction in those countries where the Agreement is in effect. In addition to regulations regarding the court structure, it also contains substantive provisions relating to the right to prevent use of an invention and allowed use by non patent proprietors (e.g. for private non-commercial use), preliminary and permanent injunctions.
Parties.
The Agreement was signed on 19 February 2013 by 24 EU member states, including all states participating in the enhanced co-operation measures except Bulgaria and Poland, while Italy, which did not join the enhanced co-operation measures, did sign the UPC agreement. The agreement remains open to accession for all remaining EU member states, and Bulgaria signed the agreement on 5 March. Meanwhile, Poland decided to wait to see how the new patent system works before joining due to concerns that it would harm their economy. While Italy is not currently participating in the unitary patent regulations, signing the UPC agreement will allow the new court to handle European patents validated in Italy. Entry into force for the UPC will take place after 13 states (including Germany, France and the United Kingdom as the three states with the most patents in force) have ratified the Agreement. As of July 2014, the agreement has been ratified by 5 states (including 1 of the required ratifiers: France).
Jurisdiction.
The Unified Patent Court will have exclusive jurisdiction in infringement and revocation proceedings involving European patents with unitary effect, and during a transition period non-exclusive jurisdiction (that the patent holder can be opt out from) regarding European patents without unitary effect in the states where the Agreement applies. It furthermore has jurisdiction to hear cases against decisions of the European Patent Office regarding unitary patents. As a court of several member states of the European Union it may (Court of First Instance) or must (Court of Appeal) ask prejudicial questions to the European Court of Justice when the interpretation of EU law (including the two unitary patent regulations, but excluding the UPC Agreement) is not obvious.
Organization.
The court would have two divisions: a court of first instance and a court of appeal. The court of appeal and the registry would have their seats in Luxembourg, while the central division of the court of first instance would have its seat in Paris. The central division would have thematic branches in London and Munich. The court of first instance may further have local and regional divisions in all member states that wish to set up such divisions.
Geographical scope of and request for unitary effect.
While the regulations formally apply to all 25 member states participating in the enhanced cooperation for a unitary patent, from the date the UPC agreement has entered into force for the first group of ratifiers, unitary patents will only extend to the territory of those participating member states where the UPC Agreement had entered into force when the unitary effect was registered. If the unitary effect territory subsequently expands to additional participating member states for which the UPC Agreement later enters into force, this will be reflected for all subsequently registered unitary patents, but the territorial scope of the unitary effect of existing unitary patents will not be extended to these states.
Unitary effect can be requested up to one month after grant of the European patent, with retroactive effect from the date of grant. However, according to the "Draft Rules Relating to Unitary Patent Protection", unitary effect would be registered only if the European patent has been granted with the same set of claims for all the 25 participating member states in the regulations, whether the unitary effect applies to them or not. European patents automatically become a bundle of "national" European patents upon grant. Upon the grant of unitary effect, the "national" European patents will retroactively be considered to never have existed in the territories where the unitary patent has effect. The unitary effect does not affect "national" European patents in states where the unitary patent does not apply. Any "national" European patents applying outside the "unitary effect" zone will co-exist with the unitary patent.
Special territories of participating member states.
As the unitary patent is introduced by an EU regulation, it is expected to not only be valid in the mainland territory of the participating member states that are party to the UPC, but also in those of their special territories that are part of the European Union. As of April 2014, this includes the following fourteen territories:
In addition to the territories above, the European Patent Convention has been extended by three member states participating in the enhanced cooperation for a unitary patent to cover some of their dependent territories outside the European Union: 
Among the dependencies in the second list, the Isle of Man intends to apply the unitary patent.
Costs.
Translation requirements as well as the requirement to pay yearly patent fees in all countries in which a European patent is designated, presently renders the European patent system costly in the European Union. In an impact assessment the European Commission estimated that the costs of obtaining a patent in all 27 EU countries would drop from over 32 000 euro (mainly due to translation costs) to 6 500 euro (for the combination of an EU, Spanish and Italian patent) due to introduction of the EU patent. Per capita costs of an EU patent were estimated at just 6 euro/million in the participating 25 countries (and 12 euro/million in the 27 EU countries for protection with an EU, Italian and Spanish patent).
How the EU Commission has presented the expected cost savings has however been sharply criticized as exaggerated and based on unrealistic assumptions. The EU Commission has notably considered the costs for validating a European patent in 27 countries while in reality only about 1% of all granted European patents are currently validated in all 27 contracting states. Based on more realistic assumptions, the cost savings are expected to be much lower than actually claimed by the Commission.
Earlier attempts.
1970s and 1980s: proposed Community Patent Convention.
Work on a Community patent started in the 1970s, but the resulting Community Patent Convention (CPC) was a failure.
The "Luxembourg Conference on the Community Patent" took place in 1975 and the Convention for the European Patent for the common market, or (Luxembourg) Community Patent Convention (CPC), was signed at Luxembourg on 15 December 1975, by the 9 member states of the European Economic Community at that time. However, the CPC never entered into force. It was not ratified by enough countries.
Fourteen years later, the Agreement relating to Community patents was made at Luxembourg on 15 December 1989. It attempted to revive the CPC project, but also failed. This Agreement consisted of an amended version of the original Community Patent Convention. Twelve states signed the Agreement: Belgium, Denmark, France, Germany, Greece, Ireland, Italy, Luxembourg, the Netherlands, Portugal, Spain, and United Kingdom. All of those states would need to have ratified the Agreement to cause it to enter into force, but only seven did so: Denmark, France, Germany, Greece, Luxembourg, the Netherlands, and United Kingdom.
Nevertheless, a majority of member states of the EEC at that time introduced some harmonisation into their national patent laws in anticipation of the entry in force of the CPC. A more substantive harmonisation took place at around the same time to take account of the European Patent Convention and the Strasbourg Convention.
2000 to 2004: EU Regulation proposal.
In 2000, renewed efforts from the European Union resulted in a Community Patent Regulation proposal, sometimes abbreviated as CPR. It provides that the patent, once it has been granted by the European Patent Office (EPO) in one of its procedural languages (English, German or French) and published in that language, with a translation of the claims into the two other procedural languages, will be valid without any further translation. This proposal is aimed to achieve a considerable reduction in translation costs.
Nevertheless, additional translations could become necessary in legal proceedings against a suspected infringer. In such a situation, a suspected infringer who has been unable to consult the text of the patent in the official language of the Member State in which he is domiciled, is presumed, until proven otherwise, not to have knowingly infringed the patent. To protect a suspected infringer who, in such a situation, has not acted in a deliberate manner, it is provided that the proprietor of the patent will not be able to obtain damages in respect of the period prior to the translation of the patent being notified to the infringer.
The proposed Community Patent Regulation should also establish a court holding exclusive jurisdiction to invalidate issued patents; thus, a Community Patent's validity will be the same in all EU member states. This court will be attached to the present European Court of Justice and Court of First Instance through use of provisions in the Treaty of Nice.
Discussion regarding the Community patent had made clear progress in 2003 when a political agreement was reached on 3 March 2003. However, one year later in March 2004 under the Irish presidency, the Competitiveness Council failed to agree on the details of the Regulation. In particular the time delays for translating the claims and the authentic text of the claims in case of an infringement remained problematic issues throughout discussions and in the end proved insoluble.
In view of the difficulties in reaching an agreement on the community patent, other legal agreements have been proposed outside the European Union legal framework to reduce the cost of translation (of patents when granted) and litigation, namely the London Agreement, which entered into force on 1 May 2008—and which has reduced the number of countries requiring translation of European patents granted nowadays under the European Patent Convention, and the corresponding costs to obtain a European patent— and the European Patent Litigation Agreement (EPLA), a proposal that has now lapsed.
Reactions to the failure.
After the council in March 2004, EU Commissioner Frits Bolkestein said that "The failure to agree on the Community Patent I am afraid undermines the credibility of the whole enterprise to make Europe the most competitive economy in the world by 2010." Adding:
It is a mystery to me how Ministers at the so-called 'Competitiveness Council' can keep a straight face when they adopt conclusions for the Spring European Council on making Europe more competitive and yet in the next breath backtrack on the political agreement already reached on the main principles of the Community Patent in March of last year. I can only hope that one day the vested, protectionist interests that stand in the way of agreement on this vital measure will be sidelined by the over-riding importance and interests of European manufacturing industry and Europe's competitiveness. That day has not yet come.
Jonathan Todd, Commission's Internal Market spokesman, declared:
Normally, after the common political approach, the text of the regulation is agreed very quickly. Instead, some Member States appear to have changed their positions. (...) It is extremely unfortunate that European industry's competitiveness, innovation and R&D are being sacrificed for the sake of preserving narrow vested interests.
European Commission President Romano Prodi, asked to evaluate his five-year term, cites as his weak point the failure of many EU governments to implement the "Lisbon Agenda", agreed in 2001. In particular, he cited the failure to agree on a Europewide patent, or even the languages to be used for such a patent, "because member states did not accept a change in the rules; they were not coherent".
Support for the Regulation.
There is support for the Community patent from various quarters. From the point of view of the European Commission the Community Patent is an essential step towards creating a level playing field for trade within the European Union. For smaller businesses, if the Community patent achieves its aim of providing a relatively inexpensive way of obtaining patent protection across a wide trading area, then there is also support. 
For larger businesses, however, other issues come into play, which have tended to dilute overall support. In general, these businesses recognise that the current European Patent system provides the best possible protection given the need to satisfy national sovereignty requirements such as regarding translation and enforcement. The Community Patent proposal was generally supported if it would do away with both of these issues, but there was some concern about the level of competence of the proposed European Patent Court. A business would be reluctant to obtain a Europe-wide patent if it ran the risk of being revoked by an inexperienced judge. Also, the question of translations would not go away – unless the users of the system could see significant change in the position of some of the countries holding out for more of a patent specification to be translated on grant or before enforcement, it was understood that larger businesses (the bulk of the users of the patent system) would be unlikely to move away from the tried and tested European Patent.
Since 2005: stalemate and new debate.
Thus, in 2005, the Community patent looked unlikely to be implemented in the near future. However, on 16 January 2006 the European Commission "launched a public consultation on how future action in patent policy to create an EU-wide system of protection can best take account of stakeholders' needs." The Community patent was one of the issues the consultation focused on. More than 2500 replies were received. According to the European Commission, the consultation showed that there is widespread support for the Community patent but not at any cost, and "in particular not on the basis of the Common Political Approach reached by EU Ministers in 2003".
In February 2007, EU Commissioner Charlie McCreevy was quoted as saying:
The proposal for an EU-wide patent is stuck in the mud. It is clear to me from discussions with member states that there is no consensus at present on how to improve the situation.
The European Commission released a white paper in April 2007 seeking to "improve the patent system in Europe and revitalise the debate on this issue." On 18 April 2007, at the European Patent Forum in Munich, Germany, Günter Verheugen, Vice-President of the European Commission, said that his proposal to support the European economy was "to have the London Agreement ratified by all member states, and to have a European patent judiciary set up, in order to achieve rapid implementation of the Community patent, which is indispensable". He further said that he believed this could be done within five years.
In October 2007, the Portuguese presidency of the Council of the European Union proposed an EU patent jurisdiction, "borrowing heavily from the rejected draft European Patent Litigation Agreement (EPLA)". In November 2007, EU ministers were reported to have made some progress towards a community patent legal system, with "some specific results" expected in 2008.
In 2008, the idea of using machine translations to translate patents was proposed to solve the language issue, which is partially responsible for blocking progress on the community patent. Meanwhile, European Commissioner for Enterprise and Industry Günter Verheugen declared at the European Patent Forum in May 2008 that there was an "urgent need" for a community patent.
Agreement in December 2009, and language issue.
In December 2009, it was reported that the Swedish EU presidency had achieved a breakthrough in negotiations concerning the community patent. The breakthrough was reported to involve setting up a single patent court for the EU, however ministers conceded much work remained to be done before the community patent would become a reality.
According to the agreed plan, the EU would accede to the European Patent Convention as a contracting state, and patents granted by the European Patent Office will, when validated for the EU, have unitary effect in the territory of the European Union. On 10 November 2010, it was announced that no agreement had been reached and that, "in spite of the progress made, [the Competitiveness Council of the European Union had] fallen short of unanimity by a small margin," with commentators reporting that the Spanish representative, citing the aim to avoid any discrimination, had "re-iterated at length the stubborn rejection of the Madrid Government of taking the 'Munich' three languages regime (English, German, French) of the European Patent Convention (EPC) as a basis for a future EU Patent."

</doc>
<doc id="6762" url="http://en.wikipedia.org/wiki?curid=6762" title="Companies law">
Companies law

Companies law (or the law of business associations) is the field of law concerning companies and other business organizations. This includes corporations, partnerships and other associations which usually carry on some form of economic or charitable activity. The most prominent kind of company, usually referred to as a "corporation", is a "juristic person", i.e. it has separate legal personality, and those who invest money into the business have limited liability for any losses the company makes, governed by corporate law. The largest companies are usually publicly listed on stock exchanges around the world. Even single individuals, also known as sole traders may incorporate themselves and limit their liability in order to carry on a business. All different forms of companies depend on the particular law of the particular country in which they reside.
The law of business organizations originally derived from the common law of England, but has evolved significantly in the 20th century. In common law countries today, the most commonly addressed forms are:
The proprietary limited company is a statutory business form in several countries, including Australia.
Many countries have forms of business entity unique to that country, although there are equivalents elsewhere. Examples are the limited liability company (LLC) and the limited liability limited partnership (LLLP) in the United States.
Other types of business organizations, such as cooperatives, credit unions and publicly owned enterprises, can be established with purposes that parallel, supersede, or even replace the profit maximization mandate of business corporations.
For a country-by-country listing of officially recognized forms of business organization, see Types of business entity.
There are various types of company that can be formed in different jurisdictions, but the most common forms of company are:
There are, however, many specific categories of corporations and other business organizations which may be formed in various countries and jurisdictions throughout the world.
US companies law.
In the United States, corporations are generally incorporated, or organized, under the laws of a particular state. The corporate law of a corporation's state of incorporation generally governs that corporation's internal governance (even if the corporation's operations take place outside of that state). The corporate laws of the various states differ - in some cases significantly - from state to state. Because of these differences, corporate lawyers are often consulted in an effort to determine the most appropriate or advantageous state in which to incorporate, and a majority of public companies in the U.S. are Delaware corporations. The federal laws of the United States and local law may also be applicable sources of corporate law.
Companies law theory.
“A corporation is described to be a person in a political capacity created by the law, to endure in perpetual succession.” Americans in the 1790s knew of a variety of corporations established for various purposes, including those of commerce, education, and religion. As the law of corporations was articulated by the Supreme Court under Chief Justice Marshall, over the first several decades of the new American state, emphasis fell, in a way which seems natural to us today, upon commercial corporations. Nonetheless, Wilson believed that, in all cases, corporations “should be erected with caution, and inspected with care.” The actions of corporations were clearly circumscribed: “To every corporation a name must be assigned; and by that name alone it can perform legal acts.” For non-binding external actions or transactions, corporations enjoyed the same latitude as private individuals; but it was with an eye to internal affairs that many saw principal advantage in incorporation. The power of making by-laws was “tacitly annexed to corporations by the very act of their establishment.” While they must not directly contradict the overarching laws of the land, the central or local government cannot be expected to regulate toward the peculiar circumstances of a given body, and so “they are invested with authority to make regulations for the management of their own interests and affairs.”
The question then arises: if corporations are to be inspected with care, what - if not the commercial or social conduct, or the by-laws - is to be inspected – and by whom? Do corporations have duties? Yes: “The general duties of every corporation may be collected from the nature and design of its institution: it should act agreeably to its nature, and fulfill the purposes for which it was formed.” Who sees that corporations are living up to those duties? “The law has provided proper persons with proper powers to visit those institutions, and to correct every irregularity, which may arise within them.” The Common Law provided for inspection by the court of king’s bench. In 1790, at least, “the powers of the court of king's bench [were] vested in the supreme court of Pennsylvania.” As for the dissolution of corporations, there seems not to have been much question that a corporation might “surrender its legal existence into the hands of that power, from which it was received. From such a surrender, the dissolution of the body corporate ensues.” Nor does there seem to have been much question that by “a judgment of forfeiture against a corporation itself, it may be dissolved.” However, Supreme Court Justice Wilson, lecturing in his unofficial capacity, at least, suggests his displeasure with the doctrine that corporate dissolution cannot be predicated “by a judgment of ouster against individuals. God forbid ― such is the sentiment of Mr. Justice Wilmot ― that the rights of the body should be lost or destroyed by the offenses of the members.”
As theorists such as Ronald Coase have pointed out, all business organizations represent an attempt to avoid certain costs associated with doing business. Each is meant to facilitate the contribution of specific resources - investment capital, knowledge, relationships, and so forth - towards a venture which will prove profitable to all contributors. Except for the partnership, all business forms are designed to provide limited liability to both members of the organization and external investors. Business organizations originated with agency law, which permits an agent to act on behalf of a principal, in exchange for the principal assuming equal liability for the wrongful acts committed by the agent. For this reason, all partners in a typical general partnership may be held liable for the wrongs committed by one partner. Those forms that provide limited liability are able to do so because the state provides a mechanism by which businesses that follow certain guidelines will be able to escape the full liability imposed under agency law. The state provides these forms because it has an interest in the strength of the companies that provide jobs and services therein, but also has an interest in monitoring and regulating their behavior.
Companies law study.
Law schools typically offer either a single upper level course on business organizations, or offer several courses covering different aspects of this area of law. The area of study examines issues such as how each major form of business entity may be formed, operated, and dissolved; the degree to which limited liability protects investors; the extent to which a business can be held liable for the acts of an agent of the business; the relative advantages and disadvantages of different types of business organizations, and the structures established by governments to monitor the buying and selling of ownership interests in large corporations.
The basic theory behind all business organizations is that, by combining certain functions within a single entity, a business (usually called a firm by economists) can operate more efficiently, and thereby realize a greater profit. Governments seek to facilitate investment in profitable operations by creating rules that protect investors in a business from being held personally liable for debts incurred by that business, either through mismanagement, or because of wrongful acts committed by the business.

</doc>
<doc id="6763" url="http://en.wikipedia.org/wiki?curid=6763" title="Cistron">
Cistron

A cistron is a gene. The term cistron is used to emphasize that genes exhibit a specific behavior in a cis-trans test; distinct positions (or loci) within a genome are cistronic.
For example, suppose a mutation at a chromosome position formula_1 is responsible for a recessive trait in a diploid organism (where chromosomes come in pairs). We say that the mutation is recessive because the organism will exhibit the wild type phenotype (ordinary trait) unless both chromosomes of a pair have the mutation (homozygous mutation). Similarly, suppose a mutation at another position, formula_2, is responsible for the same recessive trait. The positions formula_1 and formula_2 are said to be within the same cistron when an organism that has the mutation at formula_1 on one chromosome and has the mutation at position formula_2 on the paired chromosome exhibits the recessive trait even though the organism is not homozygous for either mutation. When instead the wild type trait is expressed, the positions are said to belong to distinct cistrons / genes.
For example, an operon is a stretch of DNA that is transcribed to create a contiguous segment of RNA, but contains more than one cistron / gene. The operon is said to be polycistronic, whereas ordinary genes are said to be monocistronic.

</doc>
<doc id="6766" url="http://en.wikipedia.org/wiki?curid=6766" title="Commonwealth">
Commonwealth

Commonwealth is a traditional English term for a political community founded for the common good. Historically it has sometimes been synonymous with "republicanism".
More recently it has been used for fraternal associations of some sovereign nations. Most notably the Commonwealth of Nations, an association primarily of former members of the British Empire, is often referred to as simply "the Commonwealth".
The English noun "commonwealth" in the sense meaning "public welfare; general good or advantage" dates from the 15th century. The original phrase "the common-wealth" or "the common weal" (echoed in the modern synonym "public weal") comes from the old meaning of "wealth", which is "well-being", and is itself a loose translation of the Latin res publica (republic). The term literally meant "common well-being". In the 17th century the definition of "commonwealth" expanded from its original sense of "public welfare" or "commonweal" to mean "a state in which the supreme power is vested in the people; a republic or democratic state".
Several countries such as Australia or the Bahamas have the official title "Commonwealth", as do four US states and two US territories.
Iceland.
The Icelandic Commonwealth or the Icelandic Free State (Icelandic: Þjóðveldið) was the state existing in Iceland between the establishment of the Althing in 930 and the pledge of fealty to the Norwegian king in 1262. It was initially established by a public consisting largely of recent immigrants from Norway who had fled the unification of that country under King Harald Fairhair.
Polish–Lithuanian Commonwealth.
"Republic" is still an alternative translation of the traditional name of the Polish–Lithuanian Commonwealth. Wincenty Kadłubek (Vincent Kadlubo, 1160–1223) used for the first time the original Latin term "res publica" in the context of Poland in his "Chronicles of the Kings and Princes of Poland". The name was used officially for the confederal country formed by Poland and Lithuania 1569–1795.
It is also often referred as "Nobles' Commonwealth" (1505–1795, i.e., before the union). In contemporary political doctrine of Polish–Lithuanian Commonwealth, "our state is a Republic (Commonwealth) under presidency of the King". The commonwealth introduced a doctrine of religious tolerance called Warsaw Confederation, had its own parliament "Sejm" (although elections were restricted to the nobility and elected kings, who were bound to certain contracts "Pacta conventa" from the beginning of the reign).
"A commonwealth of good counsaile" was the title of the 1607 English translation of the work of Wawrzyniec Grzymała Goślicki "De optimo senatore" that presented to English readers many of the ideas present in the political system of the Polish–Lithuanian Commonwealth.
England.
The Commonwealth of England was the official name of the political unit ("de facto" military rule in the name of parliamentary supremacy) that replaced the Kingdom of England (after the English Civil War) from 1649–53 and 1659–60, under the rule of Oliver Cromwell and his son and successor Richard. From 1653 to 1659, although still legally known as a Commonwealth, the republic, united with the former Kingdom of Scotland, operated under different institutions (at times as a "de facto" monarchy) and is known by historians as the Protectorate. The Commonwealth of England formed the first republic in the English-speaking world. In an English context, it is sometimes referred to as the "Old Commonwealth".
National.
Australia.
The term also served when six Australian colonies federated to form the Commonwealth of Australia in 1901. The Commonwealth of Australia Constitution Act created a federal system, in which power is divided between the federal, or national, government and the States—the evolved status of the colonies. The Constitution stipulated that Australia was a constitutional monarchy, where the Head of State is the British (or, since 1942, Australian) monarch, who is represented at the federal level by a Governor-General, and at the state level by six Governors, one for each state. The Parliament of Australia was derived from the British and American systems to form a uniquely Australian system. It is largely based on the British Westminster System, adopting many of its practices and precedents, but with a similar structure—House of Representatives, and Senate—to the U.S. Congress. In an Australian context, the term "Commonwealth" (capitalised) thus refers to the federal government and "Commonwealth of Australia" is the official name of the country.
Dominica.
The small Caribbean republic of Dominica has used the official style "Commonwealth of Dominica" since 1970.
The Bahamas.
The Bahamas uses the official style "Commonwealth of The Bahamas".
United States.
The term "commonwealth" has one of two political meanings within the United States:
U.S. states.
Four states in the United States officially designate themselves as "commonwealths". All four were original colonies or parts thereof (Kentucky was originally a part of the land grant of the Colony of Virginia) and share a strong influence of colonial common law in some of their laws and institutions. The four are:
U.S. insular areas.
"Commonwealth" is also used in the United States to describe the political relationship between the United States and the overseas unincorporated territories:
United Kingdom of Great Britain and Northern Ireland.
Commonwealth of Britain Bill.
Labour MP Tony Benn sponsored a Commonwealth of Britain Bill several times between 1991 and 2001, intended to abolish the monarchy and establish a British republic. It never reached second reading.
International.
Commonwealth of Nations.
The Commonwealth of Nations—formerly the British Commonwealth—is a voluntary association of 53 independent sovereign states, most of which were once part of the British Empire. The Commonwealth's membership includes both republics and monarchies. The head of the Commonwealth of Nations is Queen Elizabeth II, who reigns as monarch directly in 16 member states known as Commonwealth realms.
Commonwealth of Independent States.
The Commonwealth of Independent States (CIS) is a loose alliance or confederation consisting of 10 of the 15 former Soviet Republics, the exceptions being Turkmenistan (a CIS associate member), Lithuania, Latvia, Estonia, and Georgia. Georgia left the CIS in August 2008 after a clash with Russia over South Ossetia. Its creation signaled the dissolution of the Soviet Union, its purpose being to "allow a civilized divorce" between the Soviet Republics. The CIS has developed as a forum by which the member-states can co-operate in economics, defense, and foreign policy.

</doc>
<doc id="6767" url="http://en.wikipedia.org/wiki?curid=6767" title="Commodore 1541">
Commodore 1541

The Commodore 1541 (also known as the CBM 1541 and VIC-1541) is a floppy disk drive (FDD) which was made by Commodore International for the Commodore 64 (C64), Commodore's most popular home computer. The best-known FDD for the C64, the 1541 is a single-sided 170-kilobyte drive for 5¼" disks. The 1541 directly followed the Commodore 1540 (meant for the VIC-20).
The disk drive uses group code recording (GCR) and contains a MOS Technology 6502 microprocessor, doubling as a disk controller and on-board disk operating system processor. The number of sectors per track varies from 17 to 21 (an early implementation of zone bit recording). The drive's built-in disk operating system is CBM DOS 2.6.
History.
Introduction.
The 1541 was priced at under US$ at its introduction. A C64 plus a 1541 cost about $900, while an Apple II with no disk drive cost $1295. The first 1541 drives produced in 1982 have a label on the front reading VIC-1541 and have an off-white case to match the VIC-20. In 1983, the 1541 was switched to having the familiar beige case and a front label reading simply "1541" along with rainbow stripes to match the Commodore 64.
By 1983, after a brutal home-computer price war that Commodore began, the C64 and 1541 cost under $500. The drive became very popular, and became difficult to find. The company claimed that the shortage occurred because 90% of C64 owners bought the 1541 compared to its 30% expectation, but the press reported what "Creative Computing" described as "an absolutely alarming return rate" because of defects. The magazine reported in March 1984 that it received three defective drives in two weeks, and the lead editorial in the December 1983 issue of "Compute!'s Gazette" reported that four of the seven drives the magazine had in its editorial offices had failed. 
The early (1982–83) 1541s have a spring-eject mechanism (Alps drive), and the disks often fail to release. This style of drive has the popular nickname "Toaster Drive", because it requires the use of a knife or other hard thin object to pry out the stuck media just like a piece of toast stuck in an actual toaster (though this is inadvisable with actual toasters). This was fixed later when Commodore changed the vendor of the drive mechanism (Mitsumi) and adopted the flip-lever Newtronics mechanism, greatly improving reliability. In addition, Commodore made the drive's controller board smaller and reduced its chip count compared to the early 1541s (which had a large PCB running the length of the case, with dozens of TTL chips). The beige-case Newtronics 1541 was produced from 1984-86.
Versions and third-party clones.
All but the very earliest non-II model 1541s can use either the Alps or Newtronics mechanism. Visually, the first models, of the "-VIC-1541" denomination, have an off-white color like the VIC-20 and VIC-1540. Then, to match the look of the C64, CBM changed the drive's color to brown-beige and the name to "Commodore 1541".
The 1541's numerous shortcomings opened a market for a number of third-party clones of the disk drive, a situation that continued for the lifetime of the C64. Well-known clones are the "Oceanic OC-118" a.k.a. "Excelerator+", the MSD Super Disk single and dual drives, the "Enhancer 2000", the "Indus GT", and "CMD" 's "FD-2000" and "FD-4000". Nevertheless, the 1541 became the first disk drive to see widespread use in the home and Commodore sold millions of the units.
In 1986, Commodore released the 1541C, a revised version that offered quieter and slightly more reliable operation and a light beige case matching the color scheme of the Commodore 64C. It was replaced in 1988 by the 1541-II, which uses an external power supply to provide cooler operation and allows the drive to have a smaller desktop footprint (the power supply "brick" being placed elsewhere, typically on the floor). Later ROM revisions fixed assorted problems, including a software bug that made the save-and-replace command unusable.
Successors.
The Commodore 1570 is an upgrade from the 1541 for use with the Commodore 128, available in Europe. It offers MFM capability for accessing CP/M disks, improved speed, and somewhat quieter operation, but was only manufactured until Commodore got its production lines going with the double-sided 1571. Finally, the small, external-power-supply-based, MFM-based Commodore 1581 3½" drive was made, giving 800 KB access to the C128 and C64. By this time, however, many CBM users had shifted their attention to the 16/32-bit Amiga, and the 1581 was mostly sold to remaining GEOS users.
Design.
Hardware.
The 1541 does not have dip switches to change the device number. If a user added more than one drive to a system the user had to open the case and cut a trace in the circuit board to permanently change the drive's device number, or hand-wire an external switch to allow it to be changed externally. It was also possible to change the number temporarily from the operating system.
The pre-II 1541s also have an internal power source, which generate much heat. The heat generation was a frequent source of humour. For example, "Compute!" stated in 1988 that "Commodore 64s used to be a favorite with amateur and professional chefs since they could compute and cook on top of their 1500-series disk drives at the same time". A series of humorous tips in "MikroBitti" in 1989 said "When programming late, coffee and kebab keep nicely warm on top of the 1541." The "MikroBitti" review of the 1541-II said that its external power source "should end the jokes about toasters".
The drive-head mechanism is notoriously easy to misalign. The most common cause of the 1541's drive head knocking and subsequent misalignment is copy-protection schemes on commercial software. The main cause of the problem is that the disk drive itself does not feature any means of detecting when the read/write head reaches track zero. Accordingly, when a disk is formatted or a disk error occurs, the unit tries to move the head 40 times in the direction of track zero (although the 1541 DOS only uses 35 tracks, the drive mechanism itself is a 40-track unit, so this ensured track zero would be reached no matter where the head was before). Once track zero is reached, every further attempt to move the head in that direction would cause it to be rammed against a solid stop: for example, if the head happened to be on track 18 (where the directory is located) before this procedure, the head would be actually moved 18 times, and then rammed against the stop 22 times. This ramming gives the characteristic "machine gun" noise and sooner or later throws the head out of alignment.
The earlier 1541s are so unreliable that "Info" magazine joked, "Sometimes it seems as if one of the original design specs ... must have said 'Mean time between failure: 10 accesses.'". Users can realign the drive themselves with a software program and a calibration disk. What the user would do is remove the drive from its case and then loosen the screws holding the stepper motor that moved the head, then with the calibration disk in the drive gently turn the stepper motor back and forth until the program shows a good alignment. The screws are then tightened and the drive is put back into its case.
A third-party fix for the 1541 appeared where the solid head stop was replaced by a sprung stop, giving the head a much easier life. The later 1571 drive (which is 1541-compatible) incorporates track-zero detection by photo-interrupter and is thus immune to the problem. Also, a software solution, which resides in the drive controller's ROM, prevents the rereads from occurring, though this could cause problems when genuine errors did occur.
Interface.
The 1541 uses a proprietary bit-serial derivative of the standardized IEEE-488 parallel interface, which is used on Commodore's earlier drives for the PET/CBM range of personal/business computers. To ensure a ready supply of inexpensive cabling for its home computer peripherals, Commodore chose standard DIN connectors for the serial interface. Disk drives and other peripherals such as printers are connected to the computer via a daisy chain scheme, necessitating only a single connector on the computer itself.
Throughput and software.
Initially, Commodore intended to use a hardware shift register (one component of the 6522 VIA) to maintain relatively brisk drive speeds with the new serial interface. However, a hardware bug with this chip prevented the initial design from working as anticipated, and the ROM code was hastily rewritten to handle the entire operation in software. According to Jim Butterfield, this causes a speed reduction by a factor of five.
As implemented on the VIC-20 and Commodore 64, CBM DOS transfers only about 300 bytes per second - compare the 300-baud data rate of the Commodore cassette storage system - which translates to about 20 minutes to copy one disk—10 minutes of reading time, and 10 minutes of writing time. However, since both the computer and the drive can easily be reprogrammed, third parties quickly wrote more efficient firmware that would speed up drive operations drastically. Without hardware modifications, some "fast loader" utilities managed to achieve speeds of up to 4 kB/s. The most common of these products are the Epyx FastLoad, the Final Cartridge, and the Action Replay plug-in ROM cartridges, which all have machine code monitor and disk editor software on board as well. The popular Commodore computer magazines of the era also entered the arena with type-in fast-load utilities, with "Compute!'s Gazette" publishing "TurboDisk" in 1985 and "RUN" publishing "Sizzle" in 1987.
Even though each 1541 has its own on-board disk controller and disk operating system, it is not normally possible for a user to command two 1541 drives to copy a disk (one drive reading and the other writing) as with older dual drives like the 4040 and 8050 that were often found with the PET computer, and which the 1541 is backward-compatible with (it can read 4040 disks but not write to them since its internal operating system is similar enough for reading but not for writing). Unfortunately, however, the routines in the previous disk operating system to enable disk copying were removed for the 1541 as it was intended to be a stand-alone unit. Originally, to copy from drive to drive, software running on the C64 was needed and it would first read from one drive into computer memory, then write out to the other. Only later when first, Fast Hack'em, then other disk backup programs, were released, was true drive-to-drive copying possible for a pair of 1541s. The user could then unplug the C64 from the drives (i.e. from the first drive in the daisy chain) and do something else with the computer as the drives proceeded to copy the entire disk. This is not a recommended practice as disconnecting the serial lead from a powered drive and/or computer can result in destruction of one or both of the port chips in the disk drive.
Media.
Each side of 170 kB is split into 683 sectors on 35 tracks, each of the sectors holding 256 bytes; the file system made each sector individually rewritable.
However, one track is reserved by DOS for directory and file allocation information (the BAM, block availability map). And since for normal files, two bytes of each physical sector are used by DOS as a pointer to the next physical track and sector of the file, only 254 out of the 256 bytes of a block are used for file contents.
If the disk side was not otherwise prepared with a custom format, (e.g. for data disks), 664 blocks would be free after formatting, giving 664 × 254 = 168,656 bytes (or almost 165 kB) for user data.
By using custom formatting and load/save routines (sometimes included in third-party DOSes, see below), all of the mechanically possible 40 tracks can be used. The reason Commodore decided not to use the upper five tracks by default (or at least more than 35) was the bad quality of some of the drive mechanisms which did not always work reliably at the highest tracks. So by reducing the number of tracks used and thus capacity, it was possible to further reduce cost - in contrast to single-density drives used e.g. in IBM PC computers of the day which save 180 kB on one side (by using a 40-track format). The 1983 Apple FileWare minifloppy drives use double-sided media, higher track pitch, and variable motor speed to achieve a storage capacity of 871 kB, or 435 kB per side.
The 1541 does not have an index hole sensor, making it straightforward to use the reverse side of a disk by flipping it. A disk can be converted to a "flippy disk" by simply cutting/punching a notch on the left-hand side, causing the drive to recognize both sides as writable. This would effectively double the storage capacity. The notch can be made with scissors, a knife, hole punch, or a disk notcher tool that is specifically designed for this task. Most soft-sectored and all hard-sectored drives would have also required an extra cut-out for the index hole — a harder modification.
Tracks 36-42 are non-standard. The bitrate is after GCR encoding, so actual data is a factor 5/4 less.
The 1541 disk typically has 35 tracks. Track 18 is reserved; the remaining tracks are available for data storage. The header is on 18/0 (track 18, sector 0) along with the BAM (block availability map), and the directory starts on 18/1 (track 18, sector 1). The file interleave is 10 blocks, while the directory interleave is 3 blocks.
Header contents: The header is similar to other Commodore disk headers, the structural differences being the BAM offset ($04) and size, and the label+ID+type offset ($90).
 $00–01 T/S reference to first directory sector (18/1)
 02 DOS version ('A')
 04-8F BAM entries (4 bytes per track: Free Sector Count + 24 bits for sectors)
 90-9F Disk Label, $A0 padded
 A2-A3 Disk ID
 A5-A6 DOS type ('2A')
Uses.
Early copy protection schemes deliberately introduced read errors on the disk, the software refusing to load unless the correct error message is returned. The general idea was that simple disk-copy programs are incapable of copying the errors. When one of these errors is encountered, the disk drive (as do many floppy disk drives) will attempt one or more reread attempts after first resetting the head to track zero. Few of these schemes had much deterrent effect, as various software companies soon released "nibbler" utilities that enabled protected disks to be copied and, in some cases, the protection removed.
Commodore copy protection sometimes depends on specific hardware configurations. "Gunship", for example, does not load if a second disk drive or printer is connected to the computer.
References.
</dl>

</doc>
<doc id="6769" url="http://en.wikipedia.org/wiki?curid=6769" title="Commodore 1581">
Commodore 1581

The Commodore 1581 is a 3½ inch double-sided double-density floppy disk drive that was first made by Commodore Business Machines (CBM) in 1987, primarily for its C64 and C128 home/personal computers. The drive stores 800 kilobytes using an MFM encoding but format different from MS-DOS (720 kB), Amiga (880 kB), and Mac Plus (800 kB) formats. With special software it's possible to read C1581 disks on an x86 PC system, and likewise, read MS-DOS and other formats of disks in the C1581 (using Big Blue Reader), provided that the PC or other floppy handles the "720 kB" size format. This capability was most frequently used to read MS-DOS disks. The drive was released in the summer of 1987 and quickly became popular with bulletin board system (BBS) operators and other users.
Like the 1541 and 1571, the 1581 has an onboard MOS Technology 6502 CPU with its own ROM and RAM, and uses a serial version of the IEEE-488 interface. Inexplicably, the drive's ROM contains commands for parallel use, although no parallel interface was available. Unlike the 1571, which is nearly 100% backward-compatible with the 1541, the 1581 has limited compatibility with Commodore's earlier drives. Although it responds to the same DOS commands, most disk utilities written prior to 1987—most notably fast loaders—are so 1541-specific that they do not work with the 1581.
The version of Commodore DOS built into the 1581 added support for partitions, which could also function as fixed-allocation subdirectories. PC-style subdirectories were rejected as being too difficult to work with in terms of block availability maps, then still much in vogue, and which for some time had been the traditional way of inquiring into block availability. When used together with the C128, it implements faster burst mode access than the Commodore 1571 5¼" drive. When using the 1581 together with the C64, however, it is almost as slow as the 1541 drive, due to limitations of the C64's ROM code. The 1581 provides a total of 3160 blocks free when formatted (a block being equal to 256 bytes). The number of permitted directory entries was also increased, to 296 entries. With a storage capacity of 800 kB, the 1581 is the highest-capacity serial-bus drive that was ever made by Commodore (the 1-MB SFD-1001 uses the parallel IEEE-488), and the only 3½" one. However, starting in 1991, Creative Micro Designs (CMD) made the FD-2000 high density (1.6 MB) and FD-4000 extended density (3.2 MB) 3½" drives, both of which offered not only a 1581-emulation mode but also 1541- and 1571-compatibility modes.
Like the 1541 and 1571, a nearly identical job queue is available to the user in zero page (except for job 0), providing for exceptional degrees of compatibility.
Unlike the cases of the 1541 and 1571, the low-level disk format used by the 1581 is similar enough to the MS-DOS format as the 1581 is built around a WD1770 FM/MFM floppy controller chip. PC floppy controllers directly connected via the ISA-bus or onboard, but not standalone USB floppy drives, are able to deal with the 1581 format without need for any special tricks. Thus, utilities to format, read, and write 1581-format disks in standard PC floppy drives under Linux and Microsoft Windows exist. This controller chip, however, was the seat of some early problems with 1581 drives when the first production runs were recalled due to a high failure rate; the problem was quickly corrected. Later versions of the 1581 drive have a smaller, more streamlined-looking external power supply provided with them.
Specifications.
This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the "relicensing" terms of the GFDL, version 1.3 or later.
1581 Image Layout.
The 1581 disk has 80 logical tracks, each with 40 logical sectors (the actual physical layout of the diskette is abstracted and managed by a hardware translation layer). The directory starts on 40/3 (track 40, sector 3). The disk header is on 40/0, and the BAM (block availability map) resides on 40/1 and 40/2.
Header Contents
 $00–01 T/S reference to first directory sector (40/3)
 02 DOS version ('D')
 04-13 Disk Label, $A0 padded
 16-17 Disk ID
 19-1A DOS type ('3D')
BAM Contents, 40/1
 $00–01 T/S to next BAM sector (40/2)
 02 DOS version ('D')
 04-05 Disk ID
 06 I/O byte
 07 Autoboot flag
 10-FF BAM entries for Tracks 1-40
BAM Contents, 40/2
 $00–01 00/FF
 02 DOS version ('D')
 04-05 Disk ID
 06 I/O byte
 07 Autoboot flag
 10-FF BAM entries for Tracks 41-80

</doc>
<doc id="6771" url="http://en.wikipedia.org/wiki?curid=6771" title="College football">
College football

College football is gridiron football played by teams of student athletes fielded by American universities, colleges, and military academies, or Canadian football played by teams of student athletes fielded by Canadian universities. It was through college football play that American football rules first gained popularity in the United States.
History.
Rugby Football in England and Canada.
Modern North American football has its origins in various games, all known as "football", played at public schools in England in the mid-19th century. By the 1840s, students at Rugby School were playing a game in which players were able to pick up the ball and run with it, a sport later known as Rugby football. The game was taken to Canada by British soldiers stationed there and was soon being played at Canadian colleges.
The first documented gridiron football match was a game played at University College, a college of the University of Toronto, November 9, 1861. One of the participants in the game involving University of Toronto students was (Sir) William Mulock, later Chancellor of the school. A football club was formed at the university soon afterward, although its rules of play at this stage are unclear.
In 1864, at Trinity College, also a college of the University of Toronto, F. Barlow Cumberland and Frederick A. Bethune devised rules based on rugby football. Modern Canadian football is widely regarded as having originated with a game played in Montreal, in 1865, when British Army officers played local civilians. The game gradually gained a following, and the Montreal Football Club was formed in 1868, the first recorded non-university football club in Canada.
First American college football game.
The first intercollegiate football game played under the rules that would eventually become modern American football rules occurred between Princeton University and Rutgers University (which was called Rutgers College at the time) on November 6, 1869. However, this game was far more like soccer than modern American football. The completion of the first ever American football season came as a result of only two total games being played.
The Princeton-Rutgers game took place on College Field, which is now the site of the College Avenue Gymnasium at Rutgers University in New Brunswick, New Jersey. Rutgers won by a score of 6 "runs" to Princeton's 4. The 1869 game between Rutgers and Princeton is the first documented game of intercollegiate "football" ever played between two American colleges, and because of this, Rutgers refers to itself as "The Birthplace of College Football". It came two years before the founding of the Rugby Football Union in England, even though rugby had been codified 24 years before this in 1845 and played by many schools, universities, and clubs. Although the Rutgers-Princeton game was undoubtedly different from what we know today as American football, it was the forerunner of what evolved into American football. Another similar game took place between Rutgers and Columbia University in 1870.
Less than six years after that first game, a game that more closely resembled modern American football occurred between Harvard University and Tufts University on June 4, 1875. Though Harvard vs. Tufts is regarded as the first intercollegiate American football game between US teams, a game played by roughly the same rules (the so-called "Boston Rules") occurred between Harvard and McGill University of Montreal in 1874. Harvard, who was trying to get away from the soccer-like game that many schools played, set out to find another school who played a game similar to the one they did. This first game was a lot like rugby but much closer to modern day football than soccer. After the captains of the two teams met, they quickly realized that the games each school played were still different. In a compromise, the teams decided to play two different games, one under each team's set of rules. On May 14, Harvard won the game under their rules, and on the following day, May 15, the game under McGill's rules ended in a scoreless tie. Harvard would eventually go on to fully adopt the McGill version of the game that included more carrying of the ball and also used an oblong ball that was easier to carry and throw. An 1869 game of intercollegiate "football" between Rutgers and Princeton is often cited as the first intercollegiate American football game, however it was an unfamiliar ancestor of today's college football, as it was played under 6-year-old soccer-style Association rules.
Rugby is adopted by American colleges.
Rutgers, Princeton and Columbia met on October 20, 1873 at the Fifth Avenue Hotel in New York City to agree on a set of rules and regulations that would allow them to play a form of football that was essentially Association football (today often called "soccer" in the US) in character. Harvard University turned down an invitation to join this group because they preferred to play a rougher version of football called "the Boston Game" in which the kicking of a round ball was the most prominent feature though a player could run with the ball, pass it, or dribble it (known as “babying”). The man with the ball could be tackled, although hitting, tripping, “hacking” (shin-kicking) and other unnecessary roughness was prohibited. There was no limit to the number of players, but there were typically ten to fifteen per side.
Harvard's decision not to join the Yale-Rutgers-Princeton-Columbia association meant that they needed to look further afield to find football opponents so when a challenge from Canada’s McGill University rugby team in Montreal was issued to Harvard, they accepted. It was agreed that two games would be played on Harvard’s Jarvis baseball field in Cambridge, Massachusetts on May 14 and 15, 1874: one to be played under Harvard rules, another under the stricter rugby regulations of McGill. Harvard beat McGill in the "Boston Game" on the Thursday and held McGill to a 0-0 tie on the Friday. The Harvard students took to the rugby rules and adopted them as their own, travelling to Montreal to play a further game of rugby in the Fall of the same year winning by three tries to nil.
Harvard then played Tufts University on June 4, 1875, again at Jarvis Field. Jarvis Field was at the time a patch of land at the northern point of the Harvard campus, bordered by Everett and Jarvis Streets to the north and south, and Oxford Street and Massachusetts Avenue to the east and west. The game was won by Tufts 1-0 and a report of the outcome of this game appeared in the Boston Daily Globe of June 5, 1875. In this game each side fielded eleven men, participants were allowed to pick up the inflated egg-shaped ball and run with it and the ball carrier was stopped by knocking him down or "tackling" him. A photograph of the 1875 Tufts team which hangs in the College Football Hall of Fame commemorates this match as the generally accepted first intercollegiate football game between two US institutions.
In 1876 at Massasoit House in Springfield, Massachusetts, Harvard persuaded Princeton and Columbia to adopt an amalgam of rugby's laws and the rules that they were then playing, thus forming the Intercollegiate Football Association (IFA). Yale initially refused to join this association because of a disagreement over the number of players to be allowed per team (relenting in 1879) and Rutgers were not invited to the meeting. The rules that they agreed upon were essentially those of rugby union at the time with the exception that points be awarded for scoring a try, not just the conversion afterwards (extra point). Incidentally, rugby was to make a similar change to its scoring system 10 years later.
Rugby becomes American football.
Walter Camp, known as the "Father of American Football", is credited with changing the game from a variation of rugby into a unique sport. Camp, who was a rugby coach, decided to come up with a new set of rules to create a game that was completely different. Camp is responsible for pioneering the play from scrimmage with initially uncontested possession for the team starting with the ball (earlier games featured a rugby scrum where possession was contested) and was also the one who decided that teams should have 4 downs to advance the ball ten yards. Camp was responsible for the eleven-man team. Camp also had a hand in popularizing the game. He published numerous articles in publications such as "Collier's Weekly" and "Harper's Weekly", and he chose the first College Football All-America Team.
Formation of the NCAA.
College football increased in popularity through the remainder of the 19th century. It also became increasingly violent. Between 1890 and 1905, 330 college athletes died as a direct result of injuries sustained on the football field. These deaths could be attributed to the mass formations and gang tackling that characterized the sport in its early years. In 1906, President Theodore Roosevelt organized a meeting among thirteen school leaders at the White House to find solutions to make the sport safer for the athletes. Because the college officials could not agree upon a change in rules, it was decided over the course of several subsequent meetings that an external governing body should be responsible. Resulting from this conference was the formation of the Intercollegiate Athletic Association of the United States in 1906. The IAAUS was the original rule making body of college football, but would go on to sponsor championships in other sports. The IAAUS would get its current name of National Collegiate Athletic Association (NCAA), in 1910 which still sets rules governing the sport. The rules committee considered widening the playing field to "open up" the game, but Harvard Stadium (the first large permanent football stadium) had recently been built at great expense; it would be rendered useless by a wider field. The rules committee legalized the forward pass instead. The first legal pass was thrown by Bradbury Robinson on September 5, 1906, playing for coach Eddie Cochems, who developed an early but sophisticated passing offense at Saint Louis University. Another rule change banned "mass momentum" plays (many of which, like the infamous "flying wedge", were sometimes literally deadly).
Even after the emergence of the professional National Football League (NFL), college football remained extremely popular throughout the U.S. 
Although the college game has a much larger margin for talent than its pro counterpart, the sheer number of fans following major colleges provides a financial equalizer for the game, with Division I programs – the highest level – playing in huge stadiums, six of which have seating capacity exceeding 100,000. In many cases, college stadiums employ bench-style seating, as opposed to individual seats with backs and arm rests. This allows them to seat more fans in a given amount of space than the typical professional stadium, which tends to have more features and comforts for fans. (Only two stadiums owned by U.S. colleges or universities—Papa John's Cardinal Stadium at the University of Louisville and FAU Stadium at Florida Atlantic University—consist entirely of chairback seating.)
College athletes, unlike players in the NFL, are not permitted by the NCAA to be paid salaries. Colleges are only allowed to provide non-monetary compensation such as athletic scholarships that provide for tuition, housing, and books.
Official rules and notable rule distinctions.
Although rules for the high school, college, and NFL games are generally consistent, there are several minor differences. The NCAA Football Rules Committee determines the playing rules for Division I (both Bowl and Championship Subdivisions), II, and III games (the National Association of Intercollegiate Athletics (NAIA) is a separate organization, but uses the NCAA rules).
Organization.
College teams mostly play other similarly sized schools through the NCAA's divisional system. Division I generally consists of the major collegiate athletic powers with larger budgets, more elaborate facilities, and (with the exception of a few conferences such as the Pioneer Football League) more athletic scholarships. Division II primarily consists of smaller public and private institutions that offer fewer scholarships than those in Division I. Division III institutions also field teams, but do not offer any scholarships.
Football teams in Division I are further divided into the Bowl Subdivision (consisting of the largest programs) and the Championship Subdivision. The Bowl Subdivision has historically not used an organized tournament to determine its champion, and instead teams compete in post-season bowl games. (That will change with the debut of the four-team College Football Playoff in 2015.)
Teams in each of these four divisions are further divided into various regional conferences.
Several organizations operate college football programs outside the jurisdiction of the NCAA:
A college that fields a team in the NCAA is not restricted from fielding teams in club or sprint football, and several colleges field two teams, a varsity (NCAA) squad and a club or sprint squad (no schools, as of 2014, field both club "and" sprint teams at the same time).
Playoff games.
Starting in the 2014 season, four Division I FBS teams will be selected at the end of regular season to compete in a playoff for the FBS national championship. The College Football Playoff will replace the Bowl Championship Series, which had been used as the selection method to determine the national championship game participants starting in the 1998 season.
At the Division I FCS level, the teams participate in a 24-team playoff (most recently expanded from 20 teams in 2013) to determine the national championship. Under the current playoff structure, the top eight teams are all seeded, and receive a bye week in the first round. The highest seed receives automatic home field advantage. Starting in 2013, non-seeded teams can only host a playoff game if both teams involved are unseeded; in such a matchup, the schools must bid for the right to host the game. Selection for the playoffs is determined by a selection committee, although usually a team must have a 7-4 record to even be considered. Losses to an FBS team count against their playoff eligibility, while wins against a Division II opponent do not count towards playoff consideration. Thus, only Division I wins (whether FBS, FCS, or FCS non-scholarship) are considered for playoff selection. The Division I National Championship game is held in Frisco, Texas.
Division II and Division III of the NCAA also participate in their own respective playoffs, crowning national champions at the end of the season. The National Association of Intercollegiate Athletics also holds a playoff.
Bowl games.
Unlike other college football divisions and most other sports—collegiate or professional—the Football Bowl Subdivision, formerly known as Division I-A college football, has historically not employed a playoff system to determine a champion. Instead, it has a series of postseason "bowl games". The annual National Champion in the Football Bowl Subdivision is then instead traditionally determined by a vote of sports writers and other non-players.
This system has been challenged often, beginning with an NCAA committee proposal in 1979 to have a four-team playoff following the bowl games. However, little headway was made in instituting a playoff tournament until 2014, given the entrenched vested economic interests in the various bowls. Although the NCAA publishes lists of claimed FBS-level national champions in its official publications, it has never recognized an official FBS national championship; this policy will continue even after the establishment of the College Football Playoff (which will not be directly run by the NCAA) in 2014. As a result, the official Division I National Champion is the winner of the Football Championship Subdivision, as it is the highest level of football with an NCAA-administered championship tournament.
The first bowl game was the 1902 Rose Bowl, played between Michigan and Stanford; Michigan won 49-0. It ended when Stanford requested and Michigan agreed to end it with 8 minutes on the clock. That game was so lopsided that the game was not played annually until 1916, when the Tournament of Roses decided to reattempt the postseason game. The term "bowl" originates from the shape of the Rose Bowl stadium in Pasadena, California, which was built in 1923 and resembled the Yale Bowl, built in 1915. This is where the name came into use, as it became known as the Rose Bowl Game. Other games came along and used the term "bowl", whether the stadium was shaped like a bowl or not.
At the Division I FBS level, teams must earn the right to be bowl eligible by winning at least 6 games during the season (teams that play 13 games in a season, which is allowed for Hawaii and any of its home opponents, must win 7 games). They are then invited to a bowl game based on their conference ranking and the tie-ins that the conference has to each bowl game. For the 2009 season, there were 34 bowl games, so 68 of the 120 Division I FBS teams were invited to play at a bowl. These games are played from mid-December to early January and most of the later bowl games are typically considered more prestigious.
After the Bowl Championship Series, additional all-star bowl games round out the post-season schedule through the beginning of February.
Division I FBS National Championship Games.
Partly as a compromise between both bowl game and playoff supporters, the NCAA created the Bowl Championship Series (BCS) in 1998 in order to create a definitive National Championship game for college football. The series included the four most prominent bowl games (Rose Bowl, Orange Bowl, Sugar Bowl, Fiesta Bowl), while the National Championship game rotated each year between one of these venues. The BCS system was slightly adjusted in 2006, as the NCAA added a fifth game to the series, called the National Championship Game. This allowed the four other BCS bowls to use their normal selection process to select the teams in their games while the top two teams in the BCS rankings would play in the new National Championship Game.
The BCS selection committee used a complicated, and often controversial, computer system to rank all Division 1-FBS teams and the top two teams at the end of the season played for the National Championship. This computer system, which factored in newspaper polls, online polls, coaches' polls, strength of schedule, and various other factors of a team's season, led to much dispute over whether the two best teams in the country were being selected to play in the National Championship Game. 
The BCS ended after the 2013 season and, starting in the 2014 season, the FBS national champion will be determined by a four-team playoff known as the College Football Playoff (CFP). A selection committee of college football experts will decide the participating teams. Six major bowl games (the Rose, Sugar, Cotton, Orange, Peach, and Fiesta) will rotate on a three-year cycle as semifinal games, with the winners advancing to the College Football Playoff National Championship. This arrangement is contractually locked in until the 2026 season.
Controversy.
College football is a controversial institution within American higher education, where the amount of money involved -- what people will pay for the entertainment provided -- is a corrupting factor within universities that they are usually ill-equipped to deal with. According to William E. Kirwan, chancellor of the University of Maryland System and co-director of the Knight Commission on Intercollegiate Athletics, "We've reached a point where big-time intercollegiate athletics is undermining the integrity of our institutions, diverting presidents and institutions from their main purpose." Football coaches often make more than the presidents of their universities which employ them. Athletes are alleged to receive preferential treatment both in academics and when they run afoul of the law. Although in theory football is an extra-curricular activity engaged in as a sideline by students, it generates a substantial profit, from which the athletes receive no direct benefit. There has been serious discussion about making student-athletes university employees to allow them to be paid.
College football outside the United States.
Canadian football, which parallels American football, is played by collegiate teams in Canada under the auspices of Canadian Interuniversity Sport. (Unlike in the United States, no junior colleges play football in Canada, and the sanctioning body for junior college athletics in Canada, CCAA, does not sanction the sport.) However, amateur football outside of colleges is played in Canada, such as in the Canadian Junior Football League. Organized competition in American football also exists at the collegiate level in Mexico (ONEFA), the UK (British Universities American Football League), Japan (Japan American Football Association, Koshien Bowl), and South Korea (Korea American Football Association).
External links.
Listen to this article ()
This audio file was created from a revision of the "College football" article dated 2006-05-29, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="6773" url="http://en.wikipedia.org/wiki?curid=6773" title="Ciprofloxacin">
Ciprofloxacin

Ciprofloxacin (INN) is an antibiotic that can treat a number of bacterial infections. It is a second-generation fluoroquinolone. Its spectrum of activity includes most strains of bacterial pathogens responsible for respiratory, urinary tract, gastrointestinal, and abdominal infections, including Gram-negative ("Escherichia coli", "Haemophilus influenzae", "Klebsiella pneumoniae", "Legionella pneumophila", "Moraxella catarrhalis", "Proteus mirabilis", and "Pseudomonas aeruginosa"), and Gram-positive (methicillin-sensitive, but not methicillin-resistant "Staphylococcus aureus", "Streptococcus pneumoniae", "Staphylococcus epidermidis", "Enterococcus faecalis", and "Streptococcus pyogenes") bacterial pathogens. Ciprofloxacin and other fluoroquinolones are valued for this broad spectrum of activity, excellent tissue penetration, and for their availability in both oral and intravenous formulations.
Ciprofloxacin is used alone or in combination with other antibacterial drugs in the empiric treatment of infections for which the bacterial pathogen has not been identified, including urinary tract infections and abdominal infections among others. It can also treat infections caused by specific pathogens known to be sensitive.
Ciprofloxacin is the most widely used of the second-generation quinolone antibiotics that came into clinical use in the late 1980s and early 1990s. In 2010, over 20 million outpatient prescriptions were written for ciprofloxacin, making it the 35th-most commonly prescribed drug, and the 5th-most commonly prescribed antibacterial, in the US. Ciprofloxacin was discovered and developed by Bayer A.G. and subsequently approved by the US Food and Drug Administration (FDA) in 1987. Ciprofloxacin has 12 FDA-approved human uses and other veterinary uses, but it is often used for unapproved uses (off-label).
Overall, the safety of ciprofloxacin and other fluoroquinolones appears to be similar to that of other antibiotics, but serious side effects occur on occasion. Some disagreement in the literature exists regarding whether fluoroquinolones produce serious adverse events at a higher rate than other broad-spectrum antibiotics. The U.S. FDA-approved label for ciprofloxacin includes a "black box" warning of increased risk of tendon damage and/or rupture and for exacerbation of muscle weakness in patients with the neurological disorder myasthenia gravis. Other side effects include nausea, vomiting, diarrhea and dizziness. 
Medical uses.
Ciprofloxacin is used to treat a wide variety of infections, including infections of bones and joints, endocarditis, gastroenteritis, malignant otitis externa, respiratory tract infections, cellulitis, urinary tract infections, prostatitis, anthrax, and chancroid.
Ciprofloxacin only treats bacterial infections; it does not treat viral infections such as the common cold. Although for certain uses including acute sinusitis, lower respiratory tract infections and uncomplicated gonorrhea, ciprofloxacin is not considered a first-line agent.
Ciprofloxacin occupies an important role in treatment guidelines issued by major medical societies for the treatment of serious infections, especially those likely to be caused by Gram-negative bacteria, including "Pseudomonas aeruginosa". For example, ciprofloxacin in combination with metronidazole is one of several first-line antibiotic regimens recommended by the Infectious Disease Society of America for the treatment of community-acquired abdominal infections in adults. It also features prominently in treatment guidelines for acute pyelonephritis, complicated or hospital-acquired urinary tract infection, acute or chronic prostatitis, certain types of endocarditis, certain skin infections, and prosthetic joint infections.
In other cases, treatment guidelines are more restrictive, recommending in most cases that older, narrower-spectrum drugs be used as first-line therapy for less severe infections to minimize fluoroquinolone-resistance development. For example, the Infectious Disease Society of America recommends the use of ciprofloxacin and other fluoroquinolones in urinary tract infections be reserved to cases of proven or expected resistance to narrower-spectrum drugs such as nitrofurantoin or trimethoprim-sulfamethoxazole. The European Association for Urology recommends ciprofloxacin as an alternative regimen for the treatment of uncomplicated urinary tract infections, but cautions that the potential for “adverse events have to be considered”.
Although approved by regulatory authorities for the treatment of respiratory infections, ciprofloxacin is not recommended for respiratory infections by most treatment guidelines due in part to its modest activity against the common respiratory pathogen "Streptococcus pneumoniae". "Respiratory quinolones" such as levofloxacin, having greater activity against this pathogen, are recommended as first line agents for the treatment of community-acquired pneumonia in patients with important co-morbidities and in patients requiring hospitalization (Infectious Diseases Society of America 2007). Similarly, ciprofloxacin is not recommended as a first-line treatment for acute sinusitis
Ciprofloxacin is approved for the treatment of gonorrhea in many countries, but this recommendation is widely regarded as obsolete due to resistance development.
Available forms.
Ciprofloxacin for systemic administration is available as immediate-release tablets, as extended-release tablets, as an oral suspension, and as a solution for intravenous infusion. It is also available for local administration as eye drops and ear drops.
Pregnancy.
The U.S. FDA categorizes ciprofloxacin in pregnancy category C. This category includes drugs for which no adequate and well-controlled studies in human pregnancy exist, and for which animal studies have suggested the potential for harm to the fetus, but potential benefits may warrant use of the drug in pregnant women
despite potential risks. An expert review of published data on experiences with ciprofloxacin use during pregnancy by the Teratogen Information System concluded therapeutic doses during 
pregnancy are unlikely to pose a substantial teratogenic risk (quantity and quality of data=fair), but the data are insufficient to state no risk exists.
Two small post-marketing epidemiology studies of mostly short-term, first-trimester exposure found that fluoroquinolones did not increase risk of major malformations, spontaneous abortions, premature birth, or low birth weight. The label notes, however, that these studies are insufficient to reliably evaluate the definitive safety or risk of less common defects by ciprofloxacin in pregnant women and their developing fetuses.
Breastfeeding.
The fluoroquinolones have also been reported as being present in the mother's milk and are passed on to the nursing child. The US FDA recommends that because of the risk of serious adverse reactions (including articular damage) in infants nursing from mothers taking ciprofloxacin, a decision should be made whether to discontinue nursing or discontinue the drug, taking into account the importance of the drug to the mother.
Children.
Oral and intravenous ciprofloxacin are approved by the FDA for use in children for only two indications due to the risk of permanent injury to the musculoskeletal system:
1) Inhalational anthrax (postexposure)
2) Complicated urinary tract infections and pyelonephritis due to "Escherichia coli", but never as first-line agents. Current recommendations by the American Academy of Pediatrics note the systemic use of ciprofloxacin in children should be restricted to infections caused by multidrug-resistant pathogens or when no safe or effective alternatives are available.
Adverse effects.
Overall, the safety of ciprofloxacin and other fluoroquinolones appears to be similar to that of other antibiotics, but serious side effects occur on occasion. Some disagreement has been reported in the literature regarding whether fluoroquinolones produce serious adverse events at a higher rate than other broad-spectrum antibiotics.
In pre-approval clinical trials, 49,038 patients received courses of ciprofloxacin. Most of the adverse events reported were described as only mild or moderate in severity, abated soon after the drug was discontinued, and required no treatment. Ciprofloxacin was discontinued because of an adverse event in 1% of orally treated patients. The most frequently reported drug-related events, from clinical trials of all formulations, all dosages, all drug-therapy durations, and for all indications of ciprofloxacin therapy, were nausea (2.5%), diarrhea (1.6%), abnormal liver function tests (1.3%), vomiting (1%), and rash (1%). Other adverse events occurred at rates of <1%.
The black box warning on the US FDA-approved ciprofloxacin label warns of an increased risk of tendinitis and tendon rupture, especially in patients who are older than 60 years, patients who also use corticosteroids, and patients with kidney, lung, or heart transplants. Tendon rupture can occur during therapy or even months after discontinuation of the drug. A case control study performed using a UK medical care database found that fluoroquinolone use was associated with a 1.9-fold increase in tendon problems. The relative risk increased to 3.2 in those over 60 years of age and to 6.2 in those over the age of 60 who were also taking corticosteroids. Among the 46,766 quinolone users in the study, 38 (0.1%) cases of Achilles tendon rupture were identified. A study performed using an Italian healthcare database reached qualitatively similar conclusions.
The other black box warning is that ciprofloxacin should not be used in patients with myasthenia gravis due to possible exacerbation of muscle weakness which may lead to breathing problems resulting in death or ventilator support. Fluoroquinolones are known to block neuromuscular transmission. <ref name="www.accessdata.fda.gov"/
"Clostridium difficile"-associated diarrhea is a serious adverse effect of ciprofloxacin and other fluoroquinolones; it is unclear whether the risk is higher than with other broad-spectrum antibiotics.
The 2013 FDA label warns of nervous system effects. Ciprofloxacin, like other fluoroquinolones, is known to trigger seizures or lower the seizure threshold, and may cause other central nervous system side effects. Headache, dizziness, and insomnia have been reported as occurring fairly commonly in postapproval review articles, along with a much lower incidence of serious CNS side effects such as tremors, psychosis, anxiety, hallucinations, paranoia, and suicide attempts, especially at higher doses. Like other fluoroquinolones, it is also known to cause peripheral neuropathy that may be irreversible, such as weakness, burning pain, tingling, or numbness.
A wide range of rare but potentially fatal side effects spontaneously reported to the US FDA or the subject of case reports published in medical journals include, but are not limited to, toxic epidermal necrolysis, Stevens-Johnson syndrome, heart arrhythmias ("torsades des pointes" or QT prolongation), allergic pneumonitis, bone marrow suppression, hepatitis or liver failure, and phototoxicity/photosensitivity. The drug should be discontinued if a rash, jaundice, or other sign of hypersentitivity occur.
Children and the elderly are at a much greater risk of experiencing adverse reactions.
Contraindications.
Two contraindications are found within the 2013 package insert:
Ciprofloxacin is also considered to be contraindicated within the pediatric population (except for the indications outlined above), pregnancy, nursing mothers, and in patients with epilepsy or other seizure disorders.
Genotoxicity and carcinogenicity studies.
Ciprofloxacin is active in six of eight "in vitro" assays used as rapid screens for the detection of genotoxic effects, but is not active in "in vivo" assays of genotoxicity. Long-term carcinogenicity studies in rats and mice resulted in no carcinogenic or tumorigenic effects due to ciprofloxacin at daily oral dose levels up to 250 and 750 mg/kg to rats and mice, respectively (about 1.7 and 2.5 times the highest recommended therapeutic dose based upon mg/m2). Results from photo co-carcinogenicity testing indicate ciprofloxacin does not reduce the time to appearance of UV-induced skin tumors as compared to vehicle control.
Interactions.
Ciprofloxacin interacts with certain foods and several other drugs leading to undesirable increases or decreases in the serum levels or distribution of one or both drugs.
Ciprofloxacin should not be taken with antacids containing magnesium or aluminum, highly buffered drugs (sevelamer, lanthanum carbonate, sucralfate, didanosine), or with supplements containing calcium, iron, or zinc. It should be taken two hours before or six hours after these products. Magnesium or aluminum antacids turn ciprofloxacin into insoluble salts that are not readily absorbed by the intestinal tract, reducing peak serum concentrations by 90% or more, leading to therapeutic failure. Additionally, it should not be taken with dairy products or calcium-fortified juices alone, as peak serum concentration and the area under the serum concentration-time curve can be reduced up to 40%. However, ciprofloxacin may be taken with dairy products or calcium-fortified juices as part of a meal.
Ciprofloxacin inhibits the drug-metabolizing enzyme CYP1A2 and thereby can reduce the clearance of drugs metabolized by that enzyme. CYP1A2 substrates that exhibit increased serum levels in ciprofloxacin-treated patients include tizanidine, theophylline, caffeine, methylxanthines, clozapine, olanzapine, and ropinirole. Co-administration of ciprofloxacin with the CYP1A2 substrate tizanidine (Zanaflex) is contraindicated due to a 583% increase in the peak serum concentrations of tizanidine when administered with ciprofloxacin as compared to administration of tizanidine alone. Use of ciprofloxacin is cautioned in patients on theophylline due to its narrow therapeutic index. The authors of one review recommended that patients being treated with ciprofloxacin reduce their caffeine intake. Evidence for significant interactions with several other CYP1A2 substrates such as cyclosporine is equivocal or conflicting.
The Committee on the Safety of Medicines and the FDA warn that central nervous system adverse effects, including seizure risk, may be increased when NSAIDs are combined with quinolones. The mechanism for this interaction may involve a synergistic increased antagonism of GABA neurotransmission.
Altered serum levels of the antiepileptic drugs phenytoin and carbamazepine (increased and decreased) have been reported in patients receiving concomitant ciprofloxacin.
Overdose.
Overdose of ciprofloxacin may result in reversible renal toxicity. Treatment of overdose includes emptying of the stomach by induced vomiting or gastric lavage, as well as administration of magnesium-, aluminum-, or calcium-containing antacids to reduce drug absorption. Renal function and urinary pH should be monitored. Important support includes adequate hydration and urine acidification if necessary to prevent crystalluria. Hemodialysis or peritoneal dialysis can only remove less than 10% of ciprofloxacin. Ciprofloxacin may be quantified in plasma or serum to monitor for drug accumulation in patients with hepatic dysfunction or to confirm a diagnosis of poisoning in acute overdose victims.
Chemical properties.
Ciprofloxacin is 1-cyclopropyl-6-fluoro-1,4-dihydro-4-oxo-7-(1-piperazinyl)-3-quinolinecarboxylic acid. Its empirical formula is C17H18FN3O3 and its molecular weight is 331.4 g/mol. It is a faintly yellowish to light yellow crystalline substance.
Ciprofloxacin hydrochloride (USP) is the monohydrochloride monohydrate salt of ciprofloxacin. It is a faintly yellowish to light yellow crystalline substance with a molecular weight of 385.8 g/mol. Its empirical formula is C17H18FN3O3HCl•H2O.
Mechanism of action.
Ciprofloxacin is a broad-spectrum antibiotic active against both Gram-positive and Gram-negative bacteria. It functions by inhibiting DNA gyrase, a type II topoisomerase, and topoisomerase IV, enzymes necessary to separate bacterial DNA, thereby inhibiting cell division.
Pharmacokinetics.
Ciprofloxacin for systemic administration is available as immediate-release tablets, extended-release tablets, an oral suspension, and as a solution for intravenous administration. 
When administered over one hour as an intravenous infusion, ciprofloxacin rapidly distributes into the tissues, with levels in some tissues exceeding those in the serum. Penetration into the central nervous system is relatively modest, with cerebrospinal fluid levels normally less than 10% of peak serum concentrations. The serum half-life of ciprofloxacin is about 4–6 hours, with 50-70% of an administered dose being excreted in the urine as unmetabolized drug. An additional 10% is excreted in urine as metabolites. Urinary excretion is virtually complete by 24 hours after administration. Dose adjustment is required in the elderly and in those with renal impairment.
Ciprofloxacin is weakly bound to serum proteins (20-40%), but is an inhibitor of the drug-metabolizing enzyme cytochrome P450 1A2, which leads to the potential for clinically important drug interactions with drugs metabolized by that enzyme.
Ciprofloxacin is about 70% orally available when administered orally, so a slightly higher dose is needed to achieve the same exposure when switching from IV to oral administration. A 750-mg immediate-release oral tablet given every 12 hours produces about the same area under the serum concentration curve (AUC) and peak serum concentration (Cmax) as a 400-mg dose given every 8 hours IV. 
The extended release oral tablets allow once-daily administration by releasing the drug more slowly in the gastrointestinal tract. These tablets contain 35% of the administered dose in an immediate-release form and 65% in a slow-release matrix. Maximum serum concentrations are achieved between 1 and 4 hours after administration. Compared to the 250- and 500-mg immediate-release tablets, the 500-mg and 1000-mg XR tablets provide higher Cmax, but the 24‑hour AUCs are equivalent.
Ciprofloxacin immediate-release tablets contain ciprofloxacin as the hydrochloride salt, and the XR tablets contain a mixture of the hydrochloride salt as the free base.
History.
The first members of the quinolone antibacterial class were relatively low-potency drugs such as nalidixic acid, used mainly in the treatment of urinary tract infections owing to their renal excretion and propensity to be concentrated in urine. In 1979, the publication of a patent filed by the pharmaceutical arm of Kyorin Seiyaku Kabushiki Kaisha disclosed the discovery of norfloxacin, and the demonstration that certain structural modifications including the attachment of a fluorine atom to the quinolone ring leads to dramatically enhanced antibacterial potency. In the aftermath of this disclosure, several other pharmaceutical companies initiated research and development programs with the goal of discovering additional antibacterial agents of the fluoroquinolone class.
The fluoroquinolone program at Bayer focused on examining the effects of very minor changes to the norfloxacin structure. In 1983, the company published "in vitro" potency data for ciprofloxacin, a fluoroquinolone antibacterial having a chemical structure differing from that of norfloxacin by the presence of a single carbon atom. This small change led to a two- to 10-fold increase in potency against most strains of Gram-negative bacteria. Importantly, this structural change led to a four-fold improvement in activity against the important Gram-negative pathogen "Pseudomonas aeruginosa", making ciprofloxacin one of the most potent known drugs for the treatment of this intrinsically antibiotic-resistant pathogen.
The oral tablet form of ciprofloxacin was approved in October 1987, just one year after the approval of norfloxacin. In 1991, the intravenous formulation was introduced. Ciprofloxacin sales reached a peak of about 2 billion euros in 2001, representing 34% of Bayer’s total pharmaceutical revenues, before Bayer's patent expired in 2004, after which annual sales have averaged around €200 million.
Society and culture.
Generic equivalents.
On 24 October 2001, The Prescription Access Litigation (PAL) filed suit to dissolve an agreement between Bayer and three of its competitors which produced generic versions of drugs (Barr Laboratories, Rugby Laboratories, and Hoechst-Marion-Roussel) that PAL claimed was blocking access to adequate supplies and cheaper, generic versions of ciprofloxacin. The plaintiffs charged that Bayer Corporation, a unit of Bayer AG, had unlawfully paid the three competing companies a total of $200 million to prevent cheaper, generic versions of ciprofloxacin from being brought to the market, as well as manipulating its price and supply. Numerous other consumer advocacy groups joined the lawsuit. On 15 October 2008, five years after Bayer's patent had expired, the United States District Court for the Eastern District of New York granted Bayer's and the other defendants' motion for summary judgment, holding that any anticompetitive effects caused by the settlement agreements between Bayer and its codefendants were within the exclusionary zone of the patent and thus could not be redressed by federal antitrust law, in effect upholding Bayer's agreement with its competitors.
Bacterial resistance.
As a result of its widespread use to treat minor infections readily treatable with older, less broad spectrum antibiotics, many bacteria have developed resistance to this drug in recent years, leaving it significantly less effective than it would have been otherwise.
Resistance to ciprofloxacin and other fluoroquinolones may evolve rapidly, even during a course of treatment. Numerous pathogens, including enterococci, "Streptococcus pyogenes" and "Klebsiella pneumoniae" (quinolone-resistant) now exhibit resistance. Widespread veterinary usage of the fluoroquinolones, particularly in Europe, has been implicated. Meanwhile, some "Burkholderia cepacia", "Clostridium innocuum" and "Enterococcus faecium" strains have developed resistance to ciprofloxacin to varying degrees.
Fluoroquinolones had become the most commonly prescribed class of antibiotics to adults in 2002. Nearly half (42%) of those prescriptions were for conditions not approved by the FDA, such as acute bronchitis, otitis media, and acute upper respiratory tract infection, according to a study supported in part by the Agency for Healthcare Research and Quality. Additionally, they were commonly prescribed for medical conditions that were not even bacterial to begin with, such as viral infections, or those to which no proven benefit existed.
Litigation.
A class action was filed against Bayer AG on behalf of employees of the Brentwood Post Office in Washington, D.C., and workers at the US Capitol, along with employees of American Media, Inc. in Florida and postal workers in general who alleged they suffered serious adverse effects from taking ciprofloxacin (Cipro) in the aftermath of the anthrax attacks in 2001. The action alleged Bayer failed to warn class members of the potential side effects of the drug, thereby violating the Pennsylvania Unfair Trade Practices and Consumer Protection Laws. The class action was defeated and the litigation abandoned by the plaintiffs.
A similar action was filed in 2003 in New Jersey by four New Jersey postal workers but was withdrawn for lack of grounds, as workers had been informed of the risks of cipro when they were given the option of taking the drug.

</doc>
<doc id="6774" url="http://en.wikipedia.org/wiki?curid=6774" title="Consubstantiation">
Consubstantiation

Consubstantiation is a theological doctrine that (like Transubstantiation) attempts to describe the nature of the Christian Eucharist in concrete metaphysical terms. It holds that during the sacrament, the fundamental "substance" of the body and blood of Christ are present "alongside" the substance of the bread and wine, which remain present. 
Use.
The doctrine of consubstantiation is often held in contrast to the doctrine of transubstantiation. While some Lutherans use the term "consubstantiation" to describe their doctrine, many reject it as not accurately reflecting the eucharistic doctrine of Martin Luther, the sacramental union. Lutherans reject the concept of consubstantiation because it substitutes what they believe to be the biblical doctrine with a philosophical construct and implies, in their view, a natural, local inclusion of the body and blood of Christ in the consecrated bread and wine of the eucharist.
In England in the late 14th century, there was a political and religious movement known as Lollardy. Among much broader goals, the Lollards affirmed a form of consubstantiation—that the Eucharist remained physically bread and wine, while becoming spiritually the body and blood of Christ. Lollardy survived up until the time of the English Reformation.
Literary critic Kenneth Burke's dramatism takes this concept and utilizes it in secular rhetorical theory to look at the dialectic of unity and difference within the context of logology.

</doc>
<doc id="6775" url="http://en.wikipedia.org/wiki?curid=6775" title="Chlorophyta">
Chlorophyta

Chlorophyta is a division of green algae, informally called chlorophytes. The name is used in two very different senses, so care is needed to determine the use by a particular author. In older classification systems, it refers to a highly paraphyletic group of "all" the green algae within the green plants (Viridiplantae) and thus includes about 7,000 species of mostly aquatic photosynthetic eukaryotic organisms. In newer classifications, it refers to one of the two clades making up the Viridiplantae, which are the chlorophytes and the streptophytes. The clade Streptophyta consists of two divisions, the Charophyta and the Embryophyta. In this sense the Chlorophyta includes only about 4,300 species.
Like the land plants (bryophytes and tracheophytes), green algae contain chlorophyll a and chlorophyll b and store food as starch in their plastids.
The division contains both unicellular and multicellular species. While most species live in freshwater habitats and a large number in marine habitats, other species are adapted to a wide range of environments. Watermelon snow, or "Chlamydomonas nivalis", of the class Chlorophyceae, lives on summer alpine snowfields. Others live attached to rocks or woody parts of trees. "Monostroma kuroshiensis", an edible green algae cultivated worldwide and most expensive among green algae, belongs to this group. Some lichens are symbiotic relationships between fungi and green algae.
Members of the Chlorophyta also form symbiotic relationships with protozoa, sponges, and cnidarians. All are flagellated, and these have an advantage of motility. Some conduct sexual reproduction, which is oogamous or isogamous.
Ecology.
Species of Chlorophyta (treated as what is now considered one of the two clades of Viridiplantae) are common inhabitants of marine, freshwater and terrestrial environments. Several species have adapted to specialised and extreme environments, such as deserts, arctic environments, hypersaline habitats, marine deep waters and deep-sea hydrothermal vents. 
 Some groups, such as the Trentepohliales are exclusively found on land. Several species of Chlorophyta live in symbiosis with a diverse range of eukaryotes, including fungi (to form lichens), ciliates, forams, cnidarians and molluscs.
 Some species of Chlorophyta are heterotrophic, either free-living or parasitic. Two common species of the heterotrophic green alga Prototheca are pathogenic and can cause the disease protothecosis in humans and animals.
Classifications.
Characteristics like type of zoid, mitosis (karyokynesis), cytokinesis, organization level, life cycle, type of gametes, cell wall polysaccharides and more recently genetic data are used for the classification of Chlorophyta.
Leliaert "et al". 2012.
Simplified phylogeny of the Chlorophyta, according to Leliaert "et al". 2012. Note that many algae before classified in Chlorophyta are replaced here in Streptophyta.
Pombert "et al". 2005.
A possible classification when Chlorophyta refers to one of the two clades of the Viridiplantae is shown below.
Hoek, Mann and Jahns 1995.
Classification of the Chlorophyta, treated as all green algae, according to Hoek, Mann and Jahns 1995.
In a note added in proof, an alternative classification is presented for the algae of the class Chlorophyceae:
Bold and Wynne 1985.
Classification of the Chlorophyta and Charophyta according to Bold and Wynne 1985.
Mattox & Stewart 1984.
Classification of the Chlorophyta according to Mattox & Stewart 1984:
Fott 1971.
Classification of the Chlorophyta according to Fott 1971.
Round 1971.
Classification of the Chlorophyta and related algae according to Round 1971.
Smith 1938.
Classification of the Chlorophyta according to Smith 1938:

</doc>
<doc id="6776" url="http://en.wikipedia.org/wiki?curid=6776" title="Capybara">
Capybara

The capybara ("Hydrochoerus hydrochaeris") is a large rodent of the genus Hydrochoerus of which the only other member is the lesser capybara ("Hydrochoerus isthmius"). The capybara is the largest rodent in the world. Close relatives are guinea pigs and rock cavies, and it is more distantly related to the agouti, chinchillas, and the coypu. Native to South America, the capybara inhabits savannas and dense forests and lives near bodies of water. It is a highly social species and can be found in groups as large as 100 individuals, but usually lives in groups of 10–20 individuals. The capybara is not a threatened species and is hunted for its meat, hide and also for a grease from its thick fatty skin which is used in the pharmaceutical trade.
Etymology.
Its common name is derived from Tupi "ka'apiûara", a complex agglutination of "kaá" (leaf) + "píi" (slender) + "ú" (eat) + "ara" (a suffix for agent nouns), meaning "one who eats slender leaves", or "grass-eater".
The scientific name, both "hydrochoerus" and "hydrochaeris", comes from Greek ὕδωρ ("hydor" = water) + χοίρος ("choiros" = pig, hog).
Classification and phylogeny.
The capybara and the lesser capybara belong to the subfamily Hydrochoerinae along with the rock cavies. The living capybaras and their extinct relatives were previously classified in their own family Hydrochoeridae. Since 2002, molecular phylogenetic studies have recognized a close relationship between "Hydrochoerus" and "Kerodon" supporting placement of both genera in a subfamily of Caviidae. Paleontological classifications have yet to incorporate this new taxonomy and continue to use Hydrochoeridae for all capybaras, while using Hydrochoerinae for the living genus and its closest fossil relatives, such as "Neochoerus". The taxonomy of fossil hydrochoerines is also in a state of flux. In recent years, the diversity of fossil hydrochoerines has been substantially reduced. This is largely due to the recognition that capybara molar teeth show strong variation in shape over the life of an individual. In one instance, material once referred to four genera and seven species on the basis of differences in molar shape is now thought to represent differently aged individuals of a single species, "Cardiatherium paranense".
Description.
The capybara has a heavy, barrel-shaped body and short head, with reddish-brown fur on the upper part of its body that turns yellowish-brown underneath. Its sweat glands can be found in the surface of the hairy portions of its skin, an unusual trait among rodents. The animal lacks under hair, and guard hair differs little from over hair. Adult capybaras grow to 106 to in length, stand 50 to tall at the withers, and typically weigh 35 to, with an average in the Venezuelan llanos of 48.9 kg. The top recorded weights are 91 kg for a wild female from Brazil and 73.5 kg for a wild male from Uruguay. The dental formula is 1.1.0.01.1.3.3. Capybaras have slightly webbed feet and vestigial tails. Their hind legs are slightly longer than their forelegs; they have three toes on their rear feet and four toes on their front feet. Their muzzles are blunt, with nostrils, and the eyes and ears are near the top of their heads. Females are slightly heavier than males.
Its karyotype has 2n = 66 and FN = 102.
Ecology.
Capybaras are semi-aquatic mammals found throughout almost all countries of South America (except Chile). They live in densely forested areas near bodies of water, such as lakes, rivers, swamps, ponds, and marshes, as well as flooded savannah and along rivers in tropical forest. Capybara have flourished in cattle ranches. They roam in home ranges averaging 10 hectares (25 acres) in high-density populations.
Many escapees from captivity can also be found in similar watery habitats around the world. Sightings are fairly common in Florida, although a breeding population has not yet been confirmed. In 2011, one was spotted in the Central Coast of California.
Diet and predation.
Capybaras are herbivores, grazing mainly on grasses and aquatic plants, as well as fruit and tree bark. They are very selective feeders and will feed on the leaves of one species and disregard other species surrounding it. They eat a greater variety of plants during the dry season, as fewer plants are available. While they eat grass during the wet season, they have to switch to more abundant reeds during the dry season. Plants that capybaras eat during the summer lose their nutritional value in the winter and therefore are not consumed at that time. The capybara's jaw hinge is not perpendicular and they thus chew food by grinding back-and-forth rather than side-to-side. Capybaras are coprophagous, meaning they eat their own feces as a source of bacterial gut flora, to help digest the cellulose in the grass that forms their normal diet, and to extract the maximum protein and vitamins from their food. They may also regurgitate food to masticate again, similar to cud-chewing by a cow. As is the case with other rodents, the front teeth of capybaras grow continually to compensate for the constant wear from eating grasses; their cheek teeth also grow continuously.
Like its cousin the guinea pig, the capybara does not have the capacity to synthesize vitamin C, and capybaras not supplemented with vitamin C in captivity have been reported to develop gum disease as a sign of scurvy.
They can have a life span of 8–10 years on average, but live less than four years in the wild, as they are "a favourite food of jaguar, puma, ocelot, eagle and caiman". The capybara is also the preferred prey of the anaconda.
Social organisation.
Capybaras are gregarious. While they sometimes live solitarily, they are more commonly found in groups that average 10–20 individuals, with two to four adult males, four to seven adult females and the remainder, juveniles. Capybara groups can consist of as many as 50 or 100 individuals during the dry season when the animals gather around available water sources. Males establish social bonds, dominance, or, general group census. They can make dog-like barks when threatened or when females are herding young. 
Capybaras have two types of scent glands; a morillo, located on the snout, and anal glands. Both sexes have these glands, but males have much larger morillos and use their anal glands more frequently. The anal glands of males are also lined with detachable hairs. A crystalline form of scent secretion is coated on these hairs and is released when in contact with objects like plants. These hairs have a longer-lasting scent mark and are tasted by other capybaras. Capybara scent-mark by rubbing their morillo on an object, or by walking over scrub and marking it with their anal glands. Capybara can spread their scent further by urinating; however, females usually mark without urinating and scent-mark less frequently than males overall. Females mark more often during the wet season when they are in estrus. In addition to objects, males will also scent-mark females.
Reproduction.
When in estrus, the female's scent changes subtly and nearby males begin pursuit. In addition, a female will alert males she is in estrus by whistling though her nose. During mating, the female has the advantage and mating choice. Capybaras mate only in water, and if a female does not want to mate with a certain male, she will either submerge or leave the water. Dominant males are highly protective of the females, but they usually cannot prevent all the subordinates from copulating. The larger the group, the harder it is for the male to watch all the females. Dominant males secure significantly more matings than each subordinate, but subordinate males, as a class, are responsible for more matings than each dominant male. The lifespan of the capybara's sperm is longer than that of other rodents.
Capybara gestation is 130–150 days, and usually produces a litter of four capybara babies, but may produce between one and eight in a single litter. Birth is on land and the female will rejoin the group within a few hours of delivering the newborn capybaras, which will join the group as soon as they are mobile. Within a week, the young can eat grass, but will continue to suckle—from any female in the group—until weaned at about 16 weeks. The young will form a group within the main group. Alloparenting has been observed in this species. Breeding peaks between April and May in Venezuela and between October and November in Mato Grosso, Brazil.
Activities.
Though quite agile on land (capable of running as fast as a horse), Capybaras are equally at home in the water. They are excellent swimmers, and can remain completely submerged for up to five minutes, an ability they use to evade predators. Capybaras can sleep in water, keeping only their noses out of the water. As temperatures increase during the day, they wallow in water and then graze during the late afternoon and early evening. They also spend a lot of time wallowing in mud. They rest around midnight and then continue to graze before dawn.
Conservation and human interaction.
Capybaras are not considered a threatened species; their population is stable throughout most of their South American range, though in some areas hunting has reduced their numbers.
Capybaras are hunted for their meat and pelts in some areas, and otherwise killed by humans who see their grazing as competition for livestock. In some areas, they are farmed, which has the effect of ensuring the wetland habitats are protected. Their survival is aided by their ability to breed rapidly.
Capybaras can be found in many areas in zoos and parks, and may live for 12 years in captivity. Capybaras are gentle and will usually allow humans to pet and hand-feed them.
The European Association of Zoos and Aquaria (EAZA) tasked Drusillas Park in Alfriston, Sussex to keep the studbook for Capybaras, to monitor captive populations in Europe. The studbook includes information about all births, deaths and movements of capybaras, as well as how they are related.
Capybaras are farmed for meat and skins in South America. The meat is considered unsuitable to eat in some areas, while in other areas it is considered an important source of protein. In parts of South America, especially in Venezuela, capybara meat is popular during Lent and Holy Week as the Catholic Church previously gave a special dispensation that allows for its consumption while other meats are generally forbidden.
Although it is illegal in some states, capybaras are occasionally kept as pets in the United States.

</doc>
<doc id="6777" url="http://en.wikipedia.org/wiki?curid=6777" title="Computer animation">
Computer animation

Computer animation, or CGI animation, is the process used for generating animated images by using computer graphics. The more general term "computer-generated imagery" encompasses both static scenes and dynamic images while computer animation "only" refers to moving images.
Modern computer animation usually uses 3D computer graphics, although 2D computer graphics are still used for stylistic, low bandwidth, and faster real-time renderings. Sometimes, the target of the animation is the computer itself, but sometimes the target is another medium, such as film.
Computer animation is essentially a digital successor to the stop motion techniques used in traditional animation with 3D models and frame-by-frame animation of 2D illustrations. Computer-generated animations are more controllable than other more physically based processes, such as constructing miniatures for effects shots or hiring extras for crowd scenes, and because it allows the creation of images that would not be feasible using any other technology. It can also allow a single graphic artist to produce such content without the use of actors, expensive set pieces, or props.
To create the illusion of movement, an image is displayed on the computer monitor and repeatedly replaced by a new image that is similar to it, but advanced slightly in time (usually at a rate of 24 or 30 frames/second). This technique is identical to how the illusion of movement is achieved with television and motion pictures.
For 3D animations, objects (models) are built on the computer monitor (modeled) and 3D figures are rigged with a virtual skeleton. For 2D figure animations, separate objects (illustrations) and separate transparent layers are used with or without a virtual skeleton. Then the limbs, eyes, mouth, clothes, etc. of the figure are moved by the animator on key frames. The differences in appearance between key frames are automatically calculated by the computer in a process known as tweening or morphing. Finally, the animation is rendered.
For 3D animations, all frames must be rendered after the modeling is complete. For 2D vector animations, the rendering process is the key frame illustration process, while tweened frames are rendered as needed. For pre-recorded presentations, the rendered frames are transferred to a different format or medium, such as film or digital video. The frames may also be rendered in real time as they are presented to the end-user audience. Low bandwidth animations transmitted via the internet (e.g. 2D Flash, X3D) often use software on the end-users computer to render in real time as an alternative to streaming or pre-loaded high bandwidth animations.
Explanation.
To trick the eye and brain into thinking they are seeing a smoothly moving object, the pictures should be drawn at around 12 frames per second (frame/s) or faster (a frame is one complete image). With rates above 75-120 frames/s no improvement in realism or smoothness is perceivable due to the way the eye and brain process images. At rates below 12 frame/s most people can detect jerkiness associated with the drawing of new images which detracts from the illusion of realistic movement. Conventional hand-drawn cartoon animation often uses 15 frames/s in order to save on the number of drawings needed, but this is usually accepted because of the stylized nature of cartoons. Because it produces more realistic imagery, computer animation demands higher frame rates to reinforce this realism.
Movie film seen in theaters in the United States runs at 24 frames per second, which is sufficient to create the illusion of continuous movement. For high resolution, adapters are used.
History.
Early digital computer animation was developed at Bell Telephone Laboratories in the 1960s by Edward E. Zajac, Frank W. Sinden, Kenneth C. Knowlton, and A. Michael Noll. Other digital animation was also practiced at the Lawrence Livermore National Laboratory.
An early step in the history of computer animation was the sequel to the 1973 movie "Westworld," a science-fiction film about a society in which robots live and work among humans. The sequel, "Futureworld" (1976), used the 3D wire-frame imagery, which featured a computer-animated hand and face both created by University of Utah graduates Edwin Catmull and Fred Parke. This imagery originally appeared in their student film "A Computer Animated Hand", which they completed in 1971.
Developments in CGI technologies are reported each year at SIGGRAPH, an annual conference on computer graphics and interactive techniques that is attended by thousands of computer professionals each year. Developers of computer games and 3D video cards strive to achieve the same visual quality on personal computers in real-time as is possible for CGI films and animation. With the rapid advancement of real-time rendering quality, artists began to use game engines to render non-interactive movies, which led to the art form Machinima.
The first feature-length computer animated film was "Toy Story" (1995), which was made by Pixar. It followed an adventure centered around toys and their owners. This groundbreaking film was also the first of many fully computer-animated movies.
Computer animation helped to create blockbuster films, such as "Toy Story 3" (2010), "Avatar" (2009), "Shrek 2" (2004), "Cars 2" (2011), "Life of Pi" (2012), and "Frozen" (2013).
Methods of animating virtual characters.
In most 3D computer animation systems, an animator creates a simplified representation of a character's anatomy, which is analogous to a skeleton or stick figure. The position of each segment of the skeletal model is defined by "animation variables", or Avars for short.
In human and animal characters, many parts of the skeletal model correspond to the actual bones, but skeletal animation is also used to animate other things, such as facial features (though other methods for facial animation exist). The character "Woody" in "Toy Story", for example, uses 700 Avars (100 in the face alone). The computer doesn't usually render the skeletal model directly (it is invisible), but it does use the skeletal model to compute the exact position and orientation of that certain character, which is eventually rendered into an image. Thus by changing the values of Avars over time, the animator creates motion by making the character move from frame to frame.
There are several methods for generating the Avar values to obtain realistic motion. Traditionally, animators manipulate the Avars directly. Rather than set Avars for every frame, they usually set Avars at strategic points (frames) in time and let the computer interpolate or 'tween' between them in a process called keyframing. Keyframing puts control in the hands of the animator and has roots in hand-drawn traditional animation.
In contrast, a newer method called motion capture makes use of live action footage. When computer animation is driven by motion capture, a real performer acts out the scene as if they were the character to be animated. His/her motion is recorded to a computer using video cameras and markers and that performance is then applied to the animated character.
Each method has its advantages and as of 2007, games and films are using either or both of these methods in productions. Keyframe animation can produce motions that would be difficult or impossible to act out, while motion capture can reproduce the subtleties of a particular actor. For example, in the 2006 film , actor Bill Nighy provided the performance for the character Davy Jones. Even though Nighy himself doesn't appear in the film, the movie benefited from his performance by recording the nuances of his body language, posture, facial expressions, etc. Thus motion capture is appropriate in situations where believable, realistic behavior and action is required, but the types of characters required exceed what can be done throughout the conventional costuming.
Creating characters and objects on a computer.
3D computer animation combines 3D models of objects and programmed or hand "keyframed" movement. These models are constructed out of geometrical vertices, faces, and edges in a 3D coordinate system. Objects are sculpted much like real clay or plaster, working from general forms to specific details with various sculpting tools. Unless a 3D model is intended to be a solid color, it must be painted with "textures" for realism. A bone/joint animation system is set up to deform the CGI model (e.g., to make a humanoid model walk). In a process known as rigging, the virtual marionette is given various controllers and handles for controlling movement. Animation data can be created using motion capture, or keyframing by a human animator, or a combination of the two.
3D models rigged for animation may contain thousands of control points – for example, "Woody" in Pixar's "Toy Story" uses 700 specialized animation controllers. Rhythm and Hues Studios labored for two years to create Aslan in the movie "", which had about 1,851 controllers (742 in the face alone). In the 2004 film "The Day After Tomorrow", designers had to design forces of extreme weather with the help of video references and accurate meteorological facts. For the 2005 remake of "King Kong", actor Andy Serkis was used to help designers pinpoint the gorilla's prime location in the shots and used his expressions to model "human" characteristics onto the creature. Serkis had earlier provided the voice and performance for Gollum in J. R. R. Tolkien's "The Lord of the Rings" trilogy.
Computer animation development equipment.
Computer animation can be created with a computer and an animation software. Some impressive animation can be achieved even with basic programs; however, the rendering can take a lot of time on an ordinary home computer. Because of this, video game animators tend to use low resolution and low polygon count renders so that the graphics can be rendered in real time on a home computer. Photorealistic animation would be impractical in this context.
Professional animators of movies, television, and video sequences on computer games make photorealistic animation with high detail. This level of quality for movie animation would take hundreds of years to create on a home computer. Instead, many powerful workstation computers are used. Graphics workstation computers use two-four processors, and they are a lot more powerful than an actual home computer and they are specialized for rendering. A large number of workstations (known as a render farm) are networked together to effectively act as a giant computer. The result is a computer-animated movie that can be completed in about one to five years (however, this process is not composed solely of rendering). A workstation typically costs $2,000-16,000 with the more expensive stations being able to render much faster due to the more technologically advanced hardware that they contain. Professionals also use digital movie cameras, motion or performance capture, bluescreens, film editing software, props, and other tools used for movie animation.
Modeling human faces.
The realistic modeling of human facial features is both one of the most challenging and sought after elements in computer-generated imagery. Computer facial animation is a highly complex field where models typically include a very large number of animation variables. Historically speaking, the first SIGGRAPH tutorials on "State of the art in Facial Animation" in 1989 and 1990 proved to be a turning point in the field by bringing together and consolidating multiple research elements and sparked interest among a number of researchers.
The Facial Action Coding System (with 46 "action units", such as "lip bite" or "squint"), which had been developed in 1976, became a popular basis for many systems. As early as 2001, MPEG-4 included 68 Face Animation Parameters (FAPs) for lips, jaws, etc., and the field has made significant progress since then and the use of facial microexpression has increased.
In some cases, an affective space, such as the PAD emotional state model, can be used to assign specific emotions to the faces of avatars. In this approach, the PAD model is used as a high level emotional space and the lower level space is the MPEG-4 Facial Animation Parameters (FAP). A mid-level Partial Expression Parameters (PEP) space is then used to in a two level structure – the PAD-PEP mapping and the PEP-FAP translation model.
Realism in the future of computer animation.
Realism in computer animation can mean making each frame look photorealistic, in the sense that the scene is rendered to resemble a photograph, or to making the animation of characters believable and lifelike. Computer animation can also be realistic with or without the photorealistic rendering.
One of the greatest challenges in computer animation has been creating human characters that look and move with the highest degree of realism. Many animated films instead feature characters who are anthropomorphic animals ("Finding Nemo", "Ice Age", "Bolt", "Madagascar", "Over the Hedge", "Rio", "Kung Fu Panda", "Alpha and Omega"), machines ("Cars", "WALL-E", "Robots"), insects ("Antz", "A Bug's Life", "The Ant Bully", "Bee Movie"), fantasy creatures and characters ("Monsters, Inc.", "Shrek", "TMNT", "Brave", "Epic"), or humans with nonrealistic cartoon-like proportions ("The Incredibles", "Despicable Me", "Up", "Megamind", "", "Planet 51", "Hotel Transylvania", "Team Fortress 2").
Part of the difficulty in making pleasing, realistic human characters is the uncanny valley, the concept where (up to a point) the human audience tends to have an increasingly negative emotional response as a human replica looks and acts more and more human. Also, some materials that commonly appear in a scene such as cloth, foliage, fluids, and hair have proven more difficult to faithfully recreate and animate than others. Consequently, special software and techniques have been developed to better simulate these specific elements. 
In theory, realistic computer animation can reach a point where it is indistinguishable from real action captured on film. When computer animation achieves this level of realism, it may have major repercussions for the film industry.
The goal of computer animation is not always to emulate live action as closely as possible. For example animation was used in Nautilus Productions the "Mystery Mardi Gras Shipwreck" documentary to model a remotely operated underwater vehicle (ROV) and the "Mardi Gras" archaeological site in 4,000 feet (1220 meters) of water in the Gulf of Mexico. Computer animation can also be tailored to mimic or substitute for other types of animation, such as traditional stop-motion animation (as shown in "Flushed Away" or "The Lego Movie"). Some of the long-standing basic principles of animation, like squash & stretch, call for movement that is not strictly realistic, and such principles still see widespread application in computer animation.
Movies.
CGI short films have been produced as independent animation since 1976, although the popularity of computer animation (especially in the field of special effects) skyrocketed during the modern era of U.S. animation. The first completely computer-animated television series was "ReBoot" in 1994, and the first completely computer-animated movie was "Toy Story" (1995).
Amateur animation.
The popularity of websites that allow members to upload their own movies for others to view has created a growing community of amateur computer animators. With utilities and programs often included free with modern operating systems, many users can make their own animated movies and shorts. Several free and open source animation software applications exist as well. A popular amateur approach to animation is via the animated GIF format, which can be uploaded and seen on the web easily.
Detailed examples and pseudocode.
In 2D computer animation, moving objects are often referred to as “sprites.” A sprite is an image that has a location associated with it. The location of the sprite is changed slightly, between each displayed frame, to make the sprite appear to move. The following pseudocode makes a sprite move from left to right:
 var "int" x := 0, y := screenHeight / 2;
 while x < screenWidth
 drawBackground()
 drawSpriteAtXY (x, y) "// draw on top of the background"
 x := x + 5 "// move to the right"
Computer animation uses different techniques to produce animations. Most frequently, sophisticated mathematics is used to manipulate complex three-dimensional polygons, apply “textures”, lighting and other effects to the polygons and finally rendering the complete image. A sophisticated graphical user interface may be used to create the animation and arrange its choreography. Another technique called constructive solid geometry defines objects by conducting boolean operations on regular shapes, and has the advantage that animations may be accurately produced at any resolution.
Let's step through the rendering of a simple image of a room with flat wood walls with a grey pyramid in the center of the room. The pyramid will have a spotlight shining on it. Each wall, the floor and the ceiling is a simple polygon, in this case, a rectangle. Each corner of the rectangles is defined by three values referred to as X, Y and Z. X is how far left and right the point is. Y is how far up and down the point is, and Z is far in and out of the screen the point is. The wall nearest us would be defined by four points: (in the order x, y, z). Below is a representation of how the wall is defined
The far wall would be:
The pyramid is made up of five polygons: the rectangular base, and four triangular sides. To draw this image the computer uses math to calculate how to project this image, defined by three-dimensional data, onto a two-dimensional computer screen.
First we must also define where our view point is, that is, from what vantage point will the scene be drawn. Our view point is inside the room a bit above the floor, directly in front of the pyramid. First the computer will calculate which polygons are visible. The near wall will not be displayed at all, as it is behind our view point. The far side of the pyramid will also not be drawn as it is hidden by the front of the pyramid.
Next each point is perspective projected onto the screen. The portions of the walls ‘farthest’ from the view point will appear to be shorter than the nearer areas due to perspective. To make the walls look like wood, a wood pattern, called a texture, will be drawn on them. To accomplish this, a technique called “texture mapping” is often used. A small drawing of wood that can be repeatedly drawn in a matching tiled pattern (like desktop wallpaper) is stretched and drawn onto the walls' final shape. The pyramid is solid grey so its surfaces can just be rendered as grey. But we also have a spotlight. Where its light falls we lighten colors, where objects blocks the light we darken colors.
Next we render the complete scene on the computer screen. If the numbers describing the position of the pyramid were changed and this process repeated, the pyramid would appear to move.
Computer-assisted vs computer-generated animation.
"To animate means "to give life to" and there are two basic ways that animators commonly do this."
Computer-assisted animation is usually classed as two-dimensional (2D) animation. Creators drawings either hand drawn (pencil to paper) or interactively drawn(drawn on the computer) using different assisting appliances and are positioned into specific software packages. Within the software package the creator will place drawings into different key frames which fundamentally create an outline of the most important movements. The computer will then fill in all the " in-between frames", commonly known as Tweening. Computer-assisted animation is basically using new technologies to cut down the time scale that traditional animation could take, but still having the elements of traditional drawings of characters or objects.
Two examples of films using computer-assisted animation are "Beauty and the Beast" and "Antz".
Computer-generated animation is known as 3-dimensional (3D) animation. Creators will design an object or character with an X,Y and Z axis. Unlike the traditional way of animation no pencil to paper drawings create the way computer generated animation works. The object or character created will then be taken into a software, key framing and tweening are also carried out in computer generated animation but are also a lot of techniques used that do not relate to traditional animation. Animators can break physical laws by using mathematical algorithms to cheat, mass, force and gravity rulings. Fundamentally, time scale and quality could be said to be a preferred way to produce animation as they are two major things that are enhanced by using computer generated animation. Another great aspect of CGA is the fact you can create a flock of creatures to act independently when created as a group. An animal's fur can be programmed to wave in the wind and lie flat when it rains instead of programming each strand of hair separately.
Three examples of computer-generated animation movies are "Toy Story", "The Incredibles" and "Shrek".

</doc>
<doc id="6778" url="http://en.wikipedia.org/wiki?curid=6778" title="Ceawlin of Wessex">
Ceawlin of Wessex

Ceawlin (also spelled Ceaulin and Caelin, died "ca." 593) was a King of Wessex. He may have been the son of Cynric of Wessex and the grandson of Cerdic of Wessex, whom the "Anglo-Saxon Chronicle" represents as the leader of the first group of Saxons to come to the land which later became Wessex. Ceawlin was active during the last years of the Anglo-Saxon invasion, with little of southern England remaining in the control of the native Britons by the time of his death.
The chronology of Ceawlin's life is highly uncertain. The historical accuracy and dating of many of the events in the later "Anglo-Saxon Chronicle" have been called into question, and his reign is variously listed as lasting seven, seventeen, or thirty-two years. The "Chronicle" records several battles of Ceawlin's between the years 556 and 592, including the first record of a battle between different groups of Anglo-Saxons, and indicates that under Ceawlin Wessex acquired significant territory, some of which was later to be lost to other Anglo-Saxon kingdoms. Ceawlin is also named as one of the eight ""bretwaldas", a title given in the "Chronicle" to eight rulers who had overlordship over southern Britain, although the extent of Ceawlin's control is not known.
Ceawlin died in 593, having been deposed the year before, possibly by his successor, Ceol. He is recorded in various sources as having two sons, Cutha and Cuthwine, but the genealogies in which this information is found are known to be unreliable.
Historical context.
The history of the sub-Roman period in Britain is poorly sourced and the subject of a number of important disagreements among historians. It appears, however, that in the fifth century raids on Britain by continental peoples developed into migrations. The newcomers included Angles, Saxons, Jutes, and Frisians. These peoples captured territory in the east and south of England, but at about the end of the fifth century, a British victory at the battle of Mons Badonicus halted the Anglo-Saxon advance for fifty years. Near the year 550, however, the British began to lose ground once more, and within twenty-five years, it appears that control of almost all of southern England was in the hands of the invaders.
The peace following the battle of Mons Badonicus is attested partly by Gildas, a monk, who wrote "De Excidio et Conquestu Britanniae" or "On the Ruin and Conquest of Britain" during the middle of the sixth century. This essay is a polemic against corruption and Gildas provides little in the way of names and dates. He appears, however, to state that peace had lasted from the year of his birth to the time he was writing. The "Anglo-Saxon Chronicle" is the other main source that bears on this period, in particular in an entry for the year 827 that records a list of the kings who bore the title "bretwalda"", or "Britain-ruler". That list shows a gap in the early sixth century that matches Gildas's version of events.
Ceawlin's reign belongs to the period of Anglo-Saxon expansion at the end of the sixth century. Though there are many unanswered questions about the chronology and activities of the early West Saxon rulers, it is clear that Ceawlin was one of the key figures in the final Anglo-Saxon conquest of southern Britain.
Early West Saxon sources.
The two main written sources for early West Saxon history are the "Anglo-Saxon Chronicle" and the West Saxon Genealogical Regnal List. The "Chronicle" is a set of annals which were compiled near the year 890, during the reign of King Alfred the Great of Wessex. They record earlier material for the older entries, which were assembled from earlier annals that no longer survive, as well as, from saga material that might have been transmitted orally. The "Chronicle" dates the arrival of the future "West Saxons" in Britain to 495, when Cerdic and his son, Cynric, land at "Cerdices ora", or Cerdic's shore. Almost twenty annals describing Cerdic's campaigns, and those of his descendants appear interspersed through the next hundred years of entries in the "Chronicle". Although these annals provide most of what is known about Ceawlin, the historicity of many of the entries is uncertain.
The West Saxon Genealogical Regnal List is a list of rulers of Wessex, including the lengths of their reigns. It survives in several forms, including as a preface to the [B] manuscript of the "Chronicle". As with "Chronicle", the list was compiled during the reign of Alfred the Great, and both the list and the "Chronicle" are influenced by the desire of their writers to use a single line of descent to trace the lineage of the Kings of Wessex through Cerdic to Gewis, a descendant of Woden and the legendary ancestor of the West Saxons. The result served the political purposes of the scribe, but is riddled with contradictions for historians.
The contradictions may be seen clearly by calculating dates by different methods from the various sources. The first event in West Saxon history, the date of which can be regarded as reasonably certain, is the baptism of Cynegils, which occurred in the late 630s, perhaps as late as 640. The "Chronicle" dates Cerdic's arrival to 495, but adding up the lengths of the reigns as given in the West Saxon Genealogical Regnal List, leads to the conclusion that Cerdic's reign might have started in 532, a difference of 37 years. Neither 495 nor 532 may be treated as reliable, however, the latter date relies on the presumption that the Regnal List is correct in presenting the Kings of Wessex as having succeeded one another, with no omitted kings, no joint kingships, and that the durations of the reigns are correct as given. None of these presumptions may be made safely.
The sources also are inconsistent on the length of Ceawlin's reign. The "Chronicle" gives it as thirty-two years, from 560 to 592, but the Regnal Lists disagree: different versions give it as seven or seventeen years. A recent detailed study of the Regnal List dates the arrival of the West Saxons in England to 538, and favours seven years as the most likely length of Ceawlin's reign, with dates of 581–588 proposed. The sources do agree that Ceawlin is the son of Cynric and he usually is named as the father of Cuthwine. There is one discrepancy in this case: the entry for 685 in the [A] version of the "Chronicle" assigns Ceawlin a son, Cutha, but in the 855 entry in the same manuscript, Cutha is listed as the son of Cuthwine. Cutha also is named as Ceawlin's brother in the [E] and [F] versions of the "Chronicle", in the 571 and 568 entries, respectively.
Whether Ceawlin is a descendant of Cerdic is a matter of debate. Subgroupings of different West Saxon lineages give the impression of separate groups, of which Ceawlin's line is one. Some of the problems in the Wessex genealogies may have come about because of efforts to integrate Ceawlin's line with the other lineages: it was very important to the West Saxons to be able to trace their ancestors back to Cerdic. Another reason for doubting the literal nature of these early genealogies is that the etymology of the names of several early members of the dynasty do not appear to be Germanic, as would be expected in the names of leaders of an apparently Anglo-Saxon dynasty. The name Ceawlin is one of the names that do not have convincing Anglo-Saxon etymologies; it seems more likely to be of native British origin.
The earliest sources do not use the term "West Saxon". According to Bede's "Ecclesiastical History of the English People", the term is interchangeable with the Gewisse, meaning the descendants of Gewis. The term "West Saxon" appears only in the late seventh century, after the reign of Cædwalla.
West Saxon expansion.
Ultimately, the kingdom of Wessex occupied the southwest of England, but the initial stages in this expansion are not apparent from the sources. Cerdic's landing, whenever it is to be dated, seems to have been near the Isle of Wight, and the annals record the conquest of the island in 530. In 534, according to the "Chronicle", Cerdic died and his son Cynric took the throne; the "Chronicle" adds that "they gave the Isle of Wight to their nephews, Stuf and Wihtgar". These records are in direct conflict with Bede, who states that the Isle of Wight was settled by Jutes, not Saxons; the archaeological record is somewhat in favour of Bede on this.
Subsequent entries in the "Chronicle" give details of some of the battles by which the West Saxons won their kingdom. Ceawlin's campaigns are not given as near the coast. They range along the Thames valley and beyond, as far as Surrey in the east and the mouth of the Severn in the west. Ceawlin clearly is part of the West Saxon expansion, but the military history of the period is difficult to understand. In what follows the dates are as given in the "Chronicle", although as noted above, these are earlier than now thought accurate.
556: Beran byrg.
The first record of a battle fought by Ceawlin is in 556, when he and his father, Cynric, fought the native Britons at "Beran byrg", or Bera's Stronghold. This now is identified as Barbury Castle, an Iron Age hill fort in Wiltshire, near Swindon. Cynric would have been king of Wessex at this time.
568: Wibbandun.
The first battle Ceawlin fought as king is dated by the "Chronicle" to 568, when he and Cutha fought with Æthelberht, the king of Kent. The entry says "Here Ceawlin and Cutha fought against Aethelberht and drove him into Kent; and they killed two ealdormen, Oslaf and Cnebba, on Wibbandun." The location of "Wibbandun", which can be translated as "Wibba's Mount", has not been identified definitely; it was at one time thought to be Wimbledon, but this now is known to be incorrect. This battle is notable as the first recorded conflict between the invading peoples: previous battles recorded in the "Chronicle" are between the Anglo-Saxons and the native Britons.
There are multiple examples of joint kingship in Anglo-Saxon history, and this may be another: it is not clear what Cutha's relationship to Ceawlin is, but it certainly is possible he was also a king. The annal for 577, below, is another possible example.
571: Bedcanford.
The annal for 571 reads: "Here Cuthwulf fought against the Britons at Bedcanford, and took four settlements: Limbury and Aylesbury, Benson and Eynsham; and in the same year he passed away." Cuthwulf's relationship with Ceawlin is unknown, but the alliteration common to Anglo-Saxon royal families suggests Cuthwulf may be part of the West Saxon royal line. The location of the battle itself is unidentified. It has been suggested that it was Bedford, but what is known of the early history of Bedford's names, does not support this. This battle is of interest because it is surprising that an area so far east should still be in Briton hands this late: there is ample archaeological evidence of early Saxon and Anglian presence in the Midlands, and historians generally have interpreted Gildas's "De Excidio" as implying that the Britons had lost control of this area by the mid-sixth century. One possible explanation is, that this annal records a reconquest of land that was lost to the Britons in the campaigns ending in the battle of Mons Badonicus.
577: The lower Severn.
The annal for 577 reads "Here Cuthwine and Ceawlin fought against the Britons, and they killed three kings, Coinmail and Condidan and Farinmail, in the place which is called Dyrham, and took three cities: Gloucester and Cirencester and Bath." This entry is all that is known of these Briton kings; their names are in an archaic form that makes it very likely that this annal derives from a much older written source. The battle itself has long been regarded as a key moment in the Saxon advance, since in reaching the Bristol Channel, the West Saxons divided the Britons west of the Severn from land communication with those in the peninsula to the south of the Channel. Wessex almost certainly lost this territory to Penda of Mercia in 628, when the "Chronicle" records that "Cynegils and Cwichelm fought against Penda at Cirencester and then came to an agreement."
It is possible that when Ceawlin and Cuthwine took Bath, they found the Roman baths still operating to some extent. Nennius, a ninth-century historian, mentions a "Hot Lake" in the land of the Hwicce, which was along the Severn, and adds "It is surrounded by a wall, made of brick and stone, and men may go there to bathe at any time, and every man can have the kind of bath he likes. If he wants, it will be a cold bath; and if he wants a hot bath, it will be hot". Bede also describes hot baths in the geographical introduction to the "Ecclesiastical History" in terms very similar to those of Nennius.
Wansdyke, an early medieval defensive linear earthwork, runs from south of Bristol to near Marlborough, Wiltshire, passing not far from Bath. It probably was built in the fifth or sixth centuries, perhaps by Ceawlin.
584: Fethan leag.
Ceawlin's last recorded victory is in 584. The entry reads "Here Ceawlin and Cutha fought against the Britons at the place which is named Fethan leag, and Cutha was killed; and Ceawlin took many towns and countless war-loot, and in anger he turned back to his own [territory]." There is a wood named "Fethelée" mentioned in a twelfth-century document that relates to Stoke Lyne, in Oxfordshire, and it now is thought that the battle of Fethan leag must have been fought in this area.
The phrase "in anger he turned back to his own" probably indicates that this annal is drawn from saga material, as perhaps are all of the early Wessex annals. It also has been used to argue that perhaps, Ceawlin did not win the battle and that the chronicler chose not to record the outcome fully – a king does not usually come home "in anger" after taking "many towns and countless war-loot". It may be that Ceawlin's overlordship of the southern Britons came to an end with this battle.
Bretwaldaship.
About 731, Bede, a Northumbrian monk and chronicler, wrote a work called the "Ecclesiastical History of the English People". The work was not primarily a secular history, but Bede provides much information about the history of the Anglo-Saxons, including a list early in the history of seven kings who, he said, held "imperium" over the other kingdoms south of the Humber. The usual translation for "imperium" is "overlordship". Bede names Ceawlin as the second on the list, although he spells it "Caelin", and adds that he was "known in the speech of his own people as Ceaulin". Bede also makes it clear that Ceawlin was not a Christian—Bede mentions a later king, Æthelberht of Kent, as "the first to enter the kingdom of heaven".
The "Anglo-Saxon Chronicle," in an entry for the year 827, repeats Bede's list, adds Egbert of Wessex, and also mentions that they were known as "bretwalda", or "Britain-ruler". A great deal of scholarly attention has been given to the meaning of this word. It has been described as a term "of encomiastic poetry", but there also is evidence that it implied a definite role of military leadership.
Bede says that these kings had authority "south of the Humber", but the span of control, at least of the earlier bretwaldas, likely was less than this. In Ceawlin's case the range of control is hard to determine accurately, but Bede's inclusion of Ceawlin in the list of kings who held "imperium", and the list of battles he is recorded as having won, indicate an energetic and successful leader who, from a base in the upper Thames valley, dominated much of the surrounding area and held overlordship over the southern Britons for some period. Despite Ceawlin's military successes, the northern conquests he made could not always be retained: Mercia took much of the upper Thames valley, and the north-eastern towns won in 571 were among territory subsequently under the control of Kent and Mercia at different times.
Bede's concept of the power of these overlords also must be regarded as the product of his eighth-century viewpoint. When the "Ecclesiastical History" was written, Æthelbald of Mercia dominated the English south of the Humber, and Bede's view of the earlier kings was doubtless strongly coloured by the state of England at that time. For the earlier "bretwaldas", such as Ælle and Ceawlin, there must be some element of anachronism in Bede's description. It also is possible that Bede only meant to refer to power over Anglo-Saxon kingdoms, not the native Britons.
Ceawlin is the second king in Bede's list. All the subsequent bretwaldas followed more or less consecutively, but there is a long gap, perhaps fifty years, between Ælle of Sussex, the first bretwalda, and Ceawlin. The lack of gaps between the overlordships of the later bretwaldas has been used to make an argument for Ceawlin's dates matching the later entries in the "Chronicle" with reasonable accuracy. According to this analysis, the next bretwalda, Æthelberht of Kent, must have been already a dominant king by the time Pope Gregory the Great wrote to him in 601, since Gregory would have not written to an underking. Ceawlin defeated Æthelberht in 568 according to the "Chronicle". Æthelberht's dates are a matter of debate, but recent scholarly consensus has his reign starting no earlier than 580. The 568 date for the battle at Wibbandun is thought to be unlikely because of the assertion in various versions of the West Saxon Genealogical Regnal List that Ceawlin's reign lasted either seven or seventeen years. If this battle is placed near the year 590, before Æthelberht has established himself as a powerful king, then the subsequent annals relating to Ceawlin's defeat and death may be reasonably close to the correct date. In any case, the battle with Æthelberht is unlikely to have been more than a few years on either side of 590. The gap between Ælle and Ceawlin, on the other hand, has been taken as supporting evidence for the story told by Gildas in "De Excidio" of a peace lasting a generation or more following a Briton victory at Mons Badonicus.
Æthelberht of Kent succeeds Ceawlin on the list of bretwaldas, but the reigns may overlap somewhat: recent evaluations give Ceawlin a likely reign of 581–588, and place Æthelberht's accession near to the year 589, but these analyses are no more than scholarly guesses. Ceawlin's eclipse in 592, probably by Ceol, may have been the occasion for Æthelberht to rise to prominence; Æthelberht very likely was the dominant Anglo-Saxon king by 597. Æthelberht's rise may have been earlier: the 584 annal, even if it records a victory, is the last victory of Ceawlin's in the "Chronicle", and the period after that may have been one of Æthelberht's ascent and Ceawlin's decline.
Wessex at Ceawlin's death.
Ceawlin lost the throne of Wessex in 592. The annal for that year reads, in part: "Here there was great slaughter at Woden's Barrow, and Ceawlin was driven out." Woden's Barrow is a tumulus, now called Adam's Grave, at Alton Priors, Wiltshire. No details of his opponent are given. The medieval chronicler William of Malmesbury, writing in about 1120, says that it was "the Angles and the British conspiring together", Alternatively, it may have been Ceol, who is supposed to have been the next king of Wessex, ruling for six years according to the West Saxon Genealogical Regnal List. According to the "Anglo-Saxon Chronicle", Ceawlin died the following year. The relevant part of the annal reads: "Here Ceawlin and Cwichelm and Crida perished." Nothing more is known of Cwichelm and Crida, although they may have been members of the Wessex royal house – their names fit the alliterative pattern common to royal houses of the time.
According to the Regnal List, Ceol was a son of Cutha, who was a son of Cynric; and Ceolwulf, his brother, reigned for seventeen years after him. It is possible that some fragmentation of control among the West Saxons occurred at Ceawlin's death: Ceol and Ceolwulf may have been based in Wiltshire, as opposed to the upper Thames valley. This split also may have contributed to Æthelberht's ability to rise to dominance in southern England. The West Saxons remained influential in military terms, however: the "Chronicle" and Bede record continued military activity against Essex and Sussex within twenty or thirty years of Ceawlin's death.

</doc>
<doc id="6779" url="http://en.wikipedia.org/wiki?curid=6779" title="Christchurch (disambiguation)">
Christchurch (disambiguation)

Christchurch is the third largest urban area in New Zealand. 
Christchurch may also refer to:

</doc>
<doc id="6780" url="http://en.wikipedia.org/wiki?curid=6780" title="CD-R">
CD-R

CDR
CD-R (Compact Disc-Recordable) is a digital optical disc storage format. A CD-R disc is a compact disc that can be written once and read arbitrarily many times.
CD-R disks (CD-Rs) are readable by most plain CD readers, i.e., CD readers manufactured prior to the introduction of CD-R. This is an advantage over CD-RW, which can be re-written but cannot be played on many plain CD readers.
History.
The CD-x, originally named CD Write-Once (WO), specification was first published in 1988 by Philips and Sony in the 'Orange Book'. The Orange Book consists of several parts, furnishing details of the CD-WO, CD-MO (Magneto-Optic), and CD-RW (ReWritable). The latest editions have abandoned the use of the term "CD-WO" in favor of "CD-R", while "CD-MO" were used very little. Written CD-Rs and CD-RWs are, in the aspect of low-level encoding and data format, fully compatible with the audio CD ("Red Book" CD-DA) and data CD ("Yellow Book" CD-ROM) standards. (Note that the Yellow Book standard for CD-ROM only specifies a high-level data format and refers to the Red Book for all physical format and low-level code details, such as track pitch, linear bit density, and bitstream encoding.) This means they use Eight-to-Fourteen Modulation, CIRC error correction, and, for CD-ROM, the third error correction layer defined in the Yellow Book. Properly written CD-R discs on blanks of less than 80 minutes length are fully compatible with the audio CD and CD-ROM standards in all details including physical specifications. 80 minute CD-R discs marginally violate the Red Book physical format specifications, and longer discs are noncompliant. CD-RW discs have lower reflectivity than CD-R or pressed (non-writable) CDs and for this reason cannot meet the Red Book standard (or come close). Some hardware compatible with Red Book CDs may have difficulty reading CD-Rs and, because of their lower reflectivity, especially CD-RWs. To the extent that CD hardware can read extended-length discs or CD-RW discs, it is because that hardware has capability beyond the minimum required by the Red Book and Yellow Book standards (the hardware is more capable than it needs to be to bear the Compact Disc logo).
CD-R recording systems available in 1990 were similar to the washing machine-sized Meridian CD Publisher, based on the two-piece rack mount Yamaha PDS audio recorder costing $35,000, not including the required external ECC circuitry for data encoding, SCSI hard drive subsystem, and MS-DOS control computer. By 1992, the cost of typical recorders was down to $10–12,000, and in September 1995, Hewlett-Packard introduced its model 4020i manufactured by Philips, which, at $995, was the first recorder to cost less than $1000.
The dye materials developed by Taiyo Yuden made it possible for CD-R discs to be compatible with Audio CD and CD-ROM discs.
Initially, in the United States, there was a market separation between "music" CD-Rs and "data" CD-Rs, the former being several times more expensive than the latter due to industry copyright arrangements with the RIAA. Physically, there is no difference between the discs save for the Disc Application Flag that identifies their type: standalone audio recorders will only accept "music" CD-Rs to enforce the RIAA arrangement, while computer CD-R drives can use either type of media to burn either type of content.
Physical characteristics.
A standard CD-R is a 1.2 mm thick disc made of polycarbonate about 120 mm (4.7 in) or 80 mm (3.150 in) diameter. The 120 mm disc has a storage capacity of 74 minutes of audio or 650 Megabytes of data. CD-R/RWs are available with capacities of 80 minutes of audio or 737,280,000 bytes (700 MiB), which they achieve by molding the disc at the tightest allowable tolerances specified in the Orange Book CD-R/CD-RW standards. The engineering margin that was reserved for manufacturing tolerance has been used for data capacity instead, leaving no tolerance for manufacturing; for these discs to be truly compliant with the Orange Book standard, the manufacturing process must be perfect .
Despite the foregoing, most CD-Rs on the market have an 80 minute capacity. There are also 90 minute/790 MiB and 99 minute/870 MiB discs, although they are less common (and depart from the Orange Book standard outright). Also, due to the limitations of the data structures in the ATIP (see below), 90 and 99 minute blanks will identify as 80 minute ones. (As the ATIP is part of the Orange Book standard, it is natural that its design does not support some nonstandard disc configurations.) Therefore, in order to use the additional capacity, these discs have to be burned using "overburn" options in the CD recording software. (Overburning itself is so named because it is outside the written standards, but, due to market demand, it has nonetheless become a de facto standard function in most CD writing drives and software for them.)
Some drives use special techniques, such as Plextor's GigaRec or Sanyo's HD-BURN, to write more data onto a given disc; these techniques are inherently deviations from the Compact Disc (Red, Yellow, and/or Orange Book) standards, making the recorded discs proprietary-formatted and not fully compatible with standard CD players and drives. However, in certain applications where discs will not be distributed or exchanged outside a private group and will not be archived for a long time, a proprietary format may be an acceptable way to obtain greater capacity (up to 1.2 GiB with GigaRec or 1.8 GiB with HD-BURN on 99 minute media). The greatest risk in using such a proprietary data storage format, assuming that it works reliably as designed, is that it may be difficult or impossible to repair or replace the hardware used to read the media if it fails, is damaged, or is lost after its original vendor discontinues it.
Nothing in the Red, Yellow or Orange Book standards prohibits disc reading/writing devices from having the capacity to read or write discs beyond the Compact Disc standards. The standards do require discs to meet precise requirements in order to be called Compact Discs, but the other discs may be called by other names; if this were not true, no DVD drive could legally bear the Compact Disc logo. While disc players and drives may have capabilities beyond the standards, enabling them to read and write nonstandard discs, there is no assurance, in the absence of explicit additional manufacturer specifications beyond normal Compact Disc logo certification, that any particular player or drive will perform beyond the standards at all or consistently. Furthermore, if the same device with no explicit performance specs beyond the Compact Disc logo initially handles nonstandard discs reliably, there is no assurance that it will not later stop doing so, and in that case, there is no assurance that it can be made to do so again by service or adjustment. Therefore, discs with capacities larger than 650 MB, and especially those larger than 700 MB, are less interchangeable among players/drives than standard discs and are not very suitable for archival use, as their readability on future equipment, or even on the same equipment at a future time, is not assured, even under the assumption that the discs will not degrade at all.
The polycarbonate disc contains a spiral groove, called the "pregroove" (because it is molded in before data are written to the disc), to guide the laser beam upon writing and reading information. The pregroove is molded into the top side of the polycarbonate disc, where the pits and lands would be molded if it were a pressed (nonrecordable) Red Book CD; the bottom side, which faces the laser beam in the player or drive, is flat and smooth. The polycarbonate disc is coated on the pregroove side with a very thin layer of organic dye. Then, on top of the dye is coated a thin, reflecting layer of silver, a silver alloy, or gold. Finally, a protective coating of a photo-polymerizable lacquer is applied on top of the metal reflector and cured with UV-light.
A blank CD-R is not "empty"; the pregroove has a wobble (the ATIP), which helps the writing laser to stay on track and to write the data to the disc at a constant rate. Maintaining a constant rate is essential to ensure proper size and spacing of the pits and lands burned into the dye layer. As well as providing timing information, the ATIP (absolute time in pregroove) is also a data track containing information about the CD-R manufacturer, the dye used and media information (disc length and so on). The pregroove is not destroyed when the data are written to the CD-R, a point which some copy protection schemes use to distinguish copies from an original CD.
There are three basic formulations of dye used in CD-Rs:
There are many hybrid variations of the dye formulations, such as Formazan by Kodak (a hybrid of cyanine and phthalocyanine).
Unfortunately, many manufacturers have added additional coloring to disguise their unstable cyanine CD-Rs in the past, so the formulation of a disc cannot be determined based purely on its color. Similarly, a gold reflective layer does not guarantee use of phthalocyanine dye. The quality of the disc is also not only dependent on the dye used, it is also influenced by sealing, the top layer, the reflective layer, and the polycarbonate. Simply choosing a disc based on its dye type may be problematic. Furthermore, correct power calibration of the laser in the writer, as well as correct timing of the laser pulses, stable disc speed, and so on, is critical to not only the immediate readability but the longevity of the recorded disc, so for archiving it is important to have not only a high quality disc but a high quality writer. In fact, a high quality writer may produce adequate results with medium quality media, but high quality media cannot compensate for a mediocre writer, and discs written by such a writer cannot achieve their maximum potential archival lifetime.
Speed.
These times only include the actual optical writing pass over the disc. For most disc recording operations, additional time is used for overhead processes, such as organizing the files and tracks, which adds to the theoretical minimum total time required to produce a disc. (An exception might be making a disk from a prepared ISO image, for which the overhead would likely be trivial.) At the lowest write speeds, this overhead takes so much less time than the actual disc writing pass that it may be negligible, but at higher write speeds, the overhead time becomes a larger proportion of the overall time taken to produce a finished disc and may add significantly to it.
Also, above 20× speed, drives use a Zoned-CLV or CAV strategy, where the advertised maximum speed is only reached near the outer rim of the disc. This is not taken into account by the above table. (If this were not done, the faster rotation that would be required at the inner tracks could cause the disc to fracture and/or could cause excessive vibration which would make accurate and successful writing impossible.)
Writing methods.
The blank disc has a pre-groove track onto which the data are written. The pre-groove track, which also contains timing information, ensures that the recorder follows the same spiral
path as a conventional CD. A CD recorder writes data to a CD-R disc by pulsing its laser to heat areas of the organic dye layer. The writing process does not produce indentations (pits); instead, the heat permanently changes the optical properties of the dye, changing the reflectivity of those areas. Using a low laser power, so as not to further alter the dye, the disc is read back in the same way as a CD-ROM. However, the reflected light is modulated not by pits, but by the alternating regions of heated and unaltered dye. The change of the intensity of the reflected laser radiation is transformed into an electrical signal, from which the digital information is recovered ("decoded"). Once a section of a CD-R is written, it cannot be erased or rewritten, unlike a CD-RW. A CD-R can be recorded in multiple sessions.
A CD recorder can write to a CD-R using several methods including:
With careful examination, the written and unwritten areas can be distinguished by the naked eye. CD-Rs are written from the center outwards, so the written area appears as an inner band with slightly different shading.
Lifespan.
Real-life (not accelerated aging) tests have revealed that some CD-Rs degrade quickly even if stored normally. The quality of a CD-R disc has a large and direct influence on longevity—low quality discs should not be expected to last very long. According to research conducted by J. Perdereau, CD-Rs are expected to have an average life expectancy of 10 years. Branding isn't a reliable guide to quality, because many brands (major as well as no name) do not manufacture their own discs. Instead they are sourced from different manufacturers of varying quality. For best results, the actual manufacturer and material components of each batch of discs should be verified.
Burned CD-Rs suffer from material degradation, just like most writable media. CD-R media have an internal layer of dye used to store data. In a CD-RW disc, the recording layer is made of an alloy of silver and other metals—indium, antimony, and tellurium. In CD-R media, the dye itself can degrade, causing data to become unreadable.
As well as degradation of the dye, failure of a CD-R can be due to the reflective surface. While silver is less expensive and more widely used, it is more prone to oxidation resulting in a non-reflecting surface. Gold on the other hand, although more expensive and no longer widely used, is an inert material, so gold-based CD-Rs do not suffer from this problem. Manufacturers have estimated that the longevity of gold-based CD-Rs to be as high as 100 years.
Labeling.
It is recommended if using adhesive-backed paper labels that the labels be specially made for CD-Rs. A balanced CD vibrates only slightly when rotated at high speed. Bad or improperly made labels, or labels applied off-center, unbalance the CD and can cause it to vibrate when it spins, which causes read errors and even risks damaging the drive.
A professional alternative to CD labels is pre-printed CDs using a 5-color silkscreen or offset press. Using a permanent marker pen is also a common practice. However, solvents from such pens can affect the dye layer.
Disposal.
Data confidentiality.
Since CD-Rs in general cannot be logically erased to any degree, the disposal of CD-Rs presents a possible security issue if they contain sensitive / private data. Destroying the data requires physically destroying the disc or data layer. Heating the disc in a microwave oven for 10–15 seconds effectively destroys the data layer by causing arcing in the metal reflective layer, but this same arcing may cause damage or excessive wear to the microwave oven. Many office paper shredders are also designed to shred CDs.
Some recent burners (Plextor, LiteOn) support erase operations on -R media, by "overwriting" the stored data with strong laser power, although the erased area cannot be overwritten with new data.
Recycling.
The polycarbonate material and possible gold or silver in the reflective layer would make CD-Rs highly recyclable. However, the polycarbonate is of very little value and the quantity of precious metals is so small that it is not profitable to recover them. Consequently, recyclers that accept CD-Rs typically do not offer compensation for donating or transporting the materials.

</doc>
<doc id="6781" url="http://en.wikipedia.org/wiki?curid=6781" title="Cytosol">
Cytosol

The cytosol or intracellular fluid (ICF) or cytoplasmic matrix is the liquid found inside cells. It is separated into compartments by membranes. For example, the mitochondrial matrix separates the mitochondrion into many compartments.
In the eukaryotic cell, the cytosol is within the cell membrane and is part of the cytoplasm, which also comprises the mitochondria, plastids, and other organelles (but not their internal fluids and structures); the cell nucleus is separate. In prokaryotes, most of the chemical reactions of metabolism take place in the cytosol, while a few take place in membranes or in the periplasmic space. In eukaryotes, while many metabolic pathways still occur in the cytosol, others are contained within organelles.
The cytosol is a complex mixture of substances dissolved in water. Although water forms the large majority of the cytosol, its structure and properties within cells is not well understood. The concentrations of ions such as sodium and potassium are different in the cytosol than in the extracellular fluid; these differences in ion levels are important in processes such as osmoregulation and cell signaling. The cytosol also contains large amounts of macromolecules, which can alter how molecules behave, through macromolecular crowding.
Although it was once thought to be a simple solution of molecules, the cytosol has multiple levels of organization. These include concentration gradients of small molecules such as calcium, large complexes of enzymes that act together to carry out metabolic pathways, and protein complexes such as proteasomes and carboxysomes that enclose and separate parts of the cytosol.
Definition.
The term cytosol was first introduced in 1965 by H.A. Lardy, and initially referred to the liquid that was produced by breaking cells apart and pelleting all the insoluble components by ultracentrifugation. Such a soluble cell extract is not identical to the soluble part of the cell cytoplasm and is usually called a cytoplasmic fraction. The term "cytosol" is now used to refer to the liquid phase of the cytoplasm in an intact cell. This excludes any part of the cytoplasm that is contained within organelles. Due to the possibility of confusion between the use of the word "cytosol" to refer to both extracts of cells and the soluble part of the cytoplasm in intact cells, the phrase "aqueous cytoplasm" has been used to describe the liquid contents of the cytoplasm of living cells.
Properties and composition.
The proportion of cell volume that is cytosol varies: for example while this compartment forms the bulk of cell structure in bacteria, in plant cells the main compartment is the large central vacuole. The cytosol consists mostly of water, dissolved ions, small molecules, and large water-soluble molecules (such as proteins). The majority of these non-protein molecules have a molecular mass of less than 300 Da. This mixture of small molecules is extraordinarily complex, as the variety of molecules that are involved in metabolism (the metabolites) is immense. For example up to 200,000 different small molecules might be made in plants, although not all these will be present in the same species, or in a single cell. Estimates of the number of metabolites in single cells such as "E. coli" and baker's yeast predict that under 1,000 are made.
Water.
Most of the cytosol is water, which makes up about 70% of the total volume of a typical cell. The pH of the intracellular fluid is 7.4. while human cytosolic pH ranges between 7.0 - 7.4, and is usually higher if a cell is growing. The viscosity of cytoplasm is roughly the same as pure water, although diffusion of small molecules through this liquid is about fourfold slower than in pure water, due mostly to collisions with the large numbers of macromolecules in the cytosol. Studies in the brine shrimp have examined how water affects cell functions; these saw that a 20% reduction in the amount of water in a cell inhibits metabolism, with metabolism decreasing progressively as the cell dries out and all metabolic activity halting when the water level reaches 70% below normal.
Although water is vital for life, the structure of this water in the cytosol is not well understood, mostly because methods such as nuclear magnetic resonance spectroscopy only give information on the average structure of water, and cannot measure local variations at the microscopic scale. Even the structure of pure water is poorly understood, due to the ability of water to form structures such as water clusters through hydrogen bonds.
The classic view of water in cells is that about 5% of this water is strongly bound in by solutes or macromolecules as water of solvation, while the majority has the same structure as pure water. This water of solvation is not active in osmosis and may have different solvent properties, so that some dissolved molecules are excluded, while others become concentrated. However, others argue that the effects of the high concentrations of macromolecules in cells extend throughout the cytosol and that water in cells behaves very differently from the water in dilute solutions. These ideas include the proposal that cells contain zones of low and high-density water, which could have widespread effects on the structures and functions of the other parts of the cell. However, the use of advanced nuclear magnetic resonance methods to directly measure the mobility of water in living cells contradicts this idea, as it suggests that 85% of cell water acts like that pure water, while the remainder is less mobile and probably bound to macromolecules.
Ions.
The concentrations of the other ions in cytosol are quite different from those in extracellular fluid and the cytosol also contains much higher amounts of charged macromolecules such as proteins and nucleic acids than the outside of the cell structure.
In contrast to extracellular fluid, cytosol has a high concentration of potassium ions and a low concentration of sodium ions. This difference in ion concentrations is critical for osmoregulation, since if the ion levels were the same inside a cell as outside, water would enter constantly by osmosis - since the levels of macromolecules inside cells are higher than their levels outside. Instead, sodium ions are expelled and potassium ions taken up by the Na⁺/K⁺-ATPase, potassium ions then flow down their concentration gradient through potassium-selection ion channels, this loss of positive charge creates a negative membrane potential. To balance this potential difference, negative chloride ions also exit the cell, through selective chloride channels. The loss of sodium and chloride ions compensates for the osmotic effect of the higher concentration of organic molecules inside the cell.
Cells can deal with even larger osmotic changes by accumulating osmoprotectants such as betaines or trehalose in their cytosol. Some of these molecules can allow cells to survive being completely dried out and allow an organism to enter a state of suspended animation called cryptobiosis. In this state the cytosol and osmoprotectants become a glass-like solid that helps stabilize proteins and cell membranes from the damaging effects of desiccation.
The low concentration of calcium in the cytosol allows calcium ions to function as a second messenger in calcium signaling. Here, a signal such as a hormone or an action potential opens calcium channels so that calcium floods into the cytosol. This sudden increase in cytosolic calcium activates other signalling molecules, such as calmodulin and protein kinase C. Other ions such as chloride and potassium may also have signaling functions in the cytosol, but these are not well understood.
Macromolecules.
Protein molecules that do not bind to cell membranes or the cytoskeleton are dissolved in the cytosol. The amount of protein in cells is extremely high, and approaches 200 mg/ml, occupying about 20-30% of the volume of the cytosol. However, measuring precisely how much protein is dissolved in cytosol in intact cells is difficult, since some proteins appear to be weakly associated with membranes or organelles in whole cells and are released into solution upon cell lysis. Indeed, in experiments where the plasma membrane of cells were carefully disrupted using saponin, without damaging the other cell membranes, only about one quarter of cell protein was released. These cells were also able to synthesize proteins if given ATP and amino acids, implying that many of the enzymes in cytosol are bound to the cytoskeleton. However, the idea that the majority of the proteins in cells are tightly bound in a network called the microtrabecular lattice is now seen as unlikely.
In prokaryotes the cytosol contains the cell's genome, within a structure known as a nucleoid. This is an irregular mass of DNA and associated proteins that control the transcription and replication of the bacterial chromosome and plasmids. In eukaryotes the genome is held within the cell nucleus, which is separated from the cytosol by nuclear pores that block the free diffusion of any molecule larger than about 10 nanometres in diameter.
This high concentration of macromolecules in cytosol causes an effect called macromolecular crowding, which is when the effective concentration of other macromolecules is increased, since they have less volume to move in. This crowding effect can produce large changes in both the rates and the position of chemical equilibrium of reactions in the cytosol. It is particularly important in its ability to alter dissociation constants by favoring the association of macromolecules, such as when multiple proteins come together to form protein complexes, or when DNA-binding proteins bind to their targets in the genome.
Organization.
Although the components of the cytosol are not separated into regions by cell membranes, these components do not always mix randomly and several levels of organization can localize specific molecules to defined sites within the cytosol.
Concentration gradients.
Although small molecules diffuse rapidly in the cytosol, concentration gradients can still be produced within this compartment. A well-studied example of these are the "calcium sparks" that are produced for a short period in the region around an open calcium channel. These are about 2 micrometres in diameter and last for only a few milliseconds, although several sparks can merge to form larger gradients, called "calcium waves". Concentration gradients of other small molecules, such as oxygen and adenosine triphosphate may be produced in cells around clusters of mitochondria, although these are less well understood.
Protein complexes.
Proteins can associate to form protein complexes, these often contain a set of proteins with similar functions, such as enzymes that carry out several steps in the same metabolic pathway. This organization can allow substrate channeling, which is when the product of one enzyme is passed directly to the next enzyme in a pathway without being released into solution. Channeling can make a pathway more rapid and efficient than it would be if the enzymes were randomly distributed in the cytosol, and can also prevent the release of unstable reaction intermediates. Although a wide variety of metabolic pathways involve enzymes that are tightly bound to each other, others may involve more loosely associated complexes that are very difficult to study outside the cell. Consequently, the importance of these complexes for metabolism in general remains unclear.
Protein compartments.
Some protein complexes contain a large central cavity that is isolated from the remainder of the cytosol. One example of such an enclosed compartment is the proteasome. Here, a set of subunits form a hollow barrel containing proteases that degrade cytosolic proteins. Since these would be damaging if they mixed freely with the remainder of the cytosol, the barrel is capped by a set of regulatory proteins that recognize proteins with a signal directing them for degradation (a ubiquitin tag) and feed them into the proteolytic cavity.
Another large class of protein compartments are bacterial microcompartments, which are made of a protein shell that encapsulates various enzymes. These compartments are typically about 100-200 nanometres across and made of interlocking proteins. A well-understood example is the carboxysome, which contains enzymes involved in carbon fixation such as RuBisCO.
Cytoskeletal sieving.
Although the cytoskeleton is not part of the cytosol, the presence of this network of filaments restricts the diffusion of large particles in the cell. For example, in several studies tracer particles larger than about 25 nanometres (about the size of a ribosome) were excluded from parts of the cytosol around the edges of the cell and next to the nucleus. These "excluding compartments" may contain a much denser meshwork of actin fibres than the remainder of the cytosol. These microdomains could influence the distribution of large structures such as ribosomes and organelles within the cytosol by excluding them from some areas and concentrating them in others.
Function.
The cytosol has no single function and is instead the site of multiple cell processes. Examples of these processes include signal transduction from the cell membrane to sites within the cell, such as the cell nucleus, or organelles. This compartment is also the site of many of the processes of cytokinesis, after the breakdown of the nuclear membrane in mitosis. Another major function of cytosol is to transport metabolites from their site of production to where they are used. This is relatively simple for water-soluble molecules, such as amino acids, which can diffuse rapidly through the cytosol. However, hydrophobic molecules, such as fatty acids or sterols, can be transported through the cytosol by specific binding proteins, which shuttle these molecules between cell membranes. Molecules taken into the cell by endocytosis or on their way to be secreted can also be transported through the cytosol inside vesicles, which are small spheres of lipids that are moved along the cytoskeleton by motor proteins.
The cytosol is the site of most metabolism in prokaryotes, and a large proportion of the metabolism of eukaryotes. For instance, in mammals about half of the proteins in the cell are localized to the cytosol. The most complete data are available in yeast, where metabolic reconstructions indicate that the majority of both metabolic processes and metabolites occur in the cytosol. Major metabolic pathways that occur in the cytosol in animals are protein biosynthesis, the pentose phosphate pathway, glycolysis and gluconeogenesis. The localization of pathways can be different in other organisms, for instance fatty acid synthesis occurs in chloroplasts in plants and in apicoplasts in apicomplexa.

</doc>
<doc id="6782" url="http://en.wikipedia.org/wiki?curid=6782" title="Compound">
Compound

Compound may refer to:
Compound may also refer to:

</doc>
<doc id="6784" url="http://en.wikipedia.org/wiki?curid=6784" title="Citizenship">
Citizenship

Citizenship is the status of a person recognized under the custom or law as being a member of a state. A person may have multiple citizenships and a person who does not have citizenship of any state is said to be stateless.
Nationality is often used as a synonym for citizenship in English – notably in international law – although the term is sometimes understood as denoting a person's membership of a nation (a large ethnic group). In some countries, e.g. the United States, the United Kingdom, "nationality" and "citizenship" can have different meanings (for more information, see Nationality#Nationality versus citizenship).
Determining factors.
A person can be a citizen for several reasons. Usually citizenship of the place of birth is automatic; in other cases an application may be required.
History.
Polis.
Many thinkers point to the concept of citizenship beginning in the early city-states of ancient Greece, although others see it as primarily a modern phenomenon dating back only a few hundred years and, for mankind, that the concept of citizenship arose with the first laws. "Polis" meant both the political assembly of the city-state as well as the entire society. Citizenship has generally been identified as a western phenomenon. There is a general view that citizenship in ancient times was a simpler relation than modern forms of citizenship, although this view has come under scrutiny. The relation of citizenship has not been a fixed or static relation, but constantly changed within each society, and that according to one view, citizenship might "really have worked" only at select periods during certain times, such as when the Athenian politician Solon made reforms in the early Athenian state.
Historian Geoffrey Hosking in his 2005 "Modern Scholar" lecture course suggested that citizenship in ancient Greece arose from an appreciation for the importance of freedom. Hosking explained:
It can be argued that this growth of slavery was what made Greeks particularly conscious of the value of freedom. After all, any Greek farmer might fall into debt and therefore might become a slave, at almost any time ... When the Greeks fought together, they fought in order to avoid being enslaved by warfare, to avoid being defeated by those who might take them into slavery. And they also arranged their political institutions so as to remain free men.—Geoffrey Hosking, 2005
Slavery permitted slaveowners to have substantial free time, and enabled participation in public life. Polis citizenship was marked by exclusivity. Inequality of status was widespread; citizens had a higher status than non-citizens, such as women, slaves or barbarians. The first form of citizenship was based on the way people lived in the ancient Greek times, in small-scale organic communities of the polis. Citizenship was not seen as a separate activity from the private life of the individual person, in the sense that there was not a distinction between public and private life. The obligations of citizenship were deeply connected into one's everyday life in the polis. These small-scale organic communities were generally seen as a new development in world history, in contrast to the established ancient civilizations of Egypt or Persia, or the hunter-gatherer bands elsewhere. From the viewpoint of the ancient Greeks, a person's public life was not separated from their private life, and Greeks did not distinguish between the two worlds according to the modern western conception. The obligations of citizenship were deeply connected with everyday life. To be truly human, one had to be an active citizen to the community, which Aristotle famously expressed: "To take no part in the running of the community's affairs is to be either a beast or a god!" This form of citizenship was based on obligations of citizens towards the community, rather than rights given to the citizens of the community. This was not a problem because they all had a strong affinity with the polis; their own destiny and the destiny of the community were strongly linked. Also, citizens of the polis saw obligations to the community as an opportunity to be virtuous, it was a source of honour and respect. In Athens, citizens were both ruler and ruled, important political and judicial offices were rotated and all citizens had the right to speak and vote in the political assembly.
Roman ideas.
In the Roman Empire, citizenship expanded from small-scale communities to the entire empire. Romans realized that granting citizenship to people from all over the empire legitimized Roman rule over conquered areas. Roman citizenship was no longer a status of political agency; it had been reduced to a judicial safeguard and the expression of rule and law. Rome carried forth Greek ideas of citizenship such as the principles of equality under the law, civic participation in government, and notions that "no one citizen should have too much power for too long", but Rome offered relatively generous terms to its captives, including chances for lesser forms of citizenship. If Greek citizenship was an "emancipation from the world of things", the Roman sense increasingly reflected the fact that citizens could act upon material things as well as other citizens, in the sense of buying or selling property, possessions, titles, goods. One historian explained:
The person was defined and represented through his actions upon things; in the course of time, the term property came to mean, first, the defining characteristic of a human or other being; second, the relation which a person had with a thing; and third, the thing defined as the possession of some person.—J. G. A. Pocock, 1988
Roman citizenship reflected a struggle between the upper-class patrician interests against the lower-order working groups known as the plebeian class. A citizen came to be understood as a person "free to act by law, free to ask and expect the law's protection, a citizen of such and such a legal community, of such and such a legal standing in that community". Citizenship meant having rights to have possessions, immunities, expectations, which were "available in many kinds and degrees, available or unavailable to many kinds of person for many kinds of reason". And the law, itself, was a kind of bond uniting people. Roman citizenship was more impersonal, universal, multiform, having different degrees and applications.
Middle Ages.
During European Middle Ages, citizenship was usually associated with cities and towns, see burgher, Grand Burgher (German "Großbürger") and Bourgeoisie. Nobility used to have privileges above commoners (see aristocracy), but the French Revolution and other revolutions revoked these privileges and made citizens.
Renaissance.
During the Renaissance, people transitioned from being subjects of a king or queen to being citizens of a city and later to a nation.:p.161 Each city had its own law, courts, and independent administration. And being a citizen often meant being subject to the city's law in addition to having power in some instances to help choose officials. City dwellers who had fought alongside nobles in battles to defend their cities were no longer content with having a subordinate social status, but demanded a greater role in the form of citizenship. Membership in guilds was an indirect form of citizenship in that it helped their members succeed financially. The rise of citizenship was linked to the rise of republicanism, according to one account, since independent citizens meant that kings had less power. Citizenship became an idealized, almost abstract, concept, and did not signify a submissive relation with a lord or count, but rather indicated the bond between a person and the state in the rather abstract sense of having rights and duties.
Modern times.
The modern idea of citizenship still respects the idea of political participation, but it is usually done through "elaborate systems of political representation at a distance" such as representative democracy. Modern citizenship is much more passive; action is delegated to others; citizenship is often a constraint on acting, not an impetus to act. Nevertheless, citizens are usually aware of their obligations to authorities, and are aware that these bonds often limit what they can do.
Different senses.
Citizenship status, under social contract theory, carries with it both rights and duties. In this sense, citizenship was described as "a bundle of rights -- primarily, political participation in the life of the community, the right to vote, and the right to receive certain protection from the community, as well as obligations." Citizenship is seen by most scholars as culture-specific, in the sense that the meaning of the term varies considerably from culture to culture, and over time. How citizenship is understood depends on the person making the determination. The relation of citizenship has never been fixed or static, but constantly changes within each society. While citizenship has varied considerably throughout history, and within societies over time, there are some common elements but they vary considerably as well. As a bond, citizenship extends beyond basic kinship ties to unite people of different genetic backgrounds. It usually signifies membership in a political body. It is often based on, or was a result of, some form of military service or expectation of future service. It usually involves some form of political participation, but this can vary from token acts to active service in government. Citizenship is a status in society. It is an ideal state as well. It generally describes a person with legal rights within a given political order. It almost always has an element of exclusion, meaning that some people are not citizens, and that this distinction can sometimes be very important, or not important, depending on a particular society. Citizenship as a concept is generally hard to isolate intellectually and compare with related political notions, since it relates to many other aspects of society such as the family, military service, the individual, freedom, religion, ideas of right and wrong, ethnicity, and patterns for how a person should behave in society. When there are many different groups within a nation, citizenship may be the only real bond which unites everybody as equals without discrimination—it is a "broad bond" linking "a person with the state" and gives people a universal identity as a legal member of a specific nation.
Modern citizenship has often been looked at as two competing underlying ideas:
Scholars suggest that the concept of citizenship contains many unresolved issues, sometimes called tensions, existing within the relation, that continue to reflect uncertainty about what citizenship is supposed to mean. Some unresolved issues regarding citizenship include questions about what is the proper balance between duties and rights. Another is a question about what is the proper balance between "political citizenship" versus "social citizenship". Some thinkers see benefits with people being absent from public affairs, since too much participation such as revolution can be destructive, yet too little participation such as total apathy can be problematic as well. Citizenship can be seen as a special elite status, and it can also be seen as a democratizing force and something that everybody has; the concept can include both senses. According to sociologist Arthur Stinchcombe, citizenship is based on the extent that a person can control one's own destiny within the group in the sense of being able to influence the government of the group.:p.150 One last distinction within citizenship is the so-called "consent descent" distinction, and this issue addresses whether citizenship is a fundamental matter determined by a person "choosing" to belong to a particular nation––by his or her "consent"––or is citizenship a matter of where a person was born––that is, by his or her "descent".
Rights and duties.
Rights.
Citizens have the following rights (subject to certain exceptions):
Duties.
The following duties are generally expected of citizens (subject to exceptions):
International.
Some intergovernmental organizations have extended the concept and terminology associated with citizenship to the international level, where it is applied to the totality of the citizens of their constituent countries combined. Citizenship at this level is a secondary concept, with rights deriving from national citizenship.
European Union.
The Maastricht Treaty introduced the concept of citizenship of the European Union. Article 17 (1) of the Treaty on European Union stated that: Citizenship of the Union is hereby established. Every person holding the nationality of a Member State shall be a citizen of the Union. Citizenship of the Union shall be additional to and not replace national citizenship.
An agreement known as the amended EC Treaty established certain minimal rights for European Union citizens. Article 12 of the amended EC Treaty guaranteed a general right of non-discrimination within the scope of the Treaty. Article 18 provided a limited right to free movement and residence in Member States other than that of which the European Union citizen is a national. Articles 18-21 and 225 provide certain political rights.
Union citizens have also extensive rights to move in order to exercise economic activity in any of the Member States which predate the introduction of Union citizenship.
Commonwealth.
The concept of "Commonwealth Citizenship" has been in place ever since the establishment of the Commonwealth of Nations. As with the EU, one holds Commonwealth citizenship only by being a citizen of a Commonwealth member state. This form of citizenship offers certain privileges within some Commonwealth countries:
Although Ireland was excluded from the Commonwealth in 1949 because it declared itself a republic, Ireland is generally treated as if it were still a member. Legislation often specifically provides for equal treatment between Commonwealth countries and Ireland and refers to "Commonwealth countries and Ireland". Ireland's citizens are not classified as foreign nationals in the United Kingdom.
Canada departed from the principle of nationality being defined in terms of allegiance in 1921. In 1935 the Irish Free State was the first to introduce its own citizenship. However, Irish citizens were still treated as subjects of the Crown, and they are still not regarded as foreign, even though Ireland is not a member of the Commonwealth. The Canadian Citizenship Act of 1947 provided for a distinct Canadian Citizenship, automatically conferred upon most individuals born in Canada, with some exceptions, and defined the conditions under which one could become a naturalized citizen. The concept of Commonwealth citizenship was introduced in 1948 in the British Nationality Act 1948. Other dominions adopted this principle such as New Zealand, by way of the British Nationality and New Zealand Citizenship Act of 1948.
Subnational.
Citizenship most usually relates to membership of the nation state, but the term can also apply at the subnational level. Subnational entities may impose requirements, of residency or otherwise, which permit citizens to participate in the political life of that entity, or to enjoy benefits provided by the government of that entity. But in such cases, those eligible are also sometimes seen as "citizens" of the relevant state, province, or region. An example of this is how the fundamental basis of Swiss citizenship is citizenship of an individual commune, from which follows citizenship of a canton and of the Confederation. Another example is Åland where the residents enjoy a special provincial citizenship within Finland, "hembygdsrätt".
The United States has a federal system in which a person is a citizen of their specific state of residence, such as New Jersey or California, as well as a citizen of the United States. State constitutions may grant certain rights above and beyond what are granted under the United States Constitution and may impose their own obligations including the sovereign right of taxation and military service; each state maintains at least one military force subject to national militia transfer service, the state's national guard, and some states maintain a second military force not subject to nationalization.
Education.
"Active citizenship" is the philosophy that citizens should work towards the betterment of their community through economic participation, public, volunteer work, and other such efforts to improve life for all citizens. In this vein, schools in some countries provide citizenship education (subject).
United Kingdom.
Citizenship is offered as a General Certificate of Secondary Education (GCSE) course in many schools in the United Kingdom. As well as teaching knowledge about democracy, parliament, government, the justice system, human rights and the UK's relations with the wider world, students participate in active citizenship, often involving a social action or social enterprise in their local community.
Ireland.
It is taught in Ireland as an exam subject for the Junior Certificate. It is known as Civic, Social and Political Education (CSPE). A new Leaving Certificate exam subject with the working title 'Politics & Society' is being developed by the National Council for Curriculum and Assessment (NCCA) and is expected to be introduced to the curriculum sometime after 2012.

</doc>
<doc id="6787" url="http://en.wikipedia.org/wiki?curid=6787" title="Chiapas">
Chiapas

Chiapas (]), officially Free and Sovereign State of Chiapas (Spanish: "Estado Libre y Soberano de Chiapas"), is one of the 31 states that, with the Federal District, make up the 32 Federal Entities of Mexico. It is divided into 122 municipalities and its capital city is Tuxtla Gutiérrez. Other important population centers in Chiapas include San Cristóbal de las Casas, Comitán, Tapachula and Arriaga. Located in Southeastern Mexico, it is the southernmost State of Mexico. It is bordered by the states of Tabasco to the north, Veracruz to the northwest and Oaxaca to the west. To the east Chiapas borders Guatemala, and to the south the Pacific Ocean.
In general, Chiapas has a humid, tropical climate. In the north, in the area bordering Tabasco, near Teapa, rainfall can average more than 3000 mm per year. In the past, natural vegetation at this region was lowland, tall perennial rainforest, but this vegetation has been destroyed almost completely to give way to agriculture and ranching. Rainfall decreases moving towards the Pacific Ocean, but it is still abundant enough to allow the farming of bananas and many other tropical crops near Tapachula. On the several parallel "sierras" or mountain ranges running along the center of Chiapas, climate can be quite temperate and foggy, allowing the development of cloud forests like those of the Reserva de la Biosfera el Triunfo, home to a handful of resplendent quetzals and horned guans.
Chiapas is home to the ancient Mayan ruins of Palenque, Yaxchilán, Bonampak, and Chinkultic. It is also home to one of the largest indigenous populations in the country with twelve federally recognized ethnicities. Much of the state’s history is centered on the subjugation of these peoples with occasional rebellions. The last of these rebellions was the 1994 Zapatista uprising, which succeeded in obtaining new rights for indigenous people.
History.
The official name of the state is Chiapas. The name derives from "Chiapan" or "Tepechiapan" the name of an indigenous population. The term is from Nahuatl and has been translated to mean "sage seed hill" and "water below the hill." After the Spanish arrived, they established two cities called Chiapas de los Indios and Chiapas de los Españoles, with the name of Provincia de Chiapas for the area around the cities. The first coat of arms for the state was created in 1535 as that of the Ciudad Real (San Cristobal de las Casas). The modern coat of arms was created by Chiapas painter Javier Vargas Ballinas.
Pre-Columbian.
Hunter gatherers began to occupy the central valley of the state around 7000 BCE, but little is known about their lives. The oldest archaeological remains in the seat are located at the Santa Elena Ranch in Ocozocoautla whose finds include tools and weapons made of stone and bone. It also includes burials. In the pre Classic period from 1800 BCE to 300 CE, agricultural villages appeared all over the state although hunter gather groups would persist for long after the era.
Recent excavations in the Soconusco region of the state indicate that the oldest civilization to appear in what is now modern Chiapas is that of the Mokaya, which were cultivating corn and living in houses as early as 1500 BCE, making them one of the oldest in Mesoamerica. There is speculation that these were the forefathers of the Olmec, migrating across the Grijalva Valley and onto the coastal plain of the Gulf of Mexico to the north, which was Olmec territory. One of these people's ancient cities is now the archeological site of Chiapa de Corzo, in which was found the oldest calendar known on a piece of ceramic with a date of 36 BCE. This is three hundred years before the Mayans developed their calendar. The descendents of Mokaya are the Mixe-Zoque.
During the pre Classic, it is known that most of Chiapas was not Olmec, but had close relations with them, especially the Olmecs of the Isthmus of Tehuantepec. Olmec influenced sculpture can be found in Chiapas and products from the state including amber, magnetite and ilmenite were exported to Olmec lands. The Olmecs came to what is now the northwest of the state looking for amber with one of the main evidences for this called the Simojovel Ax.
Mayan civilization began in the pre Classic period as well but did not come into prominence until the Classic period (300-900 CE). Development of this culture was agricultural villages during the pre Classic period with city building during the Classic as social stratification became more complex. The Mayans built cities on the Yucatán Peninsula and west into Guatemala. In Chiapas, Mayas sites are concentrated along the state's borders with Tabasco and Guatemala, near Mayan sites in those entities. Most of this area belongs to the Lacandon Jungle.
Mayan civilization in the Lacandon is marked by rising exploitation of rainforest resources, rigid social stratification, feverent nationalism and waging war against neighboring peoples. At its height, it had large cities, writing and of sciences such as mathematics and astronomy. Cities were centered on large political and ceremonial structures elaborately decorated with murals and inscriptions. Among these cities are Palenque, Bonampak, Yaxchilan, Chinkultic, Toniná and Tenón. The Mayan civilization had vast trade networks and large markets trading in goods such as animal skins, indigo, amber, vanilla and quetzal feathers. It is not known what ended the civilization but theories range from over population, natural disasters, disease and loss of natural resources through over exploitation or climate change.
Nearly all Mayan cities collapsed around the same time, 900 CE. From then until 1500 CE, social organization of the region fragmented into much smaller units and social structure became much less complex. There was some influence from the rising powers of central Mexico but two main indigenous groups emerged during this time, the Zoques and the various Mayan descendents. The Chiapans, for whom the state is named, migrated into the center of the state during this time and settled around Chiapa de Corzo, the old Mixe–Zoque stronghold. There is evidence that the Aztecs appeared in the center of the state around Chiapa de Corza in the 15th century, but were unable to displace the native Chiapa tribe. However, they had enough influence so that the name of this area and of the state would come from Nahuatl.
Colonial period.
When the Spanish arrived in the 16th century, they found the indigenous peoples divided into Mayan and non-Mayan, with the latter dominated by the Zoques and Chiapa. The first contact between Spaniards and the people of Chiapas came in 1522, when Hernán Cortés sent tax collectors to the area after Aztec Empire was subdued. The first military incursion was headed by Luis Marín, who arrived in 1523. For three years, Marín was able to subjugate a number of the local peoples, but met with fierce resistance from the Tzotzils in the highlands. The Spanish colonial government then sent a new expedition under Diego de Mazariegos. Mazariegos had more success than his predecessor, but many indigenous preferred to commit suicide rather than submit to the Spanish. One famous example of this is the Battle of Tepetchia, where many jumped to their deaths in the Sumidero Canyon.
Indigenous resistance was weakened by continual warfare with the Spaniards as well as disease, and by 1530, almost all of the indigenous peoples of the area had been subdued with the exception of the Lacandons in the deep jungles who actively resisted until 1695. However, the main two groups, the Tzotzils and Tzeltals of the central highlands were subdued enough to establish the first Spanish city, today called San Cristóbal de las Casas, in 1528. It was one of two settlements initially called Villa Real de Chiapa de los Españoles and the other called Chiapa de los Indios.
Soon after, the encomienda system was introduced, which reduced most of the indigenous population to serfs and many even as slaves, paid as a form of tribute. The conquistadors brought previously unknown diseases. This, as well as overwork on plantations, dramatically decreased the indigenous population. The Spanish also established missions, mostly under the Dominicans, with the Diocese of Chiapas established in 1538 by Pope Paul III. The Dominican evangelizers became early advocates of the indigenous' plight, with Bartolomé de las Casas winning a battle with the passing of a law in 1542 for their protection. This order also worked to make sure that communities would keep their indigenous name with a saint’s prefix leading to names such as San Juan Chamula and San Lorenzo Zinacantán. He also advocated adapting the teaching of Christianity to indigenous language and culture. The encomienda system that had perpetrated much of the abuse of the indigenous peoples fell away by the end of the 16th century, and was replaced by haciendas. However, the use and misuse of Indian labor remained a large part of Chiapas politics into modern times. This treatment and tribute payments would create an undercurrent of resentment in the indigenous population that passed on from generation to generation. One uprising against high tribute payments occurs in the Tzeltal communities in the Los Alto region in 1712. Soon, the Tzoltzils and Ch’ols joined the Tzeltales in rebellion, but within a year, the government was able to extinguish the rebellion.
As of 1778, Thomas Kitchin described Chiapas as "the metropolis of the original Mexicans," with a population of approximately 20,000, and consisting mainly of indigenous peoples. The Spanish introduced new crops such as sugar cane, wheat, barley and indigo as main economic staples along native ones such as corn, cotton, cacao and beans. Livestock such as cattle, horses and sheep were introduced as well. Regions would specialize in certain crops and animals depending on local conditions and for many of these regions, communication and travel were difficult. Most Europeans and their descendents tended to concentrate in cities such as Ciudad Real, Comitán, Chiapa and Tuxtla. Intermixing of the races was prohibited by colonial law but by the end of the 17th century there was a significant mestizo population. Added to this was a population of African slaves brought in by the Spanish in the middle of the 16th century due to the loss of native workforce.
Initially, "Chiapas" referred to the first two cities established by the Spanish in what is now the center of the state and the area surrounding them. Two other regions were also established, the Soconusco and Tuxtla, all under the regional colonial government of Guatemala. Chiapas, Soconusco and Tuxla regions were united to the first time as an "intendencia" in 1790 as an administrative region under the name of Chiapas. However, within this intendencia, the division between Chiapas and Soconusco regions would remain strong and have consequences at the end of the colonial period.
19th century.
Since the colonial period, Chiapas had been relatively isolated from colonial authorities in Mexico City and regional authorities in Guatemala. One reason for this was the rugged terrain but the other was that much of Chiapas was not attractive to the Spanish for its lack of mineral wealth or large areas of arable land. This isolation spared it from battles related to Independence. José María Morelos y Pavón did enter the city of Tonalá but incurred no resistance. The only other insurgent activity was the publication of a newspaper called "El Pararrayos" by Matías de Córdova in San Cristóbal de las Casas.
However, this isolation, along with strong internal divisions in the intendencia would cause political crisis after insurgents captured Mexico City in 1821 to end the Mexican War of Independence. During this war, a group of influential merchants and ranchers sought the establishment of the Free State of Chiapas. This group became known as the "La Familia Chiapaneca." However, this alliance did not last with the lowlands preferring inclusion among the new republics of Central America and the highlands annexation to Mexico. In 1821, a number of cities in Chiapas, starting in Comitán declared the state's separation from the Spanish empire. In 1823, Guatemala became part of the United Provinces of Central America, which united to form a federal republic that would last from 1823 to 1839. With the exception of the pro-Mexican Ciudad Real (San Cristóbal) and some others, many Chiapanecan towns and villages favored a Chiapas independent of Mexico and some favored unification with Guatemala.
However, the elite in the highland cities pushed for incorporation into Mexico. In 1822, then Emperor Agustín de Iturbide decreed that Chiapas was part of Mexico. In 1823, the Junta General de Gobierno was held and Chiapas declared independence again. In July 1824, the Soconusco District of southwestern Chiapas split off from Chiapas, announcing that it would join the Central American Federation. In September of the same year, a referendum was held on whether the intendencia would join Central America or Mexico, with many of the elite endorsing union with Mexico. This referendum ended in favor of incorporation with Mexico (allegedly through manipulation of the elite in the highlands), but the Soconusco region maintained a neutral status until 1842, when Oaxacans under General Antonio López de Santa Anna occupied the area, and declared it reincorporated into Mexico. The elites of the area would not accept this until 1844. Guatemala would not recognize Mexico's annexation of the Soconusco region until 1895 even though a final border between Chiapas and the country was finalized until 1882. The State of Chiapas was officially declared in 1824, with its first constitution in 1826. Ciudad Real was renamed San Cristóbal de las Casas in 1828.
In the decades after the official end of the war, the Chiapas and Soconusco provinces became united with power concentrating into San Cristóbal de las Casas. The state's society evolved into three distinct spheres: indigenous peoples, mestizos from the farms and haciendas and the Spanish colonial cities. Most of the political struggles were between the latter two groups especially over who would control the indigenous labor force. Economically, the state lost one of its main crops, indigo, to synthetic dyes. There was a small experiment with democracy in the form of "open city councils" but it was short lived because voting was heavily rigged.
The Universidad Pontificia y Literaria de Chiapas was founded in 1826, with Mexico's second teacher’s college founded in the state in 1828.
The Mexico-wide struggles between Liberals, who favored federalism and Conservatives, who favored centralized autocratic government did not lead to any military battles in the state but it strongly affected the local politics. In Chiapas, the Liberal-Conservative division had its own twist. Much of the division between the highland and lowland ruling families was for whom the Indians should work for and for how long as the main shortage was of labor. These families split into Liberals in the lowlands, who wanted further reform and Conservatives in the highlands who still wanted to keep some of the traditional colonial and church privileges. For most of the early and mid 19th century, Conservatives held most of the power and were concentrated in the larger cites of San Cristóbal de las Casas, Chiapa (de Corzo), Tuxtla and Comitán. As Liberals gained the upper hand nationally in the mid-19th century, one Liberal politician Ángel Albino Corzo gained control of the state. Corzo became the primary exponent of Liberal ideas in the southeast of Mexico and defended the Palenque and Pichucalco areas from annexation by Tabasco. However, Corzo's rule would end in 1875, when he opposed the regime of Porfirio Díaz.
Liberal land reforms would have negative effects on the state's indigenous population unlike in other areas of the country. Liberal governments expropriated lands that were previously held by the Spanish Crown and Catholic Church in order to sell them into private hands. This was not only motivated by ideology, but also due to the need to raise money. However, many of these lands had been in a kind of "trust" with the local indigenous populations, who worked them. Liberal reforms took away this arrangement and many of these lands fell into the hands of large landholders who when made the local Indian population work for three to five days a week just for the right to continue to cultivate the lands. This requirement caused many to leave and look for employment elsewhere. Most became "free" workers on other farms, but they were often paid only with food and basic necessities from the farm shop. If this was not enough, these workers became indebted to these same shops and then unable to leave.
The opening up of these lands also allowed many whites and mestizos (often called Ladinos in Chiapas) to encroach on what had been exclusively indigenous communities in the state. These communities had had almost no contact with the Ladino world, except for a priest. The new Ladino landowners occupied their acquired lands as well as others, such as shopkeepers, opened up businesses in the center of Indian communities. In 1848, a group of Tzeltals plotted to kill the new mestizos in their midst, but this plan was discovered, and was punished by th removal of large number of the community’s male members. The changing social order had severe negative effects on the indigenous population with alcoholism spreadings, leading to more debts as it was expensive. The struggles between Conservatives and Liberals nationally disrupted commerce and confused power relations between Indian communities and Ladino authorities. It also resulted in some brief respites for Indians during times when the instability led to uncollected taxes.
One other effect that Liberal land reforms had was the start of coffee plantations, especially in the Soconusco region. One reason for this push in this area was that Mexico was still working to strengthen its claim on the area against Guatemala’s claims on the region. The land reforms brought colonists from other areas of the country as well as foreigners from England, the United States and France. These forieign immigrants would introduce coffee production to the areas, as well as modern machineray and professional administration of coffee plantations. Eventually, this production of coffee would become the state's most important crop.
Although the Liberals had mostly triumphed in the state and the rest of the country by the 1860s, Conservatives still held considerable power in Chiapas. Liberal politicians sought to solidify their power among the indigenous groups by weakening the Church. The more radical of these even allowed indigenous groups the religious freedoms to return to a number of native rituals and beliefs such as pilgrimages to natural shrines such as mountains and waterfalls.
This culminated in the Chiapas "caste war", which was an uprising the Tzotzils beginning in 1868. The basis of the uprising was the establishment of the "three stones cult" in Tzajahemal. Agustina Gómez Checheb was a girl tending her father’s sheep when three stones fell from the sky. Collecting them, she put them on her father’s altar and soon claimed that the stone communicated with her. Word of this soon spread and the "talking stones" of Tzajahemel soon became a local indigenous pilgrimage site. The cult was taken over by one pilgrim, Pedro Díaz Cuzcat, who also claimed to be able to communicate with the stones, and had knowledge of Catholic ritual, becoming a kind of priest. However, this challenged the traditional Catholic faith and non Indians began to denounce the cult. Stories about the cult include embellishments such as the crucifixion of a young Indian boy.
This led to the arrest of Checheb and Cuzcat in December 1868. This caused resentment among the Tzotzils. Although the Liberals had earlier supported the cult, Liberal landowners had also lost control of much of their Indian labor and Liberal politicians were having a harder time collecting taxes from indigenous communities. An Indian army gathered at Zontehuitz then attacked various villages and haciendas. By the following June the city of San Cristóbal was surrounded by several thousand Indians, who offered the exchanged of several Ladino captives for their religious leaders and stones. Chiapas governor Dominguéz come to San Cristóbal with about three hundred heavily armed men, who then attacked the Indian force armed only with sticks and machetes. The indigenous force was quickly dispersed and routed with government troops pursuing pockets of guerrilla resistance in the mountains until 1870. The event effectively returned control of the indigenous workforce back to the highland elite.
Modern Japanese immigration to Mexico began in 1897 when the first thirty five migrants arrived in Chiapas to work on coffee farms. This makes Mexico the first Latin American country to receive organized Japanese immigration. Although this colony ultimately failed, there remains a small Japanese community in Acacoyagua, Chiapas.
The Porfirio Díaz era at the end of the 19th century and beginning of the 20th was initially thwarted by regional bosses called caciques, bolstered by a wave of Spanish and mestizo farmers who migrated to the state and added to the elite group of wealthy landowning families. There was some technological progress such as a highway from San Cristóbal to the Oaxaca border and the first telephone line in the 1880s, but Porfirian era economic reforms would not begin until 1891 with Governor Emilio Rabasa. This governor took on the local and regional caciques and centralized power into the state capital, which he moved from San Cristóbal de las Casas to Tuxtla in 1892. He modernized public administration, transportation and promoted education. Rabasa also introduced telegraph, limited public schooling, sanitation and road construction, including a route from San Cristóbal to Tuxtla then Oaxaca, which signaled the beginning of favoritism of development in the central valley over the highlands. He also changed state policies to favor foreign investment, favored large land mass consolidation for the production of cash crops such as henequen, rubber, guayule, cochineal and coffee. Agricultural production boomed, especially coffee, which induced the construction of port facilities in Tonalá. The economic expansion and investment in roads also increased access to tropical commodities such as hardwoods, rubber and chicle.
These still required cheap and steady labor to be provided by the indigenous population. By the end of the 19th century, the four main indigenous groups, Tzeltals, Tzotzils, Tojolabals and Ch’ols were living in "reducciones" or reservations, isolated from one another. Conditions on the farms of the Porfirian era was serfdom, as bad if not worse than for other indigenous and mestizo populations leading to the Mexican Revolution. While this coming event would affect the state, Chiapas did not follow the uprisings in other areas that would end the Porfirian era.
20th century to the present.
In the early 20th century and into the Mexican Revolution, the production of coffee was particularly important but labor-intensive. This would lead to a practice called "enganche" (hook) where recruiter would lure workers with advanced pay and other incentives such as alcohol and then trap them with debts for travel and other items to be worked off. This practice would lead to a kind of indentured servitude and uprisings in areas of the state, although they never led to large rebel armies as in other parts of Mexico.
A small war broke out between Tuxtla Gutiérrez and San Cristobal in 1911. San Cristóbal, allied with San Juan Chamula, tried to regain the state's capital but the effort failed. San Cristobal de las Casas, who had a very limited budget, to the extent that it had to ally with San Juan Chamula, and Tuxtla Gutierrez, which was enough only a small ragtag army to beat overwhelmingly the army helped by chamulas from San Cristobal. There were three years of peace after that until troops allied with Venustiano Carranza entered in 1914 taking over the government, with the aim of imposing the Ley de Obreros to address wrongs done to the state's mostly indigenous workers. Conservatives responded violently months later as they were certain the Carranza forces would take their lands. This was mostly in the way of guerrilla actions headed by farm owners who called themselves the Mapaches, which continued for six years, until Carranza was assassinated and Álvaro Obregón became president of Mexico. This allowed the Mapaches to gain political power in the state and effectively stop many of the social reforms happening in other parts of Mexico.
However, these Mapaches would continue to fight against socialists and communists in Mexico from 1920 to 1936 to maintain their control over the state. In general, the elite landowners also allied with the nationally dominant Institutional Revolutionary Party (PRI) so that they could block land reforms in this way as well. The Mapaches were first defeated in 1925 when an alliance of socialists and former Carranza loyalists had Carlos A. Vidal selected as governor, although he was assassinated two years later. The last of the Mapache resistance was over come in the early 1930s by Governor Victorico Grajales, who pursued President Lázaro Cárdenas' social and economic policies including persecution of the Church. These policies would have some success in redistributing lands and organizing indigenous workers but the state would remain relatively isolated for the rest of the 20th century.
The territory was reorganized into municipalities in 1916. The current state constitution was written in 1921.
There was political stability from the 1940s to the early 1970s; however, regionalism regained with people thinking of themselves as from their local city or municipality over the state. This regionalism impeded the economy as local authorities restrained outside goods. For this reason, construction of highways and communications were pushed to help with economic development. Most of the work was done around Tuxtla Gutiérrez and Tapachula. This included the Sureste railroad connecting northern municipalities such as Pichucalco, Salto de Agua, Palenque, Catazajá and La Libertad. The Cristobal Colon highway linked Tuxtla to the Guatemalan border. Other highways included El Escopetazo to Pichucalco, a highway between San Cristóbal and Palenque with branches to Cuxtepeques and La Frailesca. This helped to integrate the state's economy, but it also permitted the political rise of communal land owners called ejidatarios.
In the mid-20th century, the state experienced a significant rise in population, which outstripped local resources, especially land in the highland areas. Since the 1930s, many indigenous and mestizos have migrated from the highland areas into the Lacandon Jungle with the populations of Altamirano, Las Margaritas, Ocosingo and Palenque rising from less than 11,000 in 1920 to over 376,000 in 2000. These migrants came to the jungle area to clear forest and grow crops and raise livestock, especially cattle. Economic development in general raised the output of the state, especially in agriculture, but it had the effect of deforesting many areas, especially the Lacandon. Added to this was there was still serf like conditions for many workers and insufficient educational infrastructure. Population continued to increase faster than the economy could absorb There were some attempts to resettle peasant farmers onto non cultivated lands, but they were met with resistance. President Gustavo Díaz Ordaz awarded a land grant to the town of Venustiano Carranza in 1967, but that land was already being used by cattle-ranchers who refused to leave. The peasants tried to take over the land anyway, but when violence broke out, they were forcibly removed.
These events began to lead to political crises in the 1970s, with more frequent land invasions and takeovers of municipal halls. This was the beginning of a process that would lead to the emergence of the Zapatista movement in the 1990s. Another important factor to this movement would be the role of the Catholic Church from the 1960s to the 1980s. In 1960, Samuel Ruiz became the bishop of the Diocese of Chiapas, centered in San Cristóbal. He supported and worked with Marist priests and nuns following an ideology called liberation theology. In 1974, he organized a statewide "Indian Congress" with representatives from the Tzeltal, Tzotzil, Tojolabal and Ch'ol peoples from 327 communities as well as Marists and the Maoist People's Union. This congress was the first of its kind with the goal of uniting the indigenous peoples politically. These efforts were also supported by leftist organizations from outside Mexico, especially to form unions of ejido organizations. These unions would later form the base of the EZLN organization. One reason for the Church's efforts to reach out to the indigenous population was that starting in the 1970s, a shift began from traditional Catholic affiliation to Protestant, Evangelical and other Christian sects.
The 1980s saw a large wave of refugees coming into the state from Central America as a number of these countries, especially Guatemala, were in the midst of violent political turmoil. The Chiapas/Guatemala border had been relatively porous with people traveling back and forth easily in the 19th and 20th centuries, much like the Mexico/U.S. border around the same time. This is in spite of tensions caused by Mexico's annexation of the Soconusco region in the 19th century. The border between Mexico and Guatemala had been traditionally poorly guarded, due to diplomatic considerations, lack of resources and pressure from landowners who need cheap labor sources.
The arrival of thousands of refugees from Central America stressed Mexico's relationship with Guatemala, at one point coming close to war as well as a politically destabilized Chiapas. Although Mexico is not a signatory to the UN Convention Relating to the Status of Refugees, international pressure forced the government to grant official protection to at least some of the refugees. Camps were established in Chiapas and other southern states, and mostly housed Mayan peoples. However, most Central American refugees from that time never received any official status, estimated by church and charity groups at about half a million from El Salvador alone. The Mexican government resisted direct international intervention in the camps, but eventually relented somewhat because of finances. By 1984, there were 92 camps with 46,000 refugees in Chiapas, concentrated in three areas, mostly near the Guatemalan border. To make matters worse, the Guatemalan army conducted raids into camps on Mexican territories with significant casualties, terrifying the refugees and local populations. From within Mexico, refugees faced threats by local governments who threatened to deport them, legally or not, and local paramilitary groups funded by those worried about the political situation in Central American spilling over into the state. The official government response was to militarize the areas around the camps, which limited international access and migration into Mexico from Central America was restricted. By 1990, it was estimated that there were over 200,000 Guatemalans and half a million from El Salvador, almost all peasant farmers and most under age twenty.
In the 1980s, the politization of the indigenous and rural populations of the state began in the 1960s and 1970s continued. In 1980, several ejido (communal land organizations) joined to form the Union of Ejidal Unions and United Peasants of Chiapas, generally called the Union of Unions or UU. It had a membership of 12,000 families from over 180 communities. By 1988, this organization joined with other to form the ARIC-Union of Unions (ARIC-UU) and took over much of the Lacandon Jungle portion of the state. Most of the members of these organization were from Protestant and Evangelical sects as well as "Word of God" Catholics affiliated with the political movements of the Diocese of Chiapas. What they held in common was indigenous identity vis-à-vis the non-indigenous, using the old 19th century "caste war" word "Ladino" for them.
The adoption of neoliberalism by the Mexican federal government clashed with the leftist political ideals of these groups, especially as the reforms began to have negative economic effects on poor farmers, especially small-scale indigenous coffee growers. This would coalese into the Zapatista movement in the 1990s. Although the Zapatista movement couched its demands and cast is role in response to contemporary issues, especially in its opposition to neoliberalism, it is one of a long line of peasant and indigenous uprisings that have occurred in the state since the colonial era. This is reflected in its indigenous vs. Ladino character. However, the movement was an economic one as well. Although rich in resources, much of the local population of the state, especially in rural areas, did not benefit from this. In the 1990s, two thirds of the states residents did not have sewage service, only a third had electricity and half did not have potable water. Over half of the schools offered education only to the third grade and most dropped out by the end of first grade. These grievances, which were strongest in the San Cristóbal and Lacandon Jungle areas, were taken up by a small leftist guerrilla band led by a man called only "Subcomandante Marcos."
This small band, called the Zapatista Army of National Liberation (Ejército Zapatista de Liberación Nacional, EZLN), came to the world's attention when on January 1, 1994, the day the NAFTA treaty went into effect. On this day, EZLN forces occupied and took over the towns of San Cristobal de las Casas, Las Margaritas, Altamirano, Ocosingo and three others. They read their proclamation of revolt to the world and then laid siege to a nearby military base, capturing weapons and releasing many prisoners from the jails. This action followed previous protests in the state in opposition to neoliberal economic policies.
Although it has been estimated at having no more than 300 armed guerrilla members, the EZLN paralyzed the Mexican government, as it could not afford the political risks of direct confrontation. The major reason for this was that the rebellion caught the attention of the national and world press, as Marcos made full use of the then-new Internet to get the group's message out, putting the spotlight on indigenous issues in Mexico in general. It was also actively supported by opposition press in Mexico City, especially "La Jornada". However, these elements did provoke the rebellion to go national. Many blamed the unrest on infiltration of leftists among the large Central American refugee population in Chiapas, and the rebellion opened up splits in the countryside with those supporting and opposing EZLN. Zapatista sympathizers have included mostly Protestants and Word of God Catholics, versus those "traditionalist" Catholics who practiced a syncretic form of Catholicism and indigenous beliefs. This split had existed in Chiapas since the 1970s, with the latter group supported by the caciques and others in the traditional power structure. Protestants and Word of God Catholics (allied directly with the bishopric in San Cristóbal) tended to oppose traditional power structures.
The reaction of the Bishop Samuel Ruiz and the Diocese of Chiapas was to offer to mediate between the rebels and authorities. However, because of this diocese's activism since the 1960s, authorities accused the clergy of being involved with the rebels. There was some ambiguity about the relationship between Ruiz and Marcos and it was a constant feature of news coverage, with many in official circles using such to discredit Ruiz. Eventually, the activities of the Zapatistas began to worry the Roman Catholic Church in general and upstage the diocese's attempts to re establish itself among Chiapan indigenous communities against Protestant evangelization. This would lead to a breach between the Church and the Zapatistas.
The Zapatista story remained in headlines for a number of years. One reason for this was the December 1997 massacre of forty-five Tzotzil peasants, mostly women and children in the Zapatista-controlled village of Acteal in the Chenhaló municipality just north of San Cristóbal. This allowed many media outlets in Mexico to step up their criticisms of the government. However, the massacre was not done by the government but by other civilians, which shows how the emergence of the Zapatista movement had divided indigenous groups.
Despite this, the armed conflict was brief, mostly because the Zapatistas did not try to gain traditional political power like many other guerilla movements. Its focus was more on trying to manipulate public opinion in order to obtain concessions from the government. This has linked the Zapatistas to other indigenous and identity-politics movements that arose in the late 20th century. The main concession that the group received was the San Andrés Accords, also known as the Law on Indian Rights and Culture. The Accords appear to grant certain indigenous zones autonomy, but this is against the Mexican constitution, so its legitimacy has been questioned. Zapatista declarations since the mid-1990s have called for a new constitution. To the present, the government has not found a solution to this problem. The revolt also pressed the government to institute anti poverty programs such as "Progresa" later called "Oportunidades" and the "Puebla-Panama Plan" aimed to increase trade between southern Mexico and Central America.
As of the late 2000s, the Zapatista movement remains popular in many indigenous communities. The uprising gave indigenous peoples a more active role in the state's politics. However, it did not solve the economic issues that many peasant farmers face, especially the lack of land to cultivate. This problem has been at crisis proportions since the 1970s, and the government's reaction has been to encourage peasant farmers—mostly indigenous—to migrate into the sparsely populated Lacandon Jungle, a trend since earlier in the century.
From the 1970s on, some 100,000 people set up homes in this rainforest area, with many being recognized as ejidos, or communal land holding organizations. These migrants included Tzeltals, Tojolabals, Ch'ols and mestizos, mostly farming corn and beans and raising livestock. However, the government changed policies in the late 1980s with establishment of the Montes Azules Biosphere Reserve as much of the Lacandon Jungle had been destroyed or severely damaged. While armed resistance had wound down, the Zapatistas have remained a strong political force, especially around San Cristóbal and the Lacandon Jungle, its traditional bases. Since the Accords, they have shifted focus in gaining autonomy for the communities they control.
Since the 1994 uprising, migration into the Lacandon Jungle has significantly increased including illegal settlements and cutting in the protected biosphere reserve. These actions are supported by the Zapatistas as part of indigenous rights, but it has put them in conflict with international environmental groups and the indigenous inhabitants of the rainforest area, the Lacandons. Environmental groups state that the settlements pose grave risks to what remains of the Lacandon, while the Zapatistas accuse them of being fronts for the government, who want to open the rainforest up to multinational corporations. Added to this is the possibility that there are significant oil and gas deposits under this area as well.
The Zapatista movement has had some successes. The agricultural sector of the economy now favors ejidos and other commonly owned land. There have been some other gains economically as well. In the last decades of the 20th century, Chiapas's traditional agricultural economy has diversified somewhat with the construction of more roads and better infrastructure by the federal and state governments. At this time, tourism has become important in some areas of the state, especially in San Cristóbal de las Casas and Palenque. Its economy is important to Mexico as a whole as well, producing coffee, corn, cacao, tobacco, sugar, fruit, vegetable and honey for export. It is also a key state for the nation's petrochemical and hydroelectric industries. A significant percentage of PEMEX's drilling and refining is based in Chiapas and Tabasco, and fifty five percent of the nations hydroelectric energy is produced in Chiapas.
However, Chiapas remains one of the poorest states in Mexico. Ninety-four of its 111 municipalities have a large percentage of the population living in poverty. In areas such as Ocosingo, Altamirano and Las Margaritas, the towns where the Zapatistas first came into prominence in 1994, 48% of the adults are illiterate. Chiapas is still considered isolated and distant from the rest of Mexico, both culturally and geographically. It has significantly underdeveloped infrastructure compared to the rest of the country and its significant indigenous population with isolationist tendencies keep the state distinct culturally. Cultural stratification, neglect and lack of investment by the Mexican federal government has exacerbated this problem.
Geography.
Political geography.
Chiapas is located in the south east of Mexico, bordering the states of Tabasco, Veracruz and Oaxaca with the Pacific Ocean to the south and Guatemala to the east. It has a territory of 74,415 km2, the eighth largest state in Mexico. The state consists of 118 municipalities organized into nine political regions called Center, Altos, Fronteriza, Frailesca, Norte, Selva, Sierra, Soconusco and Istmo-Costa. There are 18 cities, twelve towns (villas) and 111 pueblos (villages). Major cities include Tuxtla Gutiérrez, San Cristóbal de las Casas, Tapachula, Palenque, Comitán, and Chiapa de Corzo.
Geographical regions.
The state has a complex geography with seven distinct regions according to the Mullerried classification system. These include the Pacific Coast Plains, the Sierra Madre de Chiapas, the Central Depression, the Central Highlands, the Eastern Mountains, the Northern Mountains and the Gulf Coast Plains. The Pacific Coast Plains is a strip of land parallel to the ocean. It is composed mostly of sediment from the mountains that border it on the northern side. It is uniformly flat, and stretches from the Bernal Mountain south to Tonalá. It has deep salty soils due to its proximity to the sea. It has mostly deciduous rainforest although most has been converted to pasture for cattle and fields for crops. It has numerous estuaries with mangroves and other aquatic vegetation.
The Sierra Madre de Chiapas runs parallel to the Pacific coastline of the state, northwest to southeast as a continuation of the Sierra Madre del Sur. This area has the highest altitudes in Chiapas including the Tacaná Volcano, which rises 4,093 meters above sea level. Most of these mountains are volcanic in origin although the nucleus is metamorphic rock. It has a wide range of climates but little arable land. It is mostly covered in middle altitude rainforest, high altitude rainforest, and forests of oaks and pines. The mountains partially block rain clouds from the Pacific, a process known as Orographic lift, which creates a particularly rich coastal region called the Soconusco. The main commercial center of the sierra is the town of Motozintla, also near the Guatemalan border.
The Central Depression is in the center of the state. It is an extensive semi flat area bordered by the Sierra Madre de Chiapas, the Central Highlands and the Northern Mountains. Within the depression there are a number of distinct valleys. The climate here can be very hot and humid in the summer, especially due to the large volume of rain received in July and August. The original vegetation was lowland deciduous rainforest with some rainforest of middle altitudes and some oaks above 1500masl.
The Central Highlands, also referred to as Los Altos, are mountains oriented from northwest to southeast with altitudes ranging from twelve to sixteen hundred meters above sea level. The western highlands are displaced faults, while the eastern highlands are mainly folds of sedimentary formations—mainly limestone, shale, and sandstone. These mountains, along the Sierra Madre of Chiapas become the Cuchumatanes where they extend over the border into Guatemala. Its topography is mountainous with many narrow valleys and karst formations called uvalas or poljés, depending on the size. Most of the rock is limestone allowing for a number of formations such as caves and sinkholes. There are also some isolated pockets of volcanic rock with the tallest peaks being the Tzontehuitz and Huitepec volcanos. There are no significant surface water systems as they are almost all underground. The original vegetation was forest of oak and pine but these have been heavily damaged. The highlands climate in the Koeppen modified classification system for Mexico is humid temperate C(m) and subhumid temperate C (w 2 ) (w). This climate exhibits a summer rainy season and a dry winter, with possibilities of frost from December to March. The Central Highlands have been the population center of Chiapas since the Conquest. European epidemics were hindered by the tierra fría climate, allowing the indigenous peoples in the highlands to retain their large numbers.
The Eastern Mountains (Montañas del Oriente) are in the east of the state, formed by various parallel mountain chains mostly made of limestone and sandstone. Its altitude varies from 500 to 1500 masl. This area receives moisture from the Gulf of Mexico with abundant rainfall and exuberant vegetation, which creates the Lacandon Jungle, one of the most important rainforests in Mexico. The Northern Mountains (Montañas del Norte) are in the north of the state. They separate the flatlands of the Gulf Coast Plains from the Central Depression. Its rock is mostly limestone. These mountains also receive large amounts of rainfall with moisture from the Gulf of Mexico giving it a mostly hot and humid climate with rains year round. In the highest elevations around 1800 masl, temperatures are somewhat cooler and do experience a winter. The terrain is rugged with small valleys whose natural vegetation is high altitude rainforest.
The Gulf Coast Plains (Llanura Costera del Golfo) stretch into Chiapas from the state of Tabasco, which gives it the alternate name of the Tabasqueña Plains. These plains are found only in the extreme north of the state. The terrain is flat and prone to flooding during the rainy season as it was built by sediments deposited by rivers and streams heading to the Gulf.
Lacandon Jungle.
The Lacandon Jungle is situated in north eastern Chiapas, centered on a series of canyonlike valleys called the Cañadas, between smaller mountain ridges oriented from northwest to southeast. The ecosystem covers an area of approximately 1.9 million hectares extending from Chiapas into northern Guatemala and southern Yucatán Peninsula and into Belize. This area contains as much as 25% of Mexico's total species diversity, most of which has not been researched. It has a predominately hot and humid climate (Am w" i g) with most rain falling from summer to part of fall, with an average of between 2300 and 2600 mm per year. There is a short dry season from March to May. The predominate wild vegetation is perennial high rainforest. The Lacandon comprises a biosphere reserve (Montes Azules); four natural protected areas (Bonampak, Yaxchilan, Chan Kin, and Lacantum); and the communal reserve (La Cojolita), which functions as a biological corridor with the area of Petén in Guatemala. Flowing within the Rainforest is the Usumacinta River, considered to be one of the largest rivers in Mexico and seventh largest in the world based on volume of water.
During the 20th century, the Lacandon has had a dramatic increase in population and along with it, severe deforestation. The population of municipalities in this area, Altamirano, Las Margaritas, Ocosingo and Palenque have risen from 11,000 in 1920 to over 376,000 in 2000. Migrants include Ch'ol, Tzeltal, Tzotzil, Tojolabal indigenous peoples along with mestizos, Guatemalan refugees and others. Most of these migrants are peasant farmers, who cut forest to plant crops. However, the soil of this area cannot support annual crop farming for more than three or four harvents. The increase in population and the need to move on to new lands has pitted migrants against each other, the native Lacandon people, and the various ecological reserves for land. It is estimated that only ten percent of the original Lacandon rainforest in Mexico remains, with the rest strip-mined, logged and farmed. It once stretched over a large part of eastern Chiapas but all that remains is along the northern edge of the Guatemalan border. Of this remaining portion, Mexico is losing over five percent each year.
The best preserved portion of the Lacandon is within the Montes Azules Biosphere Reserve. It is centered on what was a commercial logging grant by the Porfirio Díaz government, which the government later nationalized. However, this nationalization and conversion into a reserve has made it one of the most contested lands in Chiapas, with the already existing ejidos and other settlements within the park along with new arrivals squatting on the land.
Soconusco.
The Soconusco region encompasses a coastal plain and a mountain range with elevations of up to 2000 meters above sea levels paralleling the Pacific Coast. The highest peak in Chiapas is the Tacaná Volcano at 4,800 meters above sea level. In accordance with an 1882 treaty, the dividing line between Mexico and Guatemala goes right over the summit of this volcano. The climate is tropical, with a number of rivers and evergreen forests in the mountains. This is Chiapas’ major coffee producing area, as it has the best soils and climate for coffee.
Before the arrival of the Spanish, this area was the principal source of cocoa seeds in the Aztec empire, which they used as currency, and for the highly prized quetzal feathers used by the nobility. It would become the first area to produce coffee, introduced by an Italian entrepreneur on the La Chacara farm. Coffee is cultivated on the slopes of these mountains mostly between 600 and 1200 masl. Mexico produces about 4 million sacks of green coffee each year, fifth in the world behind Brazil, Colombia, Indonesia and Vietnam. Most producers are small with plots of land under five hectares. From November to January, the annual crop is harvested and processed employing thousands of seasonal workers. Lately, a number of coffee haciendas have been developing tourism infrastructure as well.
Environment and protected areas.
Chiapas is located in the tropical belt of the planet, but the climate is moderated in many areas by altitude. For this reason, there are hot, semi-hot, temperate and even cold climates. There are areas with abundant rainfall year round along with those that receive most of their rain from May to October with a dry season from November to April. The mountain areas affect wind and moisture flow over the state, concentrating moisture in certain areas of the state. They also are responsible for some cloud-covered rainforest areas in the Sierra Madre.
Chiapas' rainforests are home to thousands of animals and plants, some of which cannot be found anywhere else in the world. Natural vegetation varies from lowland to highland tropical forest, pine and oak forests in the highest altitudes and plains area with some grassland. Chiapas is ranked second in forest resources in Mexico with valued woods such as pine, cypress, "Liquidambar", oak, cedar, mahogany and more. The Lacandon Jungle is one of the last major tropical rainforests in the northern hemisphere with an extension of 600,000 ha. It contains about sixty percent of Mexico’s tropical tree species, 3,500 species of plants, 1,157 species of invertebrates and over 500 of vertebrate species. Chiapas has one of the greatest diversities in wildlife in the Americas. There are more than 100 species of amphibians, 700 species of birds, fifty of mammals and just over 200 species of reptiles. In the hot lowlands, there are armadillos, monkeys, pelicans, wild boar, jaguars, crocodiles, iguanas and many others. In the temperate regions there are species such as bobcats, salamanders, a large red lizard Abronia lythrochila, weasels, opossums, deer, ocelots and bats. The coastal areas have large quantities of fish, turtles, and crustaceans, with many species in danger of extinction or endangered as they are endemic only to this area. The total biodiversity of the state is estimated at over 50,000 species of plants and animals. The diversity of species is not limited to the hot lowlands but in the higher altitudes as well with mesophile forests, oak/pine forests in the Los Altos, Northern Mountains and Sierra Madre and the extensive estuaries and mangrove wetlands along the coast.
Chiapas has about thirty percent of Mexico’s fresh water resources. The Sierra Madre divides them into those that flow to the Pacific and those that flow to the Gulf of Mexico. Most of the first are short rivers and streams; most longer ones flow to the Gulf. Most Pacific side rivers do not drain directly into this ocean but into lagoons and estuaries. The two largest rivers are the Grijalva and the Usumacinta, with both part of the same system. The Grijalva has four dams built on it the Belisario Dominguez (La Angostura); Manuel Moreno Torres (Chicoasén); Nezahualcóyotl (Malpaso); and Angel Albino Corzo (Peñitas). The Usumacinta divides the state from Guatemala and is the longest river in Central America. In total, the state has 110,000 ha of surface waters, 260 km of coastline, control of 96,000 km2 of ocean, 75,230 ha of estuaries and ten lake systems. Laguna Miramar is a lake in the Montes Azules reserve and the largest in the Lacandon Jungle at 40 km in diameter. The color of its waters varies from indigo to emerald green and in ancient times, there were settlements on its islands and its caves on the shoreline. The Catazajá Lake is 28 km north of the city of Palenque. It is formed by rainwater captured as it makes it way to the Usumacinta River. It contains wildlife such as manatees and iguanas and it is surrounded by rainforest. Fishing on this lake is an ancient tradition and the lake has an annual bass fishing tournament. The Welib Já Waterfall is located on the road between Palenque and Bonampak.
The state has thirty-six protected areas at the state and federal levels along with 67 areas protected by various municipalities. The Sumidero Canyon National Park was decreed in 1980 with an extension of 21,789 ha. It extends over two of the regions of the state, the Central Depression and the Central Highlands over the municipalities of Tuxtla Gutiérrez, Nuevo Usumacinta, Chiapa de Corzo and San Fernando. The canyon has steep and vertical sides that rise to up to 1000 meters from the river below with mostly tropical rainforest but some areas with xerophile vegetation such as cactus can be found. The river below, which has cut the canyon over the course of twelve million years, is called the Grijalva. The canyon is emblematic for the state as it is featured in the state seal. The Sumidero Canyon was once the site of a battle between the Spaniards and Chiapanecan Indians. Many Chiapanecans chose to throw themselves from the high edges of the canyon rather than be defeated by Spanish forces. Today, the canyon is a popular destination for ecotourism. Visitors often take boat trips down the river that runs through the canyon and enjoy the area's natural beauty including the many birds and abundant vegetation.
The Montes Azules Integral Biosphere Reserve was decreed in 1978. It is located in the northeast of the state in the Lacandon Jungle. It covers 331,200 ha in the municipalities of Maravilla Tenejapa, Ocosingo and Las Margaritas. It conserves highland perennial rainforest. The jungle is in the Usumacinta River basin east of the Chiapas Highlands. It is recognized by the United Nations Environment Programme for its global biological and cultural significance. In 1992, the 61,874 ha Lacantun Reserve, which includes the Classic Maya archaeological sites of Yaxchilan and Bonampak, was added to the biosphere reserve.
Agua Azul Waterfall Protection Area is in the Northern Mountains in the municipality of Tumbalá. It covers an area of 2,580 ha of rainforest and pine-oak forest, centered on the waterfalls it is named after. It is located in an area locally called the "Mountains of Water", as many rivers flow through there on their way to the Gulf of Mexico. The rugged terrain encourages waterfalls with large pools at the bottom, that the falling water has carved into the sedimentary rock and limestone. Agua Azul is one of the best known in the state. The waters of the Agua Azul River emerge from a cave that forms a natural bridge of thirty meters and five small waterfalls in succession, all with pools of water at the bottom. In addition to Agua Azul, the area has other attractions—such as the Shumuljá River, which contains rapids and waterfalls, the Misol Há Waterfall with a thirty-meter drop, the Bolón Ajau Waterfall with a fourteen-meter drop, the Gallito Copetón rapids, the Blacquiazules Waterfalls, and a section of calm water called the Agua Clara.
The El Ocote Biosphere Reserve was decreed in 1982 located in the Northern Mountains at the boundary with the Sierra Madre del Sur in the municipalities of Ocozocoautla, Cintalapa and Tecpatán. It has a surface area of 101,288.15 ha and preserves a rainforest area with karst formations. The Lagunas de Montebello National Park was decreed in 1959 and consists of 7371 ha near the Guatemalan border in the municipalities of La Independencia and La Trinitaria. It contains two of the most threatened ecosystems in Mexico the "cloud rainforest" and the Soconusco rainforest. The El Triunfo Biosphere Reserve, decreed in 1990, is located in the Sierra Madre de Chiapas in the municipalities of Acacoyagua, Ángel Albino Corzo, Montecristo de Guerrero, La Concordia, Mapastepec, Pijijiapan, Siltepec and Villa Corzo near the Pacific Ocean with 119,177.29 ha. It conserves areas of tropical rainforest and many freshwater systems endemic to Central America. It is home to around 400 species of birds including several rare species such as the horned guan, the quetzal and the azure-rumped tanager. The Palenque National Forest is centered on the archaeological site of the same name and was decreed in 1981. It is located in the municipality of Palenque where the Northern Mountains meet the Gulf Coast Plain. It extends over 1381 ha of tropical rainforest. The Laguna Bélgica Conservation Zone is located in the north west of the state in the municipality of Ocozocoautla. It covers forty-two hectares centered on the Bélgica Lake. The El Zapotal Ecological Center was established in 1980. Nahá – Metzabok is an area in the Lacandon Jungle whose name means "place of the black lord" in Nahuatl. It extends over 617.49 km2 and in 2010, it was included in the World Network of Biosphere Reserves. Two main communities in the area are called Nahá and Metzabok. They were established in the 1940s, but the oldest communities in the area belong to the Lacandon people. The area has large numbers of wildlife including endangered species such as eagles, quetzals and jaguars.
Demographics.
General statistics.
As of 2010, the population is 4,796,580, the eighth most populous state in Mexico. The 20th century saw large population growth in Chiapas. From fewer than one million inhabitants in 1940, the state had about two million in 1980, and over 4 million in 2005. Overcrowded land in the highlands was relieved when the rainforest to the east was subject to land reform. Cattle ranchers, loggers, and subsistence farmers migrated to the rain forest area. The population of the Lacandon was only one thousand people in 1950, but by the mid-1990s this had increased to 200 thousand. As of 2010, 78% lives in urban communities with 22% in rural communities. While birthrates are still high in the state, they have come down in recent decades from 7.4 per woman in 1950. However, these rates still mean significant population growth in raw numbers. About half of the state's population is under age 20, with an average age of 19. In 2005, there were 924,967 households, 81% headed by men and the rest by women. Most households were nuclear families (70.7%) with 22.1% consisting of extended families.
More migrate out of Chiapas than migrate in, with emigrants leaving for Tabasco, Oaxaca, Veracruz, State of Mexico and the Federal District primarily.
While Catholics remain the majority, their numbers has dropped as many have converted to Protestant sects in recent decades. The National Presbyterian Church in Mexico has a large followers in Chiapas; some estimate that 40% of the population are followers of the Presbyterian church.
There are a number of people in the state with African features. These are the descendents of slaves brought to the state in the 16th century. There are also those with predominantly European features who are the descendents of the original Spanish colonizers as well as later immigrants to Mexico. The latter mostly came at the end of the 19th and early 20th century under the Porfirio Díaz regime to start plantations.
Indigenous population.
Numbers and influence.
Over the history of Chiapas, there have been three main indigenous groups: the Mixes-Zoques, the Mayas and the Chiapa . Today, there are an estimated fifty-six linguistic groups. As of the 2005 Census, there were 957,255 people who spoke an indigenous language out of a total population of about 3.5 million. Of this one million, one third do not speak Spanish. Out of Chiapas' 111 municipios, ninety-nine have significant indigenous populations. Twenty two municipalities have indigenous populations over 90 percent, and 36 municipalities have native populations exceeding 50 percent. However, despite population growth in indigenous villages, the percentage of indigenous to non indigenous continues to fall with less than 35% indigenous. Indian populations are concentrated in a few areas, with the largest concentration of indigenous-language-speaking individuals is living in five of Chiapas's nine economic regions: Los Altos, Selva, Norte, Fronteriza, and Sierra. The remaining four regions, Centro, Frailesca, Soconusco, and Costa, have populations that are considered to be dominantly mestizo .
The state has about 13.5% of all of Mexico's indigenous population, and it has been ranked among the ten "most indianized" states, with only Campeche, Oaxaca, Quintana Roo and Yucatán having been ranked above it between 1930 and the present. These indigenous peoples have been historically resistant to assimilation into the broader Mexican society, with it best seen in the retention rates of indigenous languages and the historic demands for autonomy over geographic areas as well as cultural domains. Much of the latter has been prominent since the Zapatista uprising in 1994.
Most of Chiapas' indigenous groups are descended from the Mayans, speaking languages that are closely related to one another, belonging to the Western Maya language group. The state was part of a large region dominated by the Mayans during the Classic period. The most numerous of these Mayan groups include the Tzeltal, Tzotzil, Ch'ol, Zoque, Tojolabal, Lacandon and Mam, which have traits in common such as syncretic religious practices, and social structure based on kinship. The most common Western Maya languages are Tzeltal and Tzotzil along with Chontal, Ch’ol, Tojolabal, Chuj, Kanjobal, Acatec, Jacaltec and Motozintlec.
Twelve of Mexico's officially recognized native peoples live in the state have conserved their language, customs, history dress and traditions to a significant degree. The primary groups include the Tzeltal, Tzotzil, Ch'ol, Tojolabal, Zoque, Chuj, Kanjobal, Mam, Jacalteco, Mochó Cakchiquel and Lacandon. Most indigenous communities are found in the municipalities of the Centro, Altos, Norte and Selva regions, with many having indigenous populations of over fifty percent. These include Bochil, Sitalá, Pantepec, Simojovel to those with over ninety percent indigenous such as San Juan Cancuc, Huixtán, Tenejapa, Tila, Oxchuc, Tapalapa, Zinacantán, Mitontic, Ocotepec, Chamula, and Chalchihuitán. The most numerous indigenous communities are the Tzeltal and Tzotzil peoples, who number about 400,000 each, together accounting for about half of the state's indigenous population. The next most numerous are the Ch’ol with about 200,000 people and the Tojolabal and Zoques, who number about 50,000 each. The top 3 municipalities in Chiapas with indigenous language speakers 3 years of age and older are: Ocosingo (133,811), Chilon (96,567), and San Juan Chamula (69,475). These three municipalities accounted for 24.8% (299,853) of all indigenous language speakers 3 years or older in the state of Chiapas, out of a total of 1,209,057 indigenous language speakers 3 years or older.
Although most indigenous language speakers are bilingual, especially in the younger generations, many of these languages have shown resilience. Four of Chiapas' indigenous languages Tzeltal, Tzotzil, Tojolabal and Chol are high-vitality languages, meaning that a high percentage of these ethnicities speak the language and that there is a high rate of monolingualism in it. It is used in over 80% of homes. Zoque is considered to be of medium-vitality with a rate of bilingualism of over 70% and home use somewhere between 65% and 80%. Maya is considered to be of low-vitality with almost all of its speakers bilingual with Spanish. The most spoken indigenous languages as of 2010 are Tzeltal with 461,236 speakers, Tzotzil with 417,462, Ch’ol with 191,947 and Zoque with 53,839. In total, there are 1,141,499 who speak an indigenous language or 27% of the total population. Of these 14% do not speak Spanish. Studies done between 1930 and 2000 have indicated that Spanish is not dramatically displacing these languages. In raw number, speakers of these languages are increasing, especially among groups with a long history of resistance to Spanish/Mexican domination. Language maintenance has been strongest in areas related to where the Zapatista uprising took plaza such as the municipalities of Altamirano, Chamula, Chanal, Larráinzar, Las Margaritas, Ocosingo, Palenque, Sabanilla, San Cristóbal de Las Casas and Simojovel.
The state's rich indigenous tradition along with its associated political uprisings, especially that of 1994, has great interest from other parts of Mexico and abroad. It has been especially appealing to a variety of academics including many anthropologists, archeologists, historians, psychologists and sociologists. The concept of "mestizo" or mixed indigenous European heritage became important to Mexico's identity by the time of Independence, but Chiapas has kept its indigenous identity to the present day. Since the 1970s, this has been supported by the Mexican government as it has shifted from cultural policies that favor a "multicultural" identity for the country. One major exception to the separatist, indigenous identity has been the case of the Chiapa people, from whom the state's name comes, who have mostly been assimilated and intermarried into the mestizo population.
Most Indigenous communities have economies based primarily on traditional agriculture such as the cultivation and processing of corn, beans and coffee as a cash crop and in the last decade, many have begun producing sugarcane and jatropha for refinement into biodiesel and ethanol for automobile fuel. The raising of livestock, particularly chicken and turkey and to a lesser extent beef and farmed fish is also a major economic activity. Many indigenous, in particular the Maya are employed in the production of traditional clothing, fabrics, textiles, wood items, artworks and traditional goods such as jade and amber works. Tourism has provided a number of a these communities with markets for their handcrafts and works, some of which are very profitable.
San Cristóbal de las Casas and San Juan Chamula maintain a strong indigenous identity. On market day, many indigenous from rural areas come into San Cristóbal to sell and buy mostly items for everyday use such as fruit, vegetables, animals, cloth, consumer goods and tools. San Juan Chamula is considered to be a center of indigenous culture, especially its elaborate festivals of Carnival and Day of Saint John. It was common for politicians, especially during Institutional Revolutionary Party's dominance to visit here during election campaigns and dress in indigenous clothing and carry a carved walking stick, a traditional sign of power. Relations between the indigenous ethnic groups is complicated. While there have been inter ethnic political activism such as that promoted by the Diocese of Chiapas in the 1970s and the Zapatista movement in the 1990s, there has also been inter-indigenous conflict as well. Much of this has been based on religion, pitting those of the traditional Catholic/indigenous beliefs who support the traditional power structure against Protestants, Evangelicals and Word of God Catholics (directly allied with the Diocese) who tend to oppose it. This is particularly significant problem among the Tzeltals and Tzotzils. Starting in the 1970s, traditional leaders in San Juan Chamula began expelling dissidents from their homes and land, amounting to about 20,000 indigenous forced to leave over a thirty-year period. It continues to be a serious social problem although authorities downplay it. Recently there has been political, social and ethnic conflict between the Tzotzil who are more urbanized and have a significant number of Protestant practitioners and the Tzeltal who are predominantly Catholic and live in smaller farming communities. Many Protestant Tzotzil have accused the Tzeltal of ethnic discrimination and intimidation due to their religious beliefs and the Tzeltal have in return accused the Tzotzil of singling them out for discrimination.
Clothing, especially women’s clothing, varies by indigenous group. For example, women in Ocosingo tend to wear a blouse with a round collar embroidered with flowers and a black skirt decorated with ribbons and tied with a cloth belt. The Lacandon people tend to wear a simple white tunic. They also make a ceremonial tunic from bark, decorated with astronomy symbols. In Tenejapa, women wear a huipil embroidered with Mayan fretwork along with a black wool rebozo. Men wear short pants, embroidered at the bottom.
Tzeltals.
The Tzeltals call themselves Winik atel, which means "working men." This is the largest ethnicity in the state, mostly living southeast of San Cristóbal with the largest number in Amatenango. Today, there are about 500,000 Tzeltal Indians in Chiapas. Tzeltal Mayan, part of the Mayan language family, today is spoken by about 375,000 people making it the fourth-largest language group in Mexico. There are two main dialects; highland (or Oxchuc) and lowland (or Bachajonteco) . This language, along with Tzotzil, is from the Tzeltalan subdivision of the Mayan language family. Lexico-statistical studies indicate that these two languages probably became differentiated from one another around 1200 Most children are bilingual in the language and Spanish although many of their grandparents are monolingual Tzeltal speakers.
Each Tzeltal community constitutes a distinct social and cultural unit with its own well-defined lands, wearing apparel, kinship system, politico-religious organization, economic resources, crafts, and other cultural features. Women are distinguished by black skirt with a wool belt and an undyed cotton bloused embroidered with flowers. Their hair is tied with ribbons and covered with a cloth. Most men do not use traditional attire. Agriculture is the basic economic activity of the Tzeltal people. Traditional Mesoamerican crops such as maize, beans, squash, and chili peppers are the most important, but a variety of other crops, including wheat, manioc, sweet potatoes, cotton, chayote, some fruits, other vegetables, and coffee.
Tzotzils.
Tzotzil speakers number just slightly less than theTzeltals at 226,000, although those of the ethnicity are probably higher. Tzotzils are found in the highlands or Los Altos and spread out towards the northeast near the border with Tabasco. However, Tzotzil communities can be found in almost every municipality of the state. They are concentrated in Chamula, Zinacantán, Chenalhó, and Simojovel. Their language is closely related to Tzeltal and distantly related to Yucatec Mayan and Lacandon. Men dress in short pants tied with a red cotton belt and a shirt that hangs down to their knees. They also wear leather huaraches and a hat decorated with ribbons. The women wear a red or blue skirt a short huipil as a blouse and use a chal or rebozo to carry babies and bundles. Tzotzil communities are governed by a katinab who is selected for life by the leaders of each neighborhood. The Tzotzils are also known for their continued use of the temazcal for hygiene and medicinal purposes.
Ch’ols.
The Ch’ols of Chiapas migrated to the northwest of the state starting about 2,000 years ago, when they were concentrated in Guatemala and Honduras. Those Ch’ols who remained in the south are distinguished by the name Chortís. Chiapas Ch’ols are closely related to the Chontal in Tabasco as well. Choles are found in Tila, Tumbalá, Sabanilla, Palenque, and Salto de Agua, with an estimated population of about 115,000 people. The Ch’ol language belongs to the Maya family and is related to Tzeltal, Tzotzil, Lacandon, Tojolabal, and Yucatec Mayan. There are three varieties of Chol (spoken in Tila, Tumbalá, and Sabanilla), all mutually intelligible. Over half of speakers are monolingual in the Chol language. Women wear a long navy blue or black skirt with a white blouse heavily embroidered with bright colors and a sash with a red ribbon. The men only occasionally use traditional dress for events such as the feast of the Virgin of Guadalupe. This dress usually includes pants, shirts and huipils made of undyed cotton, with leather huaraches, a carrying sack and a hat. The fundamental economic activity of the Ch’ols is agriculture. They primarily cultivate corn and beans, as well as sugar cane, rice, coffee, and some fruits. They have Catholic beliefs strongly influenced by native ones. Harvests are celebrated on the Feast of Saint Rose on 30 August.
Tojolabals.
The Totolabals are estimated at 35,000 in the highlands. According to oral tradition, the Tojolabales came north from Guatemala. The largest community is Ingeniero González de León in the La Cañada region, an hour outside the municipal seat of Las Margaritas. Tojolabales are also found in Comitán, Trinitaria, Altamirano and La Independencia. This area is filled with rolling hills with a temperate and moist climate. There are fast moving rivers and jungle vegetation. Tojolabal is related to Kanjobal, but also to Tzeltal and Tzotzil. However, most of the youngest of this ethnicity speak Spanish. Women dress traditionally since childhood with brightly colored skirts decorated with lace or ribbons and a blouse decorated with small ribbons and they cover their heads with kerchiefs. They embroider many of their own clothes but do not sell them. Married women arrange their hair in two braids and single women wear it loose decorated with ribbons. Men no longer wear traditional garb daily as it is considered too expensive to make.
Zoques.
The Zoques are found in 3,000 square kilometers the center and west of the state scattered among hundreds of communities. These were one of the first native peoples of Chiapas, with archeological ruins tied to them dating back as far as 3500 BCE. Their language is not Mayan but rather related to Mixe, which is found in Oaxaca and Veracruz. By the time the Spanish arrived, they had been reduced in number and territory. Their ancient capital was Quechula, which was covered with water by the creation of the Malpaso Dam, along with the ruins of Guelegas, which was first buried by an eruption of the Chichonal volcano. There are still Zoque ruins at Janepaguay, the Ocozocuautla and La Ciénega valleys.
Lacandons.
The Lacandons are one of the smallest native indigenous groups of the state with a population estimated between 600 and 1000. They are mostly located in the communities of Lacanjá, Chansayab and Mensabak in the Lacandon Jungle. They live near the ruins of Bonampak and Yaxchilan and local lore states that the gods resided here when they lived on Earth. They inhabit about a million hectares of rainforest but from the 16th century to the present, migrants have taken over the area, most of which are indigenous from other areas of Chiapas. This dramatically altered their lifestyle and worldview. Traditional Lacandon shelters are huts made with fonds and wood with an earthen floor, but this has mostly given way to modern structures.
Mochós.
The Mochós or Motozintlecos are concentrated in the municipality of Motozintla on the Guatemalan border. According to anthropologists, these people are an "urban" ethnicity as they are mostly found in the neighborhoods of the municipal seat. Other communities can be found near the Tacaná volcano, and in the municipalities of Tuzantán and Belisario Dominguez. The name "Mochó" comes from a response many gave the Spanish who they could not understand and means "I don't know." This community is in the process of disappearing as their numbers shrink.
Mams.
The Mams are a Mayan ethnicity that numbers about 20,000 found in thirty municipalities, especially Tapachula, Motozintla, El Porvenir, Cacahoatán and Amatenango in the southeastern Sierra Madre of Chiapas. The Mame language is one of the most ancient Mayan languages with 5,450 Mame speakers were tallied in Chiapas in the 2000 census. These people first migrated to the border region between Chiapas and Guatemala at the end of the nineteenth century, establishing scattered settlements. In the 1960s, several hundred migrated to the Lacandon rain forest near the confluence of the Santo Domingo and Jataté Rivers. Those who live in Chiapas are referred to locally as the "Mexican Mam (or Mame)" to differientiate them from those in Guatemala. Most live around the Tacaná volcano, which the Mams call "our mother" as it is considered to be the source of the fertility of the area's fields. The masculine deity is the Tajumulco volcano, which is in Guatemala.
Guatemalan migrant groups.
In the last decades of the 20th century, Chiapas received a large number of indigenous refugees, especially from Guatemala, many of whom remain in the state. These have added ethnicities such as the Kekchi, Chuj, Ixil, Kanjobal, K'iche' and Cakchikel to the population. The Kanjobal mainly live along the border between Chiapas and Guatemala, with almost 5,800 speakers of the language tallied in the 2000 census. It is believed that a significant number of these Kanjobal-speakers may have been born in Guatemala and immigrated to Chiapas, maintaining strong cultural ties to the neighboring nation.
Economy.
Economic editators.
Chiapas accounts for 1.73% of Mexico's GDP. The primary sector, agriculture, produces 15.2% of the states GDP. The secondary sector, mostly energy production, but also commerce, services and tourism, accounts for 21.8%. The percentage of the GDP by commerce in services is rising while that of agriculture is falling. The state is divided into nine economic regions. These regions were established in the 1980s in order to facilitate statewide economic planning. Many of these regions are based on state and federal highway systems. These include Centro, Altos, Fronteriza, Frailesca, Norte, Selva, Sierra, Soconusco and Istmo-Costa.
Despite being rich in resources, Chiapas, along with Oaxaca and Guerrero, lags behind the rest of the country in almost all socioeconomic indicators. s of 2005[ [update]], there were 889,420 residential units, with 71% having running water, 77.3% having sewerage, and 93.6% having electricity. Construction of these units is varied from modern construction of block and concrete to those constructed of wood and laminate. Because of it high economic marginalization, more people migrate from Chiapas than migrate to it. Most of its socioeconomic indicators are the lowest in the country including income, education, health and housing. It has a significantly higher percentage of illiteracy than the rest of the country although that situation has improved since the 1970s when over 45% were illiterate and in the 1980s when about 32% were. The tropical climate presents health challenges, with most illnesses related to the gastro-intestinal tract and parasites. As of 2005, the state has 1,138 medical facilities: 1098 outpatient and 40 inpatient. Most are run by IMSS and ISSSTE and other government agencies. The implementation of NAFTA has had negative effects on the economy, often by lowering prices for agricultural products. It has also worked to make the southern states of Mexico poorer in comparison to those in the north with over 90% of the poorest municipalities in the south of the country. As of 2006, 31.8% work in communal services, social services and personal services. 18.4% work in financial services, insurance and real estate, 10.7% work in commerce, restaurants and hotels, 9.8% work in construction, 8.9% in utilities, 7.8% in transportation, 3.4% in industry (excluding handcrafts), and 8.4% in agriculture.
Although until the 1960s, many indigenous communities were considered by scholars to be autonomous and economically isolated, this was never the case. Economic conditions began forcing many to migrate to work, especially in agriculture for non- indigenous. However, unlike many other migrant workers, most indigenous in Chiapas have remained strongly tied to their home communities. A study as early as the 1970s showed that 77 percent of heads of household migrated outside of the Chamula municipality as local land did not produce sufficiently to support families. In the 1970s, cuts in the price of corn forced many large landowners to convert their fields into pasture for cattle, displacing many hired laborers as cattle required less work. These agricultural laborers began to work for the government on infrastructure projects financed by oil revenue. It is estimated that in the 1980s to 1990s as many as 100,000 indigenous people moved from the mountain areas into cities in Chiapas, with some moving out of the state to Mexico City, Cancún and Villahermosa in search of employment.
Agriculture, livestock, forestry and fishing.
Agriculture, livestock, forestry and fishing employ over 53% of the state’s population; however, its productivity is considered to be low. Agriculture includes both seasonal and perennial plants. Major crops include corn, beans, sorghum, soybeans, peanuts, sesame seeds, coffee, cacao, sugar cane, mangos, bananas, and palm oil. These crops take up 95% of the cultivated land in the state and 90% of the agricultural production. Only four percent of fields are irrigated with the rest dependent on rainfall either seasonally or year round. Chiapas ranks second among the Mexican states in the production of cacao, the product used to make chocolate, and is responsible for about 60 percent of Mexico's total coffee output. The production of bananas, cacao and corn make Chiapas Mexico's second largest agricultural producer overall.
Coffee is the state's most important cash crop with a history from the 19th century. The crop was introduced in 1846 by Jeronimo Manchinelli who brought 1,500 seedlings from Guatemala on his farm La Chacara. This was followed by a number of other farms as well. Coffee production intensified during the regime of Porfirio Díaz and the Europeans who came to own many of the large farms in the area. By 1892, there were 22 coffee farms in the region, among them Nueva Alemania, Hamburgo, Chiripa, Irlanda, Argovia, San Francisco, and Linda Vista in the Soconusco region. Since then coffee production has grown and diversified to include large plantations, the use and free and forced labor and a significant sector of small producers. While most coffee is grown in the Soconusco, other areas grow it, including the municipalities of Oxchuc, Pantheló, El Bosque, Tenejapa, Chenalhó, Larráinzar, and Chalchihuitán, with around six thousand producers. It also includes organic coffee producers with 18 million tons grown annually 60,000 producers. One third of these producers are indigenous women and other peasant farmers who grow the coffee under the shade of native trees without the use of agro chemicals. Some of this coffee is even grown in environmentally protected areas such as the El Triunfo reserve, where ejidos with 14,000 people grow the coffee and sell it to cooperativers who sell it to companies such as Starbucks, but the main market is Europe. Some growers have created cooperatives of their own to cut out the middleman.
Ranching occupies about three million hectares of natural and induced pasture, with about 52% of all pasture induced. Most livestock is done by families using traditional methods. Most important are meat and dairy cattle, followed by pigs and domestic fowl. These three account for 93% of the value of production. Annual milk production in Chiapas totals about 180 million liters per year. The state's cattle production, along with timber from the Lacandon Jungle and energy output gives it a certain amount of economic clouts compared to other states in the region.
Forestry is mostly based on conifers and common tropical species producing 186,858 m3 per year at a value of 54,511,000 pesos. Exploited non-wood species include the Camedor palm tree for its fronds. The fishing industry is underdeveloped but includes the capture of wild species as well as fish farming. Fish production is generated both from the ocean as well as the many freshwater rivers and lakes. In 2002, 28,582 tons of fish valued at 441.2 million pesos was produced. Species include tuna, shark, shrimp, mojarra and crab.
Industry and energy.
The state's abundant rivers and streams have been dammed to provide about fifty five percent of the country's hydroelectric energy. Much of this is sent to other states accounting for over six percent of all of Mexico's energy output. Main power stations are located at Malpaso, La Angostura, Chicoasén and Peñitas, which produce about eight percent of Mexico's hydroelectric energy. Manuel Moreno Torres plant on the Grijalva River the most productive in Mexico. All of the hydroelectric plants are owned and operated by the Federal Electricity Commission (Comisión Federal de Electricidad, CFE).
Chiapas is rich in petroleum reserves. Oil production began during the 1980s and Chiapas has become the fourth largest producer of crude oil and natural gas among the Mexican states. Many reserves are yet untapped, but between 1984 and 1992, PEMEX drilled nineteen oil wells in the Lacandona Jungle. Currently, petroleum reserves are found in the municipalities of Juárez, Ostuacán, Pichucalco and Reforma in the north of the state with 116 wells accounting for about 6.5% of the country's oil production. It also provides about a quarter of the country’s natural gas. This production equals 222964 cuft of natural gas and 17,565,000 barrels of oil per year.
Industry is limited to small and micro enterprises and include auto parts, bottling, fruit packing, coffee and chocolate processing, production of lime, bricks and other construction materials, sugar mills, furniture making, textiles, printing and the production of handcrafts. The two largest enterprises is the Comisión Federal de Electricidad and a Petróleos Mexicanos refinery. Chiapas opened its first assembly plant in 2002, a fact that highlights the historical lack of industry in this area.
Handcrafts.
Chiapas is one of the states that produces a wide variety of handcrafts and folk art in Mexico. One reason for this is its many indigenous ethnicities who produce traditional items out of identity as well as commercial reasons. One commercial reason is the market for crafts provided by the tourism industry. Another is that most indigenous communities can no longer provide for their own needs through agriculture. The need to generate outside income has led to many indigenous women producing crafts communally, which has not only had economic benefits but also involved them in the political process as well. Unlike many other states, Chiapas has a wide variety of wood resources such as cedar and mahogany as well as plant species such as reeds, ixtle and palm. It also has minerals such as obsidian, amber, jade and several types of clay and animals for the production of leather, dyes from various insects used to create the colors associated with the region. Items include various types of handcrafted clothing, dishes, jars, furniture, roof tiles, toys, musical instruments, tools and more.
Chiapas’ most important handcraft is textiles, most of which is cloth weaved on a backstrap loom. Indigenous girls often learn how to sew and embroider before they learn how to speak Spanish. They are also taught how to make natural dyes from insects, and weaving techniques. Many of the items produced are still for day-to-day use, often dyed in bright colors with intricate embroidery. They include skirts, belts, rebozos, blouses, huipils and shoulder wraps called chals. Designs are in red, yellow, turquoise blue, purple, pink, green and various pastels and decorated with designs such as flowers, butterflies, and birds, all based on local flora and fauna. Commercially, indigenous textiles are most often found in San Cristóbal de las Casas, San Juan Chamula and Zinacantán. The best textiles are considered to be from Magdalenas, Larráinzar, Venustiano Carranza and Sibaca.
One of the main minerals of the state is amber, much of which is 25 million years old, with quality comparable to that found in the Dominican Republic. Chiapan amber has a number of unique qualities, including much that is clear all the way through and some with fossilized insects and plants. Most Chiapan amber is worked into jewelry including pendants, rings and necklaces. Colors vary from white to yellow/orange to a deep red, but there are also green and pink tones as well. Since pre-Hispanic times, native peoples have believed amber to have healing and protective qualities. The largest amber mine is in Simojovel, a small village 130 km from Tuxtla Gutiérrez, which produces 95% of Chiapas' amber. Other mines are found in Huitiupán, Totolapa, El Bosque, Pueblo Nuevo Solistahuacán, Pantelhó and San Andrés Duraznal. According to the Museum of Amber in San Cristóbal, almost 300 kg of amber is extracted per month from the state. Prices vary depending on quality and color.
The major center for ceramics in the state is the city of Amatenango del Valle, with its barro blanco (white clay) pottery. The most traditional ceramic in Amatenango and Aguacatenango is a type of large jar called a cantaro used to transport water and other liquids. Many pieces created from this clay are ornamental as well as traditional pieces for everyday use such as comals, dishes, storage containers and flowerpots. All pieces here are made by hand using techniques that go back centuries. Other communities that produce ceramics include Chiapa de Corzo, Tonalá, Ocuilpa, Suchiapa and San Cristóbal de las Casas.
Wood crafts in the state center on furniture, brightly painted sculptures and toys. The Tzotzils of San Juan de Chamula are known for their sculptures as well as for their sturdy furniture. Sculptures are made from woods such as cedar, mahogany and strawberry tree. Another town noted for their sculptures is Tecpatán. The making lacquer to use in the decoration of wooden and other items goes back to the colonial period. The best-known area for this type of work, called "laca" is Chiapa de Corzo, which has a museum dedicated to it. One reason this type of decoration became popular in the state was that it protected items from the constant humidity of the climate. Much of the laca in Chiapa de Corzo is made in the traditional way with natural pigments and sands to cover gourds, dipping spoons, chests, niches and furniture. It is also used to create the Parachicos masks.
Traditional Mexican toys, which have all but disappeared in the rest of Mexico, are still readily found here and include the cajita de la serpiente, yo yos, ball in cup and more. Other wooden items include masks, cooking utensils, and tools. One famous toy is the "muñecos zapatistas" (Zapatista dolls), which are based on the revolutionary group that emerged in the 1990s.
Tourism and general commerce/services.
Ninety four percent of the state's commercial outlets are small retail stores with about 6% wholesalers. There are 111 municipal markets, 55 tianguis, three wholesale food markets and 173 large vendors of basic staple products. The service sector is the most important to the economy, with mostly commerce, warehousing and tourism.
Tourism brings large numbers of visitors to the state each year. Most of Chiapas' tourism is based on its culture, colonial cities and ecology. The state has a total of 491 ranked hotels with 12,122 rooms. There are also 780 other establishments catering primarily to tourism, such as services and restaurants.
There are three main tourist routes: the Maya Route, the Colonial Route and the Coffee Route. The Maya Route runs along the border with Guatemala in the Lacandon Jungle and includes the sites of Palenque, Bonampak, Yaxchilan along with the natural attractions of Agua Azul Waterfalls, Misol-Há Waterfall, and the Catazajá Lake. Palenque is the most important of these sites, and one of the most important tourist destinations in the state. Yaxchilan was a Mayan city along the Usumacinta River. It developed between 350 and 810 CE. Bonampak is known for its well preserved murals. These Mayan sites have made the state an attraction for international tourism. These sites contain a large number of structures, most of which date back thousands of years, especially to the sixth century. In addition to the sites on the Mayan Route, there are others within the state away from the border such as Toniná, near the city of Ocosingo.
The Colonial Route is mostly in the central highlands with a significant number of churches, monasteries and other structures from the colonial period along with some from the 19th century and even into the early 20th. The most important city on this route is San Cristóbal de las Casas, located in the Los Altos region in the Jovel Valley. The historic center of the city is filled with tiled roofs, patios with flowers, balconies, Baroque facades along with Neoclassical and Moorish designs. It is centered on a main plaza surrounded by the cathedral, the municipal palace, the Portales commercial area and the San Nicolás church. In addition, it has museums dedicated to the state’s indigenous cultures, one to amber and one to jade, both of which have been mined in the state. Other attractions along this route include Comitán de Domínguez and Chiapa de Corzo, along with small indigenous communities such as San Juan Chamula. The state capital of Tuxtla Gutiérrez does not have many colonial era structures left, but it lies near the area's most famous natural attraction of the Sumidero Canyon. This canyon is popular with tourists who take boat tours into it on the Grijalva River to see such features such as caves (La Cueva del Hombre, La Cueva del Silencio) and the Christmas Tree, which is a rock and plant formation on the side of one of the canyon walls created by a seasonal waterfall.
The Coffee Route begins in Tapachula and follows a mountainous road into the Suconusco regopm. The route passes through Puerto Chiapas, a port with modern infrastructure for shipping exports and receiving international cruises. The route visits a number of coffee plantations, such as Hamburgo, Chiripa, Violetas, Santa Rita, Lindavista, Perú-París, San Antonio Chicarras and Rancho Alegre. These haciendas provide visitors with the opportunity to see how coffee is grown and initially processed on these farms. They also offer a number of ecotourism activities such as mountain climbing, rafting, rappelling and mountain biking. There are also tours into the jungle vegetation and the Tacaná Volcano. In addition to coffee, the region also produces most of Chiapas’ soybeans, bananas and cacao.
The state has a large number of ecological attractions most of which are connected to water. The main beaches on the coastline include Puerto Arista, Boca del Cielo, Playa Linda, Playa Aventuras, Playa Azul and Santa Brigida. Others are based on the state's lakes and rivers. Laguna Verde is a lake in the Coapilla municipality. The lake is generally green but its tones constantly change through the day depending on how the sun strikes it. In the early morning and evening hours there can also be blue and ochre tones as well. The El Chiflón Waterfall is part of an ecotourism center located in a valley with reeds, sugarcane, mountains and rainforest. It is formed by the San Vicente River and has pools of water at the bottom popular for swimming. The Las Nubes Ecotourism center is located in the Las Margaritas municipality near the Guatemalan border. The area features a number of turquoise blue waterfalls with bridges and lookout points set up to see them up close.
Still others are based on conservation, local culture and other features. The Las Guacamayas Ecotourism Center is located in the Lacandon Jungle on the edge of the Montes Azules reserve. It is centered on the conservation of the red macaw, which is in danger of extinction. The Tziscao Ecotourism Center is centered on a lake with various tones. It is located inside the Lagunas de Montebello National Park, with kayaking, mountain biking and archery. Lacanjá Chansayab is located in the interior of the Lacandon Jungle and a major Lacandon people community. It has some activities associated with ecotourism such as mountain biking, hiking and cabins. The Grutas de Rancho Nuevo Ecotourism Center is centered on a set of caves in which appear capricious forms of stalagmite and stalactites. There is also horseback riding as well.
Culture.
Architecture.
Architecture in the state begins with the archeological sites of the Mayans and other groups who established color schemes and other details that echo in later structures. After the Spanish subdued the area, the building of Spanish style cities began, especially in the highland areas.
Many of the colonial era buildings area related to Dominicans who came from Seville. This Spanish city had much Arabic influence in its architecture. This Arabic influence was transferred to form part of the colonial architecture in Chiapas, especially for structures dating from the 16th to 18th centuries. However, there are a number of architectural styles and influences present in Chiapas colonial structures, including colors and patterns from Oaxaca and Central America along with indigenous ones from Chiapas.
The main colonial structures are the cathedral and Santo Domingo church of San Cristóbal, the Santo Domingo monastery and La Pila in Chiapa de Corzo. The San Cristóbal cathedral has a Baroque facade that was begun in the 16th century but by the time it was finished in the 17th, it had a mix of Spanish, Arabic, and indigenous influences. It is one of the most elaborately decorated in Mexico.
The churches and former monasteries of Santo Domingo, La Merced and San Francisco have ornamentation similar to that of the cathedral. The main structures in Chiapa de Corzo are the Santo Domingo monastery and the La Pila fountain. Santo Domingo has indigenous decorative details such as double headed eagles as well as a statue of the founding monk. In San Cristóbal, the Diego de Mazariegos house has a Plateresque facade, while that of Francisco de Montejo, built later in the 18th century has a mix of Baroque and Neoclassical. Art Deco structures can be found in San Cristóbal and Tapachula in public buildings as well as a number of rural coffee plantations from the Porfirio Díaz era.
Art and literature.
Art in Chiapas is based on the use of color and has strong indigenous influence. This dates back to cave paintings such as those found in Sima de las Cotorras near Tuxtla Gutiérrez and the caverns of Rancho Nuevo where human remains and offerings were also found. The best-known pre Hispanic artwork is the Maya murals of Bonampak, which are the only Mesoamerican murals to have been preserved for over 1500 years. In general, Mayan artwork stands out for its precise depiction of faces and its narrative form. Indigenous forms derive from this background and continue into the colonial period with the use of indigenous color schemes in churches and into modern structures such as the municipal palace in Tapachula. Since the colonial period, the state has produced a large number of painter and sculptures. Noted 20th century artists include Lázaro Gómez, Ramiro Jiménez Chacón, Héctor Ventura Cruz, Máximo Prado Pozo, and Gabriel Gallegos Ramos.
The two best-known poets from the state include Jaime Sabines and Rosario Castellanos, both from prominent Chiapan families. The first was a merchant and diplomat and the second was a teacher, diplomat, theatre director and the director of the Instituto Nacional Indigenista. Jaime Sabines is widely regarded as Mexico’s most influential contemporary poet. His work celebrates everyday people in common settings.
Music.
The most important instrument in the state is the marimba. In the pre Hispanic period, indigenous peoples had already been producing music with wooden instruments. The marimba was introduced by African slaves brought to Chiapas by the Spanish. However, it achieved its widespread popularity in the early 20th century due to the formation of the Cuarteto Marimbistico de los Hermanos Gómez in 1918, who popularized the instrument and the popular music they play not only in Chiapas but in various parts of Mexico and into the United States. Along with Cuban Juan Arozamena, they composed the piece "Las chiapanecas" considered to be the unofficial anthem of the state. In the 1940s, they were also featured in a number of Mexican films. Marimbas are constructed in Venustiano Carranza, Chiapas de Corzo and Tuxtla Gutiérrez.
Cuisine.
Like the rest of Mesoamerica, the basic diet has been based on corn and Chiapas cooking retains strong indigenous influence. One important ingredient is chipilin, a fragrant and strongly flavored herb and hoja santa, the large anise-scented leaves used in much of southern Mexican cuisine. Chiapan dishes do not incorporate many chili peppers as part of their dishes. Rather, chili peppers are most often found in the condiments. One reason for that is that a local chili pepper, called the simojovel, is far too hot to use except very sparingly. Chiapan cuisine tends to rely more on slightly sweet seasonings in their main dishes such as cinnamon, plantains, prunes and pineapple are often found in meat and poultry dishes.
Tamales are a major part of the diet and often include chipilín mixed into the dough and hoja santa, within the tamale itself or used to wrap it. One tamale native to the state is the "picte", a fresh sweet corn tamale. Tamales juacanes are filled with a mixture of black beans, dried shrimp, and pumpkin seeds.
Meats are centered on the European introduced beef, pork and chicken as many native game animals are in danger of extinction. Meat dishes are frequently accompanied by vegetables such as squash, chayote and carrots. Black beans are the favored type. Beef is favored, especially a thin cut called tasajo usually served in a sauce. Pepita con tasajo is a common dish at festivals especially in Chiapa de Corzo. It consists of a squash seed based sauced over reconstituted and shredded dried beef. As a cattle raising area, beef dishes in Palenque are particularly good. Pux-Xaxé is a stew with beef organ meats and mole sauce made with tomato, chili bolita and corn flour. Tzispolá is a beef broth with chunks of meat, chickpeas, cabbage and various types of chili peppers. Pork dishes include cochito, which is pork in an adobo sauce. In Chiapa de Corzo, their version is cochito horneado, which is a roast suckling pig flavored with adobo. Seafood is a strong component in many dishes along the coast. Turula is dried shrimp with tomatoes. Sausages, ham and other cold cuts are most often made and consumed in the highlands.
In addition to meat dishes, there is chirmol, a cooked tomato sauced flavored with chili pepper, onion and cilantro and zats, butterfly caterpillars from the Altos de Chiapas that are boiled in salted water, then sautéed in lard and eaten with tortillas, limes, and green chili pepper.
Sopa de pan consists of layers of bread and vegetables covered with a broth seasoned with saffron and other flavorings. A Comitán speciality is hearts of palm salad in vinaigrette and Palenque is known for many versions of fried plaintains, including filled with black beans or cheese.
Cheese making is important, especially in the municipalities of Ocosingo, Rayon and Pijijiapan. Ocosingo has its own self-named variety, which is shipped to restaurants and gourmet shops in various parts of the country. Regional sweets include crystallized fruit, coconut candies, flan and compotes. San Cristobal is noted for its sweets, as well as chocolates, coffee and baked goods.
While Chiapas is known for good coffee, there are a number of other local beverages. The oldest is pozol, originally the name for a fermented corn dough. This dough has its origins in the pre Hispanic period. To make the beverage, the dough is dissolved in water and usually flavored with cocoa and sugar, but sometimes it is left to ferment further. It is then served very cold with lots of ice. Taxcalate is a drink made from a powder of toasted corn, achiote, cinnamon and sugar prepared with milk or water. Pumbo is a beverage made with pineapple, club soda, vodka, sugar syrup and lots of ice. Posh is a drink distilled from sugar cane.
Religion.
Like in the rest of Mexico, Christianity was imposed on the native population by the Spanish conquistadors. Catholic beliefs were mixed with indigenous ones to form what is now called "traditionalist" Catholic belief. The Diocese of Chiapas comprises almost the entire state, and centered on San Cristobal de las Casas. It was founded in 1538 by Pope Paul III to evangelize the area with its most famous bishop of that time Bartolomé de las Casas. Evangelization focused on grouping indigenous peoples into communities centered on a church. This bishop not only had these people evangelized in their own language, he worked to introduce many of the crafts still practiced today. While still a majority, only sixty-eight percent of Chiapas residents profess the Catholic faith as of 2010, compared to 83% of the rest of the country.
Many indigenous people mix Christianity with Indian beliefs. One particular area where this is strong is the central highlands in small communities such as San Juan Chamula. In one church in San Cristobal, Mayan rites including the sacrifice of animals is permitted inside the church to ask for good health or to "ward off the evil eye."
Starting in the 1970s, there has been a shift away from traditional Catholic affiliation to Protestant, Evangelical and other Christian denominations. Presbyterians and Pentecostals attracted a large number of converts, with percentages of Protestants in the state rising from five percent in 1970 to twenty-one percent in 2000. This shift has had a political component as well, with those making the switch tending to identify across ethnic boundaries, especially across indigenous ethnic boundaries and being against the traditional power structure. The National Presbyterian Church in Mexico is particularly strong in Chiapas, the state can be described as one of the strongholds of the denomination.
To counter this, the Diocese of Chiapas began to actively re-evangelize among the indigenous populations, and working on their behalf politically as well, following an ideology called liberation theology. Those attracted by this movement call themselves "Word of God" Catholics and identify directly with the Diocese, rather than with local Catholic authorities. Both Protestants and Word of God Catholics tend to oppose traditional cacique leadership and often worked to prohibit the sale of alcohol. The latter had the effect of attracting many women to both movements.
The growing number of Protestants, Evangelicals and Word of God Catholics challenging traditional authority has caused religious strife in a number of indigenous communities. Tensions have been strong, at times, especially in rural areas such as San Juan Chamula. Tension among the groups reached its peak in the 1990s with a large number of people injured during open clashes. In the 1970s, caciques began to expel dissidents from their communities for challenging their power, initially with the use of violence. By 2000, more than 20,000 people had been displaced, but state and federal authorities did not act to stop the expulsions. Today, the situation has quieted but the tension remains, especially in very isolated communities.
Archeology.
The earliest population of Chiapas was in the coastal Soconusco region, where the Chantuto peoples appeared, going back to 5500 BC. This was the oldest Mesoamerican culture discovered to date.
The largest and best-known archeological sites in Chiapas belong to the Mayan civilization. Apart from a few works by Franciscan friars, knowledge of Maya civilisation largely disappeared after the Spanish Conquest. In the mid-19th century, John Lloyd Stephens and Frederick Catherwood traveled though the sites in Chiapas and other Mayan areas and published their writings and illustrations. This led to serious work on the culture including the deciphering of its hieroglyphic writing.
In Chiapas, principal Mayan sites include Palenque, Toniná, Bonampak, Chinkoltic and Tenam Puentes, all or near in the Lacandon Jungle. They are technically more advanced than earlier Olmec sites, which can best be seen in the detailed sculpting and novel construction techniques, including structures of four stories in height. Mayan sites are not only noted for large numbers of structures, but also for glyphs, other inscriptions, and artwork that has provided a relatively complete history of many of the sites.
Palenque is the most important Mayan and archeological site. Tthough much smaller than the huge sites at Tikal or Copán, Palenque contains some of the finest architecture, sculpture and stucco reliefs the Mayans ever produced. The history of the Palenque site begins in 431 with its height under Pakal I (615-683), Chan-Bahlum II (684-702) and Kan-Xul who reigned between 702 and 721. However, the power of Palenque would be lost by the end of the century. Pakal’s tomb was not discovered inside the Temple of Inscriptions until 1949. Today, Palenque is a World Heritage Site and one of the best-known sites in Mexico.
Yaxchilan flourished in the 8th and 9th centuries. The site contains impressive ruins, with palaces and temples bordering a large plaza upon a terrace above the Usumacinta River. The architectural remains extend across the higher terraces and the hills to the south of the river, overlooking both the river itself and the lowlands beyond. Yaxchilan is known for the large quantity of excellent sculpture at the site, such as the monolithic carved stelae and the narrative stone reliefs carved on lintels spanning the temple doorways. Over 120 inscriptions have been identified on the various monuments from the site. The major groups are the Central Acropolis, the West Acropolis and the South Acropolis. The South Acropolis occupies the highest part of the site. The site is aligned with relation to the Usumacinta River, at times causing unconventional orientation of the major structures, such as the two ballcourts.
The city of Bonampak features some of the finest remaining Maya murals. The realistically rendered paintings depict human sacrifices, musicians and scenes of the royal court. In fact the name means “painted murals.” It is centered on a large plaza and has a stairway that leads to the Acropolis. There are also a number of notable steles.
Toniná is near the city of Ocosingo with its main features being the Casa de Piedra (House of Stone) and Acropolis. The latter is a series of seven platforms with various temples and steles. This site was a ceremonial center that flourished between 600 and 900 CE.
Pre-Mayan cultures.
While the Mayan sites are the best-known, there are a number of other important sites in the state, including many older than the Maya civilization. 
The oldest sites are in the coastal Soconusco region. This includes the Mokaya culture, the oldest ceramic culture of Mesoamerica. Later, Paso de la Amada became important. Many of these sites are in Mazatan, Chiapas area.
Izapa became an important pre-Mayan site as well.
There are also other ancient sites including Tapachula and Tepcatán, and Pijijiapan. These sites contain numerous embankments and foundations that once lay beneath pyramids and other buildings. Some of these buildings have disappeared and others have been covered by jungle for about 3,000 years, unexplored.
Pijijiapan and Izapa are on the Pacific coast and were the most important pre Hispanic cities for about 1,000 years, as the most important commercial centers between the Mexican Plateau and Central America. Sima de las Cotorras is a sinkhole 140 meters deep with a diameter of 160 meters in the municipality of Ocozocoautla. It contains ancient cave paintings depicting warriors, animals and more. It is best known as a breeding area for parrots, thousands of which leave the area at once at dawn and return at dusk. The state as its Museo Regional de Antropologia e Historia located in Tuxtla Gutiérrez focusing on the pre Hispanic peoples of the state with a room dedicated to its history from the colonial period.
Education.
The average number of years of schooling is 6.7, which is the beginning of middle school, compared to the Mexico average of 8.6. 16.5% have no schooling at all, 59.6% have only primary school/secondary school, 13.7% finish high school or technical school and 9.8 go to university. Eighteen out of every 100 people 15 years or older cannot read or write, compared to 7/100 nationally. Most of Chiapas’ illiterate population are indigenous women, who are often prevented from going to school. School absenteeism and dropout rates are highest among indigenous girls.
There are an estimated 1.4 million students in the state from preschool on up. The state has about 61,000 teachers and just over 17,000 centers of educations. Preschool and primary schools are divided into modalities called general, indigenous, private and community educations sponsored by CONAFE. Middle school is divided into technical, telesecundaria (distance education) and classes for working adults. About 98% of the student population of the state is in state schools. Higher levels of education include "professional medio" (vocational training), general high school and technology-focused high school. At this level, 89% of students are in public schools. There are 105 universities and similar institutions with 58 public and 47 private serving over 60,500 students.
The state university is the Universidad Autónoma de Chiapas (UNACH). It was begun when an organization to establish a state level institution was formed in 1965, with the university itself opening its doors ten years later in 1975. The university project was partially supported by UNESCO in Mexico. It integrated older schools such as the Escuela de Derecho (Law School), which originated in 1679; the Escuela de Ingeniería Civil (School of Civil Engineering), founded in 1966; and the Escuela de Comercio y Administración, which was located in Tuxtla Gutiérrez.
Infrastructure.
The state has approximately 22,517 km of highway with 10,857 federally maintained and 11,660 maintained by the state. Almost all of these kilometers are paved. Major highways include the Las Choapas-Raudales-Ocozocoautla, which links the state to Oaxaca, Veracruz, Puebla and Mexico City. Major airports include Llano San Juan in Ocozocoautla, Francisco Sarabia National Airport (which was replaced by Ángel Albino Corzo International Airport) in Tuxtla Gutiérrez and Corazón de María Airport (which closed in 2010) in San Cristóbal de las Casas. These are used for domestic flights with the airports in Palenque and Tapachula providing international service into Guatemala. There are 22 other airfields in twelve other municipalities. Rail lines extend over 547.8 km. There are two major lines: one in the north of the state that links the center and southeast of the country, and the Costa Panamericana route, which runs from Oaxaca to the Guatemalan border.
There are thirty six AM radio stations and sixteen FM stations. There are thirty seven local television stations and sixty six repeaters.
Chiapas' main port is just outside the city of Tapachula called the Puerto Chiapas. It faces 3,361 meters of ocean, with 3,060 m2 of warehouse space. Next to it, there is an industrial park that covers 2,340,000 m2. Puerto Chiapas has 60,000 m2 of area with a capacity to receive 1,800 containers as well as refrigerated containers. The port serves the state of Chiapas and northern Guatemala. Puerto Chiapas serves to import and export products across the Pacific to Asia, the United States, Canada and South America. It also has connections with the Panama Canal. There is an international airport located eleven km away as well as a railroad terminal ending at the port proper. Over the past five years the port has grown with its newest addition being a terminal for cruise ships with tours to the Izapa site, the Coffee Route, the city of Tapachula, Pozuelos Lake and an Artesanal Chocolate Tour. Principal exports through the port include banana and banana trees, corn, fertilizer and tuna.
Sports.
The capital of the state has a professional football soccer team called Chiapas F.C., located in Tuxtla Gutiérrez. This team made Mexico's first division in 2002. The team changed its symbol and colors to orange and black in 2010. It participated in the Copa Libertadores de América in 2011.

</doc>
<doc id="6788" url="http://en.wikipedia.org/wiki?curid=6788" title="Chrysler Building">
Chrysler Building

The Chrysler Building is an Art Deco style skyscraper in New York City, located on the east side of Manhattan in the Turtle Bay area at the intersection of 42nd Street and Lexington Avenue. At 1,046 ft, the structure was the world's tallest building for 11 months before it was surpassed by the Empire State Building in 1931. It is still the tallest brick building in the world, albeit with an internal steel skeleton. After the destruction of the World Trade Center, it was again the second-tallest building in New York City until December 2007, when the spire was raised on the 1,200-foot (365.8 m) Bank of America Tower, pushing the Chrysler Building into third position. In addition, The New York Times Building, which opened in 2007, is exactly level with the Chrysler Building in height. Both buildings were then pushed into 4th position, when the under construction One World Trade Center surpassed their height.
The Chrysler Building is a classic example of Art Deco architecture and considered by many contemporary architects to be one of the finest buildings in New York City. In 2007, it was ranked ninth on the "List of America's Favorite Architecture" by the American Institute of Architects. It was the headquarters of the Chrysler Corporation from 1930 until the mid-1950s. Although the building was built and designed specifically for the car manufacturer, the corporation did not pay for the construction of it and never owned it, as Walter P. Chrysler decided to pay for it himself, so that his children could inherit it.
History.
The Chrysler Building was designed by architect William Van Alen for a project of Walter P. Chrysler. When the ground breaking occurred on September 19, 1928, there was an intense competition in New York City to build the world's tallest skyscraper. Despite a frantic pace (the building was built at an average rate of four floors per week), no workers died during the construction of this skyscraper.
Design beginnings.
Van Alen's original design for the skyscraper called for a decorative jewel-like glass crown. It also featured a base in which the showroom windows were tripled in height and topped by 12 stories with glass-wrapped corners, creating an impression that the tower appeared physically and visually light as if floating in mid-air. The height of the skyscraper was also originally designed to be 246 m. However, the design proved to be too advanced and costly for building contractor William H. Reynolds, who disapproved of Van Alen's original plan. The design and lease were then sold to Walter P. Chrysler, who worked with Van Alen and redesigned the skyscraper for additional stories; it was eventually revised to be 282 m tall. As Walter Chrysler was the chairman of the Chrysler Corporation and intended to make the building into Chrysler's headquarters, various architectural details and especially the building's gargoyles were modeled after Chrysler automobile products like the hood ornaments of the Plymouth; they exemplify the machine age in the 1920s ("see below").
Construction.
Construction commenced on September 19, 1928. In total, almost 400,000 rivets were used and approximately 3,826,000 bricks were manually laid, to create the non-loadbearing walls of the skyscraper. Contractors, builders and engineers were joined by other building-services experts to coordinate construction.
Prior to its completion, the building stood about even with a rival project at 40 Wall Street, designed by H. Craig Severance. Severance increased the height of his project and then publicly claimed the title of the world's tallest building. (This distinction excluded structures that were not fully habitable, such as the Eiffel Tower.) In response, Van Alen obtained permission for a 38 m long spire and had it secretly constructed inside the frame of the building. The spire was delivered to the site in four different sections. On October 23, 1929, the bottom section of the spire was hoisted onto the top of the building's dome and lowered into the 66th floor of the building. The other remaining sections of the spire were hoisted and riveted to the first one in sequential order in just 90 minutes.
Completion.
Upon completion, May 20, 1930, the added height of the spire allowed the Chrysler Building to surpass 40 Wall Street as the tallest building in the world and the Eiffel Tower as the tallest structure. It was the first man-made structure to stand taller than 1000 ft. Van Alen's satisfaction in these accomplishments was likely muted by Walter Chrysler's later refusal to pay the balance of his architectural fee. Less than a year after it opened to the public on May 27, 1930, the Chrysler Building was surpassed in height by the Empire State Building, but the Chrysler Building is still the world's tallest steel-supported brick building. As of November 2, 2011, the building's height was surpassed by the under construction One World Trade Center at the height of 1,106 feet.
Property.
The east building wall of the base out of which the tower rises runs at a slant to the Manhattan street grid, following a property line that predated the Commissioners' Plan of 1811. The land on which the Chrysler Building stands was donated to The Cooper Union for the Advancement of Science and Art in 1902. The land was originally leased to William H. Reynolds, but, when he was unable to raise money for the project, the building and the development rights to the land were acquired by Walter P. Chrysler in 1928. Contrary to popular belief, the Chrysler Corporation was never involved in the construction or ownership of the Chrysler Building, although it was built and designed for the corporation and served as its headquarters until the mid-1950s. It was a project of Walter P. Chrysler for his children.
The ownership of the building has changed several times. The Chrysler family sold the building in 1953 to William Zeckendorf, and in 1957 it was purchased by Sol Goldman and Alex DiLorenzo, and owned by Massachusetts Mutual Life Insurance Company. The lobby was refurbished and the facade renovated in 1978–1979. The building was owned by Jack Kent Cooke in 1979. The spire underwent a restoration that was completed in 1995. In 1998, Tishman Speyer Properties and the Travelers Insurance Group bought the Chrysler Building and the adjoining Kent Building in 1997 for about $220 million (equal to $<br>{Inflation} - Amount must not have "" prefix: 220.   million in 2014) from a consortium of banks and the estate of Jack Kent Cooke. Tishman Speyer Properties had negotiated a 150-year lease on the land from Cooper Union and the college continues to own both the land under the Chrysler Building and the building itself. Cooper Union's name is on the deed.
In 2001, a 75% stake in the building management contract was sold, for US$300 million (equal to $<br>{Inflation} - Amount must not have "" prefix: 300.   million in 2014), to TMW, the German arm of an Atlanta-based investment fund. On June 11, 2008 it was reported that the Abu Dhabi Investment Council was in negotiations to buy TMW's 75% economic interest, and a 15% interest from Tishman Speyer Properties in the building, and a share of the Trylons retail structure next door for US$800 million. On July 9, 2008 it was announced that the transaction had been completed, and that the Abu Dhabi Investment Council was now the 90% owner of the building.
Architecture.
The Chrysler Building is considered a leading example of Art Deco architecture. The corners of the 61st floor are graced with eagles; on the 31st floor, the corner ornamentation are replicas of the 1929 Chrysler radiator caps. The building is constructed of masonry, with a steel frame, and metal cladding. The building currently contains a total of 3,862 windows on its facades. Inside, there are four banks of 8 elevators designed by the Otis Elevator Corporation. The building was declared a National Historic Landmark in 1976, and a New York City Landmark in 1978.
The Chrysler Building is also renowned and recognized for its terraced crown. Composed of seven radiating terraced arches, Van Alen's design of the crown is a cruciform groin vault constructed into seven concentric members with transitioning setbacks, mounted up one behind another. The stainless-steel cladding is ribbed and riveted in a radiating sunburst pattern with many triangular vaulted windows, transitioning into smaller segments of the seven narrow setbacks of the facade of the terraced crown. The entire crown is clad with silvery "Enduro KA-2" metal, an austenitic stainless steel developed in Germany by Krupp and marketed under the trade name "Nirosta" (a German acronym for "nichtrostender Stahl", meaning "non-rusting steel").
When the building first opened, it contained a public viewing gallery on the 71st floor, which was closed to the public in 1945. This floor is now the highest occupied floor of the Chrysler Building, it was occupied by an office space management firm in 1986. The private Cloud Club occupied a three-floor high space from the 66th–68th floors, but closed in the late 1970s. Above the 71st floor, the stories of the building are designed mostly for exterior appearance, functioning mainly as landings for the stairway to the spire. These top stories are very narrow with low, sloped ceilings, and are useful only for holding radio-broadcasting and other mechanical and electrical equipment.
Television station WCBS-TV (Channel 2) originally transmitted from the top of the Chrysler in the 1940s and early 1950s, before moving to the Empire State Building. For many years, WPAT-FM and WTFM (now WKTU) also used the Chrysler Building as a transmission site, but they also moved to the Empire State Building by the 1970s. There are currently no commercial broadcast stations located at the Chrysler Building.
There are two sets of lighting in the top spires and decoration. The first are the V-shaped lighting inserts in the steel of the building itself. Added later were groups of floodlights that are on mast arms directed back at the building. This allows the top of the building to be lit in many colors for special occasions. This lighting was installed by electrician Charles Londner and crew during construction.
Representation.
The Chrysler Building has been shown in several movies that take place in New York. In the summer of 2005, New York's own Skyscraper Museum asked one hundred architects, builders, critics, engineers, historians, and scholars, among others, to choose their 10 favorites among 25 New York towers. The Chrysler Building came in first place as 90% of them placed the building in their top-10 favorite buildings.
The Chrysler Building's distinctive profile has inspired similar skyscrapers worldwide, including One Liberty Place in Philadelphia.
References.
Notes
Further reading
</dl>

</doc>
<doc id="6794" url="http://en.wikipedia.org/wiki?curid=6794" title="Comet Shoemaker–Levy 9">
Comet Shoemaker–Levy 9

Comet Shoemaker–Levy 9 (formally designated D/1993 F2) was a comet that broke apart and collided with Jupiter in July 1994, providing the first direct observation of an extraterrestrial collision of Solar System objects. This generated a large amount of coverage in the popular media, and the comet was closely observed by astronomers worldwide. The collision provided new information about Jupiter and highlighted its role in reducing space debris in the inner Solar System.
The comet was discovered by astronomers Carolyn and Eugene M. Shoemaker and David Levy. Shoemaker–Levy 9, at the time captured by and orbiting Jupiter, was located on the night of March 24, 1993, in a photograph taken with the 40 cm Schmidt telescope at the Palomar Observatory in California. It was the first comet observed to be orbiting a planet, and had probably been captured by the planet around 20 – 30 years earlier.
Calculations showed that its unusual fragmented form was due to a previous closer approach to Jupiter in July 1992. At that time, the orbit of Shoemaker–Levy 9 passed within Jupiter's Roche limit, and Jupiter's tidal forces had acted to pull apart the comet. The comet was later observed as a series of fragments ranging up to 2 km in diameter. These fragments collided with Jupiter's southern hemisphere between July 16 and July 22, 1994, at a speed of approximately 60 km/s or 216000 km/h. The prominent scars from the impacts were more easily visible than the Great Red Spot and persisted for many months.
Discovery.
While conducting a program of observations designed to uncover near-Earth objects, the Shoemakers and Levy discovered Comet Shoemaker–Levy 9 on the night of March 24, 1993 in a photograph taken with the 0.4 m Schmidt telescope at the Palomar Observatory in California. The comet was thus a serendipitous discovery, but one that quickly overshadowed the results from their main observing program.
Comet Shoemaker–Levy 9 was the ninth periodic comet (a comet whose orbital period is 200 years or less) discovered by the Shoemakers and Levy, hence its name. It was their eleventh comet discovery overall including their discovery of two non-periodic comets, which use a different nomenclature. The discovery was announced in IAU Circular 5725 on March 27, 1993.
The discovery image gave the first hint that comet Shoemaker–Levy 9 was an unusual comet, as it appeared to show multiple nuclei in an elongated region about 50 arcseconds long and 10 arcseconds wide. Brian G. Marsden of the Central Bureau for Astronomical Telegrams noted that the comet lay only about 4 degrees from Jupiter as seen from Earth, and that while this could of course be a line of sight effect, its apparent motion in the sky suggested that it was physically close to the giant planet. Because of this, he suggested that the Shoemakers and David Levy had discovered the fragments of a comet that had been disrupted by Jupiter's gravity.
Jupiter-orbiting comet.
Orbital studies of the new comet soon revealed that it was orbiting Jupiter rather than the Sun, unlike all other comets known at the time. Its orbit around Jupiter was very loosely bound, with a period of about 2 years and an apojove (the point in the orbit farthest from the planet) of 0.33 AU. Its orbit around the planet was highly eccentric ("e" = 0.9986).
Tracing back the comet's orbital motion revealed that it had been orbiting Jupiter for some time. It seems most likely that it was captured from a solar orbit in the early 1970s, although the capture may have occurred as early as the mid-1960s. Several other observers found images of the comet in precovery images obtained before March 24, including Kin Endate from a photograph exposed on March 15, S. Otomo on March 17, and a team led by Eleanor Helin from images on March 19. No precovery images dating back to earlier than March 1993 have been found. Before the comet was captured by Jupiter, it was probably a short-period comet with an aphelion just inside Jupiter's orbit, and a perihelion interior to the asteroid belt.
The volume of space within which an object can be said to orbit Jupiter is defined by Jupiter's Hill sphere (also called the Roche sphere). When the comet passed Jupiter in the late 1960s or early 1970s, it happened to be near its aphelion, and found itself slightly within Jupiter's Hill sphere. Jupiter's gravity nudged the comet towards it. Because the comet's motion with respect to Jupiter was very small, it fell almost straight toward Jupiter, which is why it ended up on a Jupiter-centric orbit of very high eccentricity – that is to say, the ellipse was nearly flattened out.
The comet had apparently passed extremely close to Jupiter on July 7, 1992, just over 40000 km above the planet's cloud tops – a smaller distance than Jupiter's radius of 70000 km, and well within the orbit of Jupiter's innermost moon Metis and the planet's Roche limit, inside which tidal forces are strong enough to disrupt a body held together only by gravity. Although the comet had approached Jupiter closely before, the July 7 encounter seemed to be by far the closest, and the fragmentation of the comet is thought to have occurred at this time. Each fragment of the comet was denoted by a letter of the alphabet, from "fragment A" through to "fragment W", a practice already established from previously observed broken-up comets.
More exciting for planetary astronomers was that the best orbital calculations suggested that the comet would pass within 45000 km of the center of Jupiter, a distance smaller than the planet's radius, meaning that there was an extremely high probability that SL9 would collide with Jupiter in July 1994. Studies suggested that the train of nuclei would plow into Jupiter's atmosphere over a period of about five days.
Predictions for the collision.
The discovery that the comet was likely to collide with Jupiter caused great excitement within the astronomical community and beyond, as astronomers had never before seen two significant Solar System bodies collide. Intense studies of the comet were undertaken, and as its orbit became more accurately established, the possibility of a collision became a certainty. The collision would provide a unique opportunity for scientists to look inside Jupiter's atmosphere, as the collisions were expected to cause eruptions of material from the layers normally hidden beneath the clouds.
Astronomers estimated that the visible fragments of SL9 ranged in size from a few hundred metres to two kilometres across, suggesting that the original comet may have had a nucleus up to 5 km across – somewhat larger than Comet Hyakutake, which became very bright when it passed close to the Earth in 1996. One of the great debates in advance of the impact was whether the effects of the impact of such small bodies would be noticeable from Earth, apart from a flash as they disintegrated like giant meteors. The most optimistic prediction was that large, asymmetric ballistic fireballs would rise above the limb of Jupiter and into sunlight to be visible from Earth. 
Other suggested effects of the impacts were seismic waves travelling across the planet, an increase in stratospheric haze on the planet due to dust from the impacts, and an increase in the mass of the Jovian ring system. However, given that observing such a collision was completely unprecedented, astronomers were cautious with their predictions of what the event might reveal.
Impacts.
Anticipation grew as the predicted date for the collisions approached, and astronomers trained terrestrial telescopes on Jupiter. Several space observatories did the same, including the Hubble Space Telescope, the ROSAT X-ray observing satellite, and significantly the Galileo spacecraft, then on its way to a rendezvous with Jupiter scheduled for 1995. While the impacts took place on the side of Jupiter hidden from Earth, Galileo, then at a distance of 1.6 AU from the planet, was able to see the impacts as they occurred. Jupiter's rapid rotation brought the impact sites into view for terrestrial observers a few minutes after the collisions.
Two other satellites made observations at the time of the impact: the Ulysses spacecraft, primarily designed for solar observations, was pointed towards Jupiter from its location 2.6 AU away, and the distant Voyager 2 probe, some 44 AU from Jupiter and on its way out of the Solar System following its encounter with Neptune in 1989, was programmed to look for radio emission in the 1–390 kHz range.
The first impact occurred at 20:13 UTC on July 16, 1994, when fragment A of the nucleus entered Jupiter's southern hemisphere at a speed of about 60 km/s. Instruments on Galileo detected a fireball which reached a peak temperature of about 24,000 K, compared to the typical Jovian cloudtop temperature of about 130 K, before expanding and cooling rapidly to about 1500 K after 40 s. The plume from the fireball quickly reached a height of over 3,000 km. A few minutes after the impact fireball was detected, Galileo measured renewed heating, probably due to ejected material falling back onto the planet. Earth-based observers detected the fireball rising over the limb of the planet shortly after the initial impact.
Despite published predictions, astronomers had not expected to see the fireballs from the impacts and did not have any idea in advance how visible the other atmospheric effects of the impacts would be from Earth. Observers soon saw a huge dark spot after the first impact. The spot was visible even in very small telescopes, and was about 6000 km (one Earth radius) across. This and subsequent dark spots were thought to have been caused by debris from the impacts, and were markedly asymmetric, forming crescent shapes in front of the direction of impact.
Over the next 6 days, 21 distinct impacts were observed, with the largest coming on July 18 at 07:33 UTC when fragment G struck Jupiter. This impact created a giant dark spot over 12,000 km across, and was estimated to have released an energy equivalent to 6,000,000 megatons of TNT (600 times the world's nuclear arsenal). Two impacts 12 hours apart on July 19 created impact marks of similar size to that caused by fragment G, and impacts continued until July 22, when fragment W struck the planet.
Observations and discoveries.
Chemical studies.
Observers hoped that the impacts would give them a first glimpse of Jupiter beneath the cloud tops, as lower material was exposed by the comet fragments punching through the upper atmosphere. Spectroscopic studies revealed absorption lines in the Jovian spectrum due to diatomic sulfur (S2) and carbon disulfide (CS2), the first detection of either in Jupiter, and only the second detection of S2 in any astronomical object. Other molecules detected included ammonia (NH3) and hydrogen sulfide (H2S). The amount of sulfur implied by the quantities of these compounds was much greater than the amount that would be expected in a small cometary nucleus, showing that material from within Jupiter was being revealed. Oxygen-bearing molecules such as sulfur dioxide were not detected, to the surprise of astronomers.
As well as these molecules, emission from heavy atoms such as iron, magnesium and silicon was detected, with abundances consistent with what would be found in a cometary nucleus. While substantial water was detected spectroscopically, it was not as much as predicted beforehand, meaning that either the water layer thought to exist below the clouds was thinner than predicted, or that the cometary fragments did not penetrate deeply enough. The relatively low levels of water were later confirmed by Galileo's atmospheric probe, which explored Jupiter's atmosphere directly.
Waves.
As predicted beforehand, the collisions generated enormous waves which swept across the planet at speeds of 450 m/s and were observed for over two hours after the largest impacts. The waves were thought to be travelling within a stable layer acting as a waveguide, and some scientists believed the stable layer must lie within the hypothesised tropospheric water cloud. However, other evidence seemed to indicate that the cometary fragments had not reached the water layer, and the waves were instead propagating within the stratosphere.
Other observations.
Radio observations revealed a sharp increase in continuum emission at a wavelength of 21 cm after the largest impacts, which peaked at 120% of the normal emission from the planet. This was thought to be due to synchrotron radiation, caused by the injection of relativistic electrons – electrons with velocities near the speed of light – into the Jovian magnetosphere by the impacts.
About an hour after fragment K entered Jupiter, observers recorded auroral emission near the impact region, as well as at the antipode of the impact site with respect to Jupiter's strong magnetic field. The cause of these emissions was difficult to establish due to a lack of knowledge of Jupiter's internal magnetic field and of the geometry of the impact sites. One possible explanation was that upwardly accelerating shock waves from the impact accelerated charged particles enough to cause auroral emission, a phenomenon more typically associated with fast-moving solar wind particles striking a planetary atmosphere near a magnetic pole.
Some astronomers had suggested that the impacts might have a noticeable effect on the Io torus, a torus of high-energy particles connecting Jupiter with the highly volcanic moon Io. High resolution spectroscopic studies found that variations in the ion density, rotational velocity, and temperatures at the time of impact and afterwards were within the normal limits.
Post-impact analysis.
One of the surprises of the impacts was the small amount of water revealed compared to prior predictions. Before the impact, models of Jupiter's atmosphere had indicated that the break-up of the largest fragments would occur at atmospheric pressures of anywhere from 30 kilopascals to a few tens of megapascals (from 0.3 to a few hundred bar), with some predictions that the comet would penetrate a layer of water and create a bluish shroud over that region of Jupiter.
Astronomers did not observe large amounts of water following the collisions, and later impact studies found that fragmentation and destruction of the cometary fragments in an 'airburst' probably occurred at much higher altitudes than previously expected, with even the largest fragments being destroyed when the pressure reached 250 kPa, well above the expected depth of the water layer. The smaller fragments were probably destroyed before they even reached the cloud layer.
Longer-term effects.
The visible scars from the impacts could be seen on Jupiter for many months. They were extremely prominent, and observers described them as more easily visible even than the Great Red Spot. A search of historical observations revealed that the spots were probably the most prominent transient features ever seen on the planet, and that while the Great Red Spot is notable for its striking color, no spots of the size and darkness of those caused by the SL9 impacts have ever been recorded before.
Spectroscopic observers found that ammonia and carbon disulfide persisted in the atmosphere for at least fourteen months after the collisions, with a considerable amount of ammonia being present in the stratosphere as opposed to its normal location in the troposphere.
Counterintuitively, the atmospheric temperature dropped to normal levels much more quickly at the larger impact sites than at the smaller sites: at the larger impact sites, temperatures were elevated over a region 15000 to wide, but dropped back to normal levels within a week of the impact. At smaller sites, temperatures 10 K higher than the surroundings persisted for almost two weeks. Global stratospheric temperatures rose immediately after the impacts, then fell to below pre-impact temperatures 2–3 weeks afterwards, before rising slowly to normal temperatures.
Frequency of impacts.
SL9 is not unique in having orbited Jupiter for a time; five comets, (including 82P/Gehrels, 147P/Kushida–Muramatsu, and 111P/Helin–Roman–Crockett) are known to have been temporarily captured by the planet.
Cometary orbits around Jupiter are unstable, as they will be highly elliptical and likely to be strongly perturbed by the Sun's gravity at apojove (the furthest point on the orbit from the planet).
By far the most massive planet in the Solar System, Jupiter can capture objects relatively frequently, but the size of SL9 makes it a rarity: one post-impact study estimated that comets 0.3 km in diameter impact the planet once in approximately 500 years and those 1.6 km in diameter do so just once in every 6,000 years.
There is very strong evidence that comets have previously been fragmented and collided with Jupiter and its satellites. During the Voyager missions to the planet, planetary scientists identified 13 crater chains on Callisto and three on Ganymede, the origin of which was initially a mystery. Crater chains seen on the Moon often radiate from large craters, and are thought to be caused by secondary impacts of the original ejecta, but the chains on the Jovian moons did not lead back to a larger crater. The impact of SL9 strongly implied that the chains were due to trains of disrupted cometary fragments crashing into the satellites.
Impact of July 19, 2009.
On July 19, 2009, a new black spot about the size of the Pacific Ocean appeared in Jupiter's southern hemisphere. Thermal infrared measurements showed the impact site was warm and spectroscopic analysis detected the production of excess hot ammonia and silica-rich dust in the upper regions of Jupiter's atmosphere. Scientists have concluded that another impact event had occurred, but this time a more compact and strong object, probably a small undiscovered asteroid, was the cause.
Jupiter as a "cosmic vacuum cleaner".
The impact of SL9 highlighted Jupiter's role as a "cosmic vacuum cleaner" (or in deference to the ancients' planetary correspondences to the major organs in the human body, a "cosmic liver") for the inner Solar System. The planet's strong gravitational influence leads to many small comets and asteroids colliding with the planet, and the rate of cometary impacts on Jupiter is thought to be between two thousand and eight thousand times higher than the rate on Earth. If Jupiter were not present, the probability of asteroid impacts with the Solar System's inner planets would be much greater.
The extinction of the dinosaurs at the end of the Cretaceous period is generally believed to have been caused by the Cretaceous–Paleogene impact event which created the Chicxulub crater, demonstrating that impacts are a serious threat to life on Earth. Astronomers have speculated that without Jupiter to mop up potential impactors, extinction events might have been more frequent on Earth, and complex life might not have been able to develop. This is part of the argument used in the Rare Earth hypothesis.
In 2009, it was shown that the presence of a smaller planet at Jupiter's position in the Solar System might increase the impact rate of comets on the Earth significantly. A planet of Jupiter's mass still seems to provide increased protection against asteroids, but the total effect on all orbital bodies within the Solar System is unclear.
External links.
Listen to this article ()
This audio file was created from a revision of the "Comet Shoemaker–Levy 9" article dated 2006-04-14, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="6796" url="http://en.wikipedia.org/wiki?curid=6796" title="Ceres Brewery">
Ceres Brewery

Ceres Brewery was a brewery company located in Aarhus, Denmark. It was part of Royal Unibrew. The factories in central Aarhus, was closed in 2008 and the grounds are now being redeveloped into a new neighbourhood of the city, known as CeresByen (The CeresCity).
History.
Ceres Brewery was founded by a grocer named Malthe Conrad Lottrup, with help from the chemists A. S. Aagard and Knud Redelien, as the city's seventh brewery. It was named after the Roman goddess Ceres, and its opening was announced in the local newspaper, "Stiftstidende", in 1856.
The brewery was successful, and Lottrup became one of the most prominent people of Aarhus. After ten years, he exspanded the brewery, adding a grand new building as his own private residence, where he entertained other local figures.
Lottrup's son-in-law, Laurits Christian Meulengracht, took over the running of the brewery after that, and was in charge for nearly thirty years, expanding it further. He then sold it to another brewery, Østjyske Bryggerier A/S.
The brewery gained more esteem in 1914, when it was made "Purveyor to the Royal Danish Court".
In 2008 the factory closed because the brewery could not live up to the expectations from its owner Royal Unibrew.

</doc>
<doc id="6799" url="http://en.wikipedia.org/wiki?curid=6799" title="COBOL">
COBOL

COBOL (, an acronym for "co"mmon "b"usiness-"o"riented "l"anguage) is a compiled English-like computer programming language designed for business use. It is imperative, procedural and, since 2002, object-oriented. COBOL is primarily used in business, finance, and administrative systems for companies and governments. In 1997, Gartner Group estimated that there were a total of 200 billion lines of COBOL in existence, which ran 80% of all business programs. COBOL is still widely used in legacy applications deployed on mainframe computers, such as large-scale batch and transaction processing jobs. But due to its declining popularity and the retirement of experienced COBOL programmers, programs are being migrated to new platforms, rewritten in modern languages or replaced with software packages. Most programming in COBOL is now purely to maintain existing applications.
COBOL was designed in 1959 by the Conference on Data Systems Languages (CODASYL) and was partly based on previous programming language design work by Grace Hopper, commonly referred to as "the (grand)mother of COBOL". It was created as part of a US Department of Defense effort to create a portable programming language for data processing. Intended as a temporary stopgap, the Department of Defense promptly forced computer manufacturers to provide it, resulting in its widespread adoption. It was standardized in 1968 and has since been revised four times. Expansions include support for structured and object-oriented programming. The current standard is "ISO/IEC 1989:2014".
COBOL has an English-like syntax, which was designed to be self-documenting and highly readable. However, it is verbose and uses over 300 reserved words. In contrast with modern, succinct syntax like codice_1, COBOL has a more English-like syntax (in this case, codice_1).
COBOL code is split into four divisions (identification, environment, data and procedure) containing a rigid hierarchy of sections, paragraphs and sentences. Lacking a large standard library, the standard specifies 43 statements, 87 functions and just one class.
Academic computer scientists were generally uninterested in business applications when COBOL was created and were not involved in its design.
COBOL has been criticized throughout its life for its verbosity, design process and poor support for structured programming, which resulted in monolithic and incomprehensible programs.
History and specification.
Background.
In the late 1950s, computer users and manufacturers were becoming concerned about the rising cost of programming. A 1959 survey had found that in any data processing installation, the programming costs $800,000 on average and that translating programs to run on new hardware would cost $600,000. At a time when new programming languages were proliferating at an ever increasing rate, the same survey suggested that if a common business-oriented language were used, conversion would be far cheaper and faster.
In April 1959, representatives from academia, computer users and manufacturers met at the University of Pennsylvania to organize a formal meeting on common business languages. Representatives among others, included Grace Hopper, inventor of the English-like data processing language FLOW-MATIC, Jean Sammet and Saul Gorn.
The group asked the Department of Defense (DoD) to sponsor an effort to create a common business language. The delegation impressed Charles A. Phillips, director of the Data System Research Staff at the DoD, who thought that they "thoroughly understood" the DoD's problems. The DoD operated 225 computers, had a further 175 on order and had spent over $200 million on implementing programs to run on them. Portable programs would save time, reduce costs and ease modernization.
Phillips agreed to sponsor the meeting and tasked the delegation with drafting the agenda.
COBOL 60.
On May 28 and 29 of 1959 (exactly one year after the Zürich ALGOL 58 meeting), a meeting was held at the Pentagon to discuss the creation of a common programming language for business. It was attended by 41 people and was chaired by Phillips. The Department of Defense was concerned about whether it could run the same data processing programs on different computers. FORTRAN, the only mainstream language at the time, lacked the features needed to write such programs.
Representatives enthusiastically described a language that could work in a wide variety of environments, from banking and insurance to utilities and inventory control. They agreed unanimously that more people should be able to program and that the new language should not be restricted by the limitations of contemporary technology. A majority agreed that the language should make maximal use of English, be capable of change, be machine-independent and be easy to use, even at the expense of power.
The meeting resulted in the creation of a steering committee and short-, intermediate- and long-range committees. The short-range committee was given to September (three months) to produce specifications for an interim language, which would then be improved upon by the other committees. Their official mission, however, was to identify the strengths and weaknesses of existing programming languages and did not explicitly direct them to create a new language.
The deadline was met with disbelief by the short-range committee.
One member, Betty Holberton, described the three-month deadline as "gross optimism" and doubted that the language really would be a stopgap.
The steering committee met on June 4 and agreed to name the entire activity as the "Committee on Data Systems Languages", or CODASYL, and to form an executive committee.
The short-range committee was made up of members representing six computer manufacturers and three government agencies. The six computer manufacturers were Burroughs Corporation, IBM, Minneapolis-Honeywell (Honeywell Labs), RCA, Sperry Rand, and Sylvania Electric Products. The three government agencies were the US Air Force, the Navy's David Taylor Model Basin, and the National Bureau of Standards (now the National Institute of Standards and Technology). The committee was chaired by Joseph Wegstein of the US National Bureau of Standards. Work began by investigating data description, statements, existing applications and user experiences.
The committee mainly examined the FLOW-MATIC, AIMACO and COMTRAN programming languages.
The FLOW-MATIC language was particularly influential because it had been implemented and because AIMACO was a derivative of it with only minor changes.
FLOW-MATIC's inventor, Grace Hopper, also served as a technical adviser to the committee. FLOW-MATIC's major contributions to COBOL were long variable names, English words for commands and the separation of data descriptions and instructions.
IBM's COMTRAN language, invented by Bob Bemer, was regarded as a competitor to FLOW-MATIC by a short-range committee made up of colleagues of Grace Hopper.
Some of its features were not incorporated into COBOL so that it would not look like IBM had dominated the design process, and Jean Sammet said in 1981 that there had been a "strong anti-IBM bias" from some committee members (herself included).
In one case, Roy Goldfinger, author of the COMTRAN manual and intermediate-range committee member, attended a subcommittee meeting to support his language and encourage the use of algebraic expressions, Grace Hopper sent a memo to the short-range committee reiterating Sperry Rand's efforts to create a language based on English.
In 1980, Grace Hopper commented that "COBOL 60 is 95% FLOW-MATIC" and that COMTRAN had had an "extremely small" influence. Furthermore, she said that she would claim that work was influenced by both FLOW-MATIC and COMTRAN only to "keep other people happy [so they] wouldn't try to knock us out".
Features from COMTRAN incorporated into COBOL included formulas, the codice_1 clause, an improved codice_4 statement, which obviated the need for GO TOs, and a more robust file management system.
The usefulness of the committee's work was subject of great debate. While some members thought the language had too many compromises and was the result of design by committee, others felt it was better than the three languages examined. Some felt the language was too complex; others, too simple.
Controversial features included those some considered useless or too advanced for data processing users. Such features included boolean expressions, formulas and table "<dfn >subscripts</dfn>" (indices). Another point of controversy was whether to make keywords context-sensitive and the effect that would have on readability. Although context-sensitive keywords were rejected, the approach was later used in PL/I and partially in COBOL from 2002. Little consideration was given to interactivity, interaction with operating systems (few existed at that time) and functions (thought of as purely mathematical and of no use in data processing).
The specifications were presented to the Executive Committee on September 4. They fell short of expectations: Joseph Wegstein noted that "it contains rough spots and requires some additions", and Bob Bemer later described them as a "hodgepodge". The subcommittee was given until December to improve it.
At a mid-September meeting, the committee discussed the new language's name. Suggestions included "BUSY" (Business System), "INFOSYL" (Information System Language) and "COCOSYL" (Common Computer Systems Language). The name "COBOL" was suggested by Bob Bemer.
In October, the intermediate-range committee received copies of the FACT language specification created by Roy Nutt. Its features impressed the committee so much that they passed a resolution to base COBOL on it.
This was a blow to the short-range committee, who had made good progress on the specification. Despite being technically superior, FACT had not been created with portability in mind or through manufacturer and user consensus. It also lacked a demonstrable implementation, allowing supporters of a FLOW-MATIC-based COBOL to overturn the resolution. RCA representative Howard Bromberg also blocked FACT, so that RCA's work on a COBOL implementation would not go to waste.
<poem>
'And what name do you want inscribed?'
I said, 'I'll write it for you.' I wrote the name down: COBOL.
'What kind of name is that?'
'Well it's a Polish name. We shortened it and got rid of a lot of unnecessary notation.'
</poem>
”
Howard Bromberg on how he bought the COBOL tombstone
It soon became apparent that the committee was too large for any further progress to be made quickly. A frustrated Howard Bromberg bought a $15 tombstone with "COBOL" engraved on it and sent it to Charles Phillips to demonstrate his displeasure.
A sub-committee was formed to analyze existing languages and was made up of six individuals:
The sub-committee did most of the work creating the specification, leaving the short-range committee to review and modify their work before producing the finished specification.
The specifications were approved by the Executive Committee on January 3, 1960, and sent to the government printing office, which printed these as "COBOL 60". The language's stated objectives were to allow efficient, portable programs to be easily written, to allow users to move to new systems with minimal effort and cost, and to be suitable for inexperienced programmers.
The CODASYL Executive Committee later created the COBOL Maintenance Committee to answer questions from users and vendors and to improve and expand the specifications.
During 1960, the list of manufacturers planning to build COBOL compilers grew. By September, five more manufacturers had joined CODASYL (Bendix, Control Data Corporation, General Electric (GE), National Cash Register and Philco), and all represented manufacturers had announced COBOL compilers. GE and IBM planned to integrate COBOL into their own languages, GECOM and COMTRAN, respectively. In contrast, International Computers and Tabulators planned to replace their language, CODEL, with COBOL.
Meanwhile, RCA and Sperry Rand worked on creating COBOL compilers. The first COBOL program ran on 17 August on an RCA 501.
On December 6 and 7, the same COBOL program (albeit with minor changes) ran on an RCA computer and a Remington-Rand Univac computer, demonstrating that compatibility could be achieved.
The relative influences of which languages were used continues to this day in the recommended advisory printed in all COBOL reference manuals:
COBOL is an industry language and is not the property of any company or group of companies, or of any organization or group of organizations.
No warranty, expressed or implied, is made by any contributor or by the CODASYL COBOL Committee as to the accuracy and functioning of the
programming system and language. Moreover, no responsibility is assumed by any contributor, or by the committee, in connection therewith. The authors and copyright holders of the copyrighted material used herein are as follows:
They have specifically authorized the use of this material, in whole or in part, in the COBOL specifications. Such authorization extends to the reproduction and use of COBOL specifications in programming manuals or similar publications.
COBOL-61 to COBOL-65.
It is rather unlikely that Cobol will be around by the end of the decade.
Anonymous, June 1960
Many logical flaws were found in "COBOL 60", leading GE's Charles Katz to warn that it could not be interpreted unambiguously. A reluctant short-term committee enacted a total cleanup and, by March 1963, it was reported that COBOL's syntax was as definable as ALGOL's, although semantic ambiguities remained.
Early COBOL compilers were primitive and slow. A 1962 US Navy evaluation found compilation speeds of 3–11 statements per minute. By mid-1964, they had increased to 11–1000 statements per minute. It was observed that increasing memory would drastically increase speed and that compilation costs varied wildly: costs per statement were between $0.23 and $18.91.
In late 1962, IBM announced that COBOL would be their primary development language and that development of COMTRAN would cease.
COBOL-60 was replaced in 1961 by COBOL-61. This was then replaced by the COBOL-61 Extended specifications in 1963, which introduced the sort and report writer facilities.
The added facilities corrected flaws identified by Honeywell in late 1959 in a letter to the short-range committee.
COBOL Edition 1965 brought further clarifications to the specifications and introduced facilities for handling mass storage files and tables.
COBOL-68.
Efforts began to standardize COBOL to overcome incompatibilities between versions. In late 1962, both ISO and the United States of America Standards Institute (now ANSI) formed groups to create standards. ANSI produced "USA Standard COBOL X3.23" in August 1968, which became the cornerstone for later versions. This version was known as American National Standard (ANS) COBOL and was adopted by ISO in 1972.
COBOL-74.
By 1970, COBOL had become the most widely used programming language in the world.
Independently of the ANSI committee, the CODASYL Programming Language Committee was working on improving the language. They described new versions in 1968, 1969, 1970 and 1973, including changes such as new inter-program communication, debugging and file merging facilities as well as improved string-handling and library inclusion features.
Although CODASYL was independent of the ANSI committee, the "CODASYL Journal of Development" was used by ANSI to identify features that were popular enough to warrant implementing.
The Programming Language Committee also liaised with ECMA and the Japanese COBOL Standard committee.
In 1974, ANSI published a revised version of (ANS) COBOL, containing new features such as file organizations, the codice_1 statement and the segmentation module.
Deleted features included the codice_1 statement, the codice_1 statement (which was replaced by codice_1) and the implementer-defined random access module (which was superseded by the new sequential and relative I/O modules). These made up 44 changes, which rendered existing statements incompatible with the new standard.
The report writer was slated to be removed from COBOL, but was reinstated before the standard was published. ISO later adopted the updated standard in 1978.
COBOL-85.
In June 1978, work began on revising COBOL-74. The proposed standard (commonly called COBOL-80) differed significantly from the previous one, causing concerns about incompatibility and conversion costs. In January 1981, Joseph T. Brophy, Senior Vice-President of Travelers Insurance, threatened to sue the standard committee because it was not upwards compatible with COBOL-74. Mr. Brophy described previous conversions of their 40-million-line code base as "non-productive" and a "complete waste of our programmer resources".
Later that year, the Data Processing Management Association (DPMA) said it was "strongly opposed" to the new standard, citing "prohibitive" conversion costs and enhancements that were "forced on the user".
During the first public review period, the committee received 2,200 responses, of which 1,700 were negative form letters.
Other responses were detailed analyses of the effect COBOL-80 would have on their systems; conversion costs were predicted to be at least 50 cents per line of code. Fewer than a dozen of the responses were in favor of the proposed standard.
In 1983, the DPMA withdrew its opposition to the standard, citing the responsiveness of the committee to public concerns. In the same year, a National Bureau of Standards study concluded that the proposed standard would present few problems. A year later, a COBOL-80 compiler was released to DEC VAX users, who noted that conversion of COBOL-74 programs posed few problems. The new codice_9 statement and inline codice_10 were particularly well received and improved productivity, thanks to simplified control flow and debugging.
The second public review drew another 1,000 (mainly negative) responses, while the last drew just 25, by which time many concerns had been addressed.
In late 1985, ANSI published the revised standard. 60 features were changed or deprecated and many were added, such as:
The standard was adopted by ISO the same year. Two amendments followed in 1989 and 1993, the first introducing intrinsic functions and the other providing corrections. ISO adopted the amendments in 1991 and 1994 respectively, before subsequently taking primary ownership and development of the standard.
COBOL 2002 and object-oriented COBOL.
In the early 1990s, work began on adding object-orientation in the next full revision of COBOL. Object-oriented features were taken from C++ and Smalltalk.
The initial estimate was to have this revision completed by 1997, and an ISO Committee Draft (CD) was available by 1997. Some vendors (including Micro Focus, Fujitsu, and IBM) introduced object-oriented syntax based on drafts of the full revision. The final approved ISO standard was approved and published in late 2002.
Fujitsu/GTSoftware, Micro Focus and RainCode introduced object-oriented COBOL compilers targeting the .NET Framework.
There were many other new features, many of which had been in the "CODASYL COBOL Journal of Development" since 1978 and had missed the opportunity to be included in COBOL-85. These other features included:
Three corrigenda were published for the standard: two in 2006 and one in 2009.
COBOL 2014.
Between 2003 and 2009, three technical reports were produced describing object finalization, XML processing and collection classes for COBOL.
COBOL 2002 suffered from poor support: no compilers completely supported the standard. Micro Focus found that it was due to a lack of user demand for the new features and due to the abolition of the NIST test suite, which had been used to test compiler conformance. The standardization process was also found to be slow and under-resourced.
COBOL 2014 includes the following changes:
Legacy.
COBOL programs are used globally in governments and businesses and are running on diverse operating systems such as z/OS, VME, Unix and Windows. In 1997, the Gartner Group reported that 80% of the world's business ran on COBOL with over 200 billion lines of code and 5 billion lines more being written annually.
Near the end of the 20th century, the year 2000 problem (Y2K) was the focus of significant COBOL programming effort, sometimes by the same programmers who had designed the systems decades before. The particular level of effort required to correct COBOL code has been attributed to the large amount of business-oriented COBOL, as business applications use dates heavily, and to fixed-length data fields. After the clean-up effort put into these programs for Y2K, a 2003 survey found that many remained in use.
The authors said that the survey data suggest "a gradual decline in the importance of Cobol in application development over the [following] 10 years unless ... integration with other languages and technologies can be adopted".
In 2006 and 2012, "Computerworld" surveys found that over 60% of organizations used COBOL (more than C++ and Visual Basic .NET) and that for half of those, COBOL was used for the majority of their internal software. 36% of managers said they planned to migrate from COBOL, and 25% said they would like to if it was cheaper. Instead, some businesses have migrated their systems from expensive mainframes to cheaper, more modern systems, while maintaining their COBOL programs.
Features.
Syntax.
COBOL has an English-like syntax, which is used to describe nearly everything in a program. For example, a condition can be expressed as  codice_1 or more concisely as  codice_1  or  codice_1. More complex conditions can be "abbreviated" by removing repeated conditions and variables. For example,  codice_1  can be shortened to codice_1. As a consequence of this English-like syntax, COBOL has over 300 keywords. Some of the keywords are simple alternative or pluralized spellings of the same word, which provides for more English-like statements and clauses; e.g., the codice_1 and codice_1 keywords can be used interchangeably, as can codice_1 and codice_1, and codice_1 and codice_1.
The syntactical elements of a COBOL program are "words", "literals", and "punctuation". Word elements include reserved keywords, user-defined identifiers, and labels, and must be separated from other words by spaces, newlines, or punctuation elements. Identifiers (for data items and files, as well as paragraph and section labels) are case-insensitive, may contain dashes for readability, and can be up to 31 characters long. Literal elements include numeric constants and quoted character (string) constants.
A COBOL program is split into four divisions: the identification division, the environment division, the data division and the procedure division. The identification division specifies the name and type of the source element and is where classes and interfaces are specified. The environment division specifies any program features that depend on the system running it, such as files and character sets. The data division is used to declare variables and parameters. The procedure division contains the program's statements. Each division is sub-divided into sections, which are made up of paragraphs.
Code format.
COBOL can be written in two formats: fixed (the default) or free. In fixed-format, code must be aligned to fit in certain areas. Until COBOL 2002, these were:
In COBOL 2002, Areas A and B were merged and extended to column 255. Also, the program name area was removed.
COBOL 2002 also introduced free-format code. Free-format code can be placed in any column of the file, like in newer languages such as C and Pascal. Comments are specified using codice_32, which can be placed anywhere and can also be used in fixed-format source code. Continuation lines are not present, and the codice_33 directive replaces the codice_34 indicator.
Identification division.
The identification division identifies the following code entity and contains the definition of a class or interface.
Object-oriented programming.
Classes and interfaces have been in COBOL since 2002. Classes have factory objects, containing class methods and variables, and instance objects, containing instance methods and variables. Inheritance and interfaces provide polymorphism. Support for generic programming is provided through parameterized classes, which can be instantiated to use any class or interface. Objects are stored as references which may be restricted to a certain type. There are two ways of called a method: the codice_1 statement, which acts similarly to codice_1, or through inline method invocation, which is analogous to using functions.
INVOKE my-class "foo" RETURNING var
MOVE my-class::"foo" TO var *> Inline method invocation
COBOL does not provide a way to hide methods. Class data can be hidden, however, by declaring it without a codice_1 clause, which leaves the user with no way to access it. Method overloading was added in COBOL 2014.
Environment division.
The environment division contains the configuration section and the input-output section. The configuration section is used to specify variable features such
as currency signs, locales and character sets. The input-output section contains file-related information.
Files.
COBOL supports three file formats, or "<dfn >organizations</dfn>": sequential, indexed and relative. In sequential files, records are contiguous and must be traversed sequentially, similarly to a linked list. Indexed files have one or more indexes which allow records to be randomly accessed and which can be sorted on them. Each record must have a unique key, but other, "<dfn >alternate</dfn>", record keys need not be unique. Implementations of indexed files vary between vendors, although common implementations, such as C‑ISAM and VSAM, are based on IBM's ISAM. Relative files, like indexed files, have a unique record key, but they do not have alternate keys. A relative record's key is its ordinal position; for example, the 10th record has a key of 10. This means that creating a record with a key of 5 may require the creation of (empty) preceding records. Relative files also allow for both sequential and random access.
A common non-standard extension is the "<dfn >line sequential</dfn>" organization, used to process text files. Records in a file are terminated by a newline and may be of varying length.
Data division.
The data division is split into six sections which declare different items: the file section, for file records; the working-storage section, for static variables; the local-storage section, for automatic variables; the linkage section, for parameters and the return value; the report section and the screen section, for text-based user interfaces.
Aggregated data.
Data items in COBOL are declared hierarchically through the use of level-numbers which indicate if a data item is part of another. An item with a higher level-number is subordinate to an item with a lower one. Top-level data items, with a level-number of 1, are called "<dfn >records</dfn>". Items that have subordinate aggregate data are called "<dfn >group items</dfn>"; those that do not are called "<dfn >elementary items</dfn>". Level-numbers used to describe standard data items are between 1 and 49.
 01 some-record. *> Aggregate group record item
 05 num PIC 9(10). *> Elementary item
 05 the-date. *> Aggregate (sub)group record item
 10 the-year PIC 9(4). *> Elementary item
 10 the-month PIC 99. *> Elementary item
 10 the-day PIC 99. *> Elementary item
In the above example, elementary item codice_1 and group item codice_1 are subordinate to the record codice_1, while elementary items codice_1, codice_1, and codice_1 are part of the group item codice_1.
Subordinate items can be disambiguated with the codice_1 (or codice_1) keyword. For example, consider the example code above along with the following example:
 01 sale-date.
 05 the-year PIC 9(4).
 05 the-month PIC 99.
 05 the-day PIC 99.
The names codice_1, codice_1, and codice_1 are ambiguous by themselves, since more than one data item is defined with those names. To specify a particular data item, for instance one of the items contained within the codice_1 group, the programmer would use codice_1 (or the equivalent codice_1). (This syntax is similar to the "dot notation" supported by most contemporary languages.)
Other data levels.
A level-number of 66 is used to declare a re-grouping of previously defined items, irrespective of how those items are structured. This data level, also known referred to by the associated <dfn >codice_1 clause</dfn>, is rarely used and, circa 1988, was usually found in old programs. Its ability to ignore the hierarchical and logical structure data meant its use was not recommended and many installations forbade its use.
 01 customer-record.
 05 cust-key PIC X(10).
 05 cust-name.
 10 cust-first-name PIC X(30).
 10 cust-last-name PIC X(30).
 05 cust-dob PIC 9(8).
 05 cust-balance PIC 9(7)V99.
 66 cust-personal-details RENAMES cust-name THRU cust-dob.
 66 cust-all-details RENAMES cust-name THRU cust-balance.
A 77 level-number indicates the item is stand-alone, and in such situations is equivalent to the level-number 01. For example, the following code declares two 77-level data items, codice_1 and codice_1, which are non-group data items that are independent of (not subordinate to) any other data items:
 77 property-name PIC X(80).
 77 sales-region PIC 9(5).
An 88 level-number declares a "<dfn >condition name</dfn>" (a so-called 88-level) which is true when its parent data item contains one of the values specified in it codice_1 clause. For example, the following code defines two 88-level condition-name items that are true or false depending on the current character data value of the codice_1 data item. When the data item contains a value of codice_1, the condition-name codice_1 is true, whereas when it contains a value of codice_1 or codice_1, the condition-name codice_1 is true. If the data item contains some other value, both of the condition-names are false.
 01 wage-type PIC X.
 88 wage-is-hourly VALUE 'H'.
 88 wage-is-yearly VALUE 'S', 'Y'.
Data types.
Standard COBOL provides the following data types:
Type safety is variable in COBOL. Numeric data is converted between different representations and sizes silently and alphanumeric data can be placed in any data item that can be stored as a string, including numeric and group data. In contrast, object references and pointers may only be assigned from items of the same type and their values may be restricted to a certain type.
PICTURE clause.
A codice_1 (or codice_1) clause is a string of characters, each of which represents a portion of the data item and what it may contain. Some picture characters specify the type of the item and how many characters or digits it occupies in memory. For example, a codice_1 indicates a decimal digit, and an codice_1 indicates that the item is signed. Other picture characters (called "<dfn >insertion</dfn>" and "<dfn >editing</dfn>" characters) specify how an item should be formatted. For example, a series of codice_1 characters define character positions as well as how a leading sign character is to be positioned within the final character data; the rightmost non-numeric character will contain the item's sign, while other character positions corresponding to a codice_1 to the left of this position will contain a space. Repeated characters can be specified more concisely by specifying a number in parentheses after a picture character; for example, codice_1 is equivalent to codice_1. Picture specifications containing only digit (codice_1) and sign (codice_1) characters define purely "<dfn >numeric</dfn>" data items, while picture specifications containing alphabetic (codice_1) or alphanumeric (codice_1) characters define "<dfn >alphanumeric</dfn>" data items. The presence of other formatting characters define "<dfn >edited numeric</dfn>" or "<dfn >edited alphanumeric</dfn>" data items.
USAGE clause.
The codice_1 clause declares the format data is stored in. Depending on the data type, it can either complement or be used instead of a codice_1 clause. While it can be used to declare pointers and object references, it is mostly geared towards specifying numeric types. These numeric formats are:
Report writer.
The report writer is a declarative facility for creating reports. The programmer need only specify the report layout and the data required to produce it, freeing them from having to write code to handle things like page breaks, data formatting, and headings and footings.
Reports are associated with report files, which are files which may only be written to through report writer statements.
 FD report-out REPORT sales-report.
Each report is defined in the report section of the data division. A report is split into report groups which define the report's headings, footings and details. Reports work around hierarchical "<dfn >control breaks</dfn>". Control breaks occur when a key variable changes it value; for example, when creating a report detailing customers' orders, a control break could occur when the program reaches a different customer's orders. Here is an example report description for a report which gives a salesperson's sales and which warns of any invalid records:
 RD sales-report
 PAGE LIMITS 60 LINES
 FIRST DETAIL 3
 CONTROLS seller-name.
 01 TYPE PAGE HEADING.
 03 COL 1 VALUE "Sales Report".
 03 COL 74 VALUE "Page".
 03 COL 79 PIC Z9 SOURCE PAGE-COUNTER.
 01 sales-on-day TYPE DETAIL, LINE + 1.
 03 COL 3 VALUE "Sales on".
 03 COL 12 PIC 99/99/9999 SOURCE sales-date.
 03 COL 21 VALUE "were".
 03 COL 26 PIC $$$$9.99 SOURCE sales-amount.
 01 invalid-sales TYPE DETAIL, LINE + 1.
 03 COL 3 VALUE "INVALID RECORD:".
 03 COL 19 PIC X(34) SOURCE sales-record.
 01 TYPE CONTROL HEADING seller-name, LINE + 2.
 03 COL 1 VALUE "Seller:".
 03 COL 9 PIC X(30) SOURCE seller-name.
The above report description describes the following layout:
Four statements control the report writer: codice_1, which prepares the report writer for printing; codice_1, which prints a report group; codice_1, which suppresses the printing of a report group; and codice_1, which terminates report processing. For the above sales report example, the procedure division might look like this:
 OPEN INPUT sales, OUTPUT report-out
 INITIATE sales-report
 PERFORM UNTIL 1 <> 1
 READ sales
 AT END
 EXIT PERFORM
 END-READ
 VALIDATE sales-record
 IF valid-record
 GENERATE sales-on-day
 ELSE
 GENERATE invalid-sales
 END-IF
 END-PERFORM
 TERMINATE sales-report
 CLOSE sales, report-out
Procedure Division.
Procedures.
The sections and paragraphs in the procedure division (collectively called procedures) can be used as labels and as simple subroutines. Unlike in other divisions, paragraphs do not need to be in sections.
Execution goes down through the procedures of a program until it is terminated.
To use procedures as subroutines, the codice_1 verb is used. This transfers control to the specified range of procedures and returns only upon reaching the end.
Unusual control flow can trigger "<dfn >mines</dfn>", which cause control in performed procedures to return at unexpected times to unexpected locations. Procedures can be reached in three ways: they can be called with codice_1, jumped to from a codice_1 or through execution "falling through" the bottom of an above paragraph. Combinations of these invoke undefined behavior, creating mines. Specifically, mines occur when execution of a range of procedures would cause control flow to go past the last statement of a range of procedures already being performed.
For example, in the code in the adjacent image, a mine is tripped at the end of codice_1 when the screen is invalid. When the screen is invalid, control jumps to the codice_1 section, which, when done, performs codice_1. This recursion triggers undefined behavior as there are now two overlapping ranges of procedures being performed. The mine is then triggered upon reaching the end of codice_1 and means control could return to one of two locations:
Statements.
COBOL 2014 has 47 statements (also called "<dfn >verbs</dfn>"), which can be grouped into the following broad categories: control flow, I/O, data manipulation and the report writer. The report writer statements are covered in the report writer section.
Control flow.
COBOL's conditional statements are codice_1 and codice_1. codice_1 is a switch-like statement with the added capability of evaluating multiple values and conditions. This can be used to implement decision tables. For example, the following might be used to control a CNC lathe: 
EVALUATE TRUE ALSO desired-speed ALSO current-speed
 WHEN lid-closed ALSO min-speed THRU max-speed ALSO LESS THAN desired-speed
 PERFORM slow-down-machine
 WHEN lid-closed ALSO min-speed THRU max-speed ALSO GREATER THAN desired-speed
 PERFORM speed-up-machine
 WHEN lid-open ALSO ANY ALSO NOT ZERO
 PERFORM emergency-stop
 WHEN OTHER
 CONTINUE
END-EVALUATE
The codice_1 statement is used to define loops which are executed until a condition is true (not while, unlike other languages). It is also used to call procedures or ranges of procedures (see the procedures section for more details). codice_1 and codice_1 call subprograms and methods, respectively. The name of the subprogram/method is contained in a string which may be a literal or a data item. Parameters can be passed by reference, by content (where a copy is passed by reference) or by value (but only if a prototype is available).
codice_1 unloads subprograms from memory. codice_1 causes the program to jump to a specified procedure.
The codice_1 statement is a return statement and the codice_1 statement stops the program. The codice_1 statement has six different formats: it can be used as a return statement, a break statement, a continue statement, an end marker or to leave a procedure.
Exceptions are raised by a codice_1 statement and caught with a handler, or "<dfn >declarative</dfn>", defined in the codice_1 portion of the procedure division. Declaratives are sections beginning with a codice_1 statement which specify the errors to handle. Exceptions can be names or objects. codice_1 is used in a declarative to jump to the statement after the one that raised the exception or to a procedure outside the codice_1. Unlike other languages, uncaught exceptions may not terminate the program and the program can proceed unaffected.
I/O.
File I/O is handled by the self-describing codice_1, codice_1, codice_1, and codice_1 statements along with a further three: codice_1, which updates a record; codice_1, which selects subsequent records to access by finding a record with a certain key; and codice_1, which releases a lock on the last record accessed.
User interaction is done using codice_1 and codice_1.
Data manipulation.
The following verbs manipulate data:
Files and tables are sorted using codice_1 and the codice_1 verb merges and sorts files. The codice_1 verb provides records to sort and codice_1 retrieves sorted records in order.
Scope termination.
Some statements, such as codice_1 and codice_1, may themselves contain statements. Such statements may be terminated in two ways: by a period ("<dfn >implicit termination</dfn>"), which terminates "all" unterminated statements contained, or by a scope terminator, which terminates the nearest matching open statement.
IF invalid-record
 IF no-more-records
 NEXT SENTENCE
 ELSE
 READ record-file
 AT END SET no-more-records TO TRUE.
IF invalid-record
 IF no-more-records
 CONTINUE
 ELSE
 READ record-file
 AT END SET no-more-records TO TRUE
 END-READ
 END-IF
END-IF
Nested statements terminated with a period are a common source of bugs. For example, examine the following code:
IF x
 DISPLAY y.
 DISPLAY z.
Here, the intent is to display codice_148 and codice_149 if condition codice_150 is true. However, codice_149 will be displayed whatever the value of codice_150 because the codice_4 statement is terminated by an erroneous period after codice_1.
Another bug is a result of the dangling else problem, when two codice_4 statements can associate with an codice_156.
IF x
 IF y
 DISPLAY a
ELSE
 DISPLAY b.
In the above fragment, the codice_156 associates with the  codice_1  statement instead of the  codice_1  statement, causing a bug. Prior to the introduction of explicit scope terminators, preventing it would require  codice_1  to be placed after the inner codice_4.
Self-modifying code.
The original COBOL specification supported the infamous  codice_1  statement, for which many compilers generated self-modifying code. codice_163 and codice_164 are procedure labels, and the single  codice_1  statement in procedure codice_163 executed after such an codice_1 statement means  codice_1  instead. Many compilers still support it,
but it was deemed obsolete in the COBOL 1985 standard and deleted in 2002.
Hello, world.
A "Hello, world" program in COBOL:
 IDENTIFICATION DIVISION.
 PROGRAM-ID. HELLO-WORLD.
 PROCEDURE DIVISION.
 DISPLAY 'Hello, world'.
 STOP RUN.
Criticism and defense.
Lack of structure.
In the 1970s, programmers began moving away from unstructured spaghetti code to the structured programming paradigm. In his letter to an editor in 1975 entitled "How do we tell truths that might hurt?" which was critical of several of COBOL's contemporaries, computer scientist and Turing Award recipient Edsger Dijkstra remarked that "The use of COBOL cripples the mind; its teaching should, therefore, be regarded as a criminal offense."
In his dissenting response to Dijkstra's article and the above "offensive statement," computer scientist Howard E. Tompkins defended structured COBOL: "COBOL programs with convoluted control flow indeed tend to 'cripple the mind'," but this was because "There are too many such business application programs written by programmers that have never had the benefit of structured COBOL taught well..."
One cause of spaghetti code was the codice_1 statement. Attempts to remove codice_1s from COBOL code, however, resulted in convoluted programs and reduced code quality. codice_1s were largely replaced by the codice_1 statement and procedures, which promoted modular programming and gave easy access to powerful looping facilities. However, codice_1 could only be used with procedures so loop bodies were not located where they were used, making programs harder to understand.
COBOL programs were infamous for being monolithic and lacking modularization.
COBOL code could only be modularized through procedures, which were found to be inadequate for large systems. It was impossible to restrict access to data, meaning a procedure could access and modify any data item. Furthermore, there was no way to pass parameters to a procedure, an omission Jean Sammet regarded as the committee's biggest mistake.
Another complication stemmed from the ability to codice_1 a specified sequence of procedures. This meant that control could jump to and return from any procedure, creating convoluted control flow and permitting a programmer to break the "single entry, single exit" rule.
This situation improved as COBOL adopted more features. COBOL-74 added subprograms, giving programmers the ability to control the data each part of the program could access. COBOL-85 then added nested subprograms, allowing programmers to hide subprograms. Further control over data and code came in 2002 when object-oriented programming, user-defined functions and user-defined data types were included.
Compatibility issues.
COBOL was intended to a be a highly portable, "common" language. However, by 2001, around 300 dialects had been created.
COBOL-85 was not fully compatible with earlier versions, and its development was controversial. Joseph T. Brophy, the CIO of Travelers Insurance, spearheaded an effort to inform users of COBOL of the heavy reprogramming costs of implementing the new standard. As a result, the ANSI COBOL Committee received more than 2,200 letters from the public, mostly negative, requiring the committee to make changes. On the other hand, conversion to COBOL-85 was thought to increase productivity in future years, thus justifying the conversion costs.
Verbose syntax.
COBOL: /koh′bol/, n.
A weak, verbose, and flabby language used by code grinders to do boring mindless things on dinosaur mainframes. [...] Its very name is seldom uttered without ritual expressions of disgust or horror.
”
The Jargon File 4.4.8.
COBOL syntax has often been criticized for its verbosity. Proponents say that this was intended to make the code self-documenting, easing program maintenance. COBOL was also intended to be easy for programmers to learn and use, while still being readable to non-technical staff such as managers.
The desire for readability led to the use of English-like syntax and structural elements, such as nouns, verbs, clauses, sentences, sections, and divisions. Yet by 1984, maintainers of COBOL programs were struggling to deal with "incomprehensible" code and the main changes in COBOL-85 were there to help ease maintenance.
Jean Sammet, a short-range committee member, noted that "little attempt was made to cater to the professional programmer, in fact people whose main interest is programming tend to be very unhappy with COBOL" which she attributed to COBOL's verbose syntax.
Isolation from the computer science community.
The COBOL community has always been isolated from the computer science community. No academic computer scientists participated in the design of COBOL; all of those on the committee came from commerce or government. Computer scientists at the time were more interested in fields like numerical analysis, physics and system programming than the commercial file-processing problems which COBOL development tackled. Jean Sammet attributed COBOL's unpopularity to an initial "snob reaction" due to its inelegance, the lack of influential computer scientists participating in the design process and a disdain for business data processing. The COBOL specification used a unique "notation", or metalanguage, to define its syntax rather than the new Backus–Naur form because few committee members had heard of it. This resulted in "severe" criticism.
Later, COBOL suffered from a shortage of material covering it; it took until 1963 for introductory books to appear. By 1985, there were twice as many books on Fortran and four times as many on BASIC as on COBOL in the Library of Congress. University professors taught more modern, state-of-the-art languages and techniques instead of COBOL which was said to have a "trade school" nature. Donald Nelson, the chair of the CODASYL COBOL committee said in 1984 that "academics ... hate COBOL" and that computer science graduates "had 'hate COBOL' drilled into them". A 2013 poll by Micro Focus found that 20% of university academics thought COBOL was outdated or dead and that 55% believed their students thought COBOL was outdated or dead. The same poll also found that only 25% of academics had COBOL programming on their curriculum even though 60% thought they should teach it.
In contrast, in 2003, COBOL featured in 80% of information systems curricula in the United States, the same proportion as C++ and Java.
Concerns about the design process.
There were doubts about the effectiveness of the design process (sometimes from those taking part in it). Short-term committee member Howard Bromberg said that there was "little control" over the development process and that it was "plagued by discontinuity of personnel and ... a lack of talent".
COBOL standards have repeatedly suffered from delays: COBOL-85 arrived five years later than hoped,
COBOL 2002 was five years late,
and COBOL 2014 was six years late.
To combat delays, the standard committee allowed the creation of optional addenda which would add features more quickly than by waiting for the next standard revision. However, some committee members raised concerns about incompatibilities between implementations and frequent modifications of the standard.
Influences on other languages.
COBOL's data structures influenced subsequent programming languages. Its record and file structure influenced PL/I and Pascal, and the codice_175 clause was a predecessor to Pascal's variant records. Explicit file structure definitions preceded the development of database management systems and aggregated data was a significant advance over Fortran's arrays.
COBOL's codice_1 facility, although considered "primitive",
influenced the development of include directives.
The focus on portability and standardization meant programs written in COBOL could be portable and facilitated the spread of the language to a wide variety of hardware platforms and operating systems. Additionally, the well-defined division structure restricts the definition of external references to the Environment Division, which simplifies platform changes in particular.
Sources.
</dl>

</doc>
<doc id="6801" url="http://en.wikipedia.org/wiki?curid=6801" title="Crew">
Crew

A crew is a body or a class of people who work at a common activity, generally in a structured or hierarchical organization. A location in which a crew works is called a crewyard or a workyard. The word has nautical resonances: the tasks involved in operating a ship, particularly a sailing ship, providing numerous specialities within a ship's crew, often organised with a chain of command. Traditional nautical usage strongly distinguishes officers from crew, though the two groups combined form the ship's company. Members of a crew are often referred to by the title "Crewman".
"Crew" also refers to the sport of rowing, where teams row competitively in racing shells.
"Crew" is used colloquially to refer to a small, tight-knit group of friends or associates engaged in criminal activity. Also used in reference to the traditional "unit" of criminals under the supervision of a caporegime in the American Mafia. However, the term is not specific to (Mafia-affiliated) organized crime. "Crew" can also refer simply to a group of friends, unrelated to crime or violence.

</doc>
<doc id="6803" url="http://en.wikipedia.org/wiki?curid=6803" title="CCD">
CCD

CCD can stand for:

</doc>
<doc id="6804" url="http://en.wikipedia.org/wiki?curid=6804" title="Charge-coupled device">
Charge-coupled device

A charge-coupled device (CCD) is a device for the movement of electrical charge, usually from within the device to an area where the charge can be manipulated, for example conversion into a digital value. This is achieved by "shifting" the signals between stages within the device one at a time. CCDs move charge between capacitive "bins" in the device, with the shift allowing for the transfer of charge between bins.
The CCD is a major piece of technology in digital imaging. In a CCD image sensor, pixels are represented by p-doped MOS capacitors. These capacitors are biased above the threshold for inversion when image acquisition begins, allowing the conversion of incoming photons into electron charges at the semiconductor-oxide interface; the CCD is then used to read out these charges. Although CCDs are not the only technology to allow for light detection, CCD image sensors are widely used in professional, medical, and scientific applications where high-quality image data is required. In applications with less exacting quality demands, such as consumer and professional digital cameras, active pixel sensors (CMOS) are generally used; the large quality advantage CCDs enjoyed early on has narrowed over time.
History.
The charge-coupled device was invented in 1969 at AT&T Bell Labs by Willard Boyle and George E. Smith.
The lab was working on semiconductor bubble memory when Boyle and Smith conceived of the design of what they termed, in their notebook, "Charge 'Bubble' Devices".
The device could be used as a shift register. The essence of the design was the ability to transfer charge along the surface of a semiconductor from one storage capacitor to the next. The concept was similar in principle to the bucket-brigade device (BBD), which was developed at Philips Research Labs during the late 1960s. The first patent (4,085,456) on the application of CCDs to imaging was assigned to Michael Tompsett.
The initial paper describing the concept listed possible uses as a memory, a delay line, and an imaging device. The first experimental device demonstrating the principle was a row of closely spaced metal squares on an oxidized silicon surface electrically accessed by wire bonds.
The first working CCD made with integrated circuit technology was a simple 8-bit shift register. This device had input and output circuits and was used to demonstrate its use as a shift register and as a crude eight pixel linear imaging device.
Development of the device progressed at a rapid rate. By 1971, Bell researchers led by Michael Tompsett were able to capture images with simple linear devices.
Several companies, including Fairchild Semiconductor, RCA and Texas Instruments, picked up on the invention and began development programs. Fairchild's effort, led by ex-Bell researcher Gil Amelio, was the first with commercial devices, and by 1974 had a linear 500-element device and a 2-D 100 x 100 pixel device. Steven Sasson, an electrical engineer working for Kodak, invented the first digital still camera using a Fairchild 100 x 100 CCD in 1975. The first KH-11 KENNAN reconnaissance satellite equipped with charge-coupled device array (800 x 800 pixels) technology for imaging was launched in December 1976. Under the leadership of Kazuo Iwama, Sony also started a large development effort on CCDs involving a significant investment. Eventually, Sony managed to mass-produce CCDs for their camcorders. Before this happened, Iwama died in August 1982; subsequently, a CCD chip was placed on his tombstone to acknowledge his contribution.
In January 2006, Boyle and Smith were awarded the National Academy of Engineering Charles Stark Draper Prize, and in 2009 they were awarded the Nobel Prize for Physics, for their invention of the CCD concept.
Michael Tompsett was awarded the 2010 National Medal of Technology and Innovation for pioneering work and electronic technologies including the design and development of the first charge coupled device (CCD) imagers. He was also awarded the 2012 IEEE Edison Medal "For pioneering contributions to imaging devices including CCD Imagers, cameras and thermal imagers".
Basics of operation.
In a CCD for capturing images, there is a photoactive region (an epitaxial layer of silicon), and a transmission region made out of a shift register (the CCD, properly speaking).
An image is projected through a lens onto the capacitor array (the photoactive region), causing each capacitor to accumulate an electric charge proportional to the light intensity at that location. A one-dimensional array, used in line-scan cameras, captures a single slice of the image, whereas a two-dimensional array, used in video and still cameras, captures a two-dimensional picture corresponding to the scene projected onto the focal plane of the sensor. Once the array has been exposed to the image, a control circuit causes each capacitor to transfer its contents to its neighbor (operating as a shift register). The last capacitor in the array dumps its charge into a charge amplifier, which converts the charge into a voltage. By repeating this process, the controlling circuit converts the entire contents of the array in the semiconductor to a sequence of voltages. In a digital device, these voltages are then sampled, digitized, and usually stored in memory; in an analog device (such as an analog video camera), they are processed into a continuous analog signal (e.g. by feeding the output of the charge amplifier into a low-pass filter), which is then processed and fed out to other circuits for transmission, recording, or other processing.
Detailed physics of operation.
Charge generation.
Before the MOS capacitors are exposed to light, they are biased into the depletion region; in n-channel CCDs, the silicon under the bias gate is slightly "p"-doped or intrinsic. The gate is then biased at a positive potential, above the threshold for strong inversion, which will eventually result in the creation of a "n" channel below the gate as in a MOSFET. However, it takes time to reach this thermal equilibrium: up to hours in high-end scientific cameras cooled at low temperature. Initially after biasing, the holes are pushed far into the substrate, and no mobile electrons are at or near the surface; the CCD thus operates in a non-equilibrium state called deep depletion.
Then, when electron–hole pairs are generated in the depletion region, they are separated by the electric field, the electrons move toward the surface, and the holes move toward the substrate. Four pair-generation processes can be identified:
The last three processes are known as dark-current generation, and add noise to the image; they can limit the total usable integration time. The accumulation of electrons at or near the surface can proceed either until image integration is over and charge begins to be transferred, or thermal equilibrium is reached. In this case, the well is said to be full. The maximum capacity of each well is known as the well depth, typically about 105 electrons per pixel.
Design and manufacturing.
The photoactive region of a CCD is, generally, an epitaxial layer of silicon. It is lightly "p" doped (usually with boron) and is grown upon a substrate material, often p++. In buried-channel devices, the type of design utilized in most modern CCDs, certain areas of the surface of the silicon are ion implanted with phosphorus, giving them an n-doped designation. This region defines the channel in which the photogenerated charge packets will travel. Simon Sze details the advantages of a buried-channel device:
This thin layer (= 0.2–0.3 micron) is fully depleted and the accumulated photogenerated charge is kept away from the surface. This structure has the advantages of higher transfer efficiency and lower dark current, from reduced surface recombination. The penalty is smaller charge capacity, by a factor of 2–3 compared to the surface-channel CCD. The gate oxide, i.e. the capacitor dielectric, is grown on top of the epitaxial layer and substrate.
Later in the process, polysilicon gates are deposited by chemical vapor deposition, patterned with photolithography, and etched in such a way that the separately phased gates lie perpendicular to the channels. The channels are further defined by utilization of the LOCOS process to produce the channel stop region.
Channel stops are thermally grown oxides that serve to isolate the charge packets in one column from those in another. These channel stops are produced before the polysilicon gates are, as the LOCOS process utilizes a high-temperature step that would destroy the gate material. The channel stops are parallel to, and exclusive of, the channel, or "charge carrying", regions.
Channel stops often have a p+ doped region underlying them, providing a further barrier to the electrons in the charge packets (this discussion of the physics of CCD devices assumes an electron transfer device, though hole transfer is possible).
The clocking of the gates, alternately high and low, will forward and reverse bias the diode that is provided by the buried channel (n-doped) and the epitaxial layer (p-doped). This will cause the CCD to deplete, near the p-n junction and will collect and move the charge packets beneath the gates—and within the channels—of the device.
CCD manufacturing and operation can be optimized for different uses. The above process describes a frame transfer CCD. While CCDs may be manufactured on a heavily doped p++ wafer it is also possible to manufacture a device inside p-wells that have been placed on an n-wafer. This second method, reportedly, reduces smear, dark current, and infrared and red response. This method of manufacture is used in the construction of interline-transfer devices.
Another version of CCD is called a peristaltic CCD. In a peristaltic charge-coupled device, the charge-packet transfer operation is analogous to the peristaltic contraction and dilation of the digestive system. The peristaltic CCD has an additional implant that keeps the charge away from the silicon/silicon dioxide interface and generates a large lateral electric field from one gate to the next. This provides an additional driving force to aid in transfer of the charge packets.
Architecture.
The CCD image sensors can be implemented in several different architectures. The most common are full-frame, frame-transfer, and interline. The distinguishing characteristic of each of these architectures is their approach to the problem of shuttering.
In a full-frame device, all of the image area is active, and there is no electronic shutter. A mechanical shutter must be added to this type of sensor or the image smears as the device is clocked or read out.
With a frame-transfer CCD, half of the silicon area is covered by an opaque mask (typically aluminum). The image can be quickly transferred from the image area to the opaque area or storage region with acceptable smear of a few percent. That image can then be read out slowly from the storage region while a new image is integrating or exposing in the active area. Frame-transfer devices typically do not require a mechanical shutter and were a common architecture for early solid-state broadcast cameras. The downside to the frame-transfer architecture is that it requires twice the silicon real estate of an equivalent full-frame device; hence, it costs roughly twice as much.
The interline architecture extends this concept one step further and masks every other column of the image sensor for storage. In this device, only one pixel shift has to occur to transfer from image area to storage area; thus, shutter times can be less than a microsecond and smear is essentially eliminated. The advantage is not free, however, as the imaging area is now covered by opaque strips dropping the fill factor to approximately 50 percent and the effective quantum efficiency by an equivalent amount. Modern designs have addressed this deleterious characteristic by adding microlenses on the surface of the device to direct light away from the opaque regions and on the active area. Microlenses can bring the fill factor back up to 90 percent or more depending on pixel size and the overall system's optical design.
The choice of architecture comes down to one of utility. If the application cannot tolerate an expensive, failure-prone, power-intensive mechanical shutter, an interline device is the right choice. Consumer snap-shot cameras have used interline devices. On the other hand, for those applications that require the best possible light collection and issues of money, power and time are less important, the full-frame device is the right choice. Astronomers tend to prefer full-frame devices. The frame-transfer falls in between and was a common choice before the fill-factor issue of interline devices was addressed. Today, frame-transfer is usually chosen when an interline architecture is not available, such as in a back-illuminated device.
CCDs containing grids of pixels are used in digital cameras, optical scanners, and video cameras as light-sensing devices. They commonly respond to 70 percent of the incident light (meaning a quantum efficiency of about 70 percent) making them far more efficient than photographic film, which captures only about 2 percent of the incident light.
Most common types of CCDs are sensitive to near-infrared light, which allows infrared photography, night-vision devices, and zero lux (or near zero lux) video-recording/photography. For normal silicon-based detectors, the sensitivity is limited to 1.1 μm. One other consequence of their sensitivity to infrared is that infrared from remote controls often appears on CCD-based digital cameras or camcorders if they do not have infrared blockers.
Cooling reduces the array's dark current, improving the sensitivity of the CCD to low light intensities, even for ultraviolet and visible wavelengths. Professional observatories often cool their detectors with liquid nitrogen to reduce the dark current, and therefore the thermal noise, to negligible levels.
Use in astronomy.
Due to the high quantum efficiencies of CCDs, linearity of their outputs (for a quantum efficiency of 100%, one count equals one photon), ease of use compared to photographic plates, and a variety of other reasons, CCDs were very rapidly adopted by astronomers for nearly all UV-to-infrared applications.
Thermal noise and cosmic rays may alter the pixels in the CCD array. To counter such effects, astronomers take several exposures with the CCD shutter closed and opened. The average of images taken with the shutter closed is necessary to lower the random noise. Once developed, the "dark frame" average image is then subtracted from the open-shutter image to remove the dark current and other systematic defects (dead pixels, hot pixels, etc.) in the CCD.
The Hubble Space Telescope, in particular, has a highly developed series of steps (“data reduction pipeline”) to convert the raw CCD data to useful images.
CCD cameras used in astrophotography often require sturdy mounts to cope with vibrations from wind and other sources, along with the tremendous weight of most imaging platforms. To take long exposures of galaxies and nebulae, many astronomers use a technique known as auto-guiding. Most autoguiders use a second CCD chip to monitor deviations during imaging. This chip can rapidly detect errors in tracking and command the mount motors to correct for them.
An interesting unusual astronomical application of CCDs, called "drift-scanning", uses a CCD to make a fixed telescope behave like a tracking telescope and follow the motion of the sky. The charges in the CCD are transferred and read in a direction parallel to the motion of the sky, and at the same speed. In this way, the telescope can image a larger region of the sky than its normal field of view. The Sloan Digital Sky Survey is the most famous example of this, using the technique to produce the largest uniform survey of the sky yet accomplished.
In addition to astronomy, CCDs are also used in astronomical analytical instrumentation such as spectrometers.
Color cameras.
Digital color cameras generally use a Bayer mask over the CCD. Each square of four pixels has one filtered red, one blue, and two green (the human eye is more sensitive to green than either red or blue). The result of this is that luminance information is collected at every pixel, but the color resolution is lower than the luminance resolution.
Better color separation can be reached by three-CCD devices (3CCD) and a dichroic beam splitter prism, that splits the image into red, green and blue components. Each of the three CCDs is arranged to respond to a particular color. Many professional video camcorders, and some semi-professional camcorders, use this technique, although developments in competing CMOS technology have made CMOS sensors, both with beam-splitters and bayer filters, increasingly popular in high-end video and digital cinema cameras. Another advantage of 3CCD over a Bayer mask device is higher quantum efficiency (and therefore higher light sensitivity for a given aperture size). This is because in a 3CCD device most of the light entering the aperture is captured by a sensor, while a Bayer mask absorbs a high proportion (about 2/3) of the light falling on each CCD pixel.
For still scenes, for instance in microscopy, the resolution of a Bayer mask device can be enhanced by microscanning technology. During the process of color co-site sampling, several frames of the scene are produced. Between acquisitions, the sensor is moved in pixel dimensions, so that each point in the visual field is acquired consecutively by elements of the mask that are sensitive to the red, green and blue components of its color. Eventually every pixel in the image has been scanned at least once in each color and the resolution of the three channels become equivalent (the resolutions of red and blue channels are quadrupled while the green channel is doubled).
Sensor sizes.
Sensors (CCD / CMOS) come in various sizes, or image sensor formats. These sizes are often referred to with an inch fraction designation such as 1/1.8″ or 2/3″ called the optical format. This measurement actually originates back in the 1950s and the time of Vidicon tubes.
Electron-multiplying CCD.
An electron-multiplying CCD (EMCCD, also known as an L3Vision CCD, a product commercialized by L2V Ltd., GB, L3CCD or Impactron CCD, a product offered by Texas Instruments) is a charge-coupled device in which a gain register is placed between the shift register and the output amplifier. The gain register is split up into a large number of stages. In each stage, the electrons are multiplied by impact ionization in a similar way to an avalanche diode. The gain probability at every stage of the register is small ("P" < 2%), but as the number of elements is large (N > 500), the overall gain can be very high (formula_1), with single input electrons giving many thousands of output electrons. Reading a signal from a CCD gives a noise background, typically a few electrons. In an EMCCD, this noise is superimposed on many thousands of electrons rather than a single electron; the devices' primary advantage is thus their negligible readout noise. It is to be noted that the use of avalanche breakdown for amplification of photo charges had already been described in the US patent US3761744 in 1973 by George E. Smith/Bell Telephone Laboratories.
EMCCDs show a similar sensitivity to Intensified CCDs (ICCDs). However, as with ICCDs, the gain that is applied in the gain register is stochastic and the "exact" gain that has been applied to a pixel's charge is impossible to know. At high gains (> 30), this uncertainty has the same effect on the signal-to-noise ratio (SNR) as halving the quantum efficiency (QE) with respect to operation with a gain of unity. However, at very low light levels (where the quantum efficiency is most important), it can be assumed that a pixel either contains an electron — or not. This removes the noise associated with the stochastic multiplication at the risk of counting multiple electrons in the same pixel as a single electron. To avoid multiple counts in one pixel due to coincident photons in this mode of operation, high frame rates are essential. The dispersion in the gain is shown in the graph on the right. For multiplication registers with many elements and large gains it is well modelled by the equation:
formula_2 if formula_3
where "P" is the probability of getting "n" output electrons given "m" input electrons and a total mean multiplication register gain of "g".
Because of the lower costs and better resolution, EMCCDs are capable of replacing ICCDs in many applications. ICCDs still have the advantage that they can be gated very fast and thus are useful in applications like range-gated imaging. EMCCD cameras indispensably need a cooling system — using either thermoelectric cooling or liquid nitrogen — to cool the chip down to temperatures in the range of -65 to. This cooling system unfortunately adds additional costs to the EMCCD imaging system and may yield condensation problems in the application. However, high-end EMCCD cameras are equipped with a permanent hermetic vacuum system confining the chip to avoid condensation issues.
The low-light capabilities of EMCCDs primarily find use in astronomy and biomedical research, among other fields. In particular, their low noise at high readout speeds makes them very useful for a variety of astronomical applications involving low light sources and transient events such as lucky imaging of faint stars, high speed photon counting photometry, Fabry-Pérot spectroscopy and high-resolution spectroscopy. More recently, these types of CCDs have broken into the field of biomedical research in low-light applications including small animal imaging, single-molecule imaging, Raman spectroscopy, super resolution microscopy as well as a wide variety of modern fluorescence microscopy techniques thanks to greater SNR in low-light conditions in comparison with traditional CCDs and ICCDs.
In terms of noise, commercial EMCCD cameras typically have clock-induced charge (CIC) and dark current (dependent on the extent of cooling) that together lead to an effective readout noise ranging from 0.01 to 1 electrons per pixel read. However, recent improvements in EMCCD technology have led to a new generation of cameras capable of producing significantly less CIC, higher charge transfer efficiency and an EM gain 5 times higher than what was previously available. These advances in low-light detection lead to an effective total background noise of 0.001 electrons per pixel read, a noise floor unmatched by any other low-light imaging device.
Frame transfer CCD.
The frame transfer CCD imager was the first imaging structure proposed for CCD Imaging by Michael Tompsett at Bell Laboratories. A frame transfer CCD is a specialized CCD, often used in astronomy and some professional video cameras, designed for high exposure efficiency and correctness.
The normal functioning of a CCD, astronomical or otherwise, can be divided into two phases: exposure and readout. During the first phase, the CCD passively collects incoming photons, storing electrons in its cells. After the exposure time is passed, the cells are read out one line at a time. During the readout phase, cells are shifted down the entire area of the CCD. While they are shifted, they continue to collect light. Thus, if the shifting is not fast enough, errors can result from light that falls on a cell holding charge during the transfer. These errors are referred to as "vertical smear" and cause a strong light source to create a vertical line above and below its exact location. In addition, the CCD cannot be used to collect light while it is being read out. Unfortunately, a faster shifting requires a faster readout, and a faster readout can introduce errors in the cell charge measurement, leading to a higher noise level.
A frame transfer CCD solves both problems: it has a shielded, not light sensitive, area containing as many cells as the area exposed to light. Typically, this area is covered by a reflective material such as aluminium. When the exposure time is up, the cells are transferred very rapidly to the hidden area. Here, safe from any incoming light, cells can be read out at any speed one deems necessary to correctly measure the cells' charge. At the same time, the exposed part of the CCD is collecting light again, so no delay occurs between successive exposures.
The disadvantage of such a CCD is the higher cost: the cell area is basically doubled, and more complex control electronics are needed.
Intensified charge-coupled device.
An intensified charge-coupled device (ICCD) is a CCD that is optically connected to an image intensifier that is mounted in front of the CCD.
An image intensifier includes three functional elements: a photocathode, a micro-channel plate (MCP) and a phosphor screen. These three elements are mounted one close behind the other in the mentioned sequence. The photons which are coming from the light source fall onto the photocathode, thereby generating photoelectrons. The photoelectrons are accelerated towards the MCP by an electrical control voltage, applied between photocathode and MCP. The electrons are multiplied inside of the MCP and thereafter accelerated towards the phosphor screen. The phosphor screen finally converts the multiplied electrons back to photons which are guided to the CCD by a fiber optic or a lens.
An image intensifier inherently includes a shutter functionality: If the control voltage between the photocathode and the MCP is reversed, the emitted photoelectrons are not accelerated towards the MCP but return to the photocathode. Thus, no electrons are multiplied and emitted by the MCP, no electrons are going to the phosphor screen and no light is emitted from the image intensifier. In this case no light falls onto the CCD, which means that the shutter is closed. The process of reversing the control voltage at the photocathode is called "gating" and therefore ICCDs are also called gateable CCD cameras.
Besides the extremely high sensitivity of ICCD cameras, which enable single photon detection, the gateability is one of the major advantages of the ICCD over the EMCCD cameras. The highest performing ICCD cameras enable shutter times as short as 200 picoseconds.
ICCD cameras are in general somewhat higher in price than EMCCD cameras because they need the expensive image intensifier. On the other hand EMCCD cameras need a cooling system to cool the EMCCD chip down to temperatures around 170 K. This cooling system adds additional costs to the EMCCD camera and often yields heavy condensation problems in the application.
ICCDs are used in night vision devices and in a large variety of scientific applications.
Blooming.
When a CCD exposure is long enough, eventually the electrons that collect in the "bins" in the brightest part of the image will overflow the bin, resulting in blooming. The structure of the CCD allows the electrons to flow more easily in one direction than another, resulting in vertical streaking.
Some anti-blooming features that can be built into a CCD reduce its sensitivity to light by using some of the pixel area for a drain structure.
James M. Early developed a vertical anti-blooming drain that would not detract from the light collection area, and so did not reduce light sensitivity.

</doc>
<doc id="6806" url="http://en.wikipedia.org/wiki?curid=6806" title="Computer memory">
Computer memory

In computing, memory refers to the devices used to store information for use in a computer. The term primary memory is used for storage systems which function at high-speed (i.e. RAM), as a distinction from secondary memory, which provides program and data storage that is slow to access but offer higher memory capacity. If needed, primary memory can be stored in secondary memory, through a memory management technique called "virtual memory". An archaic synonym for memory is store.
The term "memory", meaning primary memory is often associated with addressable semiconductor memory, i.e. integrated circuits consisting of silicon-based transistors, used for example as primary memory but also other purposes in computers and other digital electronic devices. 
There are two main types of semiconductor memory: volatile and non-volatile. Examples of non-volatile memory are flash memory (sometimes used as secondary, sometimes primary computer memory) and ROM/PROM/EPROM/EEPROM memory (used for firmware such as boot programs). Examples of volatile memory are primary memory (typically dynamic RAM, DRAM), and fast CPU cache memory (typically static RAM, SRAM, which is fast but energy-consuming and offer lower memory capacity per area unit than DRAM). 
Most semiconductor memory is organized into memory cells or bistable flip-flops, each storing one bit (0 or 1). Flash memory organization includes both one bit per memory cell and multiple bits per cell (called MLC, Multiple Level Cell). The memory cells are grouped into words of fixed word length, for example 1, 2, 4, 8, 16, 32, 64 or 128 bit. Each word can be accessed by a binary address of "N" bit, making it possible to store 2 raised by "N" words in the memory. This implies that processor registers normally are not considered as memory, since they only store one word and do not include an addressing mechanism.
The term storage is often used to describe secondary memory such as tape, magnetic disks and optical discs (CD-ROM and DVD-ROM).
History.
In the early 1940s, memory technology mostly permitted a capacity of a few bytes. The first electronic programmable digital computer, the ENIAC, using thousands of octal-base radio vacuum tubes, could perform simple calculations involving 20 numbers of ten decimal digits which were held in the vacuum tube accumulators.
The next significant advance in computer memory came with acoustic delay line memory, developed by J. Presper Eckert in the early 1940s. Through the construction of a glass tube filled with mercury and plugged at each end with a quartz crystal, delay lines could store bits of information within the quartz and transfer it through sound waves propagating through mercury. Delay line memory would be limited to a capacity of up to a few hundred thousand bits to remain efficient.
Two alternatives to the delay line, the Williams tube and Selectron tube, originated in 1946, both using electron beams in glass tubes as means of storage. Using cathode ray tubes, Fred Williams would invent the Williams tube, which would be the first random access computer memory. The Williams tube would prove more capacious than the Selectron tube (the Selectron was limited to 256 bits, while the Williams tube could store thousands) and less expensive. The Williams tube would nevertheless prove to be frustratingly sensitive to environmental disturbances.
Efforts began in the late 1940s to find non-volatile memory. Jay Forrester, Jan A. Rajchman and An Wang developed magnetic core memory, which allowed for recall of memory after power loss. Magnetic core memory would become the dominant form of memory until the development of transistor-based memory in the late 1960s.
Developments in technology and economies of scale have made possible so-called Very Large Memory (VLM) computers. 
The term "memory" when used with reference to computers generally refers to Random Access Memory or RAM.
Volatile memory.
Volatile memory is computer memory that requires power to maintain the stored information. Most modern semiconductor volatile memory is either Static RAM (see SRAM) or dynamic RAM (see DRAM). SRAM retains its contents as long as the power is connected and is easy to interface to but uses six transistors per bit. Dynamic RAM is more complicated to interface to and control and needs regular refresh cycles to prevent its contents being lost. However, DRAM uses only one transistor and a capacitor per bit, allowing it to reach much higher densities and, with more bits on a memory chip, be much cheaper per bit.
SRAM is not worthwhile for desktop system memory, where DRAM dominates, but is used for their cache memories. SRAM is commonplace in small embedded systems, which might only need tens of kilobytes or less. Forthcoming volatile memory technologies that hope to replace or compete with SRAM and DRAM include Z-RAM, TTRAM, A-RAM and ETA RAM.
Non-volatile memory.
Non-volatile memory is computer memory that can retain the stored information even when not powered. Examples of non-volatile memory include read-only memory (see ROM), flash memory, most types of magnetic computer storage devices (e.g. hard disks, floppy discs and magnetic tape), optical discs, and early computer storage methods such as paper tape and punched cards.
Forthcoming non-volatile memory technologies include FeRAM, CBRAM, PRAM, SONOS, RRAM, Racetrack memory, NRAM and Millipede.
Management of memory.
Proper management of memory is vital for a computer system to operate properly. Modern operating systems have complex systems to properly manage memory. Failure to do so can lead to bugs, slow performance, and at worst case, takeover by viruses and malicious software.
Nearly everything a computer programmer does requires him or her to consider how to manage memory. Even storing a number in memory requires the programmer to specify how the memory should store it.
Memory management bugs.
Improper management of memory is a common cause of bugs.
Early computer systems.
In early computer systems, programs typically specified the location to write memory and what data to put there. This location was a physical location on the actual memory hardware. The slow processing of such computers did not allow for the complex memory management systems used today. Also, as most such systems were single-task, sophisticated systems were not required as much.
This approach has its pitfalls. If the location specified is incorrect, this will cause the computer to write the data to some other part of the program. The results of an error like this are unpredictable. In some cases, the incorrect data might overwrite memory used by the operating system. Computer crackers can take advantage of this to create viruses and malware.
Virtual memory.
Virtual memory is a system where all physical memory is controlled by the operating system. When a program needs memory, it requests it from the operating system. The operating system then decides what physical location to place the memory in.
This offers several advantages. Computer programmers no longer need to worry about where the memory is physically stored or whether the user's computer will have enough memory. It also allows multiple types of memory to be used. For example, some memory can be stored in physical RAM chips while other memory is stored on a hard drive. This drastically increases the amount of memory available to programs. The operating system will place actively used memory in physical RAM, which is much faster than hard disks. When the amount of RAM is not sufficient to run all the current programs, it can result in a situation where the computer spends more time moving memory from RAM to disk and back than it does accomplishing tasks; this is known as thrashing.
Virtual memory systems usually include protected memory, but this is not always the case.
Protected memory.
Protected memory is a system where each program is given an area of memory to use and is not permitted to go outside that range. Use of protected memory greatly enhances both the reliability and security of a computer system.
Without protected memory, it is possible that a bug in one program will alter the memory used by another program. This will cause that other program to run off of corrupted memory with unpredictable results. If the operating system's memory is corrupted, the entire computer system may crash and need to be rebooted. At times programs intentionally alter the memory used by other programs. This is done by viruses and malware to take over computers.
Protected memory assigns programs their own areas of memory. If the operating system detects that a program has tried to alter memory that does not belong to it, the program is terminated. This way, only the offending program crashes, and other programs are not affected by the error.
Protected memory systems almost always include virtual memory as well.

</doc>
<doc id="6809" url="http://en.wikipedia.org/wiki?curid=6809" title="CDC">
CDC

CDC may refer to:

</doc>
<doc id="6811" url="http://en.wikipedia.org/wiki?curid=6811" title="Centers for Disease Control and Prevention">
Centers for Disease Control and Prevention

The Centers for Disease Control and Prevention is the leading national public health institute of the United States. The CDC is a federal agency under the Department of Health and Human Services and is headquartered in unincorporated DeKalb County, Georgia, a few miles northeast of the Atlanta city limits.
Its main goal is to protect public health and safety through the control and prevention of disease, injury, and disability. The CDC focuses national attention on developing and applying disease control and prevention. It especially focuses its attention on infectious disease, food borne pathogens, environmental health, occupational safety and health, health promotion, injury prevention and educational activities designed to improve the health of United States citizens. In addition, the CDC researches and provides information on non-infectious diseases such as obesity and diabetes and is a founding member of the International Association of National Public Health Institutes.
History.
The Communicable Diseases Center was founded July 1, 1946, as the successor to the World War II Malaria Control in War Areas program of the Office of National Defense Malaria Control Activities. Preceding its founding, organizations with global influence in malaria control were the Malaria Commission of the League of Nations and the Rockefeller Foundation. The Rockefeller Foundation greatly supported malaria control, sought to have the governments take over some of its efforts, and collaborated with the agency.
The new agency was a branch of the U.S. Public Health Service and Atlanta was chosen as the location because malaria was endemic in the Southern United States. The agency changed names (see infobox on top) before adopting the name "Communicable Disease Center" in 1946. Offices were located on the sixth floor of the Volunteer Building on Peachtree Street. With a budget at the time of about $1 million, 59 percent of its personnel were engaged in mosquito abatement and habitat control with the objective of control and eradication of malaria in the United States (see National Malaria Eradication Program).
Among its 369 employees, the main jobs at CDC were originally entomology and engineering. In CDC's initial years, more than six and a half million homes were sprayed, mostly with DDT. In 1946, there were only seven medical officers on duty and an early organization chart was drawn, somewhat fancifully, in the shape of a mosquito. Under , the CDC continued to advocate for public health issues and pushed to extend its responsibilities to many other communicable diseases. In 1947, CDC made a token payment of $10 to Emory University for 15 acre of land on Clifton Road in DeKalb County, still the home of CDC headquarters today. CDC employees collected the money to make the purchase. The benefactor behind the “gift” was Robert W. Woodruff, chairman of the board of The Coca-Cola Company. Woodruff had a long-time interest in malaria control, which had been a problem in areas where he went hunting. The same year, the PHS transferred its San Francisco based plague laboratory into the CDC as the Epidemiology Division, and a new Veterinary Diseases Division was established. An Epidemic Intelligence Service (EIS) was established in 1951, originally due to biological warfare concerns arising from the Korean War; it evolved into two-year postgraduate training program in epidemiology, and a prototype for Field Epidemiology Training Programs (FETP), now found in numerous countries, reflecting CDC's influence in promoting this model internationally.
The mission of CDC expanded beyond its original focus on malaria to include sexually transmitted diseases when the Venereal Disease Division of the U.S. Public Health Service (PHS) was transferred to the CDC in 1957. Shortly thereafter, Tuberculosis Control was transferred (in 1960) to the CDC from PHS, and then in 1963 the Immunization program was established.
It became the "National Communicable Disease Center (NCDC)" effective July 1, 1967. The organization was renamed the "Center for Disease Control (CDC)" on June 24, 1970, and "Centers for Disease Control" effective October 14, 1980. An act of the United States Congress appended the words "and Prevention" to the name effective October 27, 1992. However, Congress directed that the initialism "CDC" be retained because of its name recognition. CDC now operates under the Department of Health and Human Services umbrella.
Currently the CDC focus has broadened to include chronic diseases, disabilities, injury control, workplace hazards, environmental health threats, and terrorism preparedness. CDC combats emerging diseases and other health risks, including birth defects, West Nile virus, obesity, avian, swine, and pandemic flu, E. coli, and bioterrorism, to name a few. The organization would also prove to be an important factor in preventing the abuse of penicillin.
In May 1994 the CDC admitted to having sent several biological warfare agents to the Iraqi government from 1984 through 1989, including Botulinum toxin, West Nile virus, Yersinia pestis and Dengue fever virus.
On April 21, 2005, the then-director of CDC, Dr. Julie Gerberding, formally announced the reorganization of CDC to "confront the challenges of 21st-century health threats". The four Coordinating Centers—established under the G. W. Bush Administration and Gerberding—"diminished the influence of national centers under [their] umbrella" and were ordered cut under the Obama Administration and Frieden in 2009.
The CDC's Biosafety Level 4 laboratories are among only about a dozen such facilities in the country, as well as one of only two official repositories of smallpox in the world. The second smallpox store resides at the State Research Center of Virology and Biotechnology VECTOR in the Russian Federation. The CDC revealed in 2014 that it had discovered several misplaced smallpox samples and that lab workers had also potentially been infected with anthrax.
Organization.
The CDC is organized into "Centers, Institutes, and Offices" (CIOs) which allow it to be responsive and effective in its interface with public health concerns. Each organizational unit implements the agency's response in a particular area of expertise. Within "Offices" are Centers, Divisions, and Branches.
The CIOs are:
Budget and workforce.
CDC’s FY2014 budget is $6.9 billion.
As of 2008, staff numbered approximately 15,000 (including 6,000 contractors and 840 Commissioned Corps officers) in 170 occupations. Eighty percent have earned bachelor's degrees or higher; almost half have advanced degrees (a master's degree or a doctorate such as a PhD, D.O., or M.D.). CDC job titles include engineer, entomologist, epidemiologist, biologist, physician, veterinarian, behaviorial scientist, nurse, medical technologist, economist, public health advisor, health communicator, toxicologist, chemist, computer scientist, and statistician.
In addition to its Atlanta headquarters, the CDC has other locations in the United States and Puerto Rico. Those locations include Anchorage; Cleveland; Cincinnati; Fort Collins; Hyattsville; Morgantown; Pittsburgh; Research Triangle Park; San Juan, Puerto Rico; Spokane, Washington; Detroit; and Washington, D.C. The CDC also conducts the Behavioral Risk Factor Surveillance System, the world’s largest, on-going telephone health survey system.
The CDC offers grants that help many organizations each year bring health, safety and awareness to surrounding communities throughout the entire United States. As a government-run department, the Centers for Disease Control and Prevention awards over 85 percent of its annual budget through these grants to accomplish its ultimate goal of disease control and quality health for all.
The CDC operates the Public Health Associate Program (PHAP), a two-year paid fellowship for recent college graduates to work in public health agencies all over the United States. PHAP was founded in 2007 and currently has 159 associates in 34 states.
Directors.
The President of the United States appoints the director of the CDC and the appointment does not require Senate confirmation. The director serves at the pleasure of the President and may be fired at any time. Sixteen directors have served the CDC or its predecessor agencies.
Foundation.
The CDC Foundation operates independently from CDC as a private, nonprofit 501(c)(3) organization incorporated in the State of Georgia. The creation of the Foundation was authorized by section 399F of the Public Health Service Act to support the mission of CDC in partnership with the private sector, including organizations, foundations, businesses, educational groups, and individuals.
Diseases.
Influenza.
The CDC has launched campaigns targeting the transmission of influenza, including the H1N1 swine flu. The CDC has launched websites including [flu.gov] to educate people in proper hygiene.
Other infectious diseases.
The CDC's website (see below) has information on other infectious diseases, including smallpox, measles, and others. The CDC runs a program that protects the public from rare and dangerous substances such as anthrax and the Ebola virus. The program, called the Select Agents Program, calls for inspections of labs in the U.S. that work with dangerous pathogens.
During the 2014 Ebola outbreak in West Africa, the CDC helped coordinate the return of two infected American aid workers for treatment at Emory University Hospital, the home of a special unit to handle highly infectious diseases.
As a response to 2014 Ebola outbreak, the U.S. House of Representatives proposed and passed a Continuing Appropriations Resolution to allocate up to $30,000,000 towards CDCP's efforts to fight the virus.
Non-infectious disease.
The CDC also works on non-infectious diseases, including obesity. 
Investigations by the DHHS Office of Inspector General (OIG).
On June 15, 2011, the OIG published a report critical of the CDC's failure to oversee recipients' use of President's Emergency Plan for AIDS Relief (PEPFAR) funds. The report read in part: 
Our review found that CDC did not always monitor recipients' use of President's Emergency Plan for AIDS Relief (PEPFAR) funds in accordance with departmental and other Federal requirements. CDC implements PEPFAR, working with ministries of health and other public health partners to combat HIV/AIDS by strengthening health systems and building sustainable HIV/AIDS programs in more than 75 countries in Africa, Asia, Central and South America, and the Caribbean. HHS receives PEPFAR funds from the Department of State through a memorandum of agreement.
There was evidence that the CDC performed some monitoring of recipients' use of PEPFAR funds. However, most of the award files did not include all required documents or evidence to demonstrate that CDC performed required monitoring on all cooperative agreements. Of the 30 cooperative agreements in our sample, the award file for only 1 agreement contained all required documents. The remaining 29 award files were incomplete. In addition, 14 of 21 files were missing audit reports. (A report was not yet due for 9 of the 30 cooperative agreements.) The lack of required documentation demonstrates that CDC has not exercised proper stewardship over Federal PEPFAR funds because it did not consistently follow departmental and other Federal requirements in monitoring PEPFAR recipients.
”
On June 5, 2012, the OIG published a report identifying vulnerabilities in vaccine management in the CDC's domestic 'Vaccines for Children' (VFC) program. The report read in part: 
Although the majority of storage temperatures we independently measured during a 2-week period were within the required ranges, VFC vaccines stored by 76 percent of the 45 selected providers were exposed to inappropriate temperatures for at least 5 cumulative hours during that period. Exposure to inappropriate temperatures can reduce vaccine potency and efficacy, increasing the risk that children are not provided with maximum protection against preventable diseases. Thirteen providers stored expired vaccines together with non-expired vaccines, increasing the risk of mistakenly administering the expired vaccine. Finally, the selected providers generally did not meet vaccine management requirements or maintain required documentation. Similarly, none of the five selected grantees met all VFC program oversight requirements, and grantee site visits were not effective in ensuring that providers met vaccine management requirements over time.
”
On the November 19, 2012, the OIG published a report critical of the CDC Namibia Office's failure to properly monitor recipients' use of PEPFAR funds. The report read in part: 
CDC's office in Windhoek, Namibia (CDC Namibia), is responsible for PEPFAR funds awarded to government agencies and for-profit and non-profit organizations (recipients) in Namibia. Our audit found that CDC Namibia did not always monitor recipients' use of PEPFAR funds in accordance with HHS and other Federal requirements. There was evidence that CDC Namibia performed some monitoring of recipients' use of PEPFAR funds. However, most of the recipient cooperative agreement files did not include required documents or evidence that CDC Namibia had monitored all cooperative agreements. CDC Namibia did not consistently monitor the cooperative agreements in accordance with HHS and other Federal requirements because it did not have written policies and procedures for the monitoring process. As a result, CDC Namibia did not have assurance that PEPFAR funds were used as intended by law. We recommended that CDC Namibia implement standard operating procedures for monitoring recipients' use of PEPFAR funds. CDC concurred with our recommendation.
”
Controversies.
For 15 years, the CDC had direct oversight over the . In the study, which lasted from 1932 to 1972, a group of African American men (nearly 400 of whom had syphilis) were studied to learn more about the disease. Notably, the disease was left untreated in the research subjects and they never gave their informed consent to serve as research subjects. The Tuskegee Study was initiated in 1932 by the Public Health Service. The .
In the wake of the 2014 Ebola crisis in the United States, columnist Michelle Malkin drew attention to CDC priorities and spending patterns on politically devised non-disease control-related priorities, including motorcycle helmet laws, video games/media imagery studies, and playground injury centers.
CDC zombie apocalypse outreach campaign.
On May 16, 2011, the Centers for Disease Control and Prevention's blog published an article instructing the public on what to do to prepare for a zombie invasion. While the article did not claim that such a scenario was possible, it did use the popular culture appeal as a means of urging citizens to prepare for all potential hazards, such as earthquakes, tornadoes, and floods.
According to David Daigle, the Associate Director for Communications, Public Health Preparedness and Response, the idea arose when his team was discussing their upcoming hurricane information campaign and Daigle mused that "we say pretty much the same things every year, in the same way, and I just wonder how many people are paying attention." A social media employee mentioned that the subject of zombies had come up a lot on Twitter when she had been tweeting about the Fukushima Daiichi nuclear disaster and radiation. The team realized that a campaign like this would most likely reach a different audience from the one that normally pays attention to hurricane preparedness warnings and went to work on the zombie campaign, launching it right before hurricane season began. "The whole idea was, if you're prepared for a zombie apocalypse, you're prepared for pretty much anything," said Daigle.
Once the blog article became popular, the CDC announced an open contest for YouTube submissions of the most creative and effective videos covering preparedness for a zombie apocalypse (or apocalypse of any kind), to be judged by the "CDC Zombie Task Force". Submissions were open until October 11, 2011. They also released a zombie themed graphic novella available on their website. Zombie-themed educational materials for teachers are available on the site.
See also.
For similar agencies elsewhere, see the list of national public health agencies.

</doc>
<doc id="6813" url="http://en.wikipedia.org/wiki?curid=6813" title="Chandrasekhar limit">
Chandrasekhar limit

The Chandrasekhar limit () is the maximum mass of a stable white dwarf star. The limit was first indicated in papers published by Wilhelm Anderson and E. C. Stoner, and was named after Subrahmanyan Chandrasekhar, the Indian astrophysicist who independently discovered and improved upon the accuracy of the calculation in 1930, at the age of 19, in India. This limit was initially ignored by the community of scientists because such a limit would logically require the existence of black holes, which were considered a scientific impossibility at the time. White dwarfs resist gravitational collapse primarily through electron degeneracy pressure. (By comparison, main sequence stars resist collapse through thermal pressure.) The Chandrasekhar limit is the mass above which electron degeneracy pressure in the star's core is insufficient to balance the star's own gravitational self-attraction. Consequently, white dwarfs with masses greater than the limit would be subject to further gravitational collapse, evolving into a different type of stellar remnant, such as a neutron star or black hole. (However, white dwarfs generally avoid this fate by exploding before they undergo collapse.) Those with masses under the limit remain stable as white dwarfs.
The currently accepted value of the limit is about 1.39 formula_1 ( 2.765 × 1030 kg).
Physics.
Electron degeneracy pressure is a quantum-mechanical effect arising from the Pauli exclusion principle. Since electrons are fermions, no two electrons can be in the same state, so not all electrons can be in the minimum-energy level. Rather, electrons must occupy a band of energy levels. Compression of the electron gas increases the number of electrons in a given volume and raises the maximum energy level in the occupied band. Therefore, the energy of the electrons will increase upon compression, so pressure must be exerted on the electron gas to compress it, producing electron degeneracy pressure. With sufficient compression, electrons are forced into nuclei in the process of electron capture, relieving the pressure.
In the nonrelativistic case, electron degeneracy pressure gives rise to an equation of state of the form formula_2, where "P" is the pressure, formula_3 is the mass density, and formula_4 is a constant. Solving the hydrostatic equation then leads to a model white dwarf which is a polytrope of index 3/2 and therefore has radius inversely proportional to the cube root of its mass, and volume inversely proportional to its mass.
As the mass of a model white dwarf increases, the typical energies to which degeneracy pressure forces the electrons are no longer negligible relative to their rest masses. The velocities of the electrons approach the speed of light, and special relativity must be taken into account. In the strongly relativistic limit, the equation of state takes the form formula_5. This will yield a polytrope of index 3, which will have a total mass, Mlimit say, depending only on K2.
For a fully relativistic treatment, the equation of state used will interpolate between the equations formula_2 for small ρ and formula_5 for large ρ.
When this is done, the model radius still decreases with mass, but becomes zero at Mlimit. This is the Chandrasekhar limit. The curves of radius against mass for the non-relativistic and relativistic models are shown in the graph. They are colored blue and green, respectively. μe has been set equal to 2.
Radius is measured in standard solar radii or kilometers, and mass in standard solar masses.
Calculated values for the limit will vary depending on the nuclear composition of the mass. Chandrasekhar, eq. (36), eq. (58), eq. (43) gives the following expression, based on the equation of state for an ideal Fermi gas:
where:
As formula_11 is the Planck mass, the limit is of the order of
A more accurate value of the limit than that given by this simple model requires adjusting for various factors, including electrostatic interactions between the electrons and nuclei and effects caused by nonzero temperature. Lieb and Yau have given a rigorous derivation of the limit from a relativistic many-particle Schrödinger equation.
History.
In 1926, the British physicist Ralph H. Fowler observed that the relationship among the density, energy and temperature of white dwarfs could be explained by viewing them as a gas of nonrelativistic, non-interacting electrons and nuclei which obeyed Fermi–Dirac statistics. This Fermi gas model was then used by the British physicist E. C. Stoner in 1929 to calculate the relationship among the mass, radius, and density of white dwarfs, assuming them to be homogeneous spheres. Wilhelm Anderson applied a relativistic correction to this model, giving rise to a maximum possible mass of approximately 1.37×1030 kg. In 1930, Stoner derived the internal energy–density equation of state for a Fermi gas, and was then able to treat the mass-radius relationship in a fully relativistic manner, giving a limiting mass of approximately (for μe=2.5) 2.19 · 1030 kg. Stoner went on to derive the pressure–density equation of state, which he published in 1932. These equations of state were also previously published by the Soviet physicist Yakov Frenkel in 1928, together with some other remarks on the physics of degenerate matter. Frenkel's work, however, was ignored by the astronomical and astrophysical community.
A series of papers published between 1931 and 1935 had its beginning on a trip from India to England in 1930,
where the Indian physicist Subrahmanyan Chandrasekhar worked on the calculation of the statistics of a degenerate Fermi gas. In these papers, Chandrasekhar solved
the hydrostatic equation together with the nonrelativistic Fermi gas equation of state, and also treated the case of a relativistic Fermi gas, giving rise to the value of the limit shown above. Chandrasekhar reviews this work in his Nobel Prize lecture. This value was also computed in 1932 by the Soviet physicist Lev Davidovich Landau, who, however, did not apply it to white dwarfs.
Chandrasekhar's work on the limit aroused controversy, owing to the opposition of the British astrophysicist Arthur Stanley Eddington. Eddington was aware that the existence of black holes was theoretically possible, and also realized that the existence of the limit made their formation possible. However, he was unwilling to accept that this could happen. After a talk by Chandrasekhar on the limit in 1935, he replied:
The star has to go on radiating and radiating and contracting and contracting until, I suppose, it gets down to a few km radius, when gravity becomes strong enough to hold in the radiation, and the star can at last find peace. … I think there should be a law of Nature to prevent a star from behaving in this absurd way!—
Eddington's proposed solution to the perceived problem was to modify relativistic mechanics so as to make the law P=K1ρ5/3 universally applicable, even for large ρ. Although Bohr, Fowler, Pauli, and other physicists agreed with Chandrasekhar's analysis, at the time, owing to Eddington's status, they were unwilling to publicly support Chandrasekhar., pp. 110–111 Through the rest of his life, Eddington held to his position in his writings, including his work on his fundamental theory. The drama associated with this disagreement is one of the main themes of "Empire of the Stars", Arthur I. Miller's biography of Chandrasekhar. In Miller's view:
Chandra's discovery might well have transformed and accelerated developments in both physics and astrophysics in the 1930s. Instead, Eddington's heavy-handed intervention lent weighty support to the conservative community astrophysicists, who steadfastly refused even to consider the idea that stars might collapse to nothing. As a result, Chandra's work was almost forgotten.—p. 150, 
Applications.
The core of a star is kept from collapsing by the heat generated by the fusion of nuclei of lighter elements into heavier ones. At various stages of stellar evolution, the nuclei required for this process will be exhausted, and the core will collapse, causing it to become denser and hotter. A critical situation arises when iron accumulates in the core, since iron nuclei are incapable of generating further energy through fusion. If the core becomes sufficiently dense, electron degeneracy pressure will play a significant part in stabilizing it against gravitational collapse.
If a main-sequence star is not too massive (less than approximately 8 solar masses), it will eventually shed enough mass to form a white dwarf having mass below the Chandrasekhar limit, which will consist of the former core of the star. For more massive stars, electron degeneracy pressure will not keep the iron core from collapsing to very great density, leading to formation of a neutron star, black hole, or, speculatively, a quark star. (For very massive, low-metallicity stars, it is also possible that instabilities will destroy the star completely.) During the collapse, neutrons are formed by the capture of electrons by protons in the process of electron capture, leading to the emission of neutrinos., pp. 1046–1047. The decrease in gravitational potential energy of the collapsing core releases a large amount of energy which is on the order of 1046 joules (100 foes). Most of this energy is carried away by the emitted neutrinos. This process is believed to be responsible for supernovae of types Ib, Ic, and II.
Type Ia supernovae derive their energy from runaway fusion of the nuclei in the interior of a white dwarf. This fate may befall carbon–oxygen white dwarfs that accrete matter from a companion giant star, leading to a steadily increasing mass. As the white dwarf's mass approaches the Chandrasekhar limit, its central density increases, and, as a result of compressional heating, its temperature also increases. This eventually ignites nuclear fusion reactions, leading to an immediate carbon detonation which disrupts the star and causes the supernova., §5.1.2
A strong indication of the reliability of Chandrasekhar's formula is that the absolute magnitudes of supernovae of Type Ia are all approximately the same; at maximum luminosity, MV is approximately -19.3, with a standard deviation of no more than 0.3., (1) A 1-sigma interval therefore represents a factor of less than 2 in luminosity. This seems to indicate that all type Ia supernovae convert approximately the same amount of mass to energy.
Super-Chandrasekhar mass supernovae.
In April 2003, the Supernova Legacy Survey observed a type Ia supernova, designated SNLS-03D3bb, in a galaxy approximately 4 billion light years away. According to a group of astronomers at the University of Toronto and elsewhere, the observations of this supernova are best explained by assuming that it arose from a white dwarf which grew to twice the mass of the Sun before exploding. They believe that the star, dubbed the "Champagne Supernova" by University of Oklahoma astronomer David R. Branch, may have been spinning so fast that centrifugal force allowed it to exceed the limit. Alternatively, the supernova may have resulted from the merger of two white dwarfs, so that the limit was only violated momentarily. Nevertheless, they point out that this observation poses a challenge to the use of type Ia supernovae as standard candles.
Since the observation of the Champagne Supernova in 2003, more very bright type Ia supernovae have been observed that are thought to have originated from white dwarfs whose masses exceeded the Chandrasekhar limit. These include SN 2006gz, SN 2007if and SN 2009dc. The super-Chandrasekhar mass white dwarfs that gave rise to these supernovae are believed to have had masses up to 2.4–2.8 solar masses. One way to potentially explain the problem of the Champagne Supernova was considering it the result of an aspherical explosion of a white dwarf. However, spectropolarimetric observations of SN 2009dc showed it had a polarization smaller than 0.3, making the large asphericity theory unlikely.
Tolman–Oppenheimer–Volkoff limit.
After a supernova explosion, a neutron star may be left behind. Like white dwarfs these objects are extremely compact and are supported by degeneracy pressure, but a neutron star is so massive and compressed that electrons and protons have combined to form neutrons, and the star is thus supported by neutron degeneracy pressure instead of electron degeneracy pressure. The limit of neutron degeneracy pressure, analogous to the Chandrasekhar limit, is known as the Tolman–Oppenheimer–Volkoff limit.

</doc>
<doc id="6814" url="http://en.wikipedia.org/wiki?curid=6814" title="Congregationalist polity">
Congregationalist polity

Congregationalist polity, often known as congregationalism, is a system of church governance in which every local church congregation is independent, ecclesiastically sovereign, or "autonomous". Among those major Protestant Christian traditions that employ congregationalism are those Congregational Churches known by the "Congregationalist" name that descended from the Anglo-American Puritan movement of the 17th century, the Baptist churches, and most of the groups brought about by the Anabaptist movement in Germany that migrated to the U.S. in the late 18th century. More recent generations have witnessed also a growing number of non-denominational churches, which are most often congregationalist in their governance. In Christianity, congregationalism is distinguished most clearly from episcopal polity, which is governance by a hierarchy of bishops. But it is also distinct from presbyterian polity, in which higher assemblies of congregational representatives can exercise considerable authority over individual congregations.
Congregationalism is not limited only to organization of Christian congregations; the principles of congregationalism have been inherited by the Unitarian Universalist Association and the Canadian Unitarian Council. Jewish synagogues and most Islamic mosques in the U.S. operate under congregational government, with no hierarchies.
Basic form.
The term "congregationalist polity" describes a form of church governance that is based on the local congregation. Each local congregation is independent and self-supporting, governed by its own members.:49 Some band into loose voluntary associations with other congregations that share similar beliefs (e.g., the Willow Creek Association).:49 Others join "conventions", such as the Southern Baptist Convention, the National Baptist Convention or the American Baptist Churches USA (formerly the Northern Baptist Convention).:49 These conventions generally provide stronger ties between congregations, including some doctrinal direction and pooling of financial resources.:49 Congregations that belong to associations and conventions are still independently governed.:49 Most non-denominational churches are organized along congregationalist lines.:49 Many do not see these voluntary associations as "denominations", because they "believe that there is no church other than the local church, and denominations are in variance to Scripture.":49
Congregational church.
The earmarks of Congregationalism can be traced back to the Pilgrim societies of the United States in the early 17th century. Congregationalism expressed the viewpoint that (1) every local church is a full realization in miniature of the entire Church of Jesus Christ; and (2) the Church, while on earth, besides the local church, can only be invisible and ideal. While other theories may insist on the truth of the former, the latter precept of congregationalism gives the entire theory a unique character among plans of church government. There is no other reference than the local congregation for the "visible church" in Congregationalism. And yet, the connection of all Christians is also asserted, albeit in a way that defenders of this view usually decline, often intentionally, to elaborate more clearly or consistently. This first, foundational principle by which congregationalism is guided results in confining it to operate with the consent of each gathering of believers.
Although "congregational rule" may seem to suggest that pure democracy reigns in congregational churches, this is seldom the case. It is granted, with few exceptions (namely in some Anabaptist churches), that God has given the government of the Church into the hands of an ordained ministry. What makes congregationalism unique is its system of checks and balances, which constrains the authority of the minister, the lay officers, and the members.
Most importantly, the boundaries of the powers of the ministers and church officers are set by clear and constant reminders of the freedoms guaranteed by the Gospel to the laity, collectively and individually. With that freedom comes the responsibility upon each member to govern himself or herself under Christ. This requires lay people to exercise great charity and patience in debating issues with one another and to seek the glory and service of God as the foremost consideration in all of their decisions.
The authority of all of the people, including the officers, is limited in the local congregation by a definition of union, or a covenant, by which the terms of their cooperation together are spelled out and agreed to. This might be something as minimal as a charter specifying a handful of doctrines and behavioral expectations, or even a statement only guaranteeing specific freedoms. Or, it may be a constitution describing a comprehensive doctrinal system and specifying terms under which the local church is connected to other local churches, to which participating congregations give their assent. In congregationalism, rather uniquely, the church is understood to be a truly voluntary association.
Finally, the congregational theory strictly forbids ministers from ruling their local churches by themselves. Not only does the minister serve by the approval of the congregation, but committees further constrain the pastor from exercising power without consent by either the particular committee, or the entire congregation. It is a contradiction of the congregational principle if a minister makes decisions concerning the congregation without the vote of these other officers.
The other officers may be called "deacons", "elders" or "session" (borrowing Presbyterian terminology), or even "vestry" (borrowing the Anglican term) — it is not their label that is important to the theory, but rather their lay status and their equal vote, together with the pastor, in deciding the issues of the church. While other forms of church government are more likely to define "tyranny" as "the imposition of unjust rule", a congregationally governed church would more likely define tyranny as "transgression of liberty" or equivalently, "rule by one man". To a congregationalist; no abuse of authority is worse than the concentration of all decisive power in the hands of one ruling body, or one person.
Following this sentiment, congregationalism has evolved over time to include even more participation of the congregation, more kinds of lay committees to whom various tasks are apportioned, and more decisions subject to the vote of the entire membership.
One of the most notable characteristics of New England (or British)-heritage Congregationalism has been its consistent leadership role in the formation of "unions" with other churches. Such sentiments especially grew strong in the late 19th and early 20th centuries, when ecumenism evolved out of a liberal, non-sectarian perspective on relations to other Christian groups that accompanied the relaxation of Calvinist stringencies held by earlier generations. The congregationalist theory of independence within a union has been a cornerstone of most ecumenical movements since the 18th century.
Baptist churches.
Most Baptists hold that no church or ecclesiastical organization has inherent authority over a Baptist church. Churches can properly relate to each other under this polity only through voluntary cooperation, never by any sort of coercion. Furthermore, this Baptist polity calls for freedom from governmental control.
Exceptions to this local form of local governance include a few churches that submit to the leadership of a body of elders, as well as the Episcopal Baptists that have an Episcopal system.
Independent Baptist churches have no formal organizational structure above the level of the local congregation. More generally among Baptists, a variety of parachurch agencies and evangelical educational institutions may be supported generously or not at all, depending entirely upon the local congregation's customs and predilections. Usually doctrinal conformity is held as a first consideration when a church makes a decision to grant or decline financial contributions to such agencies, which are legally external and separate from the congregations they serve. These practices also find currency among non-denominational fundamentalist or charismatic fellowships, many of which derive from Baptist origins, culturally if not theologically.
Most Southern Baptist and National Baptist congregations, by contrast, generally relate more closely to external groups such as mission agencies and educational institutions than do those of independent persuasion. However, they adhere to a very similar ecclesiology, refusing to permit outside control or oversight of local affairs.
Churches of Christ.
Church government is congregational rather than denominational. Churches of Christ purposefully have no central headquarters, councils, or other organizational structure above the local church level.:214:103:124 Rather, the independent congregations are a network with each congregation participating at its own discretion in various means of service and fellowship with other congregations (see Sponsoring church (Churches of Christ)).:124 Churches of Christ are linked by their shared commitment to restoration principles.:106
Congregations are generally overseen by a plurality of elders (also known in some congregations as shepherds, bishops, or pastors) who are sometimes assisted in the administration of various works by deacons.:124:47,54–55 Elders are generally seen as responsible for the spiritual welfare of the congregation, while deacons are seen as responsible for the non-spiritual needs of the church.:531 Deacons serve under the supervision of the elders, and are often assigned to direct specific ministries.:531 Successful service as a deacon is often seen as preparation for the eldership.:531 Elders and deacons are chosen by the congregation based on the qualifications found in and .:53,48–52:323,335 Congregations look for elders who have a mature enough understanding of scripture to enable them to supervise the minister and to teach, as well as to perform "governance" functions.:298 In lieu of willing men who meet these qualifications, congregations are sometimes overseen by the congregation's men in general.
While the early Restoration Movement had a tradition of itinerant preachers rather than "located Preachers", during the 20th century a long-term, formally trained congregational minister became the norm among Churches of Christ.:532 Ministers are understood to serve under the oversight of the elders.:298 While the presence of a long-term professional minister has sometimes created "significant "de facto" ministerial authority" and led to conflict between the minister and the elders, the eldership has remained the "ultimate locus of authority in the congregation".:531
Churches of Christ hold to the priesthood of all believers. No special titles are used for preachers or ministers that would identify them as "clergy".:106:112–113 Churches of Christ emphasize that there is no distinction between "clergy" and "laity" and that every member has a gift and a role to play in accomplishing the work of the church.:38–40

</doc>
<doc id="6816" url="http://en.wikipedia.org/wiki?curid=6816" title="Cavalry">
Cavalry

Cavalry (from French "cavalerie", cf. "cheval" 'horse') or horsemen were soldiers or warriors who fought mounted on horseback. Cavalry were historically the most mobile of the combat arms. An individual soldier in the cavalry is known by a number of designations such as cavalryman, horseman, dragoon, or trooper.
The designation of cavalry was not usually given to any military force that used other animals, such as camels or mules. Infantry who moved on horseback, but dismounted to fight on foot, were known in the 17th and early 18th centuries as dragoons, a class of mounted infantry which later evolved into cavalry proper while retaining their historic title.
From earliest times cavalry had the advantage of improved mobility, making it an
instrument which multiplied the fighting value of even the smallest forces, allowing them to outflank and avoid, to surprise and overpower, to retreat and escape according to the requirements of the moment.
A man fighting from horseback also had the advantages of greater height, speed, and inertial mass over an opponent on foot. Another element of horse mounted warfare is the psychological impact a mounted soldier can inflict on an opponent.
The speed, mobility and shock value of the cavalry was greatly appreciated and exploited in armed forces in the Ancient and Middle Ages; some forces were mostly cavalry, particularly in nomadic societies of Asia, notably the Mongol armies. In Europe cavalry became increasingly armoured (heavy), and eventually became known for the mounted knights. During the 17th century cavalry in Europe lost most of its armor, ineffective against the muskets and cannon which were coming into use, and by the mid-19th century armor had mainly fallen into disuse, although some regiments retained a small thickened cuirass that offered protection against lances and sabres and some protection against shot.
In the period between the World Wars, many cavalry units were converted into motorized infantry and mechanized infantry units, or reformed as tank troops. However, some cavalry still served during the World War II, notably in the Red Army, the Mongolian People's Army, the Royal Italian Army and the Polish Land Forces. Most cavalry units that are horse-mounted in modern armies serve in purely ceremonial roles, or as mounted infantry in difficult terrain such as mountains or heavily forested areas. Modern usage of the term refers to specialist units equipped with tanks ("armored cavalry") or aircraft ("air cavalry").
Role of cavalry.
In many modern armies, the term "cavalry" is still often used to refer to units that are a combat arm of the armed forces which in the past filled the traditional horse-borne land combat light cavalry roles. These include scouting, skirmishing with enemy reconnaissance elements to deny them knowledge of own disposition of troops, forward security, offensive reconnaissance by combat, defensive screening of friendly forces during retrograde movement, retreat, restoration of command and control, deception, battle handover and passage of lines, relief in place, linkup, breakout operations, and raiding. The shock role, traditionally filled by heavy cavalry, is generally filled by units with the "armored" designation.
History.
Origins.
Before the Iron Age, the role of cavalry on the battlefield was largely performed by light chariots. The chariot originated with the Sintashta-Petrovka culture in Central Asia and spread by nomadic or semi-nomadic Indo-Iranians. The chariot was quickly adopted by settled peoples both as a military technology and an object of ceremonial status, especially by the pharaohs of the New Kingdom of Egypt as well as the Assyrian army and Babylonian royalty.
The power of mobility given by mounted units was recognized early on, but was offset by the difficulty of raising large forces and by the inability of horses (then mostly small) to carry heavy armor. Cavalry techniques were an innovation of equestrian nomads of the Central Asian and Iranian steppe and pastoralist tribes such as the Persian Parthians and Sarmatians.
The photograph above right shows Assyrian cavalry from reliefs of 865–860 BC. At this time, the men had no spurs, saddles, saddle cloths, or stirrups. Fighting from the back of a horse was much more difficult than mere riding. The cavalry acted in pairs; the reins of the mounted archer were controlled by his neighbour's hand. Even at this early time, cavalry used swords, shields, and bows. The sculpture implies two types of cavalry, but this might be a simplification by the artist. Later images of Assyrian cavalry show saddle cloths as primitive saddles, allowing each archer to control his own horse.
As early as 490 BC a breed of large horses was bred in the Nisaean plain in Media to carry men with increasing amounts of armour (Herodotus 7,40 & 9,20). But large horses were still very exceptional at this time. Excepting a few ineffective attempts to revive scythed chariots, and continuing far eastern use, the use of chariots in battle was obsolete in civilized nations by the time of the Persian defeat at the hands of Alexander the Great, but chariots remained in use for ceremonial purposes such as carrying the victorious general in a Roman triumph, or for racing. The southern Britons met Julius Caesar with chariots in 55 and 54 BC, but by the time of the Roman conquest of Britain a century later chariots were mostly obsolete, even in Britannia. However, the last mention of chariot use in battle was at Mons Graupius, in 84 AD. Chariots remained in use in ancient China throughout the Warring States period.
Ancient Greece: city-states, Thebes, Thessaly and Macedonia.
During the classical Greek period cavalry were usually limited to those citizens who could afford expensive war-horses. Three types of cavalry became common: light cavalry, whose riders, armed with javelins, could harass and skirmish; heavy cavalry, whose troopers, using lances, had the ability to close with their opponents; and finally those whose equipment allowed them to fight either on horseback or foot. The role of horsemen did however remain secondary to that of the hoplites or heavy infantry who comprised the main strength of the citizen levies of the various city states.
Cavalry played a relatively minor role in ancient Greek city-states, with conflicts decided by massed armored infantry. However, Thebes produced Pelopidas, her first great cavalry commander, whose tactics and skills were absorbed by Phillip II of Macedon when Phillip was a guest-hostage in Thebes. Thessaly was widely known for producing competent cavalrymen, and later experiences in wars both with and against the Persians taught the Greeks the value of cavalry in skirmishing and pursuit. The Athenian author and soldier Xenophon in particular advocated the creation of a small but well-trained cavalry force; to that end, he wrote several manuals on horsemanship and cavalry operations.
The Macedonian Kingdom in the north, on the other hand, developed a strong cavalry force that culminated in the "hetairoi" (Companion cavalry) of Philip II of Macedon and Alexander the Great. In addition to these heavy cavalry, the Macedonian army also employed lighter horsemen called prodromoi for scouting and screening, as well as the Macedonian pike phalanx and various kinds of light infantry. There were also the "Ippiko" (or "Horserider"), Greek "heavy" cavalry, armed with kontos (or cavalry lance), and sword. These wore leather armour or mail plus a helmet. They were medium rather than heavy cavalry, meaning that they were better suited to be scouts, skirmishers, and pursuers rather than front line fighters. This combination of cavalry and infantry helped to break enemy lines and were used effectively to dominate the opponents of the kingdom.
The effectiveness of this combined-arms system was most dramatically demonstrated in Alexander's conquest of Persia, Bactria, and northwestern India.
Roman Republic and Early Empire.
The cavalry in the early Roman Republic remained the preserve of the wealthy landed class known as the "equites"—men who could afford the expense of maintaining a horse in addition to arms and armor heavier than those of the common legions. As the class grew to be more of a social elite instead of a functional property-based military grouping, the Romans began to employ Italian socii for filling the ranks of their cavalry.The weakness of Roman cavalry was demonstrated by Hannibal Barca during the second Punic war where he used his superior mounted forces to win several battles. The most notable of these was the Battle of Cannae, when he inflicted a catastrophic defeat on the Romans. At about the same time the Romans began to recruit foreign auxiliary cavalry from among Gauls, Iberians, and Numidians, the last being highly valued as mounted skirmishers and scouts (see Numidian cavalry). Julius Caesar had a high opinion of his escort of Germanic mixed cavalry, giving rise to the "Cohortes Equitatae". Early emperors maintained an ala of Batavian cavalry as their personal bodyguards until the unit was dismissed by Galba after the Batavian Rebellion.
For the most part, Roman cavalry during the Republic functioned as an adjunct to the legionary infantry and formed only one-fifth of the showing force. This does not mean that its utility should be underestimated, as its strategic role in scouting, skirmishing, and outpost duties was crucial to the Romans' capability to conduct operations over long distances in hostile or unfamiliar territory. On some occasions Roman cavalry also proved its ability to strike a decisive tactical blow against a weakened or unprepared enemy, such as the final charge at the Battle of Aquilonia.
After defeats such as the Battle of Carrhae, the Romans learned the importance of large cavalry formations from the Parthians. They would begin to substantially increase both the numbers and the training standards of the cavalry in their employ, just as nearly a thousand years earlier the first Iranians to reach the Iranian Plateau forced the Assyrians to undertake a similar reform. Nonetheless, the Romans would continue to rely mainly on their heavy infantry supported by auxiliary cavalry.
Late Roman Empire and the Migration Period.
In the army of the late Roman Empire, cavalry played an increasingly important role. The Spatha, the classical sword throughout most of the 1st millennium was adopted as the standard model for the Empire's cavalry forces.
The most widespread employment of heavy cavalry at this time was found in the forces of the Parthians and their Iranian Sassanid successors. Both, but especially the former, were famed for the cataphract (fully armored cavalry armed with lances) even though the majority of their forces consisted of lighter horse archers. The West first encountered this eastern heavy cavalry during the Hellenistic period with further intensive contacts during the eight centuries of the Roman–Persian wars. At first the Parthians' mobility greatly confounded the Romans, whose armoured close-order infantry proved unable to match the speed of the Parthians. However, later the Romans would successfully adapt such heavy armor and cavalry tactics by creating their own units of cataphracts and "clibanarii".
The decline of the Roman infrastructure made it more difficult to field large infantry forces, and during the 4th and 5th centuries cavalry began to take a more dominant role on the European battlefield, also in part made possible by the appearance of new, larger breeds of horses. The replacement of the Roman saddle by variants on the Scythian model, with pommel and cantle, was also a significant factor as was the adoption of stirrups and the concomitant increase in stability of the rider's seat. Armored Cataphracts began to be deployed in eastern Europe and the near East, following the precedents established by Persian forces, as the main striking force of the armies in contrast to the earlier roles of cavalry as scouts, raiders, and outflankers.
The late Roman cavalry tradition and the mounted nobility of the Germanic invaders both contributed to the development of mediaeval knightly cavalry.
Arabs.
The Islamic Prophet Muhammad made use of cavalry in many of his military campaigns including the Expedition of Dhu Qarad, and the expedition of Zaid ibn Haritha in al-Is which took place in September, 627 AD, 5th month of 6 AH of the Islamic calendar.
Early organized Arab mounted forces under the Rashidun caliphate comprised a light cavalry armed with lance and sword. Its main role was to attack the enemy flanks and rear. These relatively lightly armored horsemen formed the most effective element of the Muslim armies during the later stages of the Islamic conquest of the Levant. The best use of this lightly armed fast moving cavalry was revealed at the Battle of Yarmouk (636 AD) in which Khalid ibn Walid, knowing the skills of his horsemen, used them to turn the tables at every critical instance of the battle with their ability to engage, disengage, then turn back and attack again from the flank or rear. A strong cavalry regiment was formed by Khalid ibn Walid which included the veterans of the campaign of Iraq and Syria. Early Muslim historians have given it the name "Mutaharrik tulai'a"( متحرك طليعة ), or the Mobile guard. This was used as an advance guard and a strong striking force to route the opposing armies with its greater mobility that give it an upper hand when maneuvering against any Byzantine army. With this mobile striking force, the conquest of Syria was made easy. 
The Battle of Talas in 751 CE was a conflict between the Arab Abbasid Caliphate and the Chinese Tang Dynasty over the control of Central Asia. Chinese infantry were routed by Arab cavalry near the bank of the River Talas.
Later Mamluks were trained as cavalry soldiers. Mamluks were to follow the dictates of al-furusiyya, a code of conduct that included values like courage and generosity but also doctrine of cavalry tactics, horsemanship, archery and treatment of wounds.
Asia.
Central Asia.
Xiongnu, Tujue, Avars, Kipchaks, Mongols, Don Cossacks and the various Turkic peoples are also examples of the horse-mounted groups that managed to gain substantial successes in military conflicts with settled agrarian and urban societies, due to their strategic and tactical mobility. As European states began to assume the character of bureaucratic nation-states supporting professional standing armies, recruitment of these mounted warriors was undertaken in order to fill the strategic roles of scouts and raiders. The best known instance of the continued employment of mounted tribal auxiliaries were the Cossack cavalry regiments of Tsarist Russia. In eastern Europe, Russia, and out onto the steppes, cavalry remained important much longer and dominated the scene of warfare until the early 17th century and even beyond, as the strategic mobility of cavalry was crucial for the semi-nomadic pastoralist lives that many steppe cultures led. Tibetans also had a tradition of cavalry warfare, in several military engagements with the Chinese Tang dynasty (618–907 AD).
East Asia.
Further east, the military history of China, specifically northern China, held a long tradition of intense military exchange between Han Chinese infantry forces of the settled dynastic empires and the mounted nomads or "barbarians" of the north. The naval history of China was centered more to the south, where mountains, rivers, and large lakes necessitated the employment of a large and well-kept navy.
In 307 BC, King Wuling of Zhao, the ancient Chinese ruler of the former State of Jin territory, ordered his military commanders and troops to adopt the trousers of the nomads as well as practice the nomads' form of mounted archery to hone their new cavalry skills. Soon afterwards the cavalry tactics employed by the State of Zhao forced their enemies in the other Warring States to adopt the same techniques in order to mount any effective attack against their swift movements on the battlefield.
The adoption of massed cavalry in China also broke the tradition of the chariot-riding Chinese aristocracy in battle, which had been in use since the ancient Shang Dynasty (c. 1600 BC-1050 BC). By this time large Chinese infantry-based armies of 100,000 to 200,000 troops were now buttressed with several hundred thousand mounted cavalry in support or as an effective striking force. The handheld pistol-and-trigger crossbow was invented in China in the 4th century BC; it was written by the Song Dynasty scholars Zeng Gongliang, Ding Du, and Yang Weide in their book "Wujing Zongyao" (1044 AD) that massed missile fire by crossbowmen was the most effective defense against enemy cavalry charges.
On many occasions the Chinese studied nomadic cavalry tactics and applied the lessons in creating their own potent cavalry forces, while in others they simply recruited the tribal horsemen wholesale into their armies; and in yet other cases nomadic empires proved eager to enlist Chinese infantry and engineering, as in the case of the Mongol Empire and its sinicized part, the Yuan Dynasty (1279–1368). The Chinese recognized early on during the Han Dynasty (202 BC-220 AD) that they were at a disadvantage in lacking the number of horses the northern nomadic peoples mustered in their armies. Emperor Wu of Han (r. 141 BC-87 BC) went to war with the Dayuan for this reason, since the Dayuan were hording a massive amount of tall, strong, Central Asian bred horses in the Hellenized–Greek region of Fergana (established slightly earlier by Alexander the Great). Although experiencing some defeats early on in the campaign, Emperor Wu's war from 104 BC to 102 BC succeeded in gathering the prized tribute of horses from Fergana.
Cavalry tactics in China were enhanced by the invention of the saddle-attached stirrup by at least the 4th century, as the oldest reliable depiction of a rider with paired stirrups was found in a Jin Dynasty tomb of the year 322 AD. The Chinese invention of the horse collar by the 5th century was also a great improvement from the breast harness, allowing the horse to haul greater weight without heavy burden on its skeletal structure.
The horse warfare of Korea was first started during the ancient Korean kingdom Gojoseon. Since at least the 3rd century BC, there was influence of northern nomadic peoples and Yemaek peoples on Korean Warfare. By roughly the 1st century BC, the ancient kingdom of Buyeo also had mounted warriors. The cavalry of Goguryeo, one of the Three Kingdoms of Korea, were called "Gaemamusa" (개마무사, 鎧馬武士), and were renowned as a fearsome heavy cavalry force. King Gwanggaeto the Great often led expeditions into the Baekje, Gaya confederacy, Buyeo, Later Yan and against Japanese invaders with his cavalry.
In the 12th century, Jurchen tribes began to violate the Goryeo-Jurchen borders, and eventually invaded Goryeo Korea. After experiencing the invasion by the Jurchen, Korean general Yun Gwan realized that Goryeo lacked efficient cavalry units. He reorganized the Goryeo military into a professional army that would contain decent and well-trained cavalry units. In 1107, the Jurchen were ultimately defeated, and surrendered to Yun Gwan. To mark the victory, General Yun built nine fortresses to the northeast of the Goryeo-Jurchen borders (동북 9성, 東北 九城).
The ancient Japanese of the Kofun period also adopted cavalry and equine culture by the 5th century AD. The emergence of the samurai aristocracy led to the development of armoured horse archers, themselves to develop into charging lancer cavalry as gunpowder weapons rendered bows obsolete.
South Asia.
In the Indian subcontinent, cavalry played a major role from the Gupta Dynasty (320-600) period onwards. India has also the oldest evidence for the introduction of toe-stirrups. 
Indian literature contains numerous references to the cavalry forces of the Central Asian horse nomads like the Sakas, Kambojas, Yavanas, Pahlavas and Paradas. Numerous Puranic texts refer to a conflict in ancient India (16th century BC) in which the cavalry forces of five nations, called five hordes ("pañca.ganan") or Kśatriya hordes ("Kśatriya ganah"), attacked and captured the throne of Ayudhya by dethroning its Vedic King Bahu
The Mahabharata, Ramayana, numerous Puranas and some foreign sources numerously attest that Kamboja cavalry was frequently requisitioned in ancient wars. V. R. Ramachandra Dikshitar writes: "Both the Puranas and the epics agree that the horses of the Sindhu and Kamboja regions were of the finest breed, and that the services of the Kambojas as cavalry troopers were requisitioned in ancient wars". J.A.O.S. writes: "Most famous horses are said to come either from Sindhu or Kamboja; of the latter (i.e. the Kamboja), the Indian epic Mahabharata speaks among the finest horsemen".
Mahabharata (950 c BC) speaks of the esteemed cavalry of the Kambojas, Sakas, Yavanas and Tusharas, all of whom had participated in the Kurukshetra war under the supreme command of Kamboja ruler Sudakshin Kamboj.
Mahabharata and Vishnudharmotari Purana especially styles the Kambojas, Yavansa, Gandharas etc. as "Ashva.yuddha.kushalah" (expert cavalrymen). In the Mahabharata war, the Kamboja cavalry along with that of the Sakas, Yavanas is reported to have been enlisted by the Kuru king Duryodhana of Hastinapura.
Herodotus (484 c BC–425 c BC) attests that the Gandarian mercenaries (i.e. "Gandharans/Kambojans" of Gandari Strapy of Achaemenids) from the 20th strapy of the Achaemenids were recruited in the army of emperor Xerxes I (486-465 BC), which he led against the Hellas. Similarly, the "men of the Mountain Land " from north of Kabol-River equivalent to medieval Kohistan (Pakistan), figure in the army of Darius III against Alexander at Arbela with a cavalry and fifteen elephants. This obviously refers to Kamboja cavalry south of Hindukush.
The Kambojas were famous for their horses, as well as cavalry-men ("asva-yuddha-Kushalah"). On account of their supreme position in horse (Ashva) culture, they were also popularly known as Ashvakas, i.e. the "horsemen" and their land was known as "Home of Horses". They are the Assakenoi and Aspasioi of the Classical writings, and the Ashvakayanas and Ashvayanas in Pāṇini's Ashtadhyayi. The Assakenoi had faced Alexander with 30,000 infantry, 20,000 cavalry and 30 war elephants. Scholars have identified the Assakenoi and Aspasioi clans of Kunar and Swat valleys as a section of the Kambojas. These hardy tribes had offered stubborn resistance to Alexander (326 c BC) during latter's campaign of the Kabul, Kunar and Swat valleys and had even extracted the praise of the Alexander's historians. These highlanders, designated as "parvatiya Ayudhajivinah" in Pāṇini's Astadhyayi, were rebellious, fiercely independent and freedom-loving cavalrymen who never easily yielded to any overlord.
The Sanskrit drama "Mudra-rakashas" by "Visakha Dutta" and the Jaina work "Parisishtaparvan" refer to Chandragupta's (320 C BC–298 c BC) alliance with Himalayan king "Parvataka". The Himalayan alliance gave Chandragupta a formidable composite army made up of the cavalry forces of the Shakas, Yavanas, Kambojas, Kiratas, Parasikas and Bahlikas as attested by Mudra-Rakashas (Mudra-Rakshasa 2). These hordes had helped Chandragupta Maurya defeat the ruler of Magadha and placed Vhandragupta on the throne, thus laying the foundations of Mauryan Dynasty in Northern India.
The cavalry of Hunas and the Kambojas is also attested in the Raghu Vamsa epic poem of Sanskrit poet Kalidasa. Raghu of Kalidasa is believed to be Chandragupta II ("Vikaramaditya") (375–413/15 AD), of the well-known Gupta Dynasty.
As late as mediaeval era, the Kamboja cavalry had also formed part of the Gurjara-Pratihara armed forces from the 8th to the 10th centuries AD. They had come to Bengal with the Pratiharas when the latter conquered part of the province.
Ancient Kambojas were constituted into military "Sanghas" and Srenis (Corporations) to manage their political and military affairs, as Arthashastra of Kautiliya as well as the Mahabharata amply attest for us. They are attested to be living as "Ayuddha-jivi" or "Shastr-opajivis" (Nation-in-arms), which also means that the Kamboja cavalry offered its military services to other nations as well. There are numerous references to Kambojas having been requisitioned as cavalry troopers in ancient wars by outside nations.
European Middle Ages.
Although Roman cavalry had no stirrups, their horned saddle allowed the combination of a firm seat with substantial flexibility. But the introduction of the wraparound saddle during the Middle Ages provided greater efficiency in mounted shock combat and the important invention of the stirrup enabled a broader array of attacks to be delivered from the back of a horse. As a greater weight of man and armor could be supported in the saddle, the probability of being dismounted in combat was significantly reduced.
In particular, a charge with the lance couched under the armpit would no longer turn into pole vaulting; this eventually led to an enormous increase in the impact of the charge. Last but not least, the introduction of spurs allowed better control of the mount during the "knightly charge" in full gallop. In western Europe there emerged what is considered the "ultimate" heavy cavalry, the knight. The knights and other similarly equipped mounted men-at-arms charged in close formation, exchanging flexibility for a massive, irresistible first charge.
The mounted men-at-arms quickly became an important force in Western European tactics. Medieval military doctrine employed them as part of a combined-arms force along with various kinds of foot troops; however medieval chroniclers tended to pay undue attention to the knights at the expense of the rank and file, which led early students of military history to suppose that this heavy cavalry was the only force that mattered on medieval European battlefields, which was not the case.
Massed English longbowmen triumphed over French cavalry at Crécy, Poitiers and Agincourt, while at Gisors (1188), Bannockburn (1314), and Laupen (1339), foot-soldiers proved their invulnerability to cavalry charges as long as they held their formation. Once the Swiss developed their pike squares for offensive as well as defensive use, infantry started to become the principal arm. This aggressive new doctrine gave the Swiss victory over a range of adversaries, and their enemies found that the only reliable way to defeat them was by the use of an even more comprehensive combined arms doctrine, as evidenced in the Battle of Marignano. The introduction of missile weapons that required less skill than the longbow, such as the crossbow and hand cannon, also helped remove the focus somewhat from cavalry elites to masses of cheap infantry equipped with easy-to-learn weapons. These missile weapons were very successfully used in the Hussite Wars, in combination with Wagenburg tactics.
This gradual rise in the dominance of infantry led to the adoption of dismounted tactics. From the earliest times knights and mounted men-at-arms had frequently dismounted to handle enemies they could not overcome on horseback, such as in the Battle of the Dyle (891) and the Battle of Bremule (1119), but after the 1350s this trend became more marked with the dismounted men-at-arms fighting as super-heavy infantry with two-handed swords and poleaxes. In any case, warfare in the Middle Ages tended to be dominated by raids and sieges rather than pitched battles, and mounted men-at-arms rarely had any choice other than dismounting when faced with the prospect of assaulting a fortified position.
Renaissance Europe.
Ironically, the rise of infantry in the early 16th century coincided with the "golden age" of heavy cavalry; a French or Spanish army at the beginning of the century could have up to half its numbers made up of various kinds of light and heavy cavalry, whereas in earlier medieval and later 17th-century armies the proportion of cavalry was seldom more than a quarter.
Knighthood largely lost its military functions and became more closely tied to social and economic prestige in an increasingly capitalistic Western society. With the rise of drilled and trained infantry, the mounted men-at-arms, now sometimes called "gendarmes" and often part of the standing army themselves, adopted the same role as in the Hellenistic age, that of delivering a decisive blow once the battle was already engaged, either by charging the enemy in the flank or attacking their commander-in-chief.
From the 1550s onwards, the use of gunpowder weapons solidified infantry's dominance of the battlefield and began to allow true mass armies to develop. This is closely related to the increase in the size of armies throughout the early modern period; heavily armored cavalrymen were expensive to raise and maintain and it took years to replace a skilled horseman or a trained horse, while arquebusiers and later musketeers could be trained and kept in the field at much lower cost, and were much easier to replace.
The Spanish tercio and later formations relegated cavalry to a supporting role. The pistol was specifically developed to try to bring cavalry back into the conflict, together with manoeuvres such as the caracole. The caracole was not particularly successful, however, and the charge (whether with sword, pistol, or lance) remained as the primary mode of employment for many types of European cavalry, although by this time it was delivered in much deeper formations and with greater discipline than before. The demi-lancers and the heavily armored sword-and-pistol reiters were among the types of cavalry whose heyday was in the 16th and 17th centuries, as for the Polish winged hussars, a heavy cavalry force that achieved great success against Swedes, Russians, and Turks.
18th-century Europe and Napoleonic Wars.
Cavalry retained an important role in this age of regularization and standardization across European armies. First and foremost they remained the primary choice for confronting enemy cavalry. Attacking an unbroken infantry force head-on usually resulted in failure, but extended linear infantry formations were vulnerable to flank or rear attacks. Cavalry was important at Blenheim (1704), Rossbach (1757), Eylau and Friedland (1807), remaining significant throughout the Napoleonic Wars.
Even with the increasing prominence of infrantry, cavalry still had an important place in armies. They usually patrolled the fringes of army encampments, with standing orders to intercept and kill suspected shirkers and deserters on sight. During battle, they chased artillery gunners away from their cannons and plugged the touchholes with iron spikes. They also hunted down infrantrymen who broke formation, as well as snipers.
The greatest cavalry charge of modern history was at the 1807 battle of Eylau, when the entire 11,000-strong French cavalry reserve, led by Maréchal Murat, launched a huge charge on and through the Russian infantry lines. However, in 1815 at the Battle of Waterloo, repeated charges by up to 9,000 French cavalrymen failed to break the line of the British and German infantry, who had formed squares.
Massed infantry was deadly to cavalry, but offered an excellent target for artillery. Once the bombardment had disordered the infantry formation, cavalry were able to rout and pursue the scattered foot soldiers. It was not until individual firearms gained accuracy and improved rates of fire that cavalry was diminished in this role as well. Even then light cavalry remained an indispensable tool for scouting, screening the army's movements, and harassing the enemy's supply lines until military aircraft supplanted them in this role in the early stages of World War I.
19th century.
By the 19th century, European cavalry fell into four main categories:
There were cavalry variations for individual nations as well: France had the "chasseurs à cheval"; Germany had the "Jäger zu Pferd"; Bavaria had the "Chevaulegers"; and Russia had Cossacks. Britain, from the mid-18th century, had Light Dragoons as light cavalry and Dragoons, Dragoon Guards and Household Cavalry as heavy cavalry. Only after the end of the Napoleonic wars were the Household Cavalry equipped with cuirasses, and some other regiments were converted to lancers. In the United States Army the cavalry were almost always dragoons. The Imperial Japanese Army had its cavalry uniformed as hussars, but they fought as dragoons.
In the Crimean War, the Charge of the Light Brigade and the Thin Red Line at the Battle of Balaclava showed the vulnerability of cavalry, when deployed without effective support.
In the early American Civil War the regular United States Army mounted rifle, dragoon, and two existing cavalry regiments were reorganized and renamed cavalry regiments, of which there were six. Over a hundred other federal and state cavalry regiments were organized, but the infantry played a much larger role in many battles due to its larger numbers, lower cost per rifle fielded, and much easier recruitment. However, cavalry saw a role as part of screening forces and in foraging and scouting. The later phases of the war saw the Federal army developing a truly effective cavalry force fighting as scouts, raiders, and, with repeating rifles, as mounted infantry.
Post Civil War, as the volunteer armies disbanded, the regular army cavalry regiments increased in number from six to ten, among them the U.S. 7th Cavalry Regiment of Little Bighorn fame, and the African-American U.S. 9th Cavalry Regiment and U.S. 10th Cavalry Regiment. These units, along with others (both cavalry and infantry), collectively became known as the Buffalo Soldiers.
These regiments, which rarely took the field as complete organizations, served throughout the American Indian Wars through the close of the frontier in the 1890s. Volunteer cavalry regiments like the Rough Riders (which consisted of cowboys, ranchers and other outdoorsmen), were a cheap and popular unit in the United States Military at that time.
During the Franco-Prussian War, at the Battle of Mars-la-Tour in 1870, a Prussian cavalry brigade decisively smashed the centre of the French battle line, after skilfully concealing their approach. This event became known as Von Bredow's Death Ride after the brigade commander Adalbert von Bredow; it would be used in the following decades to argue that massed cavalry charges still had a place on the modern battlefield.
19th-century imperial expansion.
Cavalry found a new role in colonial campaigns (irregular warfare), where modern weapons were lacking and the slow moving infantry-artillery train or fixed fortifications were often ineffective against indigenous insurgents (unless the latter offered a fight on an equal footing, as at Tel-el-Kebir, Omdurman, etc.). Cavalry "flying columns" proved effective, or at least cost-effective, in many campaigns—although an astute native commander (like Samori in western Africa, Shamil in the Caucasus, or any of the better Boer commanders) could turn the tables and use the greater mobility of their cavalry to offset their relative lack of firepower compared with European forces.
The British Indian Army maintained about forty regiments of cavalry, officered by British and manned by Indian sowars (cavalrymen). Among the more famous regiments in the lineages of the modern Indian and Pakistani armies are:
Several of these formations are still active, though they now are armoured formations, for example the Guides Cavalry in Pakistan.
The French Army maintained substantial cavalry forces in Algeria and Morocco from 1830 until the end of the Second World War. Much of the Mediterranean coastal terrain was suitable for mounted action and there was a long established culture of horsemanship amongst the Arab and Berber inhabitants. The French forces included Spahis, Chasseurs d' Afrique, Foreign Legion cavalry and mounted Goumiers.Both Spain and Italy raised cavalry regiments from amongst the indigenous horsemen of their respective North African territories.
Imperial Germany employed mounted formations in South West Africa as part of the Schutztruppen (colonial army) garrisoning the territory.
First World War.
Pre-war developments.
At the beginning of the 20th century all armies still maintained substantial cavalry forces, although there was contention over whether their role should revert to that of mounted infantry (the historic dragoon function). Following the experience of the South African War of 1899-1902 (where mounted Boer citizen commandos fighting on foot from cover proved superior to regular cavalry) the British Army withdrew lances for all but ceremonial purposes and placed a new emphasis on training for dismounted action.
In 1908 however the six British lancer regiments in existence resumed use of this impressive but obsolete weapon for active service. In 1882 the Imperial Russian Army converted all its line hussar and lancer regiments to dragoons, with an emphasis on mounted infantry training. In 1910 these regiments reverted to their historic roles, designations and uniforms.
By 1909 official regulations dictating the role of the Imperial German cavalry had been revised to indicate an increasing realization of the realities of modern warfare. The massive cavalry charge in three waves which had previously marked the end of annual maneuvers was discontinued and a new emphasis was placed in training on scouting, raiding and pursuit; rather than main battle involvement.
In spite of significant experience in mounted warfare in Morocco during 1908-14, the French cavalry remained a highly conservative institution. The traditional tactical distinctions between heavy, medium and light cavalry branches were retained. French cuirassiers wore breastplates and plumed helmets unchanged from the Napoleonic period, during the early months of World War I. Dragoons were similarly equipped, though they did not wear cuirasses and did carry lances. Light cavalry were described as being "a blaze of colour". French cavalry of all branches were well mounted and were trained to change position and charge at full gallop.
Cavalry during opening stages.
In August 1914 all combatant armies still retained substantial numbers of cavalry and the mobile nature of the opening battles on both Eastern and Western Fronts provided a number of instances of traditional cavalry actions, though on a smaller and more scattered scale than those of previous wars. The Imperial German cavalry, while as colourful and traditional as any in peacetime appearance, had adopted a practice of falling back on infantry support when any substantial opposition was encountered. These cautious tactics aroused derision amongst their more conservative French and Russian opponents but proved appropriate to the new nature of warfare. A single attempt by the German army, on 12 August 1914, to use six regiments of massed cavalry to cut off the Belgian field army from Antwerp foundered when they were driven back in disorder by rifle fire. Once the front lines stabilised on the Western Front, a combination of barbed wire, machine guns and rapid fire rifles proved deadly to horse mounted troops.
On the Eastern Front a more fluid form of warfare arose from flat open terrain favorable to mounted warfare. On the outbreak of war in 1914 the bulk of the Russian cavalry was deployed at full strength in frontier garrisons and during the period that the main armies were mobilizing scouting and raiding into East Prussia and Austrian Galacia was undertaken by mounted troops trained to fight with sabre and lance in the traditional style. On 21 August 1914 the 4th Austro-Hungarian "Kavalleriedivison" fought a major mounted engagement at Jaroslavic with the Russian 10th Cavalry Division, in what was arguably the final historic battle to involve thousands of horsemen on both sides. While this was the last massed cavalry encounter on the Eastern Front, the absence of good roads limited the use of mechanized transport and even the technologically advanced Imperial German Army continued to deploy up to twenty-four horse-mounted divisions in the East, as late as 1917.
Cavalry in Europe 1915–18.
For the remainder of the War on the Western Front cavalry had virtually no role to play. The British and French armies dismounted many of their cavalry regiments and used them in infantry and other roles: the Life Guards for example spent the last months of the War as a machine gun corps; and the Australian Light Horse served as light infantry during the Gallipoli campaign. In September 1914 cavalry comprised 9.28% of the total manpower of the British Expeditionary Force in France—by July 1918 this proportion had fallen to 1.65%. The French cavalry numbered 102,000 in May 1915 but had been reduced to 63,000 by October 1918. 
The German Army dismounted nearly all their cavalry in the West, maintaining only one mounted division on that front by January 1917. At the same date however the Central Powers still had twenty-four divisions of horse cavalry active or in reserve along the Russian Front.
Some cavalry were retained as mounted troops behind the lines in anticipation of a penetration of the opposing trenches that it seemed would never come. Tanks, introduced on the Western Front by the British in September 1916, had the capacity to achieve such breakthroughs but did not have the reliable range to exploit them. In their first major use at the Battle of Cambrai (1917), the plan was for a cavalry division to follow behind the tanks, however they were not able to cross a canal because a tank had broken the only bridge. It was not until the German Army had been forced to retreat in the Hundred Days Offensive of 1918, that cavalry were again able to operate in their intended role. There was a successful charge by the British 7th Dragoon Guards on the last day of the war.
In the wider spaces of the Eastern Front a more fluid form of warfare continued and there was still a use for mounted troops. Some wide-ranging actions were fought, again mostly in the early months of the war. However, even here the value of cavalry was overrated and the maintenance of large mounted formations at the front by the Russian Army put a major strain on the railway system, to little strategic advantage. In February 1917 the Russian regular cavalry (exclusive of Cossacks) was reduced by nearly a third from its peak number of 200,000, as two squadrons of each regiment were dismounted and incorporated into additional infantry battalions.
Cavalry and mounted infantry in the Middle East.
In the Middle East, during the Sinai and Palestine Campaign mounted forces (British, Indian, Ottoman, Australian, Arab and New Zealand) retained an important strategic role both as mounted infantry and cavalry.
In Egypt the mounted infantry formations like the New Zealand Mounted Rifles Brigade and Australian Light Horse of ANZAC Mounted Division operating as mounted infantry, drove German and Ottoman forces back from Romani to Magdhaba and Rafa and out of the Egyptian Sinai Peninsular in 1916.
After a stalemate on the Gaza—Beersheba line between March and October 1917, Beersheba was captured by the Australian Mounted Division's 4th Light Horse Brigade. Their mounted charge succeeded after a coordinated attack by the British Infantry and Yeomanry cavalry and the Australian and New Zealand Light Horse and Mounted Rifles brigades. A series of coordinated attacks by these Egyptian Expeditionary Force infantry and mounted troops were also successful at the Battle of Mughar Ridge, during which the British infantry divisions and the Desert Mounted Corps drove two Ottoman armies back to the Jaffa—Jerusalem line. The infantry with mainly dismounted cavalry and mounted infantry fought in the Judean Hills to eventually almost encircle Jerusalem which was occupied shortly after.
During a pause in operations necessitated by the Spring Offensive in 1918 on the Western Front joint infantry and mounted infantry attacks towards Amman and Es Salt resulted in retreats back to the Jordan Valley which continued to be occupied by mounted divisions during the summer of 1918.
The Australian Mounted Division was armed with swords and in September, after the successful breaching of the Ottoman line on the Mediterranean coast by the British Empire infantry XXI Corps was followed by cavalry attacks by the 4th Cavalry Division, 5th Cavalry Division and Australian Mounted Divisions which almost encircled two Ottoman armies in the Judean Hills forcing their retreat. Meanwhile Chaytor's Force of infantry and mounted infantry in ANZAC Mounted Division held the Jordan Valley, covering the right flank to later advance eastwards to capture Es Salt and Amman and half of a third Ottoman army. A subsequent pursuit by the 4th Cavalry Division and the Australian Mounted Division followed by the 5th Cavalry Division to Damascus. Armoured cars and 5th Cavalry Division lancers were continuing the pursuit of Ottoman units north of Aleppo when the Armistice of Mudros was signed by the Ottoman Empire.
Post–World War I.
A combination of military conservatism in almost all armies and post-war financial constraints prevented the lessons of 1914-18 being acted on immediately. There was a general reduction in the number of cavalry regiments in the British, French, Italian and other Western armies but it was still argued with conviction (for example in the 1922 edition of the "Encyclopædia Britannica") that mounted troops had a major role to play in future warfare. The 1920s saw an interim period during which cavalry remained as a proud and conspicuous element of all major armies, though much less so than prior to 1914.
Cavalry was extensively used in the Russian Civil War and the Soviet-Polish War. The last major cavalry battle was the Battle of Komarów in 1920, between Poland and the Russian Bolsheviks. Colonial warfare in Morocco, Syria, the Middle East and the North West Frontier of India provided some opportunities for mounted action against enemies lacking advanced weaponry.
The post-war German Army (Reichsheer) was permitted a large proportion of cavalry (18 regiments or 16.4% of total manpower) under the conditions of the Treaty of Versailles. The U.S. Cavalry abandoned its sabres in 1934 and commenced the conversion of its horsed regiments to mechanized cavalry, starting with the First Regiment of Cavalry in January 1933.
In the British Army, all cavalry regiments were mechanised between 1929 and 1941, redefining their role from horse to armoured vehicles to form the Royal Armoured Corps together with the Royal Tank Regiment.
The thirty-nine regiments of the British Indian Army were reduced to twenty-one as the result of a series of amalgamations immediately following World War I. The new establishment remained unchanged until 1936 when three regiments were redesignated as permanent training units, each with six, still mounted, regiments linked to them. In 1938 the process of mechanism began with the conversion of a full cavalry brigade (two Indian regiments and one British) to armoured car and tank units. By the end of 1940 all of the Indian cavalry had been mechanised, receiving light tanks, armoured cars or 15cwt trucks. The last horsed regiment of the Indian Army (other than the Viceregal Bodyguard and some Indian States Forces regiments) was the 19th King George's Own Lancers which had its last mounted parade at Rawalpindi on 28 October 1939. This unit still exists (though in the Pakistan Army) with an armour TOE.
During the 1930s the French Army experimented with integrating mounted and mechanised cavalry units into larger formations. Dragoon regiments were converted to motorised infantry (trucks and motor cycles), and cuirassiers to armoured units; while light cavalry (Chasseurs a' Cheval, Hussars and Spahis) remained as mounted sabre squadrons. The theory was that mixed forces comprising these diverse units could utilise the strengths of each according to circumstances. In practice mounted troops proved unable to keep up with fast moving mechanised units over any distance.
World War II cavalry.
While most armies still maintained cavalry units at the outbreak of World War II in 1939, significant mounted action was largely restricted to the Polish, Balkan and Soviet campaigns.
Polish.
A popular myth is that Polish cavalry armed with lances charged German tanks during the September 1939 campaign. This arose from misreporting of a single clash on 1 September near Krojanty, when two squadrons of the Polish 18th Lancers armed with sabres scattered German infantry before being caught in the open by German armoured cars.
Two examples illustrate how the myth developed. First, because motorised vehicles were in short supply, the Poles used horses to pull anti-tank weapons into position. Second, there were a few incidents when Polish cavalry was trapped by German tanks, and attempted to fight free. However, this did not mean that the Polish army chose to attack tanks with horse cavalry. Later, on the Eastern Front, the Red Army did deploy cavalry units effectively against the Germans.
A more correct term would be "mounted infantry" instead of "cavalry", as horses were primarily used as a means of transportation, for which they were very suitable in view of the very poor road conditions in pre-war Poland. Another myth describes Polish cavalry as being armed with both sabres and lances; lances were used for peacetime ceremonial purposes only and the primary weapon of the Polish cavalryman in 1939 was a rifle. Individual equipment did include a sabre, probably because of well-established tradition, but in the case of a melee combat this secondary weapon would probably be more effective than a rifle and bayonet. Moreover, the Polish cavalry brigade order of battle in 1939 included, apart from the mounted soldiers themselves, light and heavy machine guns (wheeled), Anti-tank rifle, model 35, anti-aircraft weapon, artillery like Bofors 37 mm anti tank gun or light and scout tanks, etc. The last cavalry vs. cavalry mutual charge in Europe took place in Poland during the battle of Krasnobrod, when Polish and German cavalry units clashed with each other.
The last classical cavalry charge of the war was that made on March 1, 1945 during the Battle of Schoenfeld by the 1st "Warsaw" Independent Cavalry Brigade. Infantry and tanks had been employed to little effect against the German position, both of which floundered in the open wetlands only to be dominated by infantry and antitank fire from the German fortifications on the forward slope of Hill 157, overlooking the wetlands. The Germans had not taken cavalry into consideration when fortifying their position which, combined with the "Warsaw"s swift assault, overran the German anti-tank guns and consolidated into an attack into the village itself, now supported by infantry and tanks.
Greek.
The Italian invasion of Greece in October 1940 saw mounted cavalry used effectively by the Greek defenders along the mountainous frontier with Albania. Three Greek cavalry regiments (two mounted and one partially mechanized) played an important role in the Italian defeat in this difficult terrain.
Soviet.
By the final stages of the war only the Soviet Union was still fielding mounted units in substantial numbers, some in combined mechanized and horse units. The advantage of this approach was that in exploitation mounted infantry could keep pace with advancing tanks. Other factors favouring the retention of mounted forces included the high quality of Russian Cossacks and other horse cavalry; and the relative lack of roads suitable for wheeled vehicles in many parts of the Eastern Front. Another consideration was that the logistic capacity required to support very large motorised forces exceeded that necessary for mounted troops.
Italian.
The last mounted saber charge by Italian cavalry occurred on August 24, 1942 at Isbuscenski (Russia), when a squadron of the "Savoia Cavalry Regiment" charged the "812th Siberian Infantry Regiment". The remainder of the regiment, together with the Novara Lancers made a dismounted attack in an action that ended with the retreat of the Russians after heavy losses on both sides. The final Italian cavalry action occurred on October 17, 1942 in Pola (Croatia) by a squadron of "Alexandria Cavalry Regiment" against a large group of Yugoslav partisans.
Other Axis.
Romanian, Hungarian and Italian cavalry were dispersed or disbanded following the retreat of the Axis forces from Russia. Germany still maintained some mounted (mixed with bicycles) SS and Cossack units until the last days of the War.
Finnish.
Finland used mounted troops against Russian forces effectively in forested terrain during winter warfare. The last Finnish cavalry unit was not disbanded until 1947.
US.
The U.S. Army's last horse cavalry actions were fought during World War II: a) by the 26th Cavalry Regiment—a small mounted regiment of Philippine Scouts which fought the Japanese during the retreat down the Bataan peninsula, until it was effectively destroyed by January 1942; and b) on captured German horses by the mounted reconnaissance section of the U.S. 10th Mountain Division in a spearhead pursuit of the German Army across the Po Valley in Italy in April 1945. The last horsed U.S. Cavalry (the Second Cavalry Division) were dismounted in March 1944.
British Empire.
All British Army cavalry regiments had been mechanised since 1 March 1942 when the Queen's Own Yorkshire Dragoons (Yeomanry) was converted to a motorised role, following mounted service against the Vichy French in Syria the previous year. The final cavalry charge by British Empire forces occurred on 21 March 1942 when a 60 strong patrol of the Burma Frontier Force encountered Japanese infantry near Toungoo airfield in central Burma. The Sikh sowars of the Frontier Force cavalry, led by Captain Arthur Sandeman of The Central India Horse (21st King George V's Own Horse), charged in the old style with sabres and most were killed.
Mongolia.
In the early stages of World War II, mounted units of the Mongolian People's Army were involved in the Battle of Khalkhin Gol against invading Japanese forces. Soviet forces under the command of Georgy Zhukov, together with Mongolian forces, defeated the Japanese Sixth army and effectively ended the Soviet–Japanese Border Wars. After the Soviet–Japanese Neutrality Pact of 1941, Mongolia remained neutral throughout most of the war, but its geographical situation meant that the country served as a buffer between Japanese forces and the Soviet Union. In addition to keeping around 10% of the population under arms, Mongolia provided half a million trained horses for use by the Soviet Army. In 1945 a partially mounted Soviet-Mongolian Cavalry Mechanized Group played a supporting role on the western flank of the Soviet invasion of Manchuria. The last active service seen by cavalry units of the Mongolian Army occurred in 1946–1948, during border clashes between Mongolia and the Republic of China.
Post–World War II to present day.
While most modern "cavalry" units have some historic connection with formerly mounted troops this is not always the case. The modern Irish Defence Force (IDF) includes a "Cavalry Corps" equipped with armoured cars and Scorpion tracked combat reconnaissance vehicles. The IDF has never included horse cavalry since its establishment in 1922 (other than a small mounted escort of Blue Hussars drawn from the Artillery Corps when required for ceremonial occasions). However, the mystique of the cavalry is such that the name has been introduced for what was always a mechanised force.
Some engagements in late 20th and early 21st century guerrilla wars involved mounted troops, particularly against partisan or guerrilla fighters in areas with poor transport infrastructure. Such units were not used as cavalry but rather as mounted infantry. Examples occurred in Afghanistan, Portuguese Africa and Rhodesia. The French Army used existing mounted squadrons of Spahis to a limited extent for patrol work during the Algerian War (1954–62) and the Swiss Army maintained a mounted dragoon regiment for combat purposes until 1973. The Portuguese Army used horse mounted cavalry with some success in the wars of independence in Angola and Mozambique in the 1960s and 1970s. During the 1964-79 Rhodesian Bush War the Rhodesian Army created an elite mounted infantry unit called Grey's Scouts to fight unconventional actions against the rebel forces of Robert Mugabe and Joshua Nkomo. The horse mounted infantry of the Scouts were effective and reportedly feared by their opponents in the rebel African forces. In the 1978 to present Afghan Civil War period there have been several instances of horse mounted combat.
South and Central American armies maintained mounted cavalry for longer than those of Europe, Asia or North America. The Mexican Army included a number of horse mounted cavalry regiments as late as the mid-1990s and the Chilean Army had five such regiments in 1983 as mounted mountain troops.
The Soviet Army retained horse cavalry divisions until 1955, and even at the dissolution of the Soviet Union in 1991, there was an independent horse mounted cavalry squadron in Kyrgyzstan.
Remaining operational mounted cavalry.
Today, the Indian Army's 61st Cavalry is reported to be the largest remaining non-ceremonial horse-mounted cavalry in the world. It was raised in 1951 from the amalgamated state cavalry squadrons of Gwailior, Jodhpur, and Mysore. While primarily utilised for ceremonial purposes, the regiment can be deployed for internal security or police roles if required. The 61st Cavalry and the President's Body Guard parade in full dress uniform in New Delhi each year in what is probably the largest assembly of traditional cavalry still to be seen in the world. Both the Indian and the Pakistani armies maintain armoured regiments with the titles of Lancers or Horse, dating back to the 19th century.
As of 2007 the Chinese People's Liberation Army employed two battalions of horse cavalry in Xinjing Military District for border patrol work. The PLA mounted units last saw action during border clashes with Vietnam in the 1970s and 80s, after which most cavalry units were disbanded as part of the major military downsizing of the 1980s. 
In the wake of the 2008 Sichuan earthquake, there have been calls to rebuild the army horse inventory for disaster relief in difficult terrain. Subsequent Chinese media reporting confirms that the Chinese Army maintains operational horse cavalry at squadron strength in the Mongolia Autonomous Region for scouting and logistical purposes.
Modern ceremonial cavalry plus armored units retaining traditional titles.
Cavalry or mounted gendarmerie units continue to be maintained for purely or primarily ceremonial purposes by the United States, British, French, Italian, Danish, Swedish, Dutch, Chilean, Portuguese, Moroccan, Algerian, Nepalese, Nigerian, Venezuelan, Brazilian, Peruvian, Paraguayan, Polish, Argentine, Senegalese, Jordanian, Pakistani, Indian, Spanish, Omani, Thai, Panamanian and Bulgarian armed forces. The Army of the Russian Federation has recently reintroduced a ceremonial mounted squadron wearing historic uniforms.
A number of armoured regiments in the British Army retain the historic designations of Hussars, Dragoons, Light Dragoons, Dragoon Guards, Lancers and Yeomanry. Only the Household Cavalry, Consisting of: The Life Guards Mounted Squadron, The Blues and Royals Mounted Squadron, The State Trumpeters of The Household Cavalry and The Household Cavalry Mounted Band are maintained for mounted (and dismounted) ceremonial duties in London.
The French Army still has regiments with the historic designations of Cuirassiers, Hussars, Chasseurs, Dragoons and Spahis. Only the cavalry of the Republican Guard and a ceremonial "fanfare" detachment of trumpeters for the cavalry/armoured branch as a whole are now mounted.
In the Canadian Army, a number of regular and reserve units have cavalry roots, including The Royal Canadian Hussars (Montreal), the Governor General's Horse Guards, Lord Strathcona's Horse, the Royal Canadian Dragoons, and the South Alberta Light Horse. Of these, only Lord Strathcona's Horse and the Governor General's Horse Guards maintain an official ceremonial horse-mounted cavalry troop or squadron.
Both the Australian and New Zealand armies follow the British practice of maintaining traditional titles (Light Horse or Mounted Rifles) for modern mechanised units. However, neither country retains a horse-mounted unit.
Several armored units of the modern United States Army retain the designation of "Armored cavalry". The United States also has "air cavalry" units equipped with helicopters. The Horse Cavalry Detachment of the U.S. Army's 1st Cavalry Division is made up of active duty soldiers, still functions as an active unit, trained to approximate the weapons, tools, equipment and techniques used by the United States Cavalry in the 1880s.
Modern cavalry units with non-combat support roles.
The First Troop Philadelphia City Cavalry is a volunteer unit within the Pennsylvania Army National Guard which serves as a combat force when in federal service but acts in a mounted disaster relief role when in state service. In addition, the Parsons' Mounted Cavalry is a Reserve Officer Training Corps unit which forms part of the Corps of Cadets at Texas A&M University.
Some individual U.S. states actively maintain cavalry units as a part of their respective state defense forces. The Maryland Defense Force includes a cavalry unit, Cavalry Troop A, which serves primarily as a ceremonial unit. The unit training includes a saber qualification course based upon the 1926 U.S. Army course. Cavalry Troop A also assists other Maryland agencies as a rural search and rescue asset. In Massachusetts, The National Lancers trace their lineage to a volunteer cavalry militia unit established in 1836 and are currently organized as an official part of the Massachusetts Organized Militia. The National Lancers maintain three units, Troops A, B, and C, which serve in a ceremonial role and assist in search and rescue missions. In July 2004, the National Lancers were ordered into active state service to guard Camp Curtis Guild during the 2004 Democratic National Convention. The Governor's Horse Guard of Connecticut maintains two companies which are trained in urban crowd control.
Light and armored cavalry.
Historically, cavalry was divided into light and armoured cavalry and horse archers. The differences were their role in combat, the size of the mount, and how much armor was worn by the mount and rider.
Early light cavalry (like the auxiliaries of the Roman army) were typically used to scout and skirmish, to cut down retreating infantry, and for defeating enemy missile troops. Armoured cavalry such as the Byzantine cataphract were used as shock troops—they would charge the main body of the enemy and, in many cases, their actions decided the outcome of the battle, hence the later term "battle cavalry".
During the Gunpowder Age, armored cavalry become obsolete. However, many units retained cuirasses and helmets for their protective value against sword and bayonet strikes and the morale boost these provide to the wearers. By this time the main difference between light and battle cavalry was their training; the former was regarded as a tool for harassment and reconnaissance, while the latter was considered best for close-order charges.
Since the development of armored warfare the distinction between light and heavy armor has persisted basically along the same lines. Armored cars and light tanks have adopted the reconnaissance role while medium and heavy tanks are regarded as the decisive shock troops.
Social status.
From the beginning of civilization to the 20th century, ownership of heavy cavalry horses has been a mark of wealth amongst settled peoples. A cavalry horse involves considerable expense in breeding, training, feeding, and equipment, and has very little productive use except as a mode of transport.
For this reason, and because of their often decisive military role, the cavalry has typically been associated with high social status. This was most clearly seen in the feudal system, where a lord was expected to enter combat armored and on horseback and bring with him an entourage of peasants on foot. If landlords and peasants came into conflict, the peasants would be ill-equipped to defeat armored knights.
In later national armies, service as an officer in the cavalry was generally a badge of high social status. For instance prior to 1914 most officers of British cavalry regiments came from a socially privileged background and the considerable expenses associated with their role generally required private means, even after it became possible for officers of the line infantry regiments to live on their pay. Options open to poorer cavalry officers in the various European armies included service with less fashionable (though often highly professional) frontier or colonial units. These included the British Indian cavalry, the Russian Cossacks or the French Chasseurs d' Afrique.
During the 19th and early 20th centuries most monarchies maintained a mounted cavalry element in their royal or imperial guards. These ranged from small units providing ceremonial escorts and palace guards through to large formations intended for active service. The mounted escort of the Spanish Royal Household provided an example of the former and the twelve cavalry regiments of the Prussian Imperial Guard an example of the latter. In either case the officers of such units were likely to be drawn from the aristocracies of their respective societies.
On film.
Some small sense of the noise and power of a cavalry charge can be gained from the 1970 film "Waterloo", which featured some 2000 cavalrymen, some of them cossacks. It included detailed displays of the horsemanship required to manage animal and weapons in large numbers at the gallop (unlike the real battle of Waterloo, where deep mud significantly slowed the horses). The Gary Cooper movie "They Came to Cordura" contains an excellent scene of a cavalry regiment deploying from march to battleline formation. A smaller-scale cavalry charge can be seen in "" (2003); although the finished scene has substantial computer-generated imagery, raw footage and reactions of the riders are shown in the Extended Version DVD Appendices.
Other films that show cavalry actions include:

</doc>
<doc id="6818" url="http://en.wikipedia.org/wiki?curid=6818" title="Citric acid cycle">
Citric acid cycle

The citric acid cycle – also known as the tricarboxylic acid (TCA) cycle or the Krebs cycle – is a series of chemical reactions used by all aerobic organisms to generate energy through the oxidation of acetate derived from carbohydrates, fats and proteins into carbon dioxide and chemical energy in the form of adenosine triphosphate (ATP). In addition, the cycle provides precursors of certain amino acids as well as the reducing agent NADH that is used in numerous other biochemical reactions. Its central importance to many biochemical pathways suggests that it was one of the earliest established components of cellular metabolism and may have originated abiogenically.
The name of this metabolic pathway is derived from citric acid (a type of tricarboxylic acid) that is consumed and then regenerated by this sequence of reactions to complete the cycle. In addition, the cycle consumes acetate (in the form of acetyl-CoA) and water, reduces NAD+ to NADH, and produces carbon dioxide as a waste byproduct. The NADH generated by the TCA cycle is fed into the oxidative phosphorylation (electron transport) pathway. The net result of these two closely linked pathways is the oxidation of nutrients to produce usable chemical energy in the form of ATP.
In eukaryotic cells, the citric acid cycle occurs in the matrix of the mitochondrion. In prokaryotic cells, such as bacteria which lack mitochondria, the TCA reaction sequence is performed in the cytosol with the proton gradient for ATP production being across the cell's surface (plasma membrane) rather than the inner membrane of the mitochondrion.
Discovery.
Several of the components and reactions of the citric acid cycle were established in the 1930s by the research of the Nobel laureate Albert Szent-Györgyi, for which he received the Nobel Prize in 1937 for his discoveries pertaining to fumaric acid, a key component of the cycle. The citric acid cycle itself was finally identified in 1937 by Hans Adolf Krebs while at the University of Sheffield, for which he received the Nobel Prize for Physiology or Medicine in 1953.
Evolution.
Components of the TCA cycle were derived from anaerobic bacteria, and the TCA cycle itself may have evolved more than once. Theoretically there are several alternatives to the TCA cycle; however, the TCA cycle appears to be the most efficient. If several TCA alternatives had evolved independently, they all appear to have converged to the TCA cycle.
Overview.
The citric acid cycle is a key metabolic pathway that unifies carbohydrate, fat, and protein metabolism. The reactions of the cycle are carried out by 8 enzymes that completely oxidize acetyl-CoA into two molecules of carbon dioxide. Through catabolism of sugars, fats, and proteins, a two-carbon organic product acetate in the form of acetyl-CoA is produced which enters the citric acid cycle. The reactions of the cycle also converts three equivalents of nicotinamide adenine dinucleotide (NAD+) into three equivalents of reduced NAD+ (NADH), one equivalent of flavin adenine dinucleotide (FAD) into one equivalent of FADH2, and one equivalent each of guanosine diphosphate (GDP) and inorganic phosphate (Pi) into one equivalent of guanosine triphosphate (GTP). The NADH and FADH2 generated by the citric acid cycle are in turn used by the oxidative phosphorylation pathway to generate energy-rich adenosine triphosphate (ATP).
One of the primary sources of acetyl-CoA is from the breakdown of sugars by glycolysis which yield pyruvate that in turn is decarboxylated by the enzyme pyruvate dehydrogenase generating acetyl-CoA according to the following reaction scheme:
The product of this reaction, acetyl-CoA, is the starting point for the citric acid cycle. Acetyl-CoA may also be obtained from the oxidation of fatty acids. Below is a schematic outline of the cycle:
Steps.
Two carbon atoms are oxidized to CO2, the energy from these reactions being transferred to other metabolic processes by GTP (or ATP), and as electrons in NADH and QH2. The NADH generated in the TCA cycle may later donate its electrons in oxidative phosphorylation to drive ATP synthesis; FADH2 is covalently attached to succinate dehydrogenase, an enzyme functioning both in the TCA cycle and the mitochondrial electron transport chain in oxidative phosphorylation. FADH2, therefore, facilitates transfer of electrons to coenzyme Q, which is the final electron acceptor of the reaction catalyzed by the Succinate:ubiquinone oxidoreductase complex, also acting as an intermediate in the electron transport chain.
The citric acid cycle is continuously supplied with new carbon in the form of acetyl-CoA, entering at step 0 below.
Mitochondria in animals, including humans, possess two succinyl-CoA synthetases: one that produces GTP from GDP, and another that produces ATP from ADP. Plants have the type that produces ATP (ADP-forming succinyl-CoA synthetase). Several of the enzymes in the cycle may be loosely associated in a multienzyme protein complex within the mitochondrial matrix.
The GTP that is formed by GDP-forming succinyl-CoA synthetase may be utilized by nucleoside-diphosphate kinase to form ATP (the catalyzed reaction is GTP + ADP → GDP + ATP).
Products.
Products of the first turn of the cycle are: "one GTP (or ATP), three NADH, one QH2, two CO2".
Because two acetyl-CoA molecules are produced from each glucose molecule, two cycles are required per glucose molecule. Therefore, at the end of two cycles, the products are: two GTP, six NADH, two QH2, and four CO2"
The above reactions are balanced if Pi represents the H2PO4− ion, ADP and GDP the ADP2− and GDP2− ions, respectively, and ATP and GTP the ATP3− and GTP3− ions, respectively.
The total number of ATP obtained after complete oxidation of one glucose in glycolysis, citric acid cycle, and oxidative phosphorylation is estimated to be between 30 and 38.
Efficiency.
The theoretical maximum yield of ATP through oxidation of one molecule of glucose in glycolysis, citric acid cycle, and oxidative phosphorylation is 38 (assuming 3 molar equivalents of ATP per equivalent NADH and 2 ATP per FADH2). In eukaryotes, two equivalents of NADH are generated in glycolysis, which takes place in the cytoplasm. Transport of these two equivalents into the mitochondria consumes two equivalents of ATP, thus reducing the net production of ATP to 36. Furthermore, inefficiencies in oxidative phosphorylation due to leakage of protons across of the mitochondrial membrane and slippage of the ATP synthase/proton pump commonly reduces the ATP yield from NADH and FADH2 to less than the theoretical maximum yield. The observed yields are, therefore, closer to ~2.5 ATP per NADH and ~1.5 ATP per FADH2, further reducing the total net production of ATP to approximately 30. An assessment of the total ATP yield with newly revised proton-to-ATP ratios provides an estimate of 29.85 ATP per glucose molecule.
Variation.
While the TCA cycle is in general highly conserved, there is significant variability in the enzymes found in different taxa (note that the diagrams on this page are specific to the mammalian pathway variant).
Some differences exist between eukaryotes and prokaryotes. The conversion of D-"threo"-isocitrate to 2-oxoglutarate is catalyzed in eukaryotes by the NAD+-dependent , while prokaryotes employ the NADP+-dependent . Similarly, the conversion of ("S")-malate to oxaloacetate is catalyzed in eukaryotes by the NAD+-dependent , while most prokaryotes utilize a quinone-dependent enzyme, .
A step with significant variability is the conversion of succinyl-CoA to succinate. Most organisms utilize , succinate–CoA ligase (ADP-forming) (despite its name, the enzyme operates in the pathway in the direction of ATP formation). In mammals a GTP-forming enzyme, succinate–CoA ligase (GDP-forming) () also operates. The level of utilization of each isoform is tissue dependent. In some acetate-producing bacteria, such as "Acetobacter aceti", an entirely different enzyme catalyzes this conversion – , succinyl-CoA:acetate CoA-transferase. This specialized enzyme links the TCA cycle with acetate metabolism in these organisms. Some bacteria, such as "Helicobacter pylori", employ yet another enzyme for this conversion – succinyl-CoA:acetoacetate CoA-transferase ().
Some variability also exists at the previous step – the conversion of 2-oxoglutarate to succinyl-CoA. While most organisms utilize the ubiquitous NAD+-dependent 2-oxoglutarate dehydrogenase, some bacteria utilize a ferredoxin-dependent 2-oxoglutarate synthase ().
Other organisms, including obligately autotrophic and methanotrophic bacteria and archaea, bypass succinyl-CoA entirely, and convert 2-oxoglutarate to succinate via succinate semialdehyde, using , 2-oxoglutarate decarboxylase, and , succinate-semialdehyde dehydrogenase.
Regulation.
The regulation of the TCA cycle is largely determined by product inhibition and substrate availability. If the cycle were permitted to run unchecked, large amounts of metabolic energy could be wasted in overproduction of reduced coenzyme such as NADH and ATP. The major eventual substrate of the cycle is ADP which gets converted to ATP. A reduced amount of ADP causes accumulation of precursor NADH which in turn can inhibit a number of enzymes. NADH, a product of all dehydrogenases in the TCA cycle with the exception of succinate dehydrogenase, inhibits pyruvate dehydrogenase, isocitrate dehydrogenase, α-ketoglutarate dehydrogenase, and also citrate synthase. Acetyl-coA inhibits pyruvate dehydrogenase, while succinyl-CoA inhibits alpha-ketoglutarate dehydrogenase and citrate synthase. When tested in vitro with TCA enzymes, ATP inhibits citrate synthase and α-ketoglutarate dehydrogenase; however, ATP levels do not change more than 10% in vivo between rest and vigorous exercise. There is no known allosteric mechanism that can account for large changes in reaction rate from an allosteric effector whose concentration changes less than 10%.
Calcium is used as a regulator. Mitochondrial matrix calcium levels can reach the tens of micromolar levels during cellular activation. It activates pyruvate dehydrogenase phosphatase which in turn activates the pyruvate dehydrogenase complex. Calcium also activates isocitrate dehydrogenase and α-ketoglutarate dehydrogenase. This increases the reaction rate of many of the steps in the cycle, and therefore increases flux throughout the pathway.
Citrate is used for feedback inhibition, as it inhibits phosphofructokinase, an enzyme involved in glycolysis that catalyses formation of fructose 1,6-bisphosphate,a precursor of pyruvate. This prevents a constant high rate of flux when there is an accumulation of citrate and a decrease in substrate for the enzyme.
Recent work has demonstrated an important link between intermediates of the citric acid cycle and the regulation of hypoxia-inducible factors (HIF). HIF plays a role in the regulation of oxygen homeostasis, and is a transcription factor that targets angiogenesis, vascular remodeling, glucose utilization, iron transport and apoptosis. HIF is synthesized consititutively, and hydroxylation of at least one of two critical proline residues mediates their interaction with the von Hippel Lindau E3 ubiquitin ligase complex, which targets them for rapid degradation. This reaction is catalysed by prolyl 4-hydroxylases. Fumarate and succinate have been identified as potent inhibitors of prolyl hydroxylases, thus leading to the stabilisation of HIF.
Major metabolic pathways converging on the TCA cycle.
Several catabolic pathways converge on the TCA cycle. Reactions that form intermediates of the TCA cycle in order to replenish them (especially during the scarcity of the intermediates) are called anaplerotic reactions.
The citric acid cycle is the third step in carbohydrate catabolism (the breakdown of sugars). Glycolysis breaks glucose (a six-carbon-molecule) down into pyruvate (a three-carbon molecule). In eukaryotes, pyruvate moves into the mitochondria. It is converted into acetyl-CoA by decarboxylation and enters the citric acid cycle.
In protein catabolism, proteins are broken down by proteases into their constituent amino acids. The carbon backbones of some of these amino acids can become a source of energy by being converted to acetyl-CoA and entering into the citric acid cycle.
In fat catabolism, triglycerides are hydrolyzed to break them into fatty acids and glycerol. In the liver the glycerol can be converted into glucose via dihydroxyacetone phosphate and glyceraldehyde-3-phosphate by way of gluconeogenesis. In many tissues, especially heart tissue, fatty acids are broken down through a process known as beta oxidation, which results in acetyl-CoA, which can be used in the citric acid cycle. Beta oxidation of fatty acids with an odd number of methylene bridges produces propionyl CoA, which is then converted into succinyl-CoA and fed into the citric acid cycle.
The total energy gained from the complete breakdown of one molecule of glucose by glycolysis, the citric acid cycle, and oxidative phosphorylation equals about 30 ATP molecules, in eukaryotes. The citric acid cycle is called an amphibolic pathway because it participates in both catabolism and anabolism.
Interactive pathway map.
"Click on genes, proteins and metabolites below to link to respective articles." 
TCA Cycle 

</doc>
<doc id="6821" url="http://en.wikipedia.org/wiki?curid=6821" title="Military engineering vehicle">
Military engineering vehicle

A military engineering vehicle is a vehicle built for the construction work or for the transportation of combat engineers on the battlefield. These vehicles may be modified civilian equipment or purpose-built military vehicles.
Types of military engineering vehicles.
Civilian and militarized heavy equipment.
Military engineering can employ a wide variety of heavy equipment in the same or similar ways to how this equipment is used outside the military. Bulldozers, cranes, graders, excavators, dump trucks, loaders, and backhoes all see extensive use by military engineers.
Military engineers may also use civilian heavy equipment which was modified for military applications. Typically, this involves adding armour for protection from battlefield hazards such as artillery, unexploded ordnance, mines, and small arms fire. Often this protection is provided by armour plates and steel jackets. Some examples of armoured civilian heavy equipment are the IDF Caterpillar D9, American D7 TPK, Canadian D6 armoured bulldozer, cranes, graders, excavators, and M35 2-1/2 ton cargo truck.
Militarized heavy equipment may also take on the form of traditional civilian equipment designed and built to unique military specifications. These vehicles typically sacrifice some depth of capability from civilian models in order to gain greater speed and independence from prime movers. Examples of this type of vehicle include high speed backhoes such as the Australian Army's High Mobility Engineering Vehicle (HMEV) from Thales or the Canadian Army's Multi-Purpose Engineer Vehicle (MPEV) from Arva.
"The main article for civilian heavy equipment is:" Heavy equipment (construction)
Armoured engineering vehicle.
Typically based on the platform of a main battle tank, these vehicles go by different names depending upon the country of use or manufacture. In the US the term "combat engineer vehicle (CEV)" is used, in the UK the term "Armoured Vehicle Royal Engineers (AVRE)" is used, while in Canada and other commonwealth nations the term "armoured engineer vehicle (AEV)" is used. There is no set template for what such a vehicle will look like, yet likely features include a large dozer blade or mine ploughs, a large calibre demolition cannon, augers, winches, excavator arms and cranes or lifting booms.
These vehicles are designed to directly conduct obstacle breaching operations and to conduct other earth-moving and engineering work on the battlefield.
Good examples of this type of vehicle include the UK Trojan AVRE, the Russian IMR, and the US M728 Combat Engineer Vehicle.
It should be noted that while the term "armoured engineer vehicle" is used specifically to describe these multi-purpose tank based engineering vehicles, that term is also used more generically in British and Commonwealth militaries to describe all heavy tank based engineering vehicles used in the support of mechanized forces. Thus, "armoured engineer vehicle" used generically would refer to AEV, AVLB, Assault Breachers, and so on.
Armored bulldozer.
The armored bulldozer is a basic tool of combat engineering. These combat engineering vehicles combine the earth moving capabilities of the bulldozer with armor which protects the vehicle and its operator in or near combat. Most are civilian bulldozers modified by addition of vehicle armor/military equipment, but some are tanks stripped of armament and fitted with a dozer blade. Some tanks have bulldozer blades while retaining their armament, but this does not make them armored bulldozers as such, because combat remains the primary role — earth moving is a secondary task.
Armoured earth mover.
Lighter and less multi-functional than the AEVs described above, these vehicles are designed to conduct earth-moving work on the battlefield. These vehicles have greater high speed mobility than traditional heavy equipment and are protected against the effects of blast and fragmentation. Good examples are the American M9 ACE and the UK FV180 Combat Engineer Tractor.
Breaching vehicle.
These vehicles are equipped with mechanical or other means for the breaching of man made obstacles. Common types of breaching vehicles include mechanical flails, mine plough vehicles, and mine roller vehicles. In some cases, these vehicles will also mount Mine-clearing line charges. Breaching vehicles may be either converted armoured fighting vehicles or purpose built vehicles. In larger militaries, converted AFV are likely to be used as "assault breachers" while the breached obstacle is still covered by enemy observation and fire, and then purpose built breaching vehicles will create additional lanes for following forces.
Good examples of breaching vehicles include the USMC M1 Assault Breacher Vehicle, the UK Aardvark JSFU, and the Singaporean Trailblazer.
Bridging vehicles.
Several types of military bridging vehicles have been developed. An armoured vehicle-launched bridge (AVLB) is typically a modified tank hull converted to carry a bridge into battle in order to support crossing ditches, small waterways, or other gap obstacles.
Another type of bridging vehicle is the truck launched bridge. The Soviet TMM bridging truck could carry and launch a 10 meter bridge that could be daisy-chained with other TMM bridges to cross larger obstacles. More recent developments have seen the conversion of AVLB and truck launched bridge with launching systems that can be mounted on either tank or truck for bridges that are capable of supporting heavy main battle tanks.
Earlier examples of bridging vehicles include a type in which a converted tank hull is the bridge. On these vehicles, the hull deck comprises the main portion of the tread way while ramps extend from the front and rear of the vehicle to allow other vehicles to climb over the bridging vehicle and cross obstacles. An example of this type of armoured bridging vehicle was the Churchill Ark used in the Second World War.
Combat engineer section carriers.
Another type of CEVs are armoured fighting vehicles which are used to transport sappers (combat engineers) and can be fitted with a bulldozer's blade and other mine-breaching devices. They are often used as APCs because of their carrying ability and heavy protection. They are usually armed with machine guns and grenade launchers and usually tracked to provide enough tractive force to push blades and rakes. Some examples are the U.S. M113 APC, IDF Puma, Nagmachon, Husky, and U.S. M1132 ESV (a Stryker variant).
Military ferries and amphibious crossing vehicles.
One of the major tasks of military engineering is crossing major rivers. Several military engineering vehicles have been developed in various nations to achieve this task. One of the more common types is the amphibious ferry such as the M3 Amphibious Rig. These vehicles are self-propelled on land, they can transform into raft type ferries when in the water, and often multiple vehicles can connect to form larger rafts or floating bridges. Other types of military ferries, such as the Soviet "Plavayushij Transportyor - Srednyj", are able to load while still on land and transport other vehicles cross country and over water.
In addition to amphibious crossing vehicles, military engineers may also employ several types of boats. Military assault boats are small boats propelled by oars or an outboard motor and used to ferry dismounted infantry across water.
Tank based combat engineering vehicles.
Most CEVs are armoured fighting vehicles that may be based on a tank chassis and have special attachments in order to breach obstacles. Such attachments may include dozer blades, mine rollers, cranes etc. An example of an engineering vehicle of this kind is a bridgelaying tank, which replaces the turret with a segmented hydraulic bridge.
The Hobart's Funnies of the Second World War were a wide variety of armoured vehicles for combat engineering tasks. They were allocated to the initial beachhead assaults by the British and Commonwealth forces in the D-Day landings
Churchill tank.
The British Churchill tank because of its good cross-country performance and capacious interior with side hatches became the most adapted with modifications, the base unit being the AVRE carrying a large demolition gun.

</doc>
<doc id="6822" url="http://en.wikipedia.org/wiki?curid=6822" title="Catalonia">
Catalonia

Catalonia (; Catalan: "Catalunya"; Occitan: "Catalonha"; Spanish: "Cataluña") is an autonomous community of Spain, and designated a "historical nationality" by its Statute of Autonomy. Catalonia comprises four provinces: Barcelona, Girona, Lleida, and Tarragona. The capital and largest city is Barcelona, the second largest city in Spain, and the centre of one of the largest metropolitan areas in Europe, and it comprises most of the territory of the former Principality of Catalonia, with the remainder now part of France. Catalonia is bordered by France and Andorra to the north, the Mediterranean Sea to the east, and the Spanish regions of Aragon and the Valencian Community to west and south respectively. The official languages are Catalan, Spanish and Aranese (an Occitan dialect).
In the 10th century the eastern counties of the March of Gothia and the Marca Hispanica became independent from the Frankish kingdom, uniting as vassals of Barcelona. In 1137 Barcelona and Aragon formed the Crown of Aragon, and Catalonia became a maritime power and the main base for the Crown of Aragon's naval power and expansionism in the Mediterranean. Medieval Catalan literature flourished. Between 1469 and 1516, the King of Aragon and the Queen of Castile married and ruled their kingdoms together, retaining all their distinct institutions, courts and Constitutions. During the Reapers' War (1640–52), Catalonia rebelled against the presence of the Castillian army in its territory, becoming a republic under French protection. Under the terms of the Treaty of the Pyrenees in 1659, which ended the wider Franco-Spanish war, Castile agreed with France to cede it the northern parts of Catalonia, mostly incorporated in the county of Roussillon. During the War of the Spanish Succession (1701–14), the Crown of Aragon sided against Philip V of Spain, whose subsequent victory led to the abolition of non-Castilian institutions in all Spain, and the replacement of Latin and other languages (like the Catalan) with the Spanish language in legal documents.
Despite the Napoleonic and Carlist Wars, Catalonia experienced economic growth and industrialisation. During the second half of the 19th century, the region saw a cultural renaissance coupled with incipient nationalism, while several workers movements appeared. In 1913, the four Catalan provinces formed a Commonwealth, and with the advent of democracy during the Second Spanish Republic (1931–39), the Generalitat of Catalonia, was restored. After the Spanish Civil War, the Francoist dictatorship enacted repressive measures, abolishing Catalan institutions and banning the official use of the Catalan language again. During the 1950s and 1960s, Catalonia saw significant economic growth and became an important tourist destination, drawing many workers from across Spain and making Barcelona one of Europe's largest industrial metropolitan areas. Since the Spanish transition to democracy (1975–82) Catalonia has recovered political and cultural autonomy and is now one of the most economically dynamic communities of Spain.
Etymology and pronunciation.
The name "Catalunya" (Catalonia) began to be used for the homeland of the "Catalans" ("Catalanenses") in the late 11th century, probably used before as a mere territorial area in reference to a group of counties that comprised part of the March of Gothia and March of Hispania under the control of the Count of Barcelona and his relatives. The origin of the name "Catalunya" is subject to diverse interpretations because of a lack of evidence.
One theory suggests that "Catalunya" (Latin "Gathia Launia") derives from the name "Gothia" or "Gauthia" ("Land of the Goths"), since the origins of the Catalan counts, lords and people were found in the March of Gothia, known as "Gothia", whence "Gothland" > "Gothlandia" > "Gothalania" > "Catalonia" theoretically derived. During the Middle Ages, Byzantine chroniclers claimed that "Catalania" derives from the local medley of Goths with Alans, initially constituting a "Goth-Alania".
Other less plausible theories suggest:
In English, "Catalonia" may be pronounced or . The native name, "Catalunya", is pronounced ] in Central Catalan, the most widely spoken variety whose pronunciation is considered standard. The Spanish name is "Cataluña" (]), and the Aranese name is "Catalonha" (]).
History.
In pre-Roman times, the area now called Catalonia, like the rest of the Mediterranean side of the Iberian Peninsula, was populated by the Iberians. Coastal trading colonies were established by the ancient Greeks, who settled around the Roses area. Both Greeks and Carthaginians briefly ruled the territory in the course of the Second Punic War and traded with the surrounding Iberian population.
After the Carthaginian defeat by the Roman Republic, Catalonia became the first area of Iberia to come under Roman rule, and became part of Roman Hispania, the westernmost part of the Roman Empire. Tarraco, now called Tarragona, was one of the most important Roman cities in Hispania.
After the fall of the Western Roman Empire, the area was conquered by the Visigoths and was ruled as part of the Visigothic Kingdom for almost two and a half centuries. In 718, it came under Muslim control and became part of the Ummayad province of Al-Andalus. From the conquest of Roussillon in 760 to the conquest of Barcelona in 801 the Frankish Empire took control of the area from the Muslims and created the counties that would later become known as Catalonia. These counties formed part of the March of Gothia and March of Hispania, a buffer zone in the south of the former province of Septimania and in the northeast of the Iberian Peninsula, to act as a defensive barrier for the Frankish Empire against the Moors of Al-Andalus.
In the Middle Ages, these counties became the basis for Catalonia under the rule of the counts of Barcelona. The counts of Barcelona were Frankish vassals nominated by the emperor of the Franks, to whom they were feudatories (801–987).
In 987, Wilfred the Hairy, count of Barcelona did not recognise Hugh Capet, making his successors (Sunyer, Borrell II, Miro I, and so on) de facto independent of the Carolingian crown. In 1137, Ramon Berenguer IV, Count of Barcelona decided to accept King Ramiro II of Aragon's proposal to marry Queen Petronila, establishing the dynastic union of the County of Barcelona with the Kingdom of Aragon, joining the Crown of Aragon and making the Catalan counties that were united under the county of Barcelona into a principality of the Aragonese Crown.
In 1258, by means of the Treaty of Corbeil, the Count of Barcelona and King of Aragon, of Mallorca and of Valencia, James I of Aragon renounced his family rights and dominions in Occitania and recognised the king of France as heir of the Carolingian Dynasty. The king of France formally relinquished his nominal feudal lordship over all the Catalan counties, excepting the County of Foix despite the opposition of the King of Aragon and Count of Barcelona. This treaty transformed the region's "de facto" union with Aragon into a "de jure" one and was the origin of the definitive separation between both geographical areas, Catalonia and the Languedoc.
As a coastal territory of the Crown of Aragon, Catalonia, Barcelona in particular, became the base of the Crown maritime dominion and lead the expansion, the power and the influence of the Crown of Aragon in the Mediterranean. Gradually, new territories such as, the Kingdom of Valencia, the Kingdom of Majorca and later Sardinia, the Kingdom of Sicily, Corsica and Duchy of Athens were incorporated under the dynasty of the House of Aragon (1164–1410) into the Crown.
The domains of the Crown of Aragon were severely affected by the Black Death pandemic and by later outbreaks of the plague. Between 1347 and 1497 Catalonia lost 37 percent of its population.
In 1410, King Martin I died without surviving descendants. As a result, by the Pact of Caspe, Ferdinand of Antequera from the Castilian dynasty of Trastámara received the Crown of Aragon as Ferdinand I of Aragon.
His grandson, King Ferdinand II of Aragon, and Queen Isabella I of Castile married in 1469, later becoming the Catholic Monarchs; subsequently, this event was seen by historiographers as the dawn of the unified Kingdom of Spain. At that point, though united by marriage, the Crowns of Castile and Aragon maintained distinct territories, each keeping its own traditional institutions, parliaments and laws. Castile commissioned in monopoly the expeditions to the Americas, and benefited from the colonial riches. Political power gradually shifted away from the Aragonese and Castilian courts to the court of the Spanish Crown.
By virtue of descent from his maternal grandparents, Ferdinand II and Isabella I, in 1516 Charles I became the first king to rule Castile and Aragon simultaneously in his own right. Following the death of his paternal (Habsburg) grandfather, Maximilian I, he was also elected Emperor of the Holy Roman Empire in 1519 as Charles V
Until 1716, Catalonia, as a principality of the Crown of Aragon, continued to retain its own Constitutions and usages and laws, which gradually were adapted until 1705, in the course of the Spanish transition from feudalism to a modern state and fueled by the kings' preference to have more centralised territories.
The Reapers' War (1640–52) saw Catalonia rebel with French help against the Spanish Crown for overstepping Catalonia's rights. Most of Catalonia was reconquered but Catalan rights were recognised. Roussillon was lost to France. Rousillon is now the Department of Pyrénées-Orientales.
The most significant conflict concerning the governing monarchy was the War of the Spanish Succession, which began when the childless Charles II of Spain, the last Spanish Habsburg, died without an heir in 1700. Charles II had chosen Philip V from the French dynasty, the Bourbons. Catalonia, like other territories that formed the Crown of Aragon, rose up in support of the Austrian Habsburg pretender Charles VI, Holy Roman Emperor in his claim for the Spanish throne as Charles III of Spain. The fight between the houses of Bourbon and Habsburg for the Spanish throne split Spain and Europe.
The fall of Barcelona on 11 September 1714 to the Bourbon king militarily ended the Habsburg claim to the Spanish throne, which became legal fact in the Treaty of Utrecht. Feeling that he had been betrayed as he was initially backed (Philip V had presided over the Corts Catalanes in 1701 and Barcelona manufactured coins for both kings during the war and supported both of them), the first Bourbon king introduced the Nueva Planta decrees that incorporated the territories of the Crown of Aragon, including Catalonia, as provinces under the Crown of Castile in 1716, terminating their separate institutions and rights, within a united administration of Spain.
In the latter half of the 19th century, Catalonia became an industrial center. To this day it remains one of the most industrialised parts of Spain. In the first third of the 20th century, Catalonia gained and lost varying degrees of autonomy several times, receiving its first statute of autonomy during the Second Spanish Republic (1931). This period was marked by political unrest and the preeminence of the Anarchists during the Spanish Civil War (1936–39). The Anarchists had been active throughout the early 20th century, achieving the first eight-hour workday in Europe in 1919.
The defeat of the Republic of Spain in the Spanish Civil War brought fascist Francisco Franco to power as dictator. His regime imposed linguistic, political and cultural restrictions across Spain. In Catalonia, any kind of public activities associated with Catalan nationalism, Anarchism, Socialism, Liberalism, Democracy or Communism, including the publication of books on those subjects or simply discussion of them in open meetings, was banned. Franco's regime banned the use of Catalan in government-run institutions and during public events. The pro-Republic of Spain President of Catalonia, Lluís Companys, was taken to Spain from his exile in the German-occupied France, tortured and executed for the crime of 'military rebellion'.
During later stages of the Franco regime, certain folkloric and religious celebrations in Catalan resumed and were tolerated. Use of Catalan in the mass media had been forbidden, but was permitted from the early 1950s in the theatre. Publishing in Catalan continued throughout the dictatorship.
The years after the war were extremely hard. Catalonia, like many other parts of Spain, had been devastated by the war. Recovery from the war damage was slow and made more difficult by the international trade embargo against Franco's dictatorial regime. By the late 1950s the country had recovered its pre-war economic levels and in the 1960s was the second fastest growing economy in the world in what became known as the Spanish Miracle. During this period there was a spectacular growth of industry and tourism in Catalonia that drew large numbers of workers to the region from across Spain and made the area around Barcelona into one of Europe's largest industrial metropolitan areas.
After Franco's death in 1975, Catalonia voted for the adoption of a democratic Spanish Constitution in 1978, in which Catalonia recovered a small political and cultural autonomy. Today, Catalonia is the most economically dynamic region of Spain. The Catalan capital and largest city, Barcelona, is a major international cultural centre and a major tourist destination.
Geography.
Climate.
The climate of Catalonia is diverse. The populated areas lying by the coast in Tarragona, Barcelona and Girona provinces feature a Mediterranean climate (Köppen "Csa"). The inland part (including the Lleida province and the inner part of Barcelona province) show a mostly continental Mediterranean climate (Köppen "Csa"). The Pyrenean peaks have a mountain (Köppen "H") or even Alpine climate (Köppen "ET") at the highest summits, while the valleys have a maritime or oceanic climate sub-type (Köppen "Cfb").
In the Mediterranean area, summers are dry and hot with sea breezes, and the maximum temperature is around 26 -. Winter is cool or slightly cold depending on the location. It snows frequently in the Pyrenees, and it occasionally snows at lower altitudes, even by the coastline. Spring and autumn are typically the rainiest seasons, except for the Pyrenean valleys, where summer is typically stormy.
The inland part of Catalonia is hotter and drier in summer. Temperature may reach 35 °C, some days even 40 °C. Nights are cooler there than at the coast, with the temperature of around 14 -. Fog is not uncommon in valleys and plains; it can be especially persistent, with freezing drizzle episodes and subzero temperatures during winter (record from −36 °C), along the Segre and in other river valleys.
Hydrography.
Most of Catalonia belongs to the Mediterranean Basin. The Catalan hydrographic network consists of two important basins, the one of the Ebro and the one that comprises the internal basins of Catalonia, all of them flow to the Mediterranean. Furthermore, there is the Garona river basin that flows to the Atlantic Ocean, but it only covers 1.7% of the Catalan territory.
The hydrographic network can be divided in two sectors, an occidental slope or Ebre river slope and one oriental slope constituted by minor rivers that flow to the Mediterranean along the Catalan coast. The first slope provides an average of 18700 hm3 per year, while the second only provides an average of 2020 hm3/year. The difference is due to the big contribution of the Ebre river, from which the Segre is an important tributary. Moreover, in Catalonia there is a relative wealth of groundwaters, although there is inequality between "comarques", given the complex geological structure of the territory. In the Pyrenees there are many small lakes, remnants of the ice age. The biggest is the one of Banyoles.
The Catalan coast is almost rectilinear, with a length over 500 km and few landforms—the most relevant are the Cap de Creus and the Gulf of Roses to the north and the Ebro Delta to the south. The Catalan Coastal Range hugs the coastline, and it is split into two segments, one between L'Estartit and the town of Blanes (the Costa Brava), and the other at the south, at the Costes del Garraf.
The principal rivers in Catalonia are the Ter, Llobregat, and the Ebre, all of which run into the Mediterranean.
Politics.
After Franco's death in 1975 and the adoption of a democratic constitution in Spain in 1978, Catalonia recovered and extended the powers that it had gained in the Statute of Autonomy of 1932 but lost with the fall of the Second Spanish Republic at the end of the Spanish Civil War in 1939.
The region has gradually achieved more autonomy since the approval of the Spanish Constitution of 1978. The Generalitat holds exclusive jurisdiction in culture, environment, communications, transportation, commerce, public safety and local government, and shares jurisdiction with the Spanish government in education, health and justice. In all, the current system grants Catalonia with "more self-government than almost any other corner in Europe".
A relatively large sector of the population supports the ideas and policies of Catalan nationalism, a political movement which defends the notion that Catalonia is a separate nation and advocates for either further political autonomy or full independence of Catalonia.
The support for Catalan nationalism ranges from a demand for further autonomy and the federalisation of Spain to the desire for independence from the rest of Spain, expressed by Catalan independentists. The first survey following the Constitutional Court ruling that cut back elements of the 2006 Statute of Autonomy, published by "La Vanguardia" on July 18, 2010, found that a majority would support independence in a referendum. In February of the same year, a poll by the Open University of Catalonia gave more or less the same results.
Other polls have shown lower support for independence, ranging from 40 to 49%. Since 2011 when the question started to be regularly surveyed by the governmental Center for Public Opinion Studies (CEO), support for Catalan independence has been on the rise. According to the CEO opinion poll from October 2012, 51% of Catalans would vote for independence, 21% against it, and 21% would either not vote or vote blank.
In hundreds of non-binding local referendums on independence, organised across Catalonia from 13 September 2009, a large majority voted for independence, although critics argued that the polls were mostly held in pro-independence areas. In December 2009, 94% of those voting backed independence from Spain, on a turn-out of 25%. The final local referendum was held in Barcelona, in April 2011. On 11 September 2012, a pro-independence march pulled in a crowd of between 600,000 (according to the Spanish Government), 1.5 million (according to the Guàrdia Urbana de Barcelona), and 2 million (according to its promoters); whereas poll results revealed that half the population of Catalonia supported secession from Spain.
Two major factors were Spain's Constitutional Court's 2010 decision to declare part of the 2006 Statute of Autonomy of Catalonia unconstitutional, as well as the fact that Catalonia contributes 19.49% of the federal government’s tax revenue, but only receives 14.03% of federal spending.
Parties that consider themselves either Catalan nationalist or independentist have been present in all Catalan governments since 1980. The largest Catalan nationalist party, Convergence and Union, ruled Catalonia from 1980 to 2003, and returned to power in the 2010 election. Between 2003 and 2010, a leftist coalition, composed by the Catalan Socialists' Party, the pro-independence Republican Left of Catalonia and the leftist-environmentalist Initiative for Catalonia-Greens, implemented policies that widened Catalan autonomy.
In the November 25, 2012 Catalan parliamentary election, sovereigntist parties supporting a secession referendum gathered 59.01% of the votes and hold 87 of the 135 seats in the Catalan Parliament. Parties supporting independence from the rest of Spain obtained 49.12% of the votes and a majority of 74 seats.
Statutes of Autonomy.
The Statute of Autonomy of Catalonia is the fundamental organic law, second only to the Spanish Constitution from which the Statute originates.
In the Spanish Constitution of 1978 Catalonia, along with the Basque Country and Galicia, was defined as a "nationality". The same constitution gave Catalonia the automatic right to autonomy, which resulted in the Statute of Autonomy of Catalonia of 1979.
Both the 1979 Statute of Autonomy and the current one, approved in 2006, state that "Catalonia, as a nationality, exercises its self-government constituted as an Autonomous Community in accordance with the Constitution and with the Statute of Autonomy of Catalonia, which is its basic institutional law, always under the law in Spain".
The Preamble of the 2006 Statute of Autonomy of Catalonia states that the Parliament of Catalonia has defined Catalonia as a nation, but that "the Spanish Constitution recognizes Catalonia's national reality as a nationality". While the Statute was approved by and sanctioned by both the Catalan and Spanish parliaments, and later by referendum in Catalonia, it has been subject to a legal challenge by the surrounding autonomous regions of Aragon, Balearic Islands and the Valencian Community, as well as by the conservative People's Party. The objections are based on various issues such as disputed cultural heritage but, especially, on the Statute's alleged breaches of the principle of "solidarity between regions" in fiscal and educational matters enshrined by the Constitution.
Spain's Constitutional Court assessed the disputed articles and on 28 June 2010, issued its judgment on the principal allegation of unconstitutionality presented by the People's Party in 2006. The judgment granted clear passage to 182 articles of the 223 that make up the fundamental text. The court approved 73 of the 114 articles that the People's Party had contested, while declaring 14 articles unconstitutional in whole or in part and imposing a restrictive interpretation on 27 others. The court accepted the specific provision that described Catalonia as a "nation", however ruled that it was a historical and cultural term with no legal weight, and that Spain remained the only nation recognised by the constitution.
Government and law.
The Catalan Statute of Autonomy establishes that Catalonia is organised politically through the "Generalitat de Catalunya", conformed by the Parliament, the Presidency of the Generalitat, the Government or Executive Council and the other institutions created by the Parliament.
The seat of the Executive Council is the city of Barcelona. Since the restoration of the Generalitat on the return of democracy in Spain, the presidents of Catalonia have been Jordi Pujol (1980–2003), Pasqual Maragall (2003–2006), José Montilla Aguilera (2006–2010) and Artur Mas incumbent as of 2010[ [update]].
Security forces.
Catalonia has its own police force, the "Mossos d'Esquadra", whose origins date back to the 18th century. Since 1980 they have been under the command of the Generalitat, and since 1994 they have expanded in number in order to replace the national "Guardia Civil" and "Policía Nacional", which report directly to the Homeland Department of Spain. The national bodies retain personnel within Catalonia to exercise functions of national scope such as overseeing ports, airports, coasts, international borders, custom offices, the identification of documents and arms control amongst others.
Most of the justice system is administered by national judicial institutions. The criminal justice system is uniform throughout Spain, while "civil law" is administered separately within Catalonia.
Navarre, the Basque Country and Catalonia are the Spanish regions with the highest degree of autonomy in terms of law enforcement.
Administrative and territorial division.
Catalonia is organised territorially into provinces, further subdivided into comarques and municipalities. The 2006 Statute of Autonomy of Catalonia establishes the administrative organisation of three local authorities: vegueries, comarques, and municipalities.
Provinces.
Catalonia is divided administratively into four provinces, the governing body of which is the "Diputació" (Spanish: "Diputación"). The four provinces and their populations are:
Municipalities.
There are at present 947 municipalities in Catalonia.
Comarques.
Comarques (Shires) are entities composed by the municipalities to manage their responsibilities and services. The current regional division has its roots in a decree of the Generalitat de Catalunya of 1936, in effect until 1939, when it was suppressed by Franco. In 1987 the Government adopted the territorial division again and in 1988 three new comarques were added (Alta Ribagorça, Pla d'Urgell and Pla de l'Estany). At present there are 41.
The comarca of Val d'Aran (Aran Valley) has a special status and its autonomous government is named Conselh Generau d'Aran.
Vegueries.
The "vegueria" is a new type of division defined as a specific territorial area for the exercise of government and inter-local cooperation with legal personality. The current Statute of Autonomy states vegueries are intended to supersede provinces in Catalonia, and take over many of functions of the comarques.
The territorial plan of Catalonia ("Pla territorial general de Catalunya") provided six general functional areas, but was amended by Law 24/2001, of December 31, recognizing the Alt Pirineu i Aran as a new functional area differentiated of Ponent. On 14 July 2010 the Catalan Parliament approved the creation of the functional area of the Penedès.
Economy.
In 2008, the regional GDP of Catalonia was €216.9 billion ($314.4 billion), the highest in Spain, and per capita GDP was €28,750 – similar to that of countries such as the United Kingdom or France. It had the fourth per capita GDP in Spain, considerably behind the Basque Country (€34,100), Madrid (autonomous community) (€34,100) and Navarra (€32,900). In that year, the GDP growth was 3.7%.
In the last years there has been a negative net relocation rate of companies based in Catalonia moving to other autonomous communities of Spain. In 2014 Calalonia lost 987 companies to other parts of Spain (mainly Madrid), getting only 602 new ones from the rest of the country.
In the context of the 2008 financial crisis, Catalonia was expected to suffer a recession amounting to almost a 2% contraction of its regional GDP in 2009. Catalonia's debt in 2012 was the highest of all Spain's autonomous communities, reaching €13,476 million, i.e. 38% of the total debt of the 17 autonomous communities.
In 2011, Catalonia ranked the 64th largest country subdivision by GDP (nominal). Catalonia belongs to the organisation Four Motors for Europe.
The distribution of sectors is as follows:
The main tourist destinations in Catalonia are the city of Barcelona, the beaches of the Costa Brava in Girona, the beaches of Costa Barcelona from Mataró to Vilanova i la Geltrú and the Costa Daurada in Tarragona. In the Pyrenees there are several ski resorts, near Lleida
Many savings banks are based in Catalonia, with 10 of the 46 Spanish savings banks having headquarters in the region. This list includes Europe's premier savings bank, La Caixa. The first private bank in Catalonia is Banc Sabadell, ranked fourth among all Spanish private banks.
The stock market of Barcelona, which in 2004 traded almost €205,000 million, is the second largest of Spain after Madrid, and Fira de Barcelona organizes international exhibitions and congresses to do with different sectors of the economy.
The main economic cost for the Catalan families is the purchase of a home. According to data from the Society of Appraisal on the 31 December 2005 Catalonia is, after Madrid, the second most expensive region in Spain for housing: 3,397 €/m² on average (see Spanish property bubble).
Demographics.
Catalonia covers an area of 32114 km2 with an official population of 7,354,411 (2008), of which non-Spanish immigrants represent about 19% according to the Spanish Statistics Institute (INE) for 2012. About 66% of the population defines itself as Roman Catholic (45% non practicing, 18% practicing), 7% as Muslim, and 29% as atheist or agnostic.
The Urban Region of Barcelona includes 5,217,864 people and covers an area of 2.268 km2, and about 1.7 million people live in a radius of 15 km2 from Barcelona. The metropolitan area of the Urban Region includes cities such as L'Hospitalet de Llobregat, Sabadell, Terrassa, Badalona, Santa Coloma de Gramenet and Cornellà de Llobregat.
In 1900, the population of Catalonia was 1,984,115 people and in 1970 it was 5,107,606. That increase was due to the demographic boom in Spain during the 60s and early 70s and also to the large-scale internal migration from the rural interior of Spain to its industrial cities. In Catalonia that wave of internal migration arrived from several regions of Spain, especially Andalusia, Murcia and Extremadura.
Immigrants from other countries settled in Catalonia in the 1990s and 2000s; a large percentage came from Africa and Latin America, and smaller numbers from Asia and Eastern Europe, often settling in urban centers such as Barcelona and industrial areas.
Languages.
According to the linguistic census held by the Government of Catalonia in 2013, a plurality claims Spanish as "their own language" (46.53% Spanish compared to 37.26% Catalan). In everyday use, 11.95% of the population claim to use both languages equally, whereas 45.92% mainly use Spanish and 35.54% mainly use Catalan. There is a significant difference between the Barcelona metropolitan area (and, to a lesser extent, the Tarragona area), where Spanish is more spoken than Catalan, and the more rural Catalonia, where Catalan clearly prevails over Spanish.
Since the Statute of Autonomy of 1979, Aranese (a dialect of Gascon Occitan) has also been official and subject to special protection in Val d'Aran. This small area of 7,000 inhabitants was the only place where a dialect of Occitan has received full official status. Then, on 9 August 2006, when the new Statute came into force, Occitan became official throughout Catalonia. Occitan is the mother tongue of 22.4% of the population of Val d'Aran. Catalan Sign Language is also officially recognised.
Originating in the historic territory of Catalonia, Catalan has enjoyed special status since the approval of the Statute of Autonomy of 1979 which declares it to be "Catalonia's own language," a term which signifies a language given special legal status within a Spanish territory, or which is historically spoken within a given region. The other languages with official status are Spanish, which has official status throughout Spain, and Aranese Occitan, which enjoys co-official status with Catalan and Spanish in the Val d'Aran.
Although not considered an "official language" in the same way as Catalan, Spanish, and Aranese, Catalan Sign Language, with about 18,000 users in Catalonia, is granted official recognition and support: "The public authorities shall guarantee the use of Catalan sign language and conditions of equality for deaf people who choose to use this language, which shall be the subject of education, protection and respect."
Under the Franco dictatorship, Catalan was excluded from the public education system and all other official use, so that for example families were not allowed to officially register children with Catalan names. Although never completely banned, Catalan language publishing was severely restricted during the early 1940s, with only religious texts and small-run self-published texts being released. Some books were published clandestinely or circumvented the restrictions by showing publishing dates prior to 1936. This policy was changed in 1946, when unrestricted publishing in Catalan resumed.
Rural–urban migration originating in other parts of Spain also reduced the social use of Catalan in urban areas and increased the use of Spanish. Lately, a similar sociolinguistic phenomenon has occurred with foreign immigration. Catalan cultural activity increased in the 1960s and Catalan classes began thanks to the initiative of associations such as Òmnium Cultural.
After the end of Franco's dictatorship, the newly established self-governing democratic institutions in Catalonia embarked on a long-term language policy to increase the use of Catalan and has, since 1983, enforced laws which attempt to protect and extend the use of Catalan. This policy, known as the "linguistic normalisation" ("normalització lingüística" in Catalan, "normalización lingüística" in Spanish) has been supported by the vast majority of Catalan political parties through the last thirty years. Some groups consider these efforts a way to discourage the use of Spanish, whereas some others, including the Catalan government and the European Union consider the policies respectful, or even as an example which "should be disseminated throughout the Union".
Today, Catalan is the main language of the Catalan autonomous government and the other public institutions that fall under its jurisdiction. Basic public education is given in Catalan, except for two hours per week of Spanish medium instruction. Businesses are required to display all information (e.g. menus, posters) in Catalan under penalty of fines. There is no obligation to display this information in either Occitan or Spanish, although there is no restriction on doing so in these or other languages. The use of fines was introduced in a 1997 linguistic law that aims to increase the public use of Catalan and defend the rights of Catalan speakers.
The law ensures that both Catalan and Spanish – being official languages – can be used by the citizens without prejudice in all public and private activities, but primary education can only be taken in Catalan language. The Generalitat uses Catalan in its communications and notifications addressed to the general population, but citizens can also receive information from the Generalitat in Spanish if they so desire. Debates in the Catalan Parliament take place almost exclusively in Catalan and the Catalan public television broadcasts programs only in Catalan.
Due to the intense immigration which Spain in general and Catalonia in particular experienced in the first decade of the 21st century, many foreign languages are spoken in various cultural communities in Catalonia, of which Rif-Berber, Moroccan Arabic, Romanian and Urdu are the more common.
Recently, some of these policies have been criticised for trying to promote Catalan by imposing fines on businesses. For example, following the passage of a March 2010 law on Catalan cinema, which establishes that half of the movies shown in Catalan cinemas must be in Catalan, a general strike of 75% of the cinemas took place. These criticisms mostly come from outside Catalonia, especially from conservative, conservative liberal and classical liberal circles of Spanish society. In Catalonia, on the other hand, there is a high social and political consensus on the language policies favoring Catalan, also among Spanish speakers and speakers of other languages.
In Catalonia, the Catalan language policy has been challenged by some anti-nationalist intellectuals like Albert Boadella. Since 2006, the liberal Citizens - Party of the Citizenry has been one of the most consistent critics of the Catalan language policy within Catalonia. The local Catalan branch of the People's Party has a more ambiguous position on the issue: on one hand, it demands a bilingual Catalan–Spanish education and a more balanced language policy that would defend Catalan without favoring it over Spanish, whereas on the other hand, a few local PP politicians have supported in their municipalities measures privileging Catalan over Spanish and it has defended some aspects of the official language policies, sometimes against the positions of its colleagues from other parts of Spain.
Religion.
Islam.
There are now in the Spanish territory 1,703,529 Muslims, most of whom 783 137 people have Moroccan nationality, followed by a Spanish 525, 842 among which include the large number of Muslim converts that took place from the end of 70s and to this day and their descendants, Ceuta and Melilla and Moroccan nationalized and the descendants of these.
Andalusia is the second region with the highest number of 266,421 Muslims, behind Catalonia reaching almost half a million (448,879), according to the demographic study of this population developed by the Union of Islamic Communities in Spain (UCIDE). They are followed by the regions of Madrid (249,643), Valencia (176,053) and Murcia (86,275). By province would emphasize Barcelona, Madrid and Murcia and Ceuta and Melilla percentage.
The third group is constituted by the Pakistani importance to 79,626, which are mainly concentrated in Catalonia, and behind them are Senegalese with 63,491 and Algerians 62,432.
Muslims currently account for 3% of the Spanish population-more than 1.7 million people, of whom about 530,000 are Spanish, 30%, and 1.17 million foreigners. The municipalities with the largest number of Muslim citizens are Barcelona, Ceuta, Melilla and Madrid, followed by Badalona (Barcelona), Cartagena (Murcia), El Ejido (Almería), Malaga, Murcia, Terrassa (Barcelona), Valencia and Zaragoza.
Transport.
Airports.
Airports in Catalonia are owned and operated by Aena (a Spanish Government entity) except two airports in Lleida which are operated by Aeroports de Catalunya (an entity belonging to the Government of Catalonia).
Commercial and passenger ports.
The two main commercial and passenger ports in Catalonia are owned and operated by (a Spanish Government entity).
The other ports of Catalonia are operated and administered by Ports de la Generalitat, a Catalan Government entity.
Roads.
There are 12000 km of roads throughout Catalonia.
The principal highways are  AP-7  ("Autopista del Mediterrani") and  A-7  ("Autovia del Mediterrani"). It follows the coast from the French border to Valencia, Murcia and Andalusia. The main roads generally radiate from Barcelona. The A-2 and AP-2 connect inland and onward to Madrid.
Other major roads are:
Public-own roads in Catalonia are either managed by the autonomous government of Catalonia (e.g.,  C-  roads) or the Spanish Government (e.g.,  AP- ,  A- ,  N-  roads).
Railways.
Catalonia saw the first railway construction in the Iberian Peninsula in 1848, linking Barcelona with Mataró. Given the topography most lines radiate from Barcelona. The city has both suburban and inter-city services. The main east coast line runs through the province connecting with the SNCF (French Railways) at Portbou on the coast.
There are two publicly owned railway companies operating in Catalonia: the Catalan FGC that operates commuter and regional services, and the Spanish national RENFE that operates long-distance and high-speed rail services (AVE and Avant) and the main commuter and regional service "Rodalies de Catalunya", administered by the Catalan government since 2010.
High-speed rail (AVE) services from Madrid currently reach Lleida, Tarragona and Barcelona. The official opening between Barcelona and Madrid took place 20 February 2008. The journey between Barcelona and Madrid now takes about two-and-a-half hours. A connection to the French high-speed TGV network has been completed, but is awaiting the completion of stations along the route to begin passenger service in April 2013. This new line (currently the LGV Perpignan- Figueres Vilafant) passes through Girona and Figueres with a tunnel through the Pyrenees. There is a direct train from Barcelona Estació de França to Paris Austerlitz along the older railway tracks.
Culture.
Symbols of Catalonia.
Catalonia has its own representative and distinctive symbols such as:
UNESCO World Heritage Sites in Catalonia.
There are several UNESCO World Heritage Sites in Catalonia:
Popular culture.
Castells are one of the main manifestations of Catalan popular culture. The activity consists in constructing human towers by competing "colles castelleres" (teams). This practice originated in the southern part of Catalonia during the 18th century. The tradition of els Castells i els Castellers was declared Masterpiece of the Oral and Intangible Heritage of Humanity by UNESCO in 2010.
The sardana is the most characteristic Catalan popular dance, other groups also practice "Ball de bastons", "moixiganga", galops or "jota" in the southern part. Musically, the "Havaneres" are also characteristic in some marine localities of the Costa Brava especially during the summer months when these songs are sung outdoors accompanied by a "cremat" of burned rum. Other music styles are Catalan rumba, Catalan rock and Nova Cançó.
In the greater celebrations other elements of the Catalan popular culture are usually present: the parades of "gegants" (giants) and "correfocs" of devils and firecrackers. Another traditional celebration in Catalonia is "La Patum de Berga" declared Masterpiece of the Oral and Intangible Heritage of Humanity by UNESCO in 25 November 2005.
In addition to traditional local Catalan culture, traditions from other parts of Spain can be found as a result of migration from other regions. On July 28, 2010, Catalonia became the second Spanish territory, after the Canary Islands, to forbid bullfighting. The ban, which went into effect on January 1, 2012, had originated in a popular petition supported by over 180,000 signatures.

</doc>
<doc id="6823" url="http://en.wikipedia.org/wiki?curid=6823" title="Constantine Kanaris">
Constantine Kanaris

Constantine Kanaris or Canaris (Greek: Κωνσταντίνος Κανάρης; 1793 or 1795 – September 2, 1877) was a Greek Prime Minister, admiral and politician who in his youth was a freedom fighter in the Greek War of Independence.
Early life.
He was born and grew up on the island of Psara, close to the island of Chios, in the Aegean. His exact year of birth is unknown. The official records of the Hellenic Navy indicate 1795 but modern Greek historians believe that 1793 is more probable.
Constantine was left an orphan at a young age. Having to support himself, he chose to became a seaman like most members of his family since the beginning of the 18th century. He was hired as a boy on the brig of his uncle Dimitris Bourekas.
Military career.
Constantine gained his fame during the Greek War of Independence (1821–1829). Unlike most other prominent figures of the War, he had never been initiated into the Filiki Eteria (Friendly Society), which played a significant role in the revolution against the Ottoman Empire, primarily by secret recruitment of supporters against the Empire.
By early 1821, it had gained enough support to declare a revolution. This declaration seems to have surprised Constantine, who was absent at Odessa. He returned to Psara in haste and was there when the island joined the Revolution on April 10, 1821.
The island formed its own fleet of ships and the famed seamen of Psara, already known for their successful naval combats against pirates and their well-equipped ships, proved to be effective at full naval war. Constantine soon distinguished himself as a fire ship captain.
At Chios, on the moonless night of June 6/June 7, 1822 forces under his command destroyed the flagship of the Turkish admiral Nasuhzade Ali Pasha (or Kara-Ali Pasha) in revenge for the Chios Massacre. The admiral was holding a celebration (Bayram), so Kanaris and his men managed to place a fire ship next to it without being noticed. When the flagship's powder store caught fire, all men aboard were instantly killed. The Ottoman casualties comprised 2000 men, both naval officers and common sailors, as well as Kara-Ali himself.
Constantine led further successful attacks against the Turkish fleet, at Tenedos in November 1822 and at Samos in August 1824. He was famously said to have encouraged himself by murmuring "Konstantí, you are going to die" every time he was approaching a Turkish warship on the fire boat he was about to detonate.
Egypt was technically a province of the Ottoman Empire at the time but its viceroy Mohammad Ali (1769–1849), had earned enough power to act independently from the Sultan and had formed his own army and naval fleet. It was headed by his adoptive son Ibrahim Pasha (1789–1848). The latter had hired a number of veteran French officers - who had served under Emperor Napoleon I and were discharged from the French army following his defeat - to help organise the new army. By 1824, it counted 100,000 men and was both better organised and better equipped than the Sultan's army.
Sultan Mahmud II offered to the viceroy the command of Crete, if he agreed to send part of this army against the Greeks. They quickly reached an agreement. The Egyptian army, under the personal command of Ibrahim Pasha, started a campaign in both land and sea against the Greeks.
The Turkish fleet captured Psara on June 21, 1824. A part of the population managed to flee the island, but those who didn't were either sold into slavery or slaughtered. The island was deserted and surviving islanders were scattered through what is now Southern Greece (see Destruction of Psara).
After the destruction of his home island, Constantine continued to lead his men into attacks against the Turks, until the Battle of Navarino of October 20, 1827. Then the Turkish-Egyptian fleet was destroyed by the combined naval forces of Britain, France and Russia.
Following the end of the war and the independence of Greece, Constantine became an officer of the new Greek Navy, reaching the rank of Admiral, and later became a prominent politician.
Political career.
Constantine Kanaris was one of the few with the personal confidence of Ioannis Kapodistrias the first Head of State of independent Greece. Kanaris served as Minister in various governments and then as Prime Minister, in the provisional government, from March 11-April 11, 1844. He served a second term (October 27, 1848 – December 24, 1849), and as Navy Minister in Mavrokordatos' 1854 cabinet.
In 1862, he was one of the few War of Independence veterans that helped in the bloodless revolution that deposed King Otto of Greece and put Prince William of Denmark on the Greek throne as King George I of Greece. Under George I, he served as a prime minister for a third term (March 17 – April 28, 1864), fourth term (August 7, 1864 – February 9, 1865) and fifth and last term (June 7 – September 14, 1877).
Kanaris died on 2 September 1877 whilst still serving in office as Prime Minister. Following his death his government remained in power until September 14, 1877 without agreeing on a replacement at its head. He was buried in the First Cemetery of Athens, where most Greek prime ministers and celebrated figures are also buried. After his death he was honored as a national hero.
To honour Kanaris, three ships of the Hellenic Navy have been named after him;
Family.
In 1817, he married Despina Maniatis, from a historical family of Psara. They had seven children:
Wilhelm Canaris, a German Admiral, speculated that he might be a descendant of Constantine Kanaris. An official genealogical family history that was researched in 1938 showed that he was unrelated and that his family was from Italy.

</doc>
<doc id="6824" url="http://en.wikipedia.org/wiki?curid=6824" title="Carl Sagan">
Carl Sagan

Carl Edward Sagan (; November 9, 1934 – December 20, 1996) was an American astronomer, cosmologist, astrophysicist, astrobiologist, author, science popularizer, and science communicator in astronomy and other natural sciences. His contributions were central to the discovery of the high surface temperatures of Venus. However, he is best known for his contributions to the scientific research of extraterrestrial life, including experimental demonstration of the production of amino acids from basic chemicals by radiation. Sagan assembled the first physical messages that were sent into space: the Pioneer plaque and the Voyager Golden Record, universal messages that could potentially be understood by any extraterrestrial intelligence that might find them.
He published more than 600 scientific papers and articles and was author, co-author or editor of more than 20 books. Sagan is known for many of his popular science books, such as "The Dragons of Eden", "Broca's Brain" and "Pale Blue Dot", and for the award-winning 1980 television series "", which he narrated and co-wrote. The most widely watched series in the history of American public television, "Cosmos" has been seen by at least 500 million people across 60 different countries. The book "Cosmos" was published to accompany the series. He also wrote the science fiction novel "Contact", the basis for a 1997 film of the same name.
Sagan always advocated scientific skeptical inquiry and the scientific method, pioneered exobiology and promoted the Search for Extra-Terrestrial Intelligence (SETI). He spent most of his career as a professor of astronomy at Cornell University, where he directed the Laboratory for Planetary Studies. Sagan and his works received numerous awards and honors, including the NASA Distinguished Public Service Medal, the National Academy of Sciences Public Welfare Medal, the Pulitzer Prize for General Non-Fiction for his book "The Dragons of Eden", and, regarding "", two Emmy Awards, the Peabody Award and the Hugo Award. He married three times and had five children. After suffering from myelodysplasia, Sagan died of pneumonia at the age of 62 on December 20, 1996.
Early life.
Carl Sagan was born in Brooklyn, New York. His father, Samuel Sagan, was an immigrant garment worker from Kamianets-Podilskyi, then Russian Empire, in today's Ukraine. His mother, Rachel Molly Gruber, was a housewife from New York. Carl was named in honor of Rachel's biological mother, Chaiya Clara, in Sagan's words, "the mother she never knew."
He had a sister, Carol, and the family lived in a modest apartment near the Atlantic Ocean, in Bensonhurst, a Brooklyn neighborhood. According to Sagan, they were Reform Jews, the most liberal of North American Judaism's four main groups. Both Sagan and his sister agreed that their father was not especially religious, but that their mother "definitely believed in God, and was active in the temple ... and served only kosher meat.":12 During the depths of the Depression, his father worked as a theater usher.
According to biographer Keay Davidson, Sagan's "inner war" was a result of his close relationship with both of his parents, who were in many ways "opposites." Sagan traced his later analytical urges to his mother, a woman who had known "extreme poverty as a child" in New York City during World War I and the 1920s.:2 As a young woman she had held her own intellectual ambitions, but they were frustrated by social restrictions: her poverty, her status as a woman and a wife, and her Jewish ethnicity. Davidson notes that she therefore "worshipped her only son, Carl. He would fulfill her unfulfilled dreams.":2
However, his "sense of wonder" came from his father, who was a "quiet and soft-hearted escapee from the Czar." In his free time he gave apples to the poor or helped soothe labor-management tensions within New York's "tumultuous" garment industry.:2 Although he was "awed" by Carl's "brilliance, his boyish chatter about stars and dinosaurs," he took his son's inquisitiveness in stride and saw it as part of his growing up.:2 In his later years as a writer and scientist, Sagan would often draw on his childhood memories to illustrate scientific points, as he did in his book, "Shadows of Forgotten Ancestors".:9 Sagan describes his parents' influence on his later thinking:
1939 World's Fair.
Sagan recalls that one of his most defining moments was when his parents took him to the 1939 New York World's Fair when he was four years old. The exhibits became a turning point in his life. He later recalled the moving map of the "America of Tomorrow" exhibit: "It showed beautiful highways and cloverleaves and little General Motors cars all carrying people to skyscrapers, buildings with lovely spires, flying buttresses—and it looked great!":14 At other exhibits, he remembered how a flashlight that shone on a photoelectric cell created a crackling sound, and how the sound from a tuning fork became a wave on an oscilloscope. He also witnessed the future media technology that would replace radio: television. Sagan wrote:
He also saw one of the Fair's most publicized events, the burial of a time capsule at Flushing Meadows, which contained mementos of the 1930s to be recovered by Earth's descendants in a future millennium. "The time capsule thrilled Carl," writes Davidson. As an adult, Sagan and his colleagues would create similar time capsules—ones that would be sent out into the galaxy; these were the Pioneer plaque and the "Voyager Golden Record" records, all of which were spinoffs of Sagan's memories of the World's Fair.:15
World War II.
During World War II Sagan's family worried about the fate of their European relatives. Sagan, however, was generally unaware of the details of the ongoing war. He writes, "Sure, we had relatives who were caught up in the Holocaust. Hitler was not a popular fellow in our household... But on the other hand, I was fairly insulated from the horrors of the war." His sister, Carol, said that their mother "above all wanted to protect Carl... She had an extraordinarily difficult time dealing with World War II and the Holocaust.":15 Sagan's book, "The Demon-Haunted World" (1996), included his memories of this conflicted period, when his family dealt with the realities of the war in Europe but tried to prevent it from undermining his optimistic spirit.
Inquisitiveness about nature.
Soon after entering elementary school he began to express a strong inquisitiveness about nature. Sagan recalled taking his first trips to the public library alone, at the age of five, when his mother got him a library card. He wanted to learn what stars were, since none of his friends or their parents could give him a clear answer:
At about age six or seven, he and a close friend took trips to the American Museum of Natural History in New York City. While there, they went to the Hayden Planetarium and walked around the museum's exhibits of space objects, such as meteorites, and displays of dinosaurs and animals in natural settings. Sagan writes about those visits:
His parents helped nurture his growing interest in science by buying him chemistry sets and reading materials. His interest in space, however, was his primary focus, especially after reading science fiction stories by writers such as H. G. Wells and Edgar Rice Burroughs, which stirred his imagination about life on other planets such as Mars. According to biographer Ray Spangenburg, these early years as Sagan tried to understand the mysteries of the planets became a "driving force in his life, a continual spark to his intellect, and a quest that would never be forgotten."
In 1947 he discovered "Astounding Science Fiction" magazine, which introduced him to more hard science fiction speculations than those in Burroughs's novels. That same year inaugurated the "flying saucer" mass hysteria with the young Carl suspecting the "discs" might be alien spaceships.
High school years.
Sagan had lived in Bensonhurst where he went to David A. Boody Junior High School. He had his bar mitzvah in Bensonhurst when he turned 13.:23 The following year, 1948, his family moved to the nearby town of Rahway, New Jersey for his father's work, where Sagan then entered Rahway High School. He graduated in 1951.:23 Rahway was an older industrial town, and the Sagans were among its few Jewish families. Today the town is best known as the site of a state prison.:23
Sagan was a straight-A student but was bored due to unchallenging classes and uninspiring teachers.:23 His teachers realized this and tried to convince his parents to send him to a private school, the administrator telling them, "This kid ought to go to a school for gifted children, he has something really remarkable.":24 This they couldn't do, partly because of the cost. 
Sagan was made president of the school's chemistry club, and at home he set up his own laboratory. He taught himself about molecules by making cardboard cutouts to help him visualize how molecules were formed: "I found that about as interesting as doing [chemical] experiments," he said.:24 Sagan remained mostly interested in astronomy as a hobby, and in his junior year made it a career goal after he learned that astronomers were paid for doing what he always enjoyed: "That was a splendid day­­­­—when I began to suspect that if I tried hard I could do astronomy full-time, not just part-time.":25
Education and scientific career.
He attended the University of Chicago, where he participated in the Ryerson Astronomical Society, received a B.A. degree in self-proclaimed "nothing" with general and special honors in 1954, a B.S. degree in physics in 1955, and an M.S. degree in physics in 1956, before earning a Ph.D. degree in 1960 with the dissertation "Physical Studies of Planets" submitted to the Department of Astronomy and Astrophysics. The title of Sagan's thesis reflects his shared interests with Gerard Kuiper, his dissertation director, who throughout the 1950s had been president of the International Astronomical Union's commission on "Physical Studies of Planets and Satellites".
During his time as an honors program undergraduate, Sagan worked in the laboratory of the geneticist H. J. Muller and wrote a thesis on the origins of life with physical chemist H. C. Urey. He used the summer months of his graduate studies to work with planetary scientist Gerard Kuiper, physicist George Gamow, and chemist Melvin Calvin. From 1960 to 1962 Sagan was a Miller Fellow at the University of California, Berkeley. From 1962 to 1968, he worked at the Smithsonian Astrophysical Observatory in Cambridge, Massachusetts. At the same time, he worked with geneticist Joshua Lederberg.
Sagan lectured and did research at Harvard University until 1968, when he moved to Cornell University in Ithaca, New York, after being denied tenure at Harvard. It has been suggested that Sagan was denied tenure in part because of his publicized scientific advocacy, which some scientists perceived as being self-promotion; Harold Urey wrote a letter to the tenure committee recommending against tenure for Sagan.
He became a full professor at Cornell in 1971, and directed the Laboratory for Planetary Studies there. From 1972 to 1981, Sagan was associate director of the Center for Radiophysics and Space Research (CRSR) at Cornell.
Sagan was associated with the U.S. space program from its inception. From the 1950s onward, he worked as an advisor to NASA, where one of his duties included briefing the Apollo astronauts before their flights to the Moon. Sagan contributed to many of the robotic spacecraft missions that explored the Solar System, arranging experiments on many of the expeditions. He conceived the idea of adding an unalterable and universal message on spacecraft destined to leave the Solar System that could potentially be understood by any extraterrestrial intelligence that might find it. Sagan assembled the first physical message that was sent into space: a gold-anodized plaque, attached to the space probe Pioneer 10, launched in 1972. Pioneer 11, also carrying another copy of the plaque, was launched the following year. He continued to refine his designs; the most elaborate message he helped to develop and assemble was the Voyager Golden Record that was sent out with the Voyager space probes in 1977. Sagan often challenged the decisions to fund the Space Shuttle and the International Space Station at the expense of further robotic missions.
Sagan taught a course on critical thinking at Cornell University until he died in 1996 from pneumonia, a few months after finding that he was in remission of myelodysplastic syndrome.
Scientific achievements.
Former student David Morrison describes Sagan as "an 'idea person' and a master of intuitive physical arguments and 'back of the envelope' calculations," and Gerard Kuiper said that "Some persons work best in specializing on a major program in the laboratory; others are best in liaison between sciences. Dr. Sagan belongs in the latter group."
Sagan's contributions were central to the discovery of the high surface temperatures of the planet Venus. In the early 1960s no one knew for certain the basic conditions of that planet's surface, and Sagan listed the possibilities in a report later depicted for popularization in a Time–Life book, "Planets". His own view was that Venus was dry and very hot as opposed to the balmy paradise others had imagined. He had investigated radio emissions from Venus and concluded that there was a surface temperature of 500 °C. As a visiting scientist to NASA's Jet Propulsion Laboratory, he contributed to the first Mariner missions to Venus, working on the design and management of the project. Mariner 2 confirmed his conclusions on the surface conditions of Venus in 1962.
Sagan was among the first to hypothesize that Saturn's moon Titan might possess oceans of liquid compounds on its surface and that Jupiter's moon Europa might possess subsurface oceans of water. This would make Europa potentially habitable. Europa's subsurface ocean of water was later indirectly confirmed by the spacecraft "Galileo". The mystery of Titan's reddish haze was also solved with Sagan's help. The reddish haze was revealed to be due to complex organic molecules constantly raining down onto Titan's surface.
He further contributed insights regarding the atmospheres of Venus and Jupiter as well as seasonal changes on Mars. He also perceived global warming as a growing, man-made danger and likened it to the natural development of Venus into a hot, life-hostile planet through a kind of runaway greenhouse effect. Sagan and his Cornell colleague Edwin Ernest Salpeter speculated about life in Jupiter's clouds, given the planet's dense atmospheric composition rich in organic molecules. He studied the observed color variations on Mars' surface and concluded that they were not seasonal or vegetational changes as most believed but shifts in surface dust caused by windstorms.
Sagan is best known, however, for his research on the possibilities of extraterrestrial life, including experimental demonstration of the production of amino acids from basic chemicals by radiation.
He is also the 1994 recipient of the Public Welfare Medal, the highest award of the National Academy of Sciences for "distinguished contributions in the application of science to the public welfare". He was denied membership in the Academy, reportedly because his media activities made him unpopular with many other scientists.
Scientific and critical thinking advocacy.
Sagan's ability to convey his ideas allowed many people to understand the cosmos better—simultaneously emphasizing the value and worthiness of the human race, and the relative insignificance of the Earth in comparison to the Universe. He delivered the 1977 series of Royal Institution Christmas Lectures in London. He hosted and, with Ann Druyan, co-wrote and co-produced the highly popular thirteen-part Public Broadcasting Service (PBS) television series "".
"Cosmos" covered a wide range of scientific subjects including the origin of life and a perspective of our place in the Universe. The series was first broadcast by PBS in 1980, winning an Emmy and a Peabody Award. It has been broadcast in more than 60 countries and seen by over 500 million people, making it the most widely watched PBS program in history. In addition, "Time" magazine ran a cover story about Sagan soon after the show broadcast, referring to him as "creator, chief writer and host-narrator of the new public television series Cosmos, [and] takes the controls of his fantasy spaceship". However, Sagan was criticized for putting too much attention into the series, with several of his classes at Cornell being cancelled and complaints from his colleagues.
Sagan was a proponent of the search for extraterrestrial life. He urged the scientific community to listen with radio telescopes for signals from potential intelligent extraterrestrial life-forms. Sagan was so persuasive that by 1982 he was able to get a petition advocating SETI published in the journal "Science" and signed by 70 scientists, including seven Nobel Prize winners. This was a tremendous increase in the respectability of this controversial field. Sagan also helped Frank Drake write the Arecibo message, a radio message beamed into space from the Arecibo radio telescope on November 16, 1974, aimed at informing potential extraterrestrials about Earth.
Sagan was chief technology officer of the professional planetary research journal "Icarus" for twelve years. He co-founded The Planetary Society, the largest space-interest group in the world, with over 100,000 members in more than 149 countries, and was a member of the SETI Institute Board of Trustees. Sagan served as Chairman of the Division for Planetary Science of the American Astronomical Society, as President of the Planetology Section of the American Geophysical Union, and as Chairman of the Astronomy Section of the American Association for the Advancement of Science (AAAS).
At the height of the Cold War, Sagan became involved in public awareness efforts for the effects of nuclear war when a 1982 mathematical climate model, titled "Twilight at Noon" suggested that a substantial nuclear exchange could trigger a nuclear twilight and upset the delicate balance of life on Earth by cooling the surface. In 1983 he was one of five authors—the "S"—in the follow-up "TTAPS" report, as the research paper came to be known, which contained the term "nuclear winter" for the first time, a term coined by his colleague Richard P. Turco. In 1984 he co-authored the book "" and in 1990 he co-authored the book "A Path Where No Man Thought: Nuclear Winter and the End of the Arms Race", which explains the nuclear winter hypothesis and with that advocates nuclear disarmament.
Sagan also wrote books to popularize science, such as "Cosmos", which reflected and expanded upon some of the themes of "A Personal Voyage" and became the best-selling science book ever published in English; "The Dragons of Eden: Speculations on the Evolution of Human Intelligence", which won a Pulitzer Prize; and "Broca's Brain: Reflections on the Romance of Science". Sagan also wrote the best-selling science fiction novel "Contact" in 1985, based on a film treatment he wrote with his wife in 1979, but he did not live to see the book's 1997 motion picture adaptation, which starred Jodie Foster and won the 1998 Hugo Award for Best Dramatic Presentation.
He wrote a sequel to "Cosmos", "Pale Blue Dot: A Vision of the Human Future in Space", which was selected as a notable book of 1995 by "The New York Times". He appeared on PBS' "Charlie Rose" program in January 1995. Sagan also wrote the introduction for Stephen Hawking's bestseller, "A Brief History of Time". Sagan was also known for his popularization of science, his efforts to increase scientific understanding among the general public, and his positions in favor of scientific skepticism and against pseudoscience, such as his debunking of the Betty and Barney Hill abduction. To mark the tenth anniversary of Sagan's death, David Morrison, a former student of Sagan's, recalled "Sagan's immense contributions to planetary research, the public understanding of science, and the skeptical movement" in "Skeptical Inquirer".
Following Saddam Hussein's threats to light Kuwait's oil wells on fire in response to any physical challenge to Iraqi control of the oil assets, Sagan and his nuclear winter/"TTAPS" colleagues warned in January 1991 in the Baltimore Sun and Wilmington Morning Star newspapers that if the fires were left to burn over a period of several months, enough smoke from the 600 or so 1991 Kuwaiti oil fires "might get so high as to disrupt agriculture in much of South Asia ..." and that this possibility should "affect the war plans"; these claims were also the subject of a televised debate between Sagan and physicist Fred Singer on 22 January, aired on the ABC News program "Nightline". In the televised debate, Sagan argued that the effects of the smoke would be similar to the effects of a nuclear winter, with Singer arguing to the contrary. After the debate, the fires burnt for many months before extinguishing efforts were complete, the results of the smoke did not produce continental sized cooling. Sagan later conceded in "The Demon-Haunted World" that the prediction did not turn out to be correct: "it "was" pitch black at noon and temperatures dropped 4°–6 °C over the Persian Gulf, but not much smoke reached stratospheric altitudes and Asia was spared".
In his later years Sagan advocated the creation of an organized search for near-Earth objects (NEO) that might impact the Earth. When others suggested creating large nuclear bombs that could be used to alter the orbit of a NEO that was predicted to hit the Earth, Sagan proposed the Deflection Dilemma: If we create the ability to deflect an asteroid away from the Earth, then we may also create the ability to deflect an asteroid towards the Earth—providing an evil power with a true doomsday bomb. His interest in the use of nuclear weapons in space grew out of his work in 1958 for the Armour Research Foundation's Project A119, concerning the possibility of detonating a nuclear device on the Lunar surface.
Sagan was a critic of Plato. Sagan said of Plato: "Science and mathematics were to be removed from the hands of the merchants and the artisans. This tendency found its most effective advocate in a follower of Pythagoras named Plato."
and "He (Plato) believed that ideas were far more real than the natural world. He advised the astronomers not to waste their time observing the stars and planets. It was better, he believed, just to think about them. Plato expressed hostility to observation and experiment. He taught contempt for the real world and disdain for the practical application of scientific knowledge. Plato's followers succeeded in extinguishing the light of science and experiment that had been kindled by Democritus and the other Ionians."
Popularizing science.
Speaking about his activities in popularizing science, Sagan said that there were at least two reasons for scientists to explain what science is about. Naked self-interest was one because much of the funding for science came from the public, and the public had a right to know how their money was being spent. If scientists increased public excitement about science, there was a good chance of having more public supporters. The other reason was the excitement of communicating one's own excitement about science to others.
"Billions and billions".
From "Cosmos" and his frequent appearances on "The Tonight Show Starring Johnny Carson", Sagan became associated with the catchphrase "billions and billions". Sagan said that he never actually used the phrase in the "Cosmos" series. The closest that he ever came was in the book "Cosmos", where he talked of "billions "upon" billions":A galaxy is composed of gas and dust and stars—billions upon billions of stars.
(Richard Feynman, a precursor to Sagan, is observed to use the phrase "billions and billions" many times in his "red books".) However, Sagan's frequent use of the word "billions", and distinctive delivery emphasizing the "b" (which he did intentionally, in place of more cumbersome alternatives such as "billions with a 'b'", in order to distinguish the word from "millions"), made him a favorite target of comic performers, including Johnny Carson, Gary Kroeger, Mike Myers, Bronson Pinchot, Penn Jillette, Harry Shearer, and others. Frank Zappa satirized the line in the song "Be in My Video", noting as well "atomic light". Sagan took this all in good humor, and his final book was entitled "", which opened with a tongue-in-cheek discussion of this catchphrase, observing that Carson was an amateur astronomer and that Carson's comic caricature often included real science.
He is also known for expressing wonderment at the vastness of space and time, as in his phrase "The total number of stars in the Universe is larger than all the grains of sand on all the beaches of the planet Earth."
As a humorous tribute to Sagan and his association with the catchphrase "billions and billions", a "sagan" has been defined as a unit of measurement equivalent to a very large number of anything.
Social concerns.
Sagan believed that the Drake equation, on substitution of reasonable estimates, suggested that a large number of extraterrestrial civilizations would form, but that the lack of evidence of such civilizations highlighted by the Fermi paradox suggests technological civilizations tend to self-destruct. This stimulated his interest in identifying and publicizing ways that humanity could destroy itself, with the hope of avoiding such a cataclysm and eventually becoming a spacefaring species. Sagan's deep concern regarding the potential destruction of human civilization in a nuclear holocaust was conveyed in a memorable cinematic sequence in the final episode of "Cosmos", called "Who Speaks for Earth?" Sagan had already resigned[] from the Air Force Scientific Advisory Board's UFO investigating Condon Committee and voluntarily surrendered his top secret clearance in protest over the Vietnam War. Following his marriage to his third wife (novelist Ann Druyan) in June 1981, Sagan became more politically active—particularly in opposing escalation of the nuclear arms race under President Ronald Reagan.
In March 1983, Reagan announced the Strategic Defense Initiative—a multi-billion dollar project to develop a comprehensive defense against attack by nuclear missiles, which was quickly dubbed the "Star Wars" program. Sagan spoke out against the project, arguing that it was technically impossible to develop a system with the level of perfection required, and far more expensive to build such a system than it would be for an enemy to defeat it through decoys and other means—and that its construction would seriously destabilize the "nuclear balance" between the United States and the Soviet Union, making further progress toward nuclear disarmament impossible.
When Soviet leader Mikhail Gorbachev declared a unilateral moratorium on the testing of nuclear weapons, which would begin on August 6, 1985—the 40th anniversary of the atomic bombing of Hiroshima—the Reagan administration dismissed the dramatic move as nothing more than propaganda, and refused to follow suit. In response, US anti-nuclear and peace activists staged a series of protest actions at the Nevada Test Site, beginning on Easter Sunday in 1986 and continuing through 1987. Hundreds of people in the "Nevada Desert Experience" group were arrested, including Sagan, who was arrested on two separate occasions as he climbed over a chain-link fence at the test site during the underground Operation Charioteer and United States's Musketeer nuclear test series of detonations.
Sagan was also a vocal advocate of the controversial notion of "Testosterone poisoning", arguing in 1992 that human males could become gripped by an "usually severe [case of] testosterone poisoning" and this could compel them to become genocidal. In his review of Moondance magazine writer Daniela Gioseffi's 1990 book "Women on War", he argues that females are the only "half" of humanity "untainted by testosterone poisoning". One chapter of his 1993 book, "Shadows of Forgotten Ancestors" is dedicated to testosterone and its alleged poisonous effects.
Personal life and beliefs.
Sagan was married three times. In 1957, he married biologist Lynn Margulis, mother of Dorion Sagan and Jeremy Sagan. After Sagan and Margulis divorced, he married artist Linda Salzman in 1968, mother of Nick Sagan. During these marriages, Sagan focused heavily on his career, a factor which may have contributed to Sagan's first divorce. In 1981, Sagan married author Ann Druyan, mother of Alexandra Rachel (Sasha) Sagan and Samuel Democritus Sagan. Sagan and Druyan remained married until his death in 1996.
Isaac Asimov described Sagan as one of only two people he ever met whose intellect surpassed his own. The other, he claimed, was the computer scientist and artificial intelligence expert Marvin Minsky.
Sagan wrote frequently about religion and the relationship between religion and science, expressing his skepticism about the conventional conceptualization of God as a sapient being. For example: Some people think God is an outsized, light-skinned male with a long white beard, sitting on a throne somewhere up there in the sky, busily tallying the fall of every sparrow. Others—for example Baruch Spinoza and Albert Einstein—considered God to be essentially the sum total of the physical laws which describe the universe. I do not know of any compelling evidence for anthropomorphic patriarchs controlling human destiny from some hidden celestial vantage point, but it would be madness to deny the existence of physical laws.
In another description of his view on the concept of God, Sagan emphatically writes: The idea that God is an oversized white male with a flowing beard who sits in the sky and tallies the fall of every sparrow is ludicrous. But if by God one means the set of physical laws that govern the universe, then clearly there is such a God. This God is emotionally unsatisfying ... it does not make much sense to pray to the law of gravity.
On atheism, Sagan commented in 1981: An atheist is someone who is certain that God does not exist, someone who has compelling evidence against the existence of God. I know of no such compelling evidence. Because God can be relegated to remote times and places and to ultimate causes, we would have to know a great deal more about the universe than we do now to be sure that no such God exists. To be certain of the existence of God and to be certain of the nonexistence of God seem to me to be the confident extremes in a subject so riddled with doubt and uncertainty as to inspire very little confidence indeed.
Sagan also commented on Christianity, stating "My long-time view about Christianity is that it represents an amalgam of two seemingly immiscible parts, the religion of Jesus and the religion of Paul. Thomas Jefferson attempted to excise the Pauline parts of the New Testament. There wasn't much left when he was done, but it was an inspiring document."
Regarding the relationship between spirituality and science, Sagan stated: "Science is not only compatible with spirituality; it is a profound source of spirituality. When we recognize our place in an immensity of light-years and in the passage of ages, when we grasp the intricacy, beauty, and subtlety of life, then that soaring feeling, that sense of elation and humility combined, is surely spiritual."
An environmental appeal, "Preserving and Cherishing the Earth", signed by Sagan with other noted scientists in January 1990, stated that "The historical record makes clear that religious teaching, example, and leadership are powerfully able to influence personal conduct and commitment... Thus, there is a vital role for religion and science."
In reply to a question in 1996 about his religious beliefs, Sagan answered, "I'm agnostic." Sagan maintained that the idea of a creator God of the Universe was difficult to prove or disprove and that the only conceivable scientific discovery that could challenge it would be an infinitely old universe. Sagan's views on religion have been interpreted as a form of pantheism comparable to Einstein's belief in Spinoza's God. His son, Dorion Sagan said, "My father believed in the God of Spinoza and Einstein, God not behind nature but as nature, equivalent to it." His last wife, Ann Druyan, stated: When my husband died, because he was so famous and known for not being a believer, many people would come up to me—it still sometimes happens—and ask me if Carl changed at the end and converted to a belief in an afterlife. They also frequently ask me if I think I will see him again. Carl faced his death with unflagging courage and never sought refuge in illusions. The tragedy was that we knew we would never see each other again. I don't ever expect to be reunited with Carl.
In 2006, Ann Druyan edited Sagan's 1985 Glasgow "Gifford Lectures in Natural Theology" into a book, "", in which he elaborates on his views of divinity in the natural world.
Sagan is also widely regarded as a freethinker or skeptic; one of his most famous quotations, in "Cosmos", was, "Extraordinary claims require extraordinary evidence" (called the "Sagan Standard" by some). This was based on a nearly identical statement by fellow founder of the Committee for the Scientific Investigation of Claims of the Paranormal, Marcello Truzzi, "An extraordinary claim requires extraordinary proof." This idea had been earlier aphorized in Théodore Flournoy's work "From India to the Planet Mars" (1899) from a longer quote by Pierre-Simon Laplace (1749–1827), a French mathematician and astronomer, as the Principle of Laplace: "The weight of the evidence should be proportioned to the strangeness of the facts."
Late in his life, Sagan's books elaborated on his skeptical, naturalistic view of the world. In "The Demon-Haunted World", he presented tools for testing arguments and detecting fallacious or fraudulent ones, essentially advocating wide use of critical thinking and the scientific method. The compilation "Billions and Billions: Thoughts on Life and Death at the Brink of the Millennium", published in 1997 after Sagan's death, contains essays written by Sagan, such as his views on abortion, and his widow Ann Druyan's account of his death as a skeptic, agnostic, and freethinker.
Sagan warned against humans' tendency towards anthropocentrism. He was the faculty adviser for the Cornell Students for the Ethical Treatment of Animals. In the "Cosmos" chapter "Blues For a Red Planet", Sagan wrote, "If there is life on Mars, I believe we should do nothing with Mars. Mars then belongs to the Martians, even if the Martians are only microbes."
Sagan was a user and advocate of marijuana. Under the pseudonym "Mr. X", he contributed an essay about smoking cannabis to the 1971 book "Marihuana Reconsidered". The essay explained that marijuana use had helped to inspire some of Sagan's works and enhance sensual and intellectual experiences. After Sagan's death, his friend Lester Grinspoon disclosed this information to Sagan's biographer, Keay Davidson. The publishing of the biography, "Carl Sagan: A Life", in 1999 brought media attention to this aspect of Sagan's life. Not long after his death, widow Ann Druyan had gone on to preside over the board of directors of the National Organization for the Reform of Marijuana Laws (NORML), a non-profit organization dedicated to reforming cannabis laws.
In 1994, engineers at Apple Computer code-named the Power Macintosh 7100 "Carl Sagan" in the hope that Apple would make "billions and billions" with the sale of the PowerMac 7100. The name was only used internally, but Sagan was concerned that it would become a product endorsement and sent Apple a cease-and-desist letter. Apple complied, but engineers retaliated by changing the internal codename to "BHA" for “Butt-Head Astronomer”. Sagan then sued Apple for libel, a form of defamation, in federal court. The court granted Apple's motion to dismiss Sagan's claims and opined in dicta that a reader aware of the context would understand Apple was "clearly attempting to retaliate in a humorous and satirical way", and that “It strains reason to conclude that Defendant was attempting to criticize Plaintiff's reputation or competency as an astronomer. One does not seriously attack the expertise of a scientist using the undefined phrase ‘butt-head’.” Sagan then sued for Apple's original use of his name and likeness, but again lost. Sagan appealed the ruling. In November 1995, an out-of-court settlement was reached and Apple's office of trademarks and patents released a conciliatory statement that “Apple has always had great respect for Dr. Sagan. It was never Apple's intention to cause Dr. Sagan or his family any embarrassment or concern.” Apple's third and final code name for the project was "LAW", short for "Lawyers are Wimps".
Sagan briefly served as an adviser on Stanley Kubrick's film "".:168 Sagan proposed that the film suggest, rather than depict, extraterrestrial superintelligence.
Sagan and UFOs.
In 1947, the year that inaugurated the "flying saucer" craze, the young Sagan suspected the "discs" might be alien spaceships.
Sagan's interest in UFO reports prompted him on August 3, 1952, to write a letter to U.S. Secretary of State Dean Acheson to ask how the United States would respond if flying saucers turned out to be extraterrestrial.:51–52 He later had several conversations on the subject in 1964 with Jacques Vallée. Though quite skeptical of any extraordinary answer to the UFO question, Sagan thought scientists should study the phenomenon, at least because there was widespread public interest in UFO reports.
Stuart Appelle notes that Sagan "wrote frequently on what he perceived as the logical and empirical fallacies regarding UFOs and the abduction experience. Sagan rejected an extraterrestrial explanation for the phenomenon but felt there were both empirical and pedagogical benefits for examining UFO reports and that the subject was, therefore, a legitimate topic of study."
In 1966 Sagan was a member of the Ad Hoc Committee to Review Project Blue Book, the U.S. Air Force's UFO investigation project. The committee concluded Blue Book had been lacking as a scientific study, and recommended a university-based project to give the UFO phenomenon closer scientific scrutiny. The result was the Condon Committee (1966–68), led by physicist Edward Condon, and in their final report they formally concluded that UFOs, regardless of what any of them actually were, did not behave in a manner consistent with a threat to national security.
Sociologist Ron Westrum writes that "The high point of Sagan's treatment of the UFO question was the AAAS' symposium in 1969. A wide range of educated opinions on the subject were offered by participants, including not only proponents such as James McDonald and J. Allen Hynek but also skeptics like astronomers William Hartmann and Donald Menzel. The roster of speakers was balanced, and it is to Sagan's credit that this event was presented in spite of pressure from Edward Condon." With physicist Thornton Page, Sagan edited the lectures and discussions given at the symposium; these were published in 1972 as "UFO's: A Scientific Debate". Some of Sagan's many books examine UFOs (as did one episode of "Cosmos") and he claimed a religious undercurrent to the phenomenon.
Sagan again revealed his views on interstellar travel in his 1980 "Cosmos" series. In one of his last written works, Sagan argued that the chances of extraterrestrial spacecraft visiting Earth are vanishingly small. However, Sagan did think it plausible that Cold War concerns contributed to governments misleading their citizens about UFOs, and that "some UFO reports and analyses, and perhaps voluminous files, have been made inaccessible to the public which pays the bills ... It's time for the files to be declassified and made generally available." He cautioned against jumping to conclusions about suppressed UFO data and stressed that there was no strong evidence that aliens were visiting the Earth either in the past or present.
Death.
After suffering from myelodysplasia, and receiving three bone marrow transplants, Sagan died of pneumonia at the age of 62 at the Fred Hutchinson Cancer Research Center in Seattle, Washington, on December 20, 1996.
He was buried at Lakeview Cemetery in Ithaca, New York.
Posthumous recognition.
The 1997 movie "Contact", based on Sagan's novel of the same name and finished after his death, ends with the dedication "For Carl". His photo can also be seen at 59:25 in the film.
In 1997 the Sagan Planet Walk was opened in Ithaca, New York. It is a walking-scale model of the Solar System, extending 1.2 km from the center of The Commons in downtown Ithaca to the Sciencenter, a hands-on museum. The exhibition was created in memory of Carl Sagan, who was an Ithaca resident and Cornell Professor. Professor Sagan had been a founding member of the museum's advisory board.
The landing site of the unmanned Mars Pathfinder spacecraft was renamed the "Carl Sagan Memorial Station" on July 5, 1997. Asteroid 2709 Sagan is named in his honor, as the Carl Sagan Institute for the search of habitable planets.
Sagan's son, Nick Sagan, wrote several episodes in the "Star Trek" franchise. In an episode of "" entitled "Terra Prime", a quick shot is shown of the relic rover "Sojourner", part of the Mars Pathfinder mission, placed by a historical marker at Carl Sagan Memorial Station on the Martian surface. The marker displays a quote from Sagan: "Whatever the reason you're on Mars, I'm glad you're there, and I wish I was with you." Sagan's student Steve Squyres led the team that landed the rovers "Spirit" and "Opportunity" successfully on Mars in 2004.
On November 9, 2001, on what would have been Sagan's 67th birthday, the Ames Research Center dedicated the site for the Carl Sagan Center for the Study of Life in the Cosmos. "Carl was an incredible visionary, and now his legacy can be preserved and advanced by a 21st century research and education laboratory committed to enhancing our understanding of life in the universe and furthering the cause of space exploration for all time", said NASA Administrator Daniel Goldin. Ann Druyan was at the Center as it opened its doors on October 22, 2006.
Sagan has at least three awards named in his honor:
In 2006, the Carl Sagan Medal was awarded to astrobiologist and author David Grinspoon, the son of Sagan's close friend Lester Grinspoon.
August 2007 the Independent Investigations Group (IIG) awarded Sagan posthumously a Lifetime Achievement Award. This honor has also been awarded to Harry Houdini and James Randi.
Beginning in 2009, a musical project known as Symphony of Science sampled several excerpts of Sagan from his series "Cosmos" and remixed them to electronic music. To date, the videos have received over 21 million views worldwide on YouTube.
In February 2015, the Finnish-based symphonic metal band Nightwish released the song "Sagan" as a non-album bonus track for their single "Élan". The song, written by the band's songwriter/composer/keyboardist Tuomas Holopainen, is an homage to the life and work of the late Carl Sagan.
Awards and honors.
</dl>
Publications.
</dl>
Further reading.
</dl>
External links.
 

</doc>
<doc id="6827" url="http://en.wikipedia.org/wiki?curid=6827" title="Cuban Missile Crisis">
Cuban Missile Crisis

The Cuban Missile Crisis, also known as the October Crisis (Spanish: "Crisis de octubre"), The Missile Scare, or the Caribbean Crisis (Russian: Карибский кризис, tr. "Karibskiy krizis"), was a 13-day (October 14–28, 1962) confrontation between the United States and the Soviet Union over Soviet ballistic missiles deployed in Cuba. It played out on television worldwide and was the closest the Cold War came to escalating into a full-scale nuclear war.
In response to the failed Bay of Pigs Invasion of 1961, and the presence of American Jupiter ballistic missiles in Italy and Turkey against the USSR with Moscow within range, Soviet leader Nikita Khrushchev decided to agree to Cuba's request to place nuclear missiles in Cuba to deter future harassment of Cuba. An agreement was reached during a secret meeting between Khrushchev and Fidel Castro in July and construction on a number of missile launch facilities started later that summer.
An election was underway in the U.S. and the White House had denied Republican charges that it was ignoring dangerous Soviet missiles 90 miles from Florida. These missile preparations were confirmed when an Air Force U-2 spy plane produced clear photographic evidence of medium-range and intermediate-range ballistic missile facilities. The United States established a military blockade to prevent further missiles from entering Cuba. It announced that they would not permit offensive weapons to be delivered to Cuba and demanded that the weapons already in Cuba be dismantled and returned to the USSR.
After a period of tense negotiations an agreement was reached between Kennedy and Khrushchev. Publicly, the Soviets would dismantle their offensive weapons in Cuba and return them to the Soviet Union, subject to United Nations verification, in exchange for a US public declaration and agreement never to invade Cuba without direct provocation. Secretly, the US also agreed that it would dismantle all US-built Jupiter MRBMs, which were deployed in Turkey and Italy against the Soviet Union but were not known to the public.
When all offensive missiles and Ilyushin Il-28 light bombers had been withdrawn from Cuba, the blockade was formally ended on November 20, 1962. The negotiations between the United States and the Soviet Union pointed out the necessity of a quick, clear, and direct communication line between Washington and Moscow. As a result, the Moscow–Washington hotline was established. A series of agreements sharply reduced U.S.-Soviet tensions for the following years.
Earlier actions by the United States.
The United States was concerned about an expansion of Communism, and a Latin American country allying openly with the USSR was regarded as unacceptable, given the US-Soviet enmity since the end of World War II. Such an involvement would also directly defy the Monroe Doctrine, a United States policy which, while limiting the United States' involvement with European colonies and European affairs, held that European powers ought not to have involvement with states in the Western Hemisphere.
The United States had been embarrassed publicly by the failed Bay of Pigs Invasion in April 1961, which had been launched under President John F. Kennedy by CIA-trained forces of Cuban exiles. Afterward, former President Eisenhower told Kennedy that "the failure of the Bay of Pigs will embolden the Soviets to do something that they would otherwise not do.":10 The half-hearted invasion left Soviet premier Nikita Khrushchev and his advisers with the impression that Kennedy was indecisive and, as one Soviet adviser wrote, "too young, intellectual, not prepared well for decision making in crisis situations ... too intelligent and too weak." US covert operations continued in 1961 with the unsuccessful Operation Mongoose.
In addition, Khrushchev's impression of Kennedy's weakness was confirmed by the President's soft response during the Berlin Crisis of 1961, particularly the building of the Berlin Wall. Speaking to Soviet officials in the aftermath of the crisis, Khrushchev asserted, "I know for certain that Kennedy doesn't have a strong background, nor, generally speaking, does he have the courage to stand up to a serious challenge." He also told his son Sergei that on Cuba, Kennedy "would make a fuss, make more of a fuss, and then agree."
In January 1962, General Edward Lansdale described plans to overthrow the Cuban Government in a top-secret report (partially declassified 1989), addressed to President Kennedy and officials involved with Operation Mongoose. CIA agents or "pathfinders" from the Special Activities Division were to be infiltrated into Cuba to carry out sabotage and organization, including radio broadcasts. In February 1962, the United States launched an embargo against Cuba, and Lansdale presented a 26-page, top-secret timetable for implementation of the overthrow of the Cuban Government, mandating that guerrilla operations begin in August and September, and in the first two weeks of October: "Open revolt and overthrow of the Communist regime."
Balance of power.
When Kennedy ran for president in 1960, one of his key election issues was an alleged "missile gap" with the Soviets leading. In fact, the United States led the Soviets by a wide margin that would only increase. In 1961, the Soviets had only four intercontinental ballistic missiles (R-7 Semyorka). By October 1962, they may have had a few dozen, although some intelligence estimates were as high as 75.
The United States, on the other hand, had 170 ICBMs and was quickly building more. It also had eight "George Washington"- and "Ethan Allen"-class ballistic missile submarines with the capability to launch 16 Polaris missiles each, with a range of 1400 mi.
Khrushchev increased the perception of a missile gap when he loudly boasted to the world that the USSR was building missiles "like sausages" whose numbers and capabilities actually were nowhere close to his assertions. The Soviet Union did have medium-range ballistic missiles in quantity, about 700 of them; however, these were very unreliable and inaccurate. Overall, the United States had a very considerable advantage in total number of nuclear warheads (27,000 against 3,600) at the time and, more importantly, in all the technologies needed to deliver them accurately.
The United States also led in missile defensive capabilities, Naval and Air power; but the USSR enjoyed a two-to-one advantage in conventional ground forces, much more pronounced in field guns and tanks (particularly in the European theater).
Soviet deployment of missiles in Cuba (Operation Anadyr).
In May 1962, Soviet Premier Nikita Khrushchev was persuaded by the idea of countering the United States' growing lead in developing and deploying strategic missiles by placing Soviet intermediate-range nuclear missiles in Cuba, despite the misgivings of the Soviet Ambassador in Havana, Alexandr Ivanovich Alexeyev who argued that Castro would not accept the deployment of these missiles. Khrushchev faced a strategic situation where the US was perceived to have a "splendid first strike" capability that put the Soviet Union at a huge disadvantage. In 1962, the Soviets had only 20 ICBMs capable of delivering nuclear warheads to the United States from inside the Soviet Union. The poor accuracy and reliability of these missiles raised serious doubts about their effectiveness. A newer, more reliable generation of ICBMs would only become operational after 1965. Therefore, Soviet nuclear capability in 1962 placed less emphasis on ICBMs than on medium and intermediate-range ballistic missiles (MRBMs and IRBMs). These missiles could hit American allies and most of Alaska from Soviet territory but not the contiguous 48 states of the US. Graham Allison, the director of Harvard University's Belfer Center for Science and International Affairs, points out, "The Soviet Union could right the nuclear imbalance by deploying new ICBMs on its own soil. But to meet the threat it faced in 1962, 1963, and 1964, it had very few options. Moving existing nuclear weapons to locations from which they could reach American targets was one."
A second reason Soviet missiles were deployed to Cuba was because Khrushchev wanted to bring West Berlin—the American/British/French-controlled democratic zone within Communist East Germany—into the Soviet orbit. The East Germans and Soviets considered western control over a portion of Berlin a grave threat to East Germany. For this reason, among others, Khrushchev made West Berlin the central battlefield of the Cold War. Khrushchev believed that if the Americans did nothing over the missile deployments in Cuba, he could muscle the West out of Berlin using said missiles as a deterrent to western counter-measures in Berlin. If the Americans tried to bargain with the Soviets after becoming aware of the missiles, Khrushchev could demand trading the missiles for West Berlin. Since Berlin was strategically more important than Cuba, the trade would be a win for Khrushchev. President Kennedy recognized this: "The advantage is, from Khrushchev's point of view, he takes a great chance but there are quite some rewards to it."
Finally, Khrushchev was also reacting in part to the nuclear threat of obsolescent Jupiter intermediate-range ballistic missiles which the United States had installed in Turkey during April 1962.
In early 1962, a group of Soviet military and missile construction specialists accompanied an agricultural delegation to Havana. They obtained a meeting with Cuban leader Fidel Castro. The Cuban leadership had a strong expectation that the US would invade Cuba again and they enthusiastically approved the idea of installing nuclear missiles in Cuba. However, according to another source, Fidel Castro objected to the missiles deployment that would have made him look like a Soviet puppet, but was persuaded that missiles in Cuba would be an irritant to the US and help the interests of the entire socialist camp. Further, the deployment would include short-range tactical weapons (with a range of 40 km, usable only against naval vessels) that would provide a "nuclear umbrella" for attacks upon the island.
By May, Khrushchev and Castro agreed to place strategic nuclear missiles secretly in Cuba. Like Castro, Khrushchev felt that a US invasion of Cuba was imminent, and that to lose Cuba would do great harm to the communist cause, especially in Latin America. He said he wanted to confront the Americans "with more than words ... the logical answer was missiles.":29 The Soviets maintained their tight secrecy, writing their plans longhand, which were approved by Rodion Malinovsky on July 4 and Khrushchev on July 7.
From the very beginning, the Soviets' operation entailed elaborate denial and deception, known in the USSR as "maskirovka". All of the planning and preparation for transporting and deploying the missiles were carried out in the utmost secrecy, with only a very few told the exact nature of the mission. Even the troops detailed for the mission were given misdirection, told they were headed for a cold region and outfitted with ski boots, fleece-lined parkas, and other winter equipment. The Soviet code name was Operation Anadyr. Anadyr was also the name of a river flowing into the Bering Sea, the name of the capital of Chukotsky District, and a bomber base in the far eastern region. All these were meant to conceal the program from both internal and external audiences.
Specialists in missile construction under the guise of "machine operators," "irrigation specialists" and "agricultural specialists" arrived in July. A total of 43,000 foreign troops would ultimately be brought in. Marshal Sergei Biryuzov, chief of the Soviet Rocket Forces, led a survey team that visited Cuba. He told Khrushchev that the missiles would be concealed and camouflaged by the palm trees.
The Cuban leadership was further upset when in September the United States Congress approved US Joint Resolution 230, which expressed Congress's resolve to prevent the creation of an externally supported military establishment. On the same day, the US announced a major military exercise in the Caribbean, PHIBRIGLEX-62, which Cuba denounced as a deliberate provocation and proof that the US planned to invade Cuba.
The Soviet leadership believed, based on their perception of Kennedy's lack of confidence during the Bay of Pigs Invasion, that he would avoid confrontation and accept the missiles as a "fait accompli".:1 On September 11, the Soviet Union publicly warned that a US attack on Cuba or on Soviet ships carrying supplies to the island would mean war. The Soviets continued their "Maskirovka" program to conceal their actions in Cuba. They repeatedly denied that the weapons being brought into Cuba were offensive in nature. On September 7, Soviet Ambassador to the United States Anatoly Dobrynin assured United States Ambassador to the United Nations Adlai Stevenson that the USSR was supplying only defensive weapons to Cuba. On September 11, the Telegrafnoe Agentstvo Sovetskogo Soyuza (Soviet News Agency TASS) announced that the Soviet Union had no need or intention to introduce offensive nuclear missiles into Cuba. On October 13, Dobrynin was questioned by former Undersecretary of State Chester Bowles about whether the Soviets plan to put offensive weapons in Cuba. He denied any such plans. And again on October 17, Soviet embassy official Georgy Bolshakov brought President Kennedy a "personal message" from Khrushchev reassuring him that "under no circumstances would surface-to-surface missiles be sent to Cuba.":494
As early as August 1962, the United States suspected the Soviets of building missile facilities in Cuba. During that month, its intelligence services gathered information about sightings by ground observers of Russian-built MiG-21 fighters and Il-28 light bombers. U-2 spyplanes found S-75 Dvina (NATO designation "SA-2") surface-to-air missile sites at eight different locations. CIA director John A. McCone was suspicious. Sending antiaircraft missiles into Cuba, he reasoned, "made sense only if Moscow intended to use them to shield a base for ballistic missiles aimed at the United States." On August 10, he wrote a memo to President Kennedy in which he guessed that the Soviets were preparing to introduce ballistic missiles into Cuba. 
With important Congressional elections scheduled for November, the Crisis became emeshed in American politics. On August 31, Senator Kenneth Keating (R-New York), who probably received his information from Cuban exiles in Florida, warned on the Senate floor that the Soviet Union may be constructing a missile base in Cuba. He charged the Kennedy Administration was covering up a major threat to the U.S.
Air Force General Curtis LeMay presented a pre-invasion bombing plan to Kennedy in September, while spy flights and minor military harassment from US forces at Guantanamo Bay Naval Base were the subject of continual Cuban diplomatic complaints to the US government.
The first consignment of R-12 missiles arrived on the night of September 8, followed by a second on September 16. The R-12 was an intermediate-range ballistic missile, capable of carrying a thermonuclear warhead. It was a single-stage, road-transportable, surface-launched, storable liquid propellant fueled missile that could deliver a megaton-class nuclear weapon. The Soviets were building nine sites—six for R-12 medium-range missiles (NATO designation "SS-4 Sandal") with an effective range of 2000 km and three for R-14 intermediate-range ballistic missiles (NATO designation "SS-5 Skean") with a maximum range of 4500 km.
Cuba positioning.
On October 7, Cuban President Osvaldo Dorticós spoke at the UN General Assembly: "If ... we are attacked, we will defend ourselves. I repeat, we have sufficient means with which to defend ourselves; we have indeed our inevitable weapons, the weapons, which we would have preferred not to acquire, and which we do not wish to employ."
Missiles reported.
The missiles in Cuba allowed the Soviets to effectively target the majority of the continental United States. The planned arsenal was forty launchers. The Cuban populace readily noticed the arrival and deployment of the missiles and hundreds of reports reached Miami. US intelligence received countless reports, many of dubious quality or even laughable, and most of which could be dismissed as describing defensive missiles. Only five reports bothered the analysts. They described large trucks passing through towns at night carrying very long canvas-covered cylindrical objects that could not make turns through towns without backing up and maneuvering. Defensive missiles could make these turns. These reports could not be satisfactorily dismissed. There was also a very sensitive source, which had to be protected at all costs: Oleg Penkovsky, a double agent in the GRU working for CIA and MI6 reported the Soviet plans and even provided details of the missile placements, which were eventually verified by U-2 flights.
Aerial images find Soviet missiles.
Despite the increasing evidence of a military build-up on Cuba, no U-2 flights were made over Cuba from September 5 until October 14. The first problem that caused the pause in reconnaissance flights took place on August 30, when a U-2 operated by the US Air Force's Strategic Air Command flew over Sakhalin Island in the Soviet Far East by mistake. The Soviets lodged a protest and the US apologized. Nine days later, a Taiwanese-operated U-2 was lost over western China, probably to a SAM. US officials were worried that one of the Cuban or Soviet SAMs in Cuba might shoot down a CIA U-2, initiating another international incident. In a meeting with members of the Committee on Overhead Reconnaissance (COMOR) on 10 September, US Secretary of State Dean Rusk and National Security Advisor McGeorge Bundy heavily restricted further U-2 flights over Cuban airspace. The resulting lack of coverage over the island for the next five weeks became known to historians as the "Photo Gap." During this period, no significant U-2 coverage was achieved over the interior of the island. US officials attempted to use a Corona photoreconnaissance satellite to obtain coverage over reported Soviet military deployments, but imagery acquired over western Cuba by a Corona KH-4 mission on 1 October was heavily covered by clouds and haze and failed to provide any usable intelligence. At the end of September, Navy reconnaissance aircraft photographed the Soviet ship "Kasimov" with large crates on its deck the size and shape of Il-28 light bombers.
In September 1962, photo interpreters from the Defense Intelligence Agency (DIA) noticed that Cuban surface-to-air missile sites were arranged in a pattern similar to those used by the Soviet Union to protect its ICBM bases, leading the wary Agency to lobby for the resumption of U-2 flights over the island. Although in the past the flights had been conducted by the CIA, due to pressure from the Defense Department, the authority was transferred to the Air Force. Following CIA's unsuccessful mission over the Soviet Union, it was thought that if another U-2 was shot down a cover story involving Air Force flights would be easier to explain than CIA flights.
When the reconnaissance missions were re-authorized on October 8, weather kept the planes from flying. The US first obtained U-2 photographic evidence of the missiles on October 14, when a U-2 flight piloted by Major Richard Heyser took 928 pictures on a path selected by DIA analysts, capturing images of what turned out to be an SS-4 construction site at San Cristóbal, Pinar del Río Province (now in Artemisa Province), in western Cuba.
President notified.
On October 15, the CIA's National Photographic Interpretation Center reviewed the U-2 photographs and identified objects that they interpreted as medium range ballistic missiles. That evening, the CIA notified the Department of State and at 8:30 pm EDT, National Security Adviser McGeorge Bundy elected to wait until morning to tell the President. Secretary of Defense Robert McNamara was briefed at midnight. The next morning, Bundy met with Kennedy and showed him the U-2 photographs and briefed him on the CIA's analysis of the images. At 6:30 pm EDT, Kennedy convened a meeting of the nine members of the National Security Council and five other key advisors, in a group he formally named the Executive Committee of the National Security Council (EXCOMM) after the fact on October 22 by the National Security Action Memorandum 196. Without informing the members of EXCOMM, President Kennedy tape recorded all of their proceedings, and Sheldon M. Stern, head of the Kennedy library has transcribed some of them.
Responses considered.
The US had no plan in place because US intelligence had been convinced that the Soviets would never install nuclear missiles in Cuba. The EXCOMM quickly discussed several possible courses of action, including:
The Joint Chiefs of Staff unanimously agreed that a full-scale attack and invasion was the only solution. They believed that the Soviets would not attempt to stop the US from conquering Cuba. Kennedy was skeptical.
They, no more than we, can let these things go by without doing something. They can't, after all their statements, permit us to take out their missiles, kill a lot of Russians, and then do nothing. If they don't take action in Cuba, they certainly will in Berlin.
Kennedy concluded that attacking Cuba by air would signal the Soviets to presume "a clear line" to conquer Berlin. Kennedy also believed that United States' allies would think of the US as "trigger-happy cowboys" who lost Berlin because they could not peacefully resolve the Cuban situation.:332
The EXCOMM then discussed the effect on the strategic balance of power, both political and military. The Joint Chiefs of Staff believed that the missiles would seriously alter the military balance, but Secretary of Defense Robert McNamara disagreed. He was convinced that the missiles would not affect the strategic balance at all. An extra forty, he reasoned, would make little difference to the overall strategic balance. The US already had approximately 5,000 strategic warheads,:261 while the Soviet Union had only 300. He concluded that the Soviets having 340 would not therefore substantially alter the strategic balance. In 1990, he reiterated that "it made "no" difference ... The military balance wasn't changed. I didn't believe it then, and I don't believe it now."
The EXCOMM agreed that the missiles would affect the "political" balance. First, Kennedy had explicitly promised the American people less than a month before the crisis that "if Cuba should possess a capacity to carry out offensive actions against the United States ... the United States would act.":674–681 Second, US credibility among their allies, and among the American people, would be damaged if they allowed the Soviet Union to "appear" to redress the strategic balance by placing missiles in Cuba. Kennedy explained after the crisis that "it would have politically changed the balance of power. It would have appeared to, and appearances contribute to reality."
On October 18, President Kennedy met with Soviet Minister of Foreign Affairs, Andrei Gromyko, who claimed the weapons were for defensive purposes only. Not wanting to expose what he already knew, and wanting to avoid panicking the American public, the President did not reveal that he was already aware of the missile build-up.
By October 19, frequent U-2 spy flights showed four operational sites. As part of the blockade, the US military was put on high alert to enforce the blockade and to be ready to invade Cuba at a moment's notice. The 1st Armored Division was sent to Georgia, and five army divisions were alerted for combat operations. The Strategic Air Command (SAC) distributed its shorter-ranged B-47 Stratojet medium bombers to civilian airports and sent aloft its B-52 Stratofortress heavy bombers.
Operational plans.
Two Operational Plans (OPLAN) were considered. OPLAN 316 envisioned a full invasion of Cuba by Army and Marine units supported by the Navy following Air Force and naval airstrikes. However, Army units in the United States would have had trouble fielding mechanized and logistical assets, while the US Navy could not supply sufficient amphibious shipping to transport even a modest armored contingent from the Army. OPLAN 312, primarily an Air Force and Navy carrier operation, was designed with enough flexibility to do anything from engaging individual missile sites to providing air support for OPLAN 316's ground forces.
Blockade ("quarantine").
Kennedy met with members of EXCOMM and other top advisers throughout October 21, considering two remaining options: an air strike primarily against the Cuban missile bases, or a naval blockade of Cuba. A full-scale invasion was not the administration's first option. Robert McNamara supported the naval blockade as a strong but limited military action that left the US in control. However, the term "blockade" was problematic. According to international law a blockade is an act of war, but the Kennedy administration did not think that the USSR would be provoked to attack by a mere blockade. Additionally, legal experts at the State Department and Justice Department concluded that a declaration of war could be avoided so long as another legal justification, based on the Rio Treaty for defense of the Western Hemisphere, was obtained via a resolution by a two-thirds vote from the members or the Organization of American States (OAS).
Admiral Anderson, Chief of Naval Operations wrote a position paper that helped Kennedy to differentiate between what they termed a "quarantine" of offensive weapons and a blockade of all materials, claiming that a classic blockade was not the original intention. Since it would take place in international waters, Kennedy obtained the approval of the OAS for military action under the hemispheric defense provisions of the Rio Treaty.
Latin American participation in the quarantine now involved two Argentine destroyers which were to report to the US Commander South Atlantic [COMSOLANT] at Trinidad on November 9. An Argentine submarine and a Marine battalion with lift were available if required. In addition, two Venezuelan destroyers (Destroyers ARV D-11 Nueva Esparta" and "ARV D-21 Zulia") and one submarine (Caribe) had reported to COMSOLANT, ready for sea by November 2. The Government of Trinidad and Tobago offered the use of Chaguaramas Naval Base to warships of any OAS nation for the duration of the "quarantine." The Dominican Republic had made available one escort ship. Colombia was reported ready to furnish units and had sent military officers to the US to discuss this assistance. The Argentine Air Force informally offered three SA-16 aircraft in addition to forces already committed to the "quarantine" operation.
This initially was to involve a naval blockade against offensive weapons within the framework of the Organization of American States and the Rio Treaty. Such a blockade might be expanded to cover all types of goods and air transport. The action was to be backed up by surveillance of Cuba. The CNO's scenario was followed closely in later implementing the "quarantine."
On October 19, the EXCOMM formed separate working groups to examine the air strike and blockade options, and by the afternoon most support in the EXCOMM shifted to the blockade option. Reservations about the plan continued to be voiced as late as the twenty-first; however, the paramount one being that once the blockade was put into effect, the Soviets would rush to complete some of the missiles. Consequently, the United States could find itself bombing operational missiles were the blockade to fail to force Khrushchev to remove the missiles already on the island.
Speech to the nation.
At 3:00 pm EDT on October 22, President Kennedy formally established the Executive Committee (EXCOMM) with National Security Action Memorandum (NSAM) 196. At 5:00 pm, he met with Congressional leaders who contentiously opposed a blockade and demanded a stronger response. In Moscow, Ambassador Kohler briefed Chairman Khrushchev on the pending blockade and Kennedy's speech to the nation. Ambassadors around the world gave notice to non-Eastern Bloc leaders. Before the speech, US delegations met with Canadian Prime Minister John Diefenbaker, British Prime Minister Harold Macmillan, West German Chancellor Konrad Adenauer, and French President Charles de Gaulle to brief them on the US intelligence and their proposed response. All were supportive of the US position.
On October 22 at 7:00 pm EDT, President Kennedy delivered a nation-wide televised address on all of the major networks announcing the discovery of the missiles.
It shall be the policy of this nation to regard any nuclear missile launched from Cuba against any nation in the Western Hemisphere as an attack by the Soviet Union on the United States, requiring a full retaliatory response upon the Soviet Union.
Kennedy described the administration's plan:
To halt this offensive buildup, a strict quarantine on all offensive military equipment under shipment to Cuba is being initiated. All ships of any kind bound for Cuba, from whatever nation or port, will, if found to contain cargoes of offensive weapons, be turned back. This quarantine will be extended, if needed, to other types of cargo and carriers. We are not at this time, however, denying the necessities of life as the Soviets attempted to do in their Berlin blockade of 1948.
During the speech a directive went out to all US forces worldwide placing them on DEFCON 3. The heavy cruiser USS "Newport News" was designated flagship for the blockade, with the USS "Leary" (DD-879) as "Newport News"‍ '​s destroyer escort.
Crisis deepens.
On October 23 at 11:24 am EDT a cable drafted by George Ball to the US Ambassador in Turkey and the US Ambassador to NATO notified them that they were considering making an offer to withdraw what the US knew to be nearly obsolete missiles from Italy and Turkey in exchange for the Soviet withdrawal from Cuba. Turkish officials replied that they would "deeply resent" any trade for the US missile's presence in their country. Two days later, on the morning of October 25, journalist Walter Lippmann proposed the same thing in his syndicated column. Castro reaffirmed Cuba's right to self-defense and said that all of its weapons were defensive and Cuba would not allow an inspection.
International response.
Three days after Kennedy's speech, the Chinese "People's Daily" announced that "650,000,000 Chinese men and women were standing by the Cuban people." In West Germany, newspapers supported the United States' response, contrasting it with the weak American actions in the region during the preceding months. They also expressed some fear that the Soviets might retaliate in Berlin. In France on October 23, the crisis made the front page of all the daily newspapers. The next day, an editorial in "Le Monde" expressed doubt about the authenticity of the CIA's photographic evidence. Two days later, after a visit by a high-ranking CIA agent, they accepted the validity of the photographs. Also in France, in the October 29 issue of "Le Figaro", Raymond Aron wrote in support of the American response.
On October 24, Pope John XXIII sent a message to the Soviet embassy in Rome to be transmitted to the Kremlin, in which he voiced his concern for peace. In this message he stated "We beg all governments not to remain deaf to this cry of humanity. That they do all that is in their power to save peace." With the permission of Kennedy and Khrushchev, this appeal went public on October 25 on radio, asking leaders to do "all in their power to preserve peace" and to "save the world from the horrors of a war". The intervention of Pope John was significant, as on the same day confrontation started to settle down. Pope John's message also appeared in "Pravda", the official Soviet newspaper, on October 26 under the headline, "We beg all rulers not to be deaf to the cry of humanity". While Pope John XXIII's role in the crisis is often overlooked, he acted as a third party in the dispute in such a way that it allowed Kennedy and Khrushchev a way to back out without either party acknowledging defeat. His public appeal essentially created a bridge between Washington and Moscow.
Soviet broadcast.
At the time, the crisis continued unabated, and on the evening of October 24, the Soviet news agency TASS broadcast a telegram from Khrushchev to President Kennedy, in which Khrushchev warned that the United States' "outright piracy" would lead to war. However, this was followed at 9:24 pm by a telegram from Khrushchev to Kennedy which was received at 10:52 pm EDT, in which Khrushchev stated, "if you weigh the present situation with a cool head without giving way to passion, you will understand that the Soviet Union cannot afford not to decline the despotic demands of the USA" and that the Soviet Union views the blockade as "an act of aggression" and their ships will be instructed to ignore it.
US alert level raised.
The United States requested an emergency meeting of the United Nations Security Council on October 25. US Ambassador to the United Nations Adlai Stevenson confronted Soviet Ambassador Valerian Zorin in an emergency meeting of the SC challenging him to admit the existence of the missiles. Ambassador Zorin refused to answer. The next day at 10:00 pm EDT, the United States raised the readiness level of SAC forces to DEFCON 2. For the only confirmed time in US history, while the B-52 bombers went on continuous airborne alert, the B-47 medium bombers were dispersed to various military and civilian airfields, and made ready to take off, fully equipped, on 15 minutes' notice. One-eighth of SAC's 1,436 bombers were on airborne alert, some 145 intercontinental ballistic missiles stood on ready alert, while Air Defense Command (ADC) redeployed 161 nuclear-armed interceptors to 16 dispersal fields within nine hours with one-third maintaining 15-minute alert status. Twenty-three nuclear-armed B-52s were sent to orbit points within striking distance of the Soviet Union so that the latter might observe that the US was serious. Jack J. Catton later estimated that about 80% of SAC's planes were ready for launch during the crisis; David A. Burchinal recalled that, by contrast,
the Russians were so thoroughly stood down, and we knew it. They didn't make any move. They did not increase their alert; they did not increase any flights, or their air defense posture. They didn't do a thing, they froze in place. We were never further from nuclear war than at the time of Cuba, never further.
"By October 22, Tactical Air Command (TAC) had 511 fighters plus supporting tankers and reconnaissance aircraft deployed to face Cuba on one-hour alert status. However, TAC and the Military Air Transport Service had problems. The concentration of aircraft in Florida strained command and support echelons; which faced critical undermanning in security, armaments, and communications; the absence of initial authorization for war-reserve stocks of conventional munitions forced TAC to scrounge; and the lack of airlift assets to support a major airborne drop necessitated the call-up of 24 Reserve squadrons."
On October 25 at 1:45 am EDT, Kennedy responded to Khrushchev's telegram, stating that the United States was forced into action after receiving repeated assurances that no offensive missiles were being placed in Cuba, and that when these assurances proved to be false, the deployment "required the responses I have announced ... I hope that your government will take necessary action to permit a restoration of the earlier situation."
Blockade challenged.
At 7:15 am EDT on October 25, the USS "Essex" and USS "Gearing" attempted to intercept the "Bucharest" but failed to do so. Fairly certain the tanker did not contain any military material, they allowed it through the blockade. Later that day, at 5:43 pm, the commander of the blockade effort ordered the USS "Joseph P. Kennedy, Jr" to intercept and board the Lebanese freighter "Marucla". This took place the next day, and the "Marucla" was cleared through the blockade after its cargo was checked.
At 5:00 pm EDT on October 25, William Clements announced that the missiles in Cuba were still actively being worked on. This report was later verified by a CIA report that suggested there had been no slow-down at all. In response, Kennedy issued Security Action Memorandum 199, authorizing the loading of nuclear weapons onto aircraft under the command of SACEUR (which had the duty of carrying out first air strikes on the Soviet Union). During the day, the Soviets responded to the blockade by turning back 14 ships presumably carrying offensive weapons.
Crisis stalemated.
The next morning, October 26, Kennedy informed the EXCOMM that he believed only an invasion would remove the missiles from Cuba. However, he was persuaded to give the matter time and continue with both military and diplomatic pressure. He agreed and ordered the low-level flights over the island to be increased from two per day to once every two hours. He also ordered a crash program to institute a new civil government in Cuba if an invasion went ahead.
At this point, the crisis was ostensibly at a stalemate. The USSR had shown no indication that they would back down and had made several comments to the contrary. The US had no reason to believe otherwise and was in the early stages of preparing for an invasion, along with a nuclear strike on the Soviet Union in case it responded militarily, which was assumed.
Secret negotiations.
At 1:00 pm EDT on October 26, John A. Scali of ABC News had lunch with Aleksandr Fomin (alias of spy Alexander Feklisov) at Fomin's request. Fomin noted, "War seems about to break out" and asked Scali to use his contacts to talk to his "high-level friends" at the State Department to see if the US would be interested in a diplomatic solution. He suggested that the language of the deal would contain an assurance from the Soviet Union to remove the weapons under UN supervision and that Castro would publicly announce that he would not accept such weapons in the future, in exchange for a public statement by the US that it would never invade Cuba. The US responded by asking the Brazilian government to pass a message to Castro that the US would be "unlikely to invade" if the missiles were removed.
Mr. President, we and you ought not now to pull on the ends of the rope in which you have tied the knot of war, because the more the two of us pull, the tighter that knot will be tied. And a moment may come when that knot will be tied so tight that even he who tied it will not have the strength to untie it, and then it will be necessary to cut that knot, and what that would mean is not for me to explain to you, because you yourself understand perfectly of what terrible forces our countries dispose.
Consequently, if there is no intention to tighten that knot and thereby to doom the world to the catastrophe of thermonuclear war, then let us not only relax the forces pulling on the ends of the rope, let us take measures to untie that knot. We are ready for this.
Letter From Chairman Khrushchev to President Kennedy, October 26, 1962
On October 26 at 6:00 pm EDT, the State Department started receiving a message that appeared to be written personally by Khrushchev. It was Saturday at 2:00 am in Moscow. The long letter took several minutes to arrive, and it took translators additional time to translate and transcribe it.
Robert Kennedy described the letter as "very long and emotional." Khrushchev reiterated the basic outline that had been stated to John Scali earlier in the day, "I propose: we, for our part, will declare that our ships bound for Cuba are not carrying any armaments. You will declare that the United States will not invade Cuba with its troops and will not support any other forces which might intend to invade Cuba. Then the necessity of the presence of our military specialists in Cuba will disappear." At 6:45 pm EDT, news of Fomin's offer to Scali was finally heard and was interpreted as a "set up" for the arrival of Khrushchev's letter. The letter was then considered official and accurate, although it was later learned that Fomin was almost certainly operating of his own accord without official backing. Additional study of the letter was ordered and continued into the night.
Crisis continues.
Direct aggression against Cuba would mean nuclear war. The Americans speak about such aggression as if they did not know or did not want to accept this fact. I have no doubt they would lose such a war. —Ernesto "Che" Guevara, October 1962
Castro, on the other hand, was convinced that an invasion of Cuba was soon at hand, and on October 26, he sent a telegram to Khrushchev that appeared to call for a preemptive nuclear strike on the US. However, in a 2010 interview, Castro said of his recommendation for the Soviets to attack America "before" they made any move against Cuba: "After I've seen what I've seen, and knowing what I know now, it wasn't worth it at all." Castro also ordered all anti-aircraft weapons in Cuba to fire on any US aircraft, whereas in the past they had been ordered only to fire on groups of two or more. At 6:00 am EDT on October 27, the CIA delivered a memo reporting that three of the four missile sites at San Cristobal and the two sites at Sagua la Grande appeared to be fully operational. They also noted that the Cuban military continued to organize for action, although they were under order not to initiate action unless attacked.
At 9:00 am EDT on October 27, Radio Moscow began broadcasting a message from Khrushchev. Contrary to the letter of the night before, the message offered a new trade, that the missiles on Cuba would be removed in exchange for the removal of the Jupiter missiles from Italy and Turkey. At 10:00 am EDT, the executive committee met again to discuss the situation and came to the conclusion that the change in the message was due to internal debate between Khrushchev and other party officials in the Kremlin.:300 Kennedy realized that he would be in an "insupportable position if this becomes Khrushchev's proposal", because: 1) The missiles in Turkey were not militarily useful and were being removed anyway; and 2) "It's gonna – to any man at the United Nations or any other rational man, it will look like a very fair trade." National Security Adviser McGeorge Bundy explained why Khrushchev's public acquiescence could not be considered: "The current threat to peace is not in Turkey, it is in Cuba."
McNamara noted that another tanker, the "Grozny", was about 600 mi out and should be intercepted. He also noted that they had not made the USSR aware of the blockade line and suggested relaying this information to them via U Thant at the United Nations.
While the meeting progressed, at 11:03 am EDT a new message began to arrive from Khrushchev. The message stated, in part,
"You are disturbed over Cuba. You say that this disturbs you because it is ninety-nine miles by sea from the coast of the United States of America. But ... you have placed destructive missile weapons, which you call offensive, in Italy and Turkey, literally next to us ... I therefore make this proposal: We are willing to remove from Cuba the means which you regard as offensive ... Your representatives will make a declaration to the effect that the United States ... will remove its analogous means from Turkey ... and after that, persons entrusted by the United Nations Security Council could inspect on the spot the fulfillment of the pledges made."
The executive committee continued to meet through the day.
Throughout the crisis, Turkey had repeatedly stated that it would be upset if the Jupiter missiles were removed. Italy's Prime Minister Fanfani, who was also Foreign Minister "ad interim", offered to allow withdrawal of the missiles deployed in Apulia as a bargaining chip. He gave the message to one of his most trusted friends, Ettore Bernabei, the general manager of RAI-TV, to convey to Arthur M. Schlesinger, Jr.. Bernabei was in New York to attend an international conference on satellite TV broadcasting. Unknown to the Soviets, the US regarded the Jupiter missiles as obsolescent and already supplanted by the Polaris nuclear ballistic submarine missiles.
On the morning of October 27, a U-2F (the third CIA U-2A, modified for air-to-air refueling) piloted by USAF Major Rudolf Anderson, departed its forward operating location at McCoy AFB, Florida. At approximately 12:00 pm EDT, the aircraft was struck by a S-75 Dvina (NATO designation "SA-2 Guideline") SAM missile launched from Cuba. The aircraft was shot down and Anderson was killed. The stress in negotiations between the USSR and the US intensified, and only much later was it learned that the decision to fire the missile was made locally by an undetermined Soviet commander acting on his own authority. Later that day, at about 3:41 pm EDT, several US Navy RF-8A Crusader aircraft on low-level photoreconnaissance missions were fired upon.
At 4:00 pm EDT, Kennedy recalled members of EXCOMM to the White House and ordered that a message immediately be sent to U Thant asking the Soviets to "suspend" work on the missiles while negotiations were carried out. During this meeting, General Maxwell Taylor delivered the news that the U-2 had been shot down. Kennedy had earlier claimed he would order an attack on such sites if fired upon, but he decided to not act unless another attack was made. In an interview 40 years later, McNamara said:
 We had to send a U-2 over to gain reconnaissance information on whether the Soviet missiles were becoming operational. We believed that if the U-2 was shot down that—the Cubans didn't have capabilities to shoot it down, the Soviets did—we believed if it was shot down, it would be shot down by a Soviet surface-to-air-missile unit, and that it would represent a decision by the Soviets to escalate the conflict. And therefore, before we sent the U-2 out, we agreed that if it was shot down we wouldn't meet, we'd simply attack. It was shot down on Friday. ... Fortunately, we changed our mind, we thought "Well, it might have been an accident, we won't attack." Later we learned that Khrushchev had reasoned just as we did: we send over the U-2, if it was shot down, he reasoned we would believe it was an intentional escalation. And therefore, he issued orders to Pliyev, the Soviet commander in Cuba, to instruct all of his batteries not to shoot down the U-2.
Drafting the response.
Emissaries sent by both Kennedy and Nikita Khrushchev agreed to meet at the Yenching Palace Chinese restaurant in the Cleveland Park neighborhood of Washington D.C. on the evening of October 27. Kennedy suggested that they take Khrushchev's offer to trade away the missiles. Unknown to most members of the EXCOMM, Robert Kennedy had been meeting with the Soviet Ambassador in Washington to discover whether these intentions were genuine. The EXCOMM was generally against the proposal because it would undermine NATO's authority, and the Turkish government had repeatedly stated it was against any such trade.
As the meeting progressed, a new plan emerged and Kennedy was slowly persuaded. The new plan called for the President to ignore the latest message and instead to return to Khrushchev's earlier one. Kennedy was initially hesitant, feeling that Khrushchev would no longer accept the deal because a new one had been offered, but Llewellyn Thompson argued that he might accept it anyway. White House Special Counsel and Adviser Ted Sorensen and Robert Kennedy left the meeting and returned 45 minutes later with a draft letter to this effect. The President made several changes, had it typed, and sent it.
After the EXCOMM meeting, a smaller meeting continued in the Oval Office. The group argued that the letter should be underscored with an oral message to Ambassador Dobrynin stating that if the missiles were not withdrawn, military action would be used to remove them. Dean Rusk added one proviso, that no part of the language of the deal would mention Turkey, but there would be an understanding that the missiles would be removed "voluntarily" in the immediate aftermath. The President agreed, and the message was sent.
At Dean Rusk's request, Fomin and Scali met again. Scali asked why the two letters from Khrushchev were so different, and Fomin claimed it was because of "poor communications." Scali replied that the claim was not credible and shouted that he thought it was a "stinking double cross." He went on to claim that an invasion was only hours away, at which point Fomin stated that a response to the US message was expected from Khrushchev shortly, and he urged Scali to tell the State Department that no treachery was intended. Scali said that he did not think anyone would believe him, but he agreed to deliver the message. The two went their separate ways, and Scali immediately typed out a memo for the EXCOMM.
Within the US establishment, it was well understood that ignoring the second offer and returning to the first put Khrushchev in a terrible position. Military preparations continued, and all active duty Air Force personnel were recalled to their bases for possible action. Robert Kennedy later recalled the mood, "We had not abandoned all hope, but what hope there was now rested with Khrushchev's revising his course within the next few hours. It was a hope, not an expectation. The expectation was military confrontation by Tuesday, and possibly tomorrow ..."
At 8:05 pm EDT, the letter drafted earlier in the day was delivered. The message read, "As I read your letter, the key elements of your proposals—which seem generally acceptable as I understand them—are as follows: 1) You would agree to remove these weapons systems from Cuba under appropriate United Nations observation and supervision; and undertake, with suitable safe-guards, to halt the further introduction of such weapon systems into Cuba. 2) We, on our part, would agree—upon the establishment of adequate arrangements through the United Nations, to ensure the carrying out and continuation of these commitments (a) to remove promptly the quarantine measures now in effect and (b) to give assurances against the invasion of Cuba." The letter was also released directly to the press to ensure it could not be "delayed."
With the letter delivered, a deal was on the table. However, as Robert Kennedy noted, there was little expectation it would be accepted. At 9:00 pm EDT, the EXCOMM met again to review the actions for the following day. Plans were drawn up for air strikes on the missile sites as well as other economic targets, notably petroleum storage. McNamara stated that they had to "have two things ready: a government for Cuba, because we're going to need one; and secondly, plans for how to respond to the Soviet Union in Europe, because sure as hell they're going to do something there."
At 12:12 am EDT, on October 27, the US informed its NATO allies that "the situation is growing shorter ... the United States may find it necessary within a very short time in its interest and that of its fellow nations in the Western Hemisphere to take whatever military action may be necessary." To add to the concern, at 6 am the CIA reported that all missiles in Cuba were ready for action.
Later on that same day, what the White House later called "Black Saturday," the US Navy dropped a series of "signaling depth charges" (practice depth charges the size of hand grenades) on a Soviet submarine (B-59) at the blockade line, unaware that it was armed with a nuclear-tipped torpedo with orders that allowed it to be used if the submarine was "hulled" (a hole in the hull from depth charges or surface fire). The decision to launch these required agreement from all three officers on board, but one of them, Vasili Arkhipov, objected and so the launch was narrowly averted.
On the same day a US U-2 spy plane made an accidental, unauthorized ninety-minute overflight of the Soviet Union's far eastern coast. The Soviets responded by scrambling MiG fighters from Wrangel Island; in turn the Americans launched F-102 fighters armed with nuclear air-to-air missiles over the Bering Sea.
On October 27, Khrushchev also received a letter from Castro – what is now known as the Armageddon Letter (dated Oct. 26) – interpreted as urging the use of nuclear force in the event of an attack on Cuba. "I believe the imperialists' aggressiveness is extremely dangerous and if they actually carry out the brutal act of invading Cuba in violation of international law and morality, that would be the moment to eliminate such danger forever through an act of clear legitimate defense, however harsh and terrible the solution would be," Castro wrote.
Crisis ends.
On October 27, after much deliberation between the Soviet Union and Kennedy's cabinet, Kennedy secretly agreed to remove all missiles set in southern Italy and in Turkey, the latter on the border of the Soviet Union, in exchange for Khrushchev removing all missiles in Cuba. There is some dispute as to whether removing the missiles from Italy was part of the secret agreement, although Khrushchev wrote in his memoirs that it was; nevertheless, when the crisis had ended McNamara gave the order to dismantle the missiles in both Italy and Turkey.
At 9:00 am EST, on October 28, a new message from Khrushchev was broadcast on Radio Moscow. Khrushchev stated that, "the Soviet government, in addition to previously issued instructions on the cessation of further work at the building sites for the weapons, has issued a new order on the dismantling of the weapons which you describe as 'offensive' and their crating and return to the Soviet Union."
Kennedy immediately responded, issuing a statement calling the letter "an important and constructive contribution to peace." He continued this with a formal letter: 
I consider my letter to you of October twenty-seventh and your reply of today as firm undertakings on the part of both our governments which should be promptly carried out ... The US will make a statement in the framework of the Security Council in reference to Cuba as follows: it will declare that the United States of America will respect the inviolability of Cuban borders, its sovereignty, that it take the pledge not to interfere in internal affairs, not to intrude themselves and not to permit our territory to be used as a bridgehead for the invasion of Cuba, and will restrain those who would plan to carry an aggression against Cuba, either from US territory or from the territory of other countries neighboring to Cuba.:103
 Kennedy's planned statement would also contain suggestions he had received from his adviser, Arthur M. Schlesinger, Jr., in a "Memorandum for the President" describing the "Post Mortem on Cuba."
The US continued the blockade, and in the following days, aerial reconnaissance proved that the Soviets were making progress in removing the missile systems. The 42 missiles and their support equipment were loaded onto eight Soviet ships. On November 2, 1962, President Kennedy addressed the United States via radio and television broadcasts regarding the dismantlement process of the Soviet R-12 missile bases located in the Caribbean region. The ships left Cuba from November 5–9. The US made a final visual check as each of the ships passed the blockade line. Further diplomatic efforts were required to remove the Soviet IL-28 bombers, and they were loaded on three Soviet ships on December 5 and 6. Concurrent with the Soviet commitment on the IL-28's, the US Government announced the end of the blockade effective at 6:45 pm EST on November 20, 1962. 
At the time when the Kennedy administration thought that the Cuban missile crisis was resolved, nuclear tactical rockets stayed in Cuba since they were not part of the Kennedy-Khrushchev understandings. However, the Soviets changed their minds, fearing possible future Cuban militant steps, and on November 22, 1962, the Soviet Deputy Prime Minister Anastas Mikoyan told Castro that those rockets with the nuclear warheads were being removed too.
In his negotiations with the Soviet Ambassador Anatoly Dobrynin, US Attorney General Robert Kennedy informally proposed that the Jupiter missiles in Turkey would be removed "within a short time after this crisis was over.":222 The last US missiles were disassembled by April 24, 1963, and were flown out of Turkey soon after.
The practical effect of this Kennedy-Khrushchev Pact was that it effectively strengthened Castro's position in Cuba, guaranteeing that the US would not invade Cuba. It is possible that Khrushchev only placed the missiles in Cuba to get Kennedy to remove the missiles from Italy and Turkey and that the Soviets had no intention of resorting to nuclear war if they were out-gunned by the Americans. Because the withdrawal of the Jupiter missiles from NATO bases in Southern Italy and Turkey was not made public at the time, Khrushchev appeared to have lost the conflict and become weakened. The perception was that Kennedy had won the contest between the superpowers and Khrushchev had been humiliated. This is not entirely the case as both Kennedy and Khrushchev took every step to avoid full conflict despite the pressures of their governments. Khrushchev held power for another two years.:102–105
Aftermath.
The compromise embarrassed Khrushchev and the Soviet Union because the withdrawal of US missiles from Italy and Turkey was a secret deal between Kennedy and Khrushchev. Khrushchev went to Kennedy thinking that the crisis was getting out of hand. The Soviets were seen as retreating from circumstances that they had started. Khrushchev's fall from power two years later was in part because of the Politburo embarrassment at both Khrushchev's eventual concessions to the US and his ineptitude in precipitating the crisis in the first place. According to Dobrynin, the top Soviet leadership took the Cuban outcome as "a blow to its prestige bordering on humiliation."
Cuba perceived the outcome as a partial betrayal by the Soviets, given that decisions on how to resolve the crisis had been made exclusively by Kennedy and Khrushchev. Castro was especially upset that certain issues of interest to Cuba, such as the status of the US Naval Base in Guantánamo, were not addressed. This caused Cuban–Soviet relations to deteriorate for years to come.:278 On the other hand, Cuba continued to be protected from invasion.
Although General Curtis LeMay told the President that he considered the resolution of the Cuban Missile Crisis the "greatest defeat in our history," his was a minority position.:335 He had pressed for an immediate invasion of Cuba as soon as the crisis began, and still favored invading Cuba even after the Soviets had withdrawn their missiles. 25 years later, LeMay still believed that "We could have gotten not only the missiles out of Cuba, we could have gotten the Communists out of Cuba at that time."
After the crisis the United States and the Soviet Union created the Moscow–Washington hotline, a direct communications link between Moscow and Washington, D.C. The purpose was to have a way that the leaders of the two Cold War countries could communicate directly to solve such a crisis. The world-wide US Forces DEFCON 3 status was returned to DEFCON 4 on November 20, 1962. U-2 pilot Major Anderson's body was returned to the United States and he was buried with full military honors in South Carolina. He was the first recipient of the newly created Air Force Cross, which was awarded posthumously.
Although Anderson was the only combatant fatality during the crisis, 11 crew members of three reconnaissance Boeing RB-47 Stratojets of the 55th Strategic Reconnaissance Wing were also killed in crashes during the period between September 27 and November 11, 1962. Further, seven crew died when a MATS Boeing C-135B Stratolifter delivering ammunition to Guantanamo Bay Naval Base stalled and crashed on approach on October 23.
Critics including Seymour Melman and Seymour Hersh suggested that the Cuban Missile Crisis encouraged US use of military means, such as in the Vietnam War. This Soviet–American confrontation was synchronous with the Sino-Indian War, dating from the US's military blockade of Cuba; historians speculate that the Chinese attack against India for disputed land was meant to coincide with the Cuban Missile Crisis.
Post-crisis revelations.
Arthur M. Schlesinger, Jr., a historian and adviser to John F. Kennedy, told National Public Radio in an interview on October 16, 2002, that Castro did not want the missiles, but that Khrushchev had pressured Castro to accept them. Castro was not completely happy with the idea but the Cuban National Directorate of the Revolution accepted them to protect Cuba against US attack, and to aid its ally, the Soviet Union.:272 Schlesinger believed that when the missiles were withdrawn, Castro was angrier with Khrushchev than he was with Kennedy because Khrushchev had not consulted Castro before deciding to remove them. Although Castro was infuriated by Khrushchev, he planned on striking the United States with remaining missiles immediately after the blockade was lifted.:311
In early 1992, it was confirmed that Soviet forces in Cuba had, by the time the crisis broke, received tactical nuclear warheads for their artillery rockets and Il-28 bombers. Castro stated that he would have recommended their use if the US invaded despite knowing Cuba would be destroyed.
Arguably the most dangerous moment in the crisis was only recognized during the Cuban Missile Crisis Havana conference in October 2002. Attended by many of the veterans of the crisis, they all learned that on October 27, 1962, the USS "Beale" had tracked and dropped signaling depth charges (the size of hand grenades) on the B-59, a Soviet Project 641 (NATO designation "Foxtrot") submarine which, unknown to the US, was armed with a 15-kiloton nuclear torpedo. Running out of air, the Soviet submarine was surrounded by American warships and desperately needed to surface. An argument broke out among three officers on the "B-59", including submarine captain Valentin Savitsky, political officer Ivan Semonovich Maslennikov, and Deputy brigade commander Captain 2nd rank (US Navy Commander rank equivalent) Vasili Arkhipov. An exhausted Savitsky became furious and ordered that the nuclear torpedo on board be made combat ready. Accounts differ about whether Commander Arkhipov convinced Savitsky not to make the attack, or whether Savitsky himself finally concluded that the only reasonable choice left open to him was to come to the surface.:303, 317 During the conference Robert McNamara stated that nuclear war had come much closer than people had thought. Thomas Blanton, director of the National Security Archive, said, "A guy called Vasili Arkhipov saved the world."
Fifty years after the crisis, Graham Allison wrote:
Fifty years ago, the Cuban missile crisis brought the world to the brink of nuclear disaster. During the standoff, US President John F. Kennedy thought the chance of escalation to war was "between 1 in 3 and even," and what we have learned in later decades has done nothing to lengthen those odds. We now know, for example, that in addition to nuclear-armed ballistic missiles, the Soviet Union had deployed 100 tactical nuclear weapons to Cuba, and the local Soviet commander there could have launched these weapons without additional codes or commands from Moscow. The US air strike and invasion that were scheduled for the third week of the confrontation would likely have triggered a nuclear response against American ships and troops, and perhaps even Miami. The resulting war might have led to the deaths of 100 million Americans and over 100 million Russians.
BBC journalist Joe Matthews published on October 13, 2012, the story behind the 100 tactical nuclear warheads mentioned by Graham Allison in the excerpt above. Khrushchev feared that Castro's hurt pride and widespread Cuban indignation over the concessions he had made to Kennedy might lead to a breakdown of the agreement between the Soviet Union and the United States. In order to prevent this Khrushchev decided to make Cuba a special offer. The offer was to give Cuba more than 100 tactical nuclear weapons that had been shipped to Cuba along with the long-range missiles, but which crucially had passed completely under the radar of US intelligence. Khrushchev concluded that because the Americans hadn't listed the missiles on their list of demands, the Soviet Union's interests would be well served by keeping them in Cuba.
Anastas Mikoyan was tasked with the negotiations with Castro over the missile transfer deal designed to prevent a breakdown in the relations between Cuba and the Soviet Union. While in Havana, Mikoyan witnessed the mood swings and paranoia of Castro, who was convinced that Moscow had made the agreement with the United States at the expense of Cuba's defense. Mikoyan, on his own initiative, decided that Castro and his military not be given control of weapons with an explosive force equal to 100 Hiroshima-sized bombs under any circumstances. He defused the seemingly intractable situation, which risked re-escalating the crisis, on November 22, 1962. During a tense, four-hour meeting, Mikoyan convinced Castro that despite Moscow's desire to help, it would be in breach of an unpublished Soviet law (which didn't actually exist) to transfer the missiles permanently into Cuban hands and provide them with an independent nuclear deterrent. Castro was forced to give way and – much to the relief of Khrushchev and the whole Soviet government – the tactical nuclear weapons were crated and returned by sea to the Soviet Union during December 1962.
Further reading.
</dl>
Historiography.
</dl>
Primary sources.
</dl>
Lesson plans.
</dl>

</doc>
<doc id="6828" url="http://en.wikipedia.org/wiki?curid=6828" title="Aquilegia">
Aquilegia

Aquilegia (common names: Granny's Bonnet or Columbine) is a genus of about 60-70 species of perennial plants that are found in meadows, woodlands, and at higher altitudes throughout the Northern Hemisphere, known for the spurred petals of their flowers.
Etymology.
The genus name "Aquilegia" is derived from the Latin word for eagle ("aquila"), because the shape of the flower petals, which are said to resemble an eagle's claw. The common name "columbine" comes from the Latin for "dove", due to the resemblance of the inverted flower to five doves clustered together.
Description.
The fruit is a follicle.
the five points that stick out further than the petals are the calix (chalis).
Relatives.
Columbines are closely related to plants in the genera "Actaea" (baneberries) and "Aconitum" (wolfsbanes/monkshoods), which like "Aquilegia" produce cardiogenic toxins.
Insects.
They are used as food plants by some Lepidoptera (butterfly and moth) caterpillars. These are mainly of noctuid moths – noted for feeding on many poisonous plants without harm – such as Cabbage Moth ("Mamestra brassicae"), Dot Moth ("Melanchra persicariae") and Mouse Moth ("Amphipyra tragopoginis"). The Engrailed ("Ectropis crepuscularia"), a geometer moth, also uses columbine as a larval foodplant.
Cultivation.
Columbine is a hardy perennial, which propagates by seed. It will grow to a height of 15 to 20 inches. It will grow in full sun; however, it prefers growing in partial shade and well drained soil, and is able to tolerate average soils and dry soil conditions. Columbine is rated at hardiness zone 3 in the USA so does not require mulching or protection in the winter.
Large numbers of hybrids are available for the garden, since the British "A. vulgaris" was joined by other European and North American varieties.
 Aquilegia species are very interfertile, and will self-sow. Some varieties are short-lived so are better treated as biennials. The following hybrid cultivars have gained the Royal Horticultural Society's Award of Garden Merit:
The British National Collection of Aquilegia is held by Mrs Carrie Thomas at Killay near Swansea.
Uses.
The flowers of various species of columbine were consumed in moderation by Native Americans as a condiment with other fresh greens, and are reported to be very sweet, and safe if consumed in small quantities. The plant's seeds and roots are highly poisonous however, and contain cardiogenic toxins which cause both severe gastroenteritis and heart palpitations if consumed as food. Native Americans used very small amounts of "Aquilegia" root as a treatment for ulcers. However, the medical use of this plant is better avoided due to its high toxicity; columbine poisonings may be fatal.
An acute toxicity test in mice has demonstrated that ethanol extract mixed with isocytisoside, the main flavonoid compound from the leaves and stems of "Aquilegia vulgaris", can be classified as non-toxic, since a dose of 3000 mg/kg did not cause mortality.
Culture.
The Colorado Blue Columbine ("A. caerulea") is the official state flower of Colorado (see also Columbine, Colorado).
Evolution.
Columbines have been important in the study of evolution. It was found that Sierra Columbine ("A. pubescens") and Crimson Columbine ("A. formosa") each have adapted specifically to a pollinator. Bees and hummingbirds are the visitors to "A. formosa", while hawkmoths would only visit "A. pubescens" when given a choice. Such a "pollination syndrome", being due to flower color and orientation controlled by their genetics, ensures reproductive isolation and can be a cause of speciation.
"Aquilegia" petals show an enormous range of petal spur length diversity ranging from a centimeter to the 15 cm spurs of "Aquilegia longissima". Selection from pollinator shifts is suggested to have driven these changes in nectar spur length. 
Interestingly, it was shown that this amazing spur length diversity is achieved solely through changing cell shape, not cell number or cell size. This suggests that a simple microscopic change can result in a dramatic evolutionarily relevant morphological change.
Species.
Columbine species include:

</doc>
<doc id="6829" url="http://en.wikipedia.org/wiki?curid=6829" title="Cache (computing)">
Cache (computing)

In computing, a cache ( ) is a component that stores data so future requests for that data can be served faster; the data stored in a cache might be the results of an earlier computation, or the duplicates of data stored elsewhere. A cache hit occurs when the requested data can be found in a cache, while a cache miss occurs when it cannot. Cache hits are served by reading data from the cache, which is faster than recomputing a result or reading from a slower data store; thus, the more requests can be served from the cache, the faster the system performs.
To be cost-effective and to enable efficient use of data, caches are relatively small. Nevertheless, caches have proven themselves in many areas of computing because access patterns in typical computer applications exhibit the locality of reference. Moreover, access patterns exhibit temporal locality if data is requested again that has been recently requested already, while spatial locality refers to requests for data physically stored close to data that has been already requested.
Operation.
Hardware implements cache as a block of memory for temporary storage of data likely to be used again. Central processing units (CPUs) and hard disk drives (HDDs) frequently use a cache, as do web browsers and web servers.
A cache is made up of a pool of entries. Each entry has a datum (piece of data) – a copy of the same datum in some backing store. Each entry also has a tag, which specifies the identity of the datum in the backing store of which the entry is a copy.
When the cache client (a CPU, web browser, operating system) needs to access a datum presumed to exist in the backing store, it first checks the cache. If an entry can be found with a tag matching that of the desired datum, the datum in the entry is used instead. This situation is known as a "cache hit". So, for example, a web browser program might check its local cache on disk to see if it has a local copy of the contents of a web page at a particular URL. In this example, the URL is the tag, and the contents of the web page is the datum. The percentage of accesses that result in cache hits is known as the "hit rate" or "hit ratio" of the cache.
The alternative situation, when the cache is consulted and found not to contain a datum with the desired tag, has become known as a "cache miss". The previously uncached datum fetched from the backing store during miss handling is usually copied into the cache, ready for the next access.
During a cache miss, the CPU usually ejects some other entry in order to make room for the previously uncached datum. The heuristic used to select the entry to eject is known as the replacement policy. One popular replacement policy, "least recently used" (LRU), replaces the least recently used entry (see cache algorithm). More efficient caches compute use frequency against the size of the stored contents, as well as the latencies and throughputs for both the cache and the backing store. This works well for larger amounts of data, longer latencies and slower throughputs, such as experienced with a hard drive and the Internet, but is not efficient for use with a CPU cache.
Writing policies.
When a system writes a datum to cache, it must at some point write that datum to backing store as well. The timing of this write is controlled by what is known as the "write policy".
There are two basic writing approaches:
A write-back cache is more complex to implement, since it needs to track which of its locations have been written over, and mark them as "dirty" for later writing to the backing store. The data in these locations are written back to the backing store only when they are evicted from the cache, an effect referred to as a "lazy write". For this reason, a read miss in a write-back cache (which requires a block to be replaced by another) will often require two memory accesses to service: one to write the replaced data from the cache back to the store, and then one to retrieve the needed datum.
Other policies may also trigger data write-back. The client may make many changes to a datum in the cache, and then explicitly notify the cache to write back the datum.
No data is returned on write operations, thus there are two approaches for situations of write-misses:
Both write-through and write-back policies can use either of these write-miss policies, but usually they are paired in this way:
Entities other than the cache may change the data in the backing store, in which case the copy in the cache may become out-of-date or "stale". Alternatively, when the client updates the data in the cache, copies of those data in other caches will become stale. Communication protocols between the cache managers which keep the data consistent are known as coherency protocols.
Applications.
CPU cache.
Small memories on or close to the CPU can operate faster than the much larger main memory. Most CPUs since the 1980s have used one or more caches, and modern high-end embedded, desktop and server microprocessors may have as many as six, each specialized for a specific function. Examples of caches with a specific function are the D-cache and I-cache (data cache and instruction cache).
GPU cache.
Earlier graphics processing units (GPUs) did not feature hardware-managed caches; however, as GPUs moved toward general-purpose computing (GPGPU), they have introduced and used progressively larger caches. For example, GT200 architecture GPUs did not feature an L2 cache, while the Fermi GPU has 768 KB of last-level cache, the Kepler GPU has 1536 KB of last-level cache, and the Maxwell GPU has 2048 KB of last-level cache.
Translation lookaside buffer.
A memory management unit (MMU) that fetches page table entries from main memory has a specialized cache, used for recording the results of virtual address to physical address translations. This specialized cache is called a translation lookaside buffer (TLB).
Disk cache.
While CPU caches are generally managed entirely by hardware, a variety of software manages other caches. The page cache in main memory, which is an example of disk cache, is managed by the operating system kernel.
While the disk buffer, which is an integrated part of the hard disk drive, is sometimes misleadingly referred to as "disk cache", its main functions are write sequencing and read prefetching. Repeated cache hits are relatively rare, due to the small size of the buffer in comparison to the drive's capacity. However, high-end disk controllers often have their own on-board cache of the hard disk drive's data blocks.
Finally, a fast local hard disk drive can also cache information held on even slower data storage devices, such as remote servers (web cache) or local tape drives or optical jukeboxes; such a scheme is the main concept of hierarchical storage management. Also, fast flash-based solid-state drives (SSDs) can be used as caches for slower rotational-media hard disk drives, working together as hybrid drives or solid-state hybrid drives (SSHDs).
Web cache.
Web browsers and web proxy servers employ web caches to store previous responses from web servers, such as web pages and images. Web caches reduce the amount of information that needs to be transmitted across the network, as information previously stored in the cache can often be re-used. This reduces bandwidth and processing requirements of the web server, and helps to improve responsiveness for users of the web.
Web browsers employ a built-in web cache, but some internet service providers or organizations also use a caching proxy server, which is a web cache that is shared among all users of that network.
Another form of cache is P2P caching, where the files most sought for by peer-to-peer applications are stored in an ISP cache to accelerate P2P transfers. Similarly, decentralised equivalents exist, which allow communities to perform the same task for P2P traffic, for example, Corelli.
Other caches.
The BIND DNS daemon caches a mapping of domain names to IP addresses, as does a resolver library.
Write-through operation is common when operating over unreliable networks (like an Ethernet LAN), because of the enormous complexity of the coherency protocol required between multiple write-back caches when communication is unreliable. For instance, web page caches and client-side network file system caches (like those in NFS or SMB) are typically read-only or write-through specifically to keep the network protocol simple and reliable.
Search engines also frequently make web pages they have indexed available from their cache. For example, Google provides a "Cached" link next to each search result. This can prove useful when web pages from a web server are temporarily or permanently inaccessible.
Another type of caching is storing computed results that will likely be needed again, or memoization. For example, ccache is a program that caches the output of the compilation, in order to speed up later compilation runs.
Database caching can substantially improve the throughput of database applications, for example in the processing of indexes, data dictionaries, and frequently used subsets of data.
A distributed cache uses networked hosts to provide scalability, reliability and performance to the application. The hosts can be co-located or spread over different geographical regions.
The difference between buffer and cache.
The semantics of a "buffer" and a "cache" are not necessarily mutually exclusive; even so, there are fundamental differences in intent between the process of caching and the process of buffering.
Fundamentally, caching realizes a performance increase for transfers of data that is being repeatedly transferred. While a caching system may realize a performance increase upon the initial (typically write) transfer of a data item, this performance increase is due to buffering occurring within the caching system.
With read caches, a data item must have been fetched from its residing location at least once in order for subsequent reads of the data item to realize a performance increase by virtue of being able to be fetched from the cache's (faster) intermediate storage rather than the data's residing location. With write caches, a performance increase of writing a data item may be realized upon the first write of the data item by virtue of the data item immediately being stored in the cache's intermediate storage, deferring the transfer of the data item to its residing storage at a later stage or else occurring as a background process. Contrary to strict buffering, a caching process must adhere to a (potentially distributed) cache coherency protocol in order to maintain consistency between the cache's intermediate storage and the location where the data resides. Buffering, on the other hand,
With typical caching implementations, a data item that is read or written for the first time is effectively being buffered; and in the case of a write, mostly realizing a performance increase for the application from where the write originated. Additionally, the portion of a caching protocol where individual writes are deferred to a batch of writes is a form of buffering. The portion of a caching protocol where individual reads are deferred to a batch of reads is also a form of buffering, although this form may negatively impact the performance of at least the initial reads (even though it may positively impact the performance of the sum of the individual reads). In practice, caching almost always involves some form of buffering, while strict buffering does not involve caching.
A buffer is a temporary memory location that is traditionally used because CPU instructions cannot directly address data stored in peripheral devices. Thus, addressable memory is used as an intermediate stage. Additionally, such a buffer may be feasible when a large block of data is assembled or disassembled (as required by a storage device), or when data may be delivered in a different order than that in which it is produced. Also, a whole buffer of data is usually transferred sequentially (for example to hard disk), so buffering itself sometimes increases transfer performance or reduces the variation or jitter of the transfer's latency as opposed to caching where the intent is to reduce the latency. These benefits are present even if the buffered data are written to the buffer once and read from the buffer once.
A cache also increases transfer performance. A part of the increase similarly comes from the possibility that multiple small transfers will combine into one large block. But the main performance-gain occurs because there is a good chance that the same data will be read from cache multiple times, or that written data will soon be read. A cache's sole purpose is to reduce accesses to the underlying slower storage. Cache is also usually an abstraction layer that is designed to be invisible from the perspective of neighboring layers.

</doc>
<doc id="6830" url="http://en.wikipedia.org/wiki?curid=6830" title="Columbus, Indiana">
Columbus, Indiana

Columbus is a city in and the county seat of Bartholomew County, Indiana, United States. The population was 44,061 at the 2010 census. Located approximately forty miles (64 km) south of Indianapolis, on the east fork of the White River, it is the state's 20th largest city. It is also the principal city of the Columbus, Indiana metropolitan statistical area which encompasses all of Bartholomew County.
"National Geographic Traveler" ranked Columbus 11th on its historic destinations list in late 2008, describing the city as "authentic, unique, and unspoiled." Columbus won the national contest "America in Bloom" in 2006, and in 2004 it was named one of "The Ten Most Playful Towns" by "Nick Jr. Family Magazine". The July 2005 edition of "GQ" magazine named Columbus one of the "62 Reasons to Love Your Country". Columbus is the headquarters of the engine company Cummins, Inc.
History.
The land which is now Columbus was bought by General John Tipton and Luke Bonesteel in 1820. General Tipton built a log cabin on Mt. Tipton, a small hill overlooking White River and the surrounding flat, heavily forested and swampy valley. The town was known as Tiptonia, named in honor of General Tipton. The town's name was changed to Columbus on March 20, 1821. General Tipton was upset by the name change and decided to leave the newly founded town. He later became the Highway Commissioner for the State of Indiana and was assigned to building a highway from Indianapolis, Indiana to Louisville, Kentucky. When the road reached Columbus, Tipton constructed the first bypass road ever built; it detoured south around the west side of Columbus en route to Seymour.
Joseph McKinney was the first to plot the town of Columbus, but no date was recorded.
It was recorded for years in the local history books that the land on which Columbus sits was donated by General Tipton; however, a deed purporting to show a sale of the land was acquired in 2003 by . The deed indicated that General Tipton actually sold the land.
A ferry was established to avoid crossing both the Flatrock and Driftwood rivers, which join only a short distance above the site of the ferry. This became a village of three or four log cabins, and a store was added in 1821. Later that year, Bartholomew County was organized by an act of the State Legislature and named to honor the famous Hoosier militiaman, General Joseph Bartholomew. Columbus was incorporated on June 28, 1864.
The first railroad in Indiana reached Columbus from Madison, Indiana in 1844. This eventually became the Madison branch of the Pennsylvania Railroad. The railroad fostered the growth of the community into one of the largest communities in Indiana, and three more railroads reached the city by 1850.
Columbus is host to the oldest theater in the State of Indiana, The Crump Theatre, which was built in 1889 by John Crump. Today the building is in a National Register District and an all-ages venue with occasional musical performances. Columbus was host to the former oldest continually operated bookstore in Indiana: Cummins Bookstore. The bookstore first began operations in 1892 and closed in late 2007.
The Irwin Union Bank building was built in 1954. It was designated a National Historic Landmark by the National Park Service in 2001 in recognition of its unique architecture. The building consists of a one-story bank structure adjacent to a three-story office annex. A portion of the office annex was built along with the banking hall in 1954. The remaining larger portion, designed by Kevin Roche John Dinkeloo and Associates, was built in 1973. Eero Saarinen designed the bank building with its glazed hall to be set off against the blank background of its three-story brick annex. Two steel and glass vestibule connectors lead from the north side of this structure to the annex. The building was designed to distance the Irwin Union Bank from traditional banking architecture, which mostly echoed imposing, neoclassical style buildings of brick or stone. Tellers were behind iron bars and removed from their customers. Saarinen worked to develop a building that would welcome customers rather than intimidate them.
Columbus has been home to many manufacturing companies, including Noblitt-Sparks Industries (which built radios under the Arvin brand in the 1930s) and Arvin Industries, now Meritor, Inc. After merging with Meritor Automotive on July 10, 2000, the headquarters of the newly created ArvinMeritor Industries was established in Troy, Michigan, the home of parent company, Rockwell International. It was announced in February 2011 that the company name would revert to Meritor, Inc. Cummins, Inc. is by far the region's largest employer, and the Infotech Park accounts for a sizable number of research jobs in Columbus proper. Just south of Columbus are the North American headquarters of , the world's largest material handling (forklift) manufacturer. Other notable industries include architecture, a discipline for which Columbus is famous worldwide. The late J. Irwin Miller (then president and chairman of Cummins Engine Company) launched the Cummins Foundation, a charitable program that helps subsidize a large number of architectural projects throughout the city by up-and-coming engineers and architects.
Early in the 20th century, Columbus also was home to a number of pioneering car manufacturers, including Reeves, which produced the unusual four-axle Octoauto and the twin rear axle Sextoauto, both around 1911.
Because Columbus is far enough from Indianapolis, it benefits tremendously from nearby commuters who recognize Columbus as a major city in its own right. Nearly 19,000 workers commute into the city from the surrounding townships and villages. In recent years city officials have explored ways to revitalize the city and return Columbus to the days when Miller's architectural innovation made it one of the most envied cities in the US. Economic development, widespread beautification innovations, various tax incentives, and increased law enforcement have helped Columbus overcome what some considered a slump during the 1980s and 1990s.
Geography.
Columbus is located at (39.213998, −85.911056). The Driftwood and Flatrock Rivers converge at Columbus to form the East Fork of the White River.
According to the United States Census Bureau, the city has a total area of 27.89 sqmi, of which 27.50 sqmi is land and 0.39 sqmi is water.
Demographics.
2010 census.
As of the census of 2010, there were 44,061 people, 17,787 households, and 11,506 families residing in the city. The population density was 1602.2 PD/sqmi. There were 19,700 housing units at an average density of 716.4 /sqmi. The racial makeup of the city was 86.9% White, 2.7% African American, 0.2% Native American, 5.6% Asian, 0.1% Pacific Islander, 2.5% from other races, and 2.0% from two or more races. Hispanic or Latino of any race were 5.8% of the population.
There were 17,787 households of which 33.5% had children under the age of 18 living with them, 48.5% were married couples living together, 11.7% had a female householder with no husband present, 4.5% had a male householder with no wife present, and 35.3% were non-families. 29.7% of all households were made up of individuals and 11.5% had someone living alone who was 65 years of age or older. The average household size was 2.43 and the average family size was 3.00.
The median age in the city was 37.1 years. 25.2% of residents were under the age of 18; 8.1% were between the ages of 18 and 24; 27.3% were from 25 to 44; 24.9% were from 45 to 64; and 14.4% were 65 years of age or older. The gender makeup of the city was 48.4% male and 51.6% female.
2000 census.
As of the census of 2000, there were 39,059 people, 15,985 households, and 10,566 families residing in the city. The population density was 1,505.3 people per square mile (581.1/km²). There were 17,162 housing units at an average density of 661.4 per square mile (255.3/km²). The racial makeup of the city was 91.32% White, 2.71% Black or African American, 0.13% Native American, 3.23% Asian, 0.05% Pacific Islander, 1.39% from other races, and 1.19% from two or more races. 2.81% of the population were Hispanic or Latino of any race.
There were 15,985 households out of which 31.8% had children under the age of 18 living with them, 51.9% were married couples living together, 11.0% had a female householder with no husband present, and 33.9% were non-families. 29.1% of all households were composed of individuals and 10.7% had someone living alone who was 65 years of age or older. The average household size was 2.39, and the average family size was 2.94.
In the city the population was spread out with 25.7% under the age of 18, 8.0% from 18 to 24 years, 29.5% from 25 to 44 years, 23.0% from 45 to 64 years, and 13.7% over the age of 65. The median age was 36 years. There were 92.8 males for every 100 females and 89.6 males for every 100 females over age 18.
The median income for a household in the city was $41,723, and the median income for a family was $52,296. Males had a median income of $40,367 versus $24,446 for females, and the per capita income was $22,055. About 6.5% of families and 8.1% of the population were below the poverty line, including 9.7% of those under age 18 and 8.8% of those age 65 or over.
Arts and culture.
Columbus is a city known for its . J. Irwin Miller, 2nd CEO and a nephew of a Co-Founder of Cummins Inc., the Columbus-headquartered diesel engine manufacturer, instituted a program in which the Cummins company paid the architects' fee, provided the client selected a firm from a list compiled by Miller. The plan was initiated with public schools and was so successful that Miller decided to defray the design costs of fire stations, public housing, and other community structures. The high number of notable public buildings and sculptures in the Columbus area, designed by such individuals as Eero Saarinen, I.M. Pei, Robert Venturi, Cesar Pelli, and Richard Meier have led to Columbus earning the nickname "Athens of the Prairie." Six buildings, built between 1942 and 1965, are National Historic Landmarks, and approximately 60 other buildings sustain the Bartholomew County seat's reputation as a showcase of modern architecture. National Geographic Magazine once devoted an entire article to the town's architecture.
Sports.
The Indiana Diesels of the Premier Basketball League play their home games at the gymnasium at Ceraland Park, with plans to move to a proposed downtown sports complex in the near future.
Columbus also boasts a roller derby league, the Terrorz of Tiny Towns. Established in 2010, this league hosts weekly practices at Columbus Skateland. The town also has two cricket teams, both which play under the name of Columbus Indiana Cricket Club; their home ground is at Ceraland park.
Parks and recreation.
Columbus boasts over 700 acres of parks and green space and over 20 miles of People Trails. These amenities, in addition to several world class athletic and community facilities, including Donner Aquatic Center, Lincoln Park Softball Complex, Hamilton Center Ice Arena, Clifty Park, Foundation for Youth/Columbus Gymnastics Center and The Commons, are managed and maintained by the Columbus Parks and Recreation Department.
Government.
Columbus uses the Mayor-Council form of government. The council consists of seven members. Five are elected from one of five wards the other two are elected at-large. The Mayor is elected in a citywide vote. The current mayor is Kristen Brown.
Notable residents.
This is a list of notable people who were born in, or who currently live, or have lived in Columbus.
Entertainment.
There is currently one mainstream movie theater, AMC 12, which shows new movies, and the Yes! Cinema shows independent, older, and foreign films from its location downtown. The landmark Crump Theatre featured occasional local performances, such as comedy and local rock or punk bands, and occasional theatrical performances.
There is a canoe livery, Blue's Canoes, that offers canoeing, rafting and kayaking trips on the nearby Driftwood River.
Education.
The Bartholomew Consolidated School Corporation operates public schools.

</doc>
<doc id="6834" url="http://en.wikipedia.org/wiki?curid=6834" title="List of computer scientists">
List of computer scientists

This is a list of computer scientists, people who do work in computer science, in particular researchers and authors.
Some persons notable as programmers are included here because they work in research as well as program. A few of these people pre-date the invention of the digital computer; they are now regarded as computer scientists because their work can be seen as leading to the invention of the computer. Others are mathematicians whose work falls within what would now be called theoretical computer science, such as complexity theory and algorithmic information theory.
 * A

</doc>
<doc id="6836" url="http://en.wikipedia.org/wiki?curid=6836" title="Cultural production and nationalism">
Cultural production and nationalism

Literature, visual arts, music, and scholarship have complex relationships with ideological forces.
The 19th Century.
In the 19th century nationalism was an especially potent influence on all of these fields. To summarize, every established national group used cultural productions to assert and strengthen a sense of national unity and destiny; less politically consolidated groups, especially those pursuing the goal of nationhood, used them in the same ways, though often with a note of determination that makes them easier to see from our contemporary point of reference.
Natural admiration for excellence and justifiable pride in a predecessor's achievements is sometimes difficult to sort out from other intentions. Dante was a great poet, the Societa Dantesca Italiana did great work in editing and publishing a usable and affordable text, but the "Divine Comedy" was certainly used by the newly unified Italian government (see History of Italy) to encourage a more homogeneous, Tuscan-influenced dialect for the whole peninsula (see Italian language).
The Academy.
This relationship between ideology and serious work is particularly ambiguous in the academic fields of historical importance. Much as 19th century science is often treated as the inventor of conceptions of evolution and race which had serious negative political and social consequences, many 19th century historians pursued what they intended as reasonably objective research projects in the history of their own and other regions either to end by themselves using the results to support nationalistic goals or to see their work used that way by others. 
More politically consolidated nations sponsored historical research projects which produced results of permanent value - such as the "Monumenta Germaniae Historica" ("Monuments of German History") project. The "MGH" is a vast series (it runs to hundreds of volumes and is still publishing) of edited primary source material essential for scholarly work on late Antiquity and the Middle Ages. However, the term "German" in the title was interpreted in the broadest possible sense, and its initial royal patronage made the connection clear between a perceived unity of Germanness in history and 19th century Germanness.

</doc>
<doc id="6839" url="http://en.wikipedia.org/wiki?curid=6839" title="CRESU experiment">
CRESU experiment

The CRESU experiment (French: "Cinétique de Réaction en Ecoulement Supersonique Uniforme", lit. Reaction Kinetics in Uniform Supersonic Flow) is an experiment investigating chemical reactions taking place at very low temperatures.
The technique involves the expansion of a gas or mixture of gases through a de Laval nozzle from a high pressure reservoir into a vacuum chamber. As it expands, the nozzle collimates the gas into a uniform supersonic beam that is essentially collision free and has a temperature that, in the centre of mass frame, can be significantly below that of the reservoir gas. Each nozzle produces a characteristic temperature. This way, any temperature between room temperature and about 10K can be achieved. There are relatively few CRESU apparatuses in existence for the simple reason that the gas throughput and pumping requirements are huge, which makes them expensive to run. Two of the leading centres have been the University of Rennes (France) and the University of Birmingham (UK). A more recent development has been a pulsed version of the CRESU, which requires far less gas and therefore smaller pumps. One might well ask why we should use such a complex method for producing low temperature gases when they could be produced much more easily using liquid helium. The answer is simple: most species have a negligible vapour pressure at such low temperatures and this means that they quickly condense on the sides of the apparatus. Essentially, the CRESU technique provides a "wall-less flow tube," which allows the kinetics of gas phase reactions to be investigated at much lower temperatures than otherwise possible.
Chemical kinetics experiments can then be carried out in a "pump-probe" fashion using a laser to initiate the reaction (for example by preparing one of the reagents by photolysis of a precursor), followed by observation of that same species (for example by laser-induced fluorescence) after a known time delay. The fluorescence signal is captured by a photomultiplier a known distance downstream of the de Laval nozzle. The time delay can be varied up to the maximum corresponding to the flow time over that known distance. By studying how quickly the reagent species disappears in the presence of differing concentrations of a (usually stable) co-reagent species the reaction rate constant at the low temperature of the CRESU flow can be determined.
Reactions studied by the CRESU technique typically have no significant activation energy barrier. In the case of neutral-neutral reactions (i.e., not involving any charged species, ions), these type of barrier-free reactions usually involve free radical species such as molecular oxygen (O2), the cyanide radical (CN) or the hydroxyl radical (OH). The energetic driving force for these reactions is typically an attractive long range intermolecular potential.
CRESU experiments have been used to show deviations from Arrhenius kinetics at low temperatures: as the temperature is reduced, the rate constant actually increases. They can explain why chemistry is so prevalent in the interstellar medium, where many different polyatomic species have been detected (by radio astronomy), but where temperatures are so low that conventional wisdom might suggest that chemical reactions do not occur.

</doc>
<doc id="6840" url="http://en.wikipedia.org/wiki?curid=6840" title="Cygwin">
Cygwin

Cygwin ( ) is a Unix-like environment and command-line interface for Microsoft Windows. Cygwin provides native integration of Windows-based applications, data, and other system resources with applications, software tools, and data of the Unix-like environment. Thus it is possible to launch Windows applications from the Cygwin environment, as well as to use Cygwin tools and applications within the Windows operating context.
Cygwin consists of two parts: a dynamic-link library (DLL) as an API compatibility layer providing a substantial part of the POSIX API functionality, and an extensive collection of software tools and applications that provide a Unix-like look and feel.
Cygwin was originally developed by Cygnus Solutions, which was later acquired by Red Hat. It is free and open source software, released under the GNU General Public License version 3. Today it is maintained by employees of Red Hat, NetApp and many other volunteers.
Description.
Cygwin consists of a library that implements the POSIX system call API in terms of Win32 system calls, a GNU development toolchain (including GCC and GDB) to allow software development, and a large number of application programs equivalent to those on Unix systems. Programmers have ported many Unix, GNU, BSD and Linux programs and packages to Cygwin, including the X Window System, K Desktop Environment 3, GNOME, Apache, and TeX. Cygwin permits installing inetd, syslogd, sshd, Apache, and other daemons as standard Windows services, allowing Microsoft Windows systems to emulate Unix and Linux servers.
Cygwin programs are installed by running Cygwin's "setup" program, which downloads the necessary program and feature package files from repositories on the Internet. Setup can install, update, and remove programs and their source code packages. A complete installation will take in excess of 36 GB of hard disk space, but usable configurations may require as little as 1 or 2 GB.
Efforts to reconcile concepts that differ between Unix and Windows systems include:
The version of gcc that comes with Cygwin has various extensions for creating Windows DLLs, specifying whether a program is a windowing or console-mode program, adding resources, etc. Support for compiling programs that do not require the POSIX compatibility layer provided by the Cygwin DLL used to be included in the default codice_13, but as of 2014[ [update]] is provided by cross-compilers contributed by the MinGW-w64 project.
Cygwin is used heavily for porting many popular pieces of software to the Windows platform. It is used to compile Sun Java, OpenOffice.org, LibreOffice, and even web server software like Lighttpd and Hiawatha.
Red Hat normally licenses the Cygwin library under the GNU General Public License version 3 with an exception to allow linking to any free and open source software whose license conforms to the Open Source Definition. Red Hat also sells commercial licenses to those who wish to redistribute programs that use the Cygwin library under proprietary terms.
History.
Cygwin began in 1995 as a project of Steve Chamberlain, a Cygnus engineer who observed that Windows NT and 95 used COFF as their object file format, and that GNU already included support for x86 and COFF, and the C library newlib. He thought it would be possible to retarget GCC and produce a cross compiler generating executables that could run on Windows. This proved practical and a prototype was quickly developed.
The next step was to attempt to bootstrap the compiler on a Windows system, requiring sufficient emulation of Unix to let the GNU configure shell script run. A Bourne shell-compatible command interpreter, such as bash, was needed and in turn a fork system call emulation and standard input/output. Windows includes similar functionality, so the Cygwin library just needed to provide a POSIX-compatible application programming interface (API) and properly translate calls and manage private versions of data, such as file descriptors.
Initially, Cygwin was called gnuwin32 (not to be confused with the current GnuWin32 project). The name was changed to Cygwin32 to emphasize Cygnus' role in creating it. When Microsoft registered the trademark Win32, the 32 was dropped to simply become Cygwin.
By 1996, other engineers had joined in, because it was clear that Cygwin would be a useful way to provide Cygnus' embedded tools hosted on Windows systems (the previous strategy had been to use DJGPP). It was especially attractive because it was possible to do a three-way cross-compile, for instance to use a hefty Sun workstation to build, say, a Windows-x-MIPS cross-compiler, which was faster than using the PC at the time. In 1999, Cygnus offered Cygwin 1.0 as a commercial product of interest in its own right although subsequent versions have not been released, instead relying on continued open source releases.
Geoffrey Noer was the project lead from 1996 to 1998. Christopher Faylor was the project lead from 1998 to mid-2014. Corinna Vinschen became co-lead since early 2004 when Faylor left Red Hat and has been lead since mid-2014, when Faylor withdrew from active participation in the project.
Features.
Cygwin's default package selection is fairly small, containing little more than the bash shell and the core file manipulation utilities expected of a Unix command line. Additional packages are available as optional installs from within Cygwin's package manager ("setup-x86.exe" - 32bit & "setup-x86_64.exe" - 64bit). These include (among many others):
The Cygwin/X project contributes an implementation of the X Window System that allows graphical Unix programs to display their user interfaces on the Windows desktop. This can be used with both local and remote programs. Cygwin ships with a fairly small number of X applications, for example:
In addition to the low-level Xlib/XCB libraries for developing X applications, Cygwin also ships with various higher-level and cross-platform GUI frameworks, including GTK+ and Qt.
The Cygwin Ports project provides many additional packages that are not available in the Cygwin distribution itself. Examples include GNOME and K Desktop Environment 3 as well as the MySQL database and the PHP scripting language.
Alternative Windows/Unix integration tools.
Several open-source and proprietary alternatives provide simultaneous access to both Windows and UNIX environments on the same hardware.
Toolsets like Microsoft Windows Services for UNIX (SFU), UWIN, MKS Toolkit for Enterprise Developers and Hamilton C shell also aim to provide a Unix-like user- and development-environment. They implement at least a shell and a set of the most popular utilities. Most include the familiar GNU and/or Unix development tools, including make, yacc, lex and a cc command which acts a wrapper around a supported C compiler. SFU also includes the GCC compiler.
MinGW provides a native software port of the GCC to Microsoft Windows, along with a set of freely-distributable import libraries and header files for the Windows API. MinGW allows developers to create native Microsoft Windows applications. In addition, a component of MinGW known as MSYS ("Minimal SYStem"), which derives from Cygwin version 1.3.3, provides a minimal Unix-like shell environment including bash and a selection of POSIX tools sufficient to enable autoconf scripts to run.
Numerous virtualization solutions provide x86 platform virtualization to run Windows and Unix-like operating systems simultaneously on the same hardware, but without the integration of the environments that Cygwin provides. Some, like VirtualBox and VMware Player run on Windows and Linux hosts and can run many other operating systems. Cooperative Linux (abbreviated "coLinux") runs a full, but modified Linux kernel like a driver under Windows, effectively making Windows and Linux two coroutines, using cooperative multitasking to switch between them.
Winelib, a part of the Wine project, is the inverse of Cygwin – it is a free and open-source compatibility layer for Unix-like operating systems on the x86 or x86-64 architecture that can allow programs written for Microsoft Windows to run on Unix-like operating systems. Unlike Cygwin, which requires "You rebuild your application from source if you want it to run on Windows", the full Wine product supports executing unmodified Windows binaries.

</doc>
<doc id="6845" url="http://en.wikipedia.org/wiki?curid=6845" title="Corinth">
Corinth

Corinth (; Greek: Κόρινθος, "Kórinthos", ]) is a city and former municipality in Corinthia, Peloponnese, Greece. Since the 2011 local government reform it is part of the municipality Corinth, of which it is the seat and a municipal unit. It is the capital of Corinthia.
It was founded as Nea Korinthos or New Corinth in 1858 after an earthquake destroyed the existing settlement of Corinth, which had developed in and around the site of ancient Corinth.
Geography.
Located about 78 km southwest of Athens, Corinth is surrounded by the coastal townlets of (clockwise) Lechaio, Isthmia, Kechries, and the inland townlets of Examilia and the archaeological site and village of ancient Corinth. Natural features around the city include the narrow coastal plain of Vocha, the Corinthian Gulf, the Isthmus of Corinth cut by its canal, the Saronic Gulf, the Oneia Mountains, and the monolithic rock of Acrocorinth, where the medieval acropolis was built.
History.
Corinth derives its name from Ancient Corinth, a city-state of antiquity. In 1858, the old city, now known as Archaia Korinthos (Αρχαία Κόρινθος), located 3 km SW of the modern city, was totally destroyed by an earthquake. Nea Korinthos or New Corinth was then built on the coast of the Gulf of Corinth, repaired after a further earthquake in 1928, and rebuilt again after a great fire in 1933.
Demographics.
Corinth is the second largest city in the region of the Peloponnese.
The Municipality of Corinth, or "Dimos Korinthion", has a population of 58,280 (2011 census) and 36,991 (2001 census). It has a population density of 95.34 per km². The municipality includes the town of Archaia Korinthos (1,770 inhabitants), the town of Examilia (1,547 inhabitants), and the smaller settlements of Xylokeriza (777 inhabitants) and Corinthia (686 inhabitants).
Economy.
Industry.
Corinth is a major industrial hub at a national level. Copper cables, petroleum products, leather, medical equipment, marble, gypsum, ceramic tiles, salt, mineral water and beverages, meat products, and gums are produced nearby. As of 2005, a period of deindustrialization has commenced as a large pipework complex, a textile factory and a meat packing facility diminished their operations.
Transport.
Roads.
Corinth is a major road hub. The A7 toll motorway for Tripoli and Kalamata, (and Sparta via A71 toll), branches off the A8/European route E94 toll motorway from Athens at Corinth. Corinth is the main entry point to the Peloponnesian peninsula, the southernmost area of continental Greece.
Railways.
The city has been connected to the Proastiakos, the Athens suburban rail network, since 2005, when the new Corinth railway station was completed.
Port.
The port of Corinth, located north of the city centre and close to the northwest entrance of the Corinth Canal, at 37 56.0’ N / 22 56.0’ E, serves the local needs of industry and agriculture. It is mainly a cargo exporting facility.
It is an artificial harbour (depth approximately 9 m, protected by a concrete mole (length approximately 930 metres, width 100 metres, mole surface 93,000 m2). A new pier finished in the late 1980s doubled the capacity of the port. The reinforced mole protects anchored vessels from strong northern winds.
Within the port operates a customs office facility and a Hellenic Coast Guard post. Sea traffic is limited to trade in the export of local produce, mainly citrus fruits, grapes, marble, aggregates and some domestic imports. The port operates as a contingency facility for general cargo ships, bulk carriers and ROROs, in case of strikes at Piraeus port. There is a ferry line (RORO) connecting Corinth to Italy.
Ferries.
There was a ferry link to Catania, Sicily and Genoa in Italy.
Canal.
The Corinth Canal, carrying ship traffic between the western Mediterranean Sea and the Aegean Sea, is about 4 km east of the city, cutting through the Isthmus of Corinth that connects the Peloponnesian peninsula to the Greek mainland, thus effectively making the former an island. The builders dug the canal through the Isthmus at sea level; no locks are employed. It is 6.4 km in length and only 21.3 m wide at its base, making it impassable for most modern ships. It now has little economic importance.
The canal was mooted in classical times and an abortive effort was made to build it in the 1st century AD. Construction finally got underway in 1881 but was hampered by geological and financial problems that bankrupted the original builders. It was completed in 1893, but due to the canal's narrowness, navigational problems and periodic closures to repair landslips from its steep walls, it failed to attract the level of traffic anticipated by its operators. It is now used mainly for tourist traffic.
Sport.
The city's association football team is Korinthos F.C. ("Π.Α.E. Κόρινθος"), established in 1999 after the merger of Pankorinthian Football Club ("Παγκορινθιακός") and Corinth Football Club ("Κόρινθος"). During the 2006-2007 season, the team played in the Greek Fourth Division's Regional Group 7. The team went undefeated that season and it earned the top spot. This granted the team a promotion to the Gamma Ethnikí (Third Division) for the 2007-2008 season. For the 2008-2009 season, Korinthos F.C. competed in the Gamma Ethniki (Third Division) southern grouping.
Twin towns — sister cities.
Corinth is twinned with:
Other locations named after Corinth.
Due to its ancient history and the presence of St. Paul the Apostle in Corinth some locations all over the world have been named Corinth.

</doc>
<doc id="6846" url="http://en.wikipedia.org/wiki?curid=6846" title="Colossae">
Colossae

Colossae (; Greek: Κολοσσαί), was an ancient city of Phrygia, on the Lycus, which is a tributary of the Maeander River. It was situated about 12 miles South East of Laodicea, and near the great road from Ephesus to the Euphrates. It has never been excavated.
Some sources distinguish Colossae from nearby Chonae (Χῶναι) now called Honaz, which they attribute to the Middle Ages; others say these were successive names for the same city.
History.
In 396 BC, during the Persian Wars, the satrap Tissaphernes was lured to Colossae and slain by an agent of the party of Cyrus the Younger. Pliny tells that the wool of Colossae gave its name ("colossinus") to the colour of the cyclamen flower. During the Hellenistic period, the town was of some mercantile importance, although by the 1st century it had dwindled greatly in size and significance.
It does not appear from his Epistle to the Colossians that St. Paul had visited this city, for it only speaks of him having heard of their faith () and since he tells Philemon of his hope to visit it upon being freed from prison (see ). To judge from the Letter to the Colossians, Epaphras was a person of some importance in the Christian community there (; ), and tradition presents him as its first bishop. Tradition also gives Philemon as the second bishop of the see. The first historically documented bishop is Epiphanius, who was not personally at the Council of Chalcedon, but whose metropolitan bishop Nunechius of Laodicea, the capital of the Roman province of Phrygia Pacatiana signed the acts on his behalf.
In Byzantine times, Colossae fell into decay (possibly due to an earthquake) and the town of Chonae arose near its ruins. The first recorded bishop of Chonae was Cosmas, who signed the acts of the Trullan Council of 692 as bishop of "Colossae or Chonae". A successor of his, named Theodosius or Dositheus, was mentioned as participating in the Second Council of Nicaea in 787 as bishop of "Chonae or Colossae". In about 858-860 the bishopric was elevated to the rank of autocephalous archdiocese and later to that of metropolitan see, but without suffragans. It is not mentioned in documents later than the end of the 14th century. No longer a residential bishopric, Colossae is today listed by the Catholic Church as a titular see.
The town was the birthplace of the Byzantine Greek writers Nicetas and Michael Choniates. In 1206–1230, it was ruled by Manuel Maurozomes.
Miracle of the Archangel Michael.
In Byzantine and Russian art, the theme of the "Miracle of the Archangel Michael at Chonae" ("Τὸ ἐν Χωναῖς/Χῶναις Θαῦμα τοῦ Ἀρχαγγέλου Μιχαήλ") is intimately linked with the site. Eastern Orthodox tradition tells that the pagans directed the stream of a river against the sanctuary of St. Michael there to destroy it, but Michael the Archangel appeared and split the rock by lightning to give a new bed to the stream, diverting the flow away from the church and sanctifying forever the waters which came from the gorge. The Orthodox celebrate a feast in commemoration of this event on 6 September. The Monastery of the Miracle (Chudov Monastery) in the Moscow Kremlin, where the Russian Tsars were baptized, was dedicated to the Feast of the Miracle at Kona.
The 5th- to 7th-century texts that refer to the miracle at Chonae formed the basis of specific paradigms for "properly approaching" angelic intermediaries for more effective prayers within the Christian culture.

</doc>
<doc id="6848" url="http://en.wikipedia.org/wiki?curid=6848" title="Charge of the Goddess">
Charge of the Goddess

 The Charge of the Goddess is an inspirational text often used in the neopagan religion of Wicca. It is usually spoken by the High Priestess after the ritual of Drawing Down the Moon. The Charge is the promise of the Goddess (embodied by the High Priestess) to all witches that she will teach and guide them. It has been called "perhaps the most important single theological document in the neo-Pagan movement".
Several versions exist, though they all have the same basic premise, that of a set of instructions given by the Great Goddess to her worshippers. The most well known version is that written by Gerald Gardner. This version includes material paraphrased from works by Aleister Crowley, primarily from Liber AL (The Book of the Law, particularly from Ch 1, spoken by Nuit, the Star Goddess), and from Liber XV (The Gnostic Mass). Other sources includes Liber LXV (Liber Cordis Cincti Serpente, or the Book of the Heart Girt with the Serpent), thus linking modern Wicca to the cosmology and revelations of Thelema. It has been shown that Gerald Gardner's book collection which was acquired by Ripley's Believe It or Not! included a copy of Crowley's The Blue Equinox which includes all of the Crowley quotations in the Charge of the Goddess.
There is also a poetic paraphrased version written by Doreen Valiente in the mid-1950s, which is contained within the traditional Gardnerian Book of Shadows.
Several different versions of a Wiccan "Charge of the God" have since been created to mirror and accompany the "Charge of the Goddess".
The Charge of the Goddess is recited during most rituals where the priestess is expected to represent, and/or embody, the Goddess within the sacred circle.
Themes.
The opening paragraph names a collection of goddesses, some derived from Greek or Roman mythology, others from Celtic or Arthurian legends, affirming a belief that these various figures represent a single Great Mother:
This theme echoes the ancient Roman belief that the Goddess Isis was known by ten thousand names and also that the Goddess still worshipped today by Wiccans and other neopagans is known under many guises but is one universal divinity.
The second paragraph is largely derived and paraphrased from the words that Aradia, the messianic daughter of Diana, speaks to her followers in Charles Godfrey Leland's 1899 book "Aradia, or the Gospel of the Witches" (London: David Nutt; various reprints). The third paragraph is largely written by Doreen Valiente, with some phrases adapted from "The Book of the Law" and "The Gnostic Mass" by Aleister Crowley.
The charge affirms that "all" acts of love and pleasure are sacred to the Goddess e.g.
"Let my worship be within the heart that rejoices,
for behold, all acts of love and pleasure are my rituals.
Therefore, let there be beauty and strength,
power and compassion, honor and humility,
mirth and reverence within you."
History.
Ancient Precedents.
In book eleven, chapter 47 of Apuleius's "The Golden Ass", Isis delivers what Ceisiwr Serith calls "essentially a charge of a goddess". This is rather different from the modern version known in Wicca, though they have the same premise, that of the rules given by a great Mother Goddess to her faithful.
The Charge of the Goddess is also known under the title "Leviter Veslis". This has been identified by the historian Ronald Hutton, cited in an article by Roger Dearnsley "The Influence of Aleister Crowley on "Ye Bok of Ye Art Magical", as a piece of medieval ecclesiastical Latin used to mean "lifting the veil." 
Origins of Wiccan Charge.
The earliest known Wiccan version is found in a document dating from the late 1940s, Gereald Gardner's ritual notebook titled "Ye Bok of Ye Art Magical" (formerly in the collection of Ripley's International, Toronto). This version draws extensively from Charles Godfrey Leland's "Aradia, or the Gospel of the Witches" and other modern sources, particularly from the works of Aleister Crowley. The oldest identifiable source is the 17th century "Centrum Naturae Concentratum" of Alipili (or Ali Puli).
It is believed to have been compiled by Gerald Gardner or possibly another member of the New Forest coven. Doreen Valiente, a student of Gardner, took his version from his "Book of Shadows" and adapted it into verse, and later into another prose version. Gardner intended his version to be a theological statement justifying the Gardnerian sequence of initiations.
Valiente felt that the influence of Crowley on the Charge was too obvious, and she did not want the Craft associated with Crowley. Gardner invited her to rewrite the Charge. She proceeded to do, her first version being into verse.
The initial verse version by Doreen Valiente consisted of eight verses, the second of which was :
Valiente was unhappy with this version, saying that "people seemed to have some difficulty with this, because of the various goddess-names which they found hard to pronounce", and so she rewrote it as a prose version, much of which differs from her initial version, and is more akin to Gardner's version. This prose version has since been modified and reproduced widely by other authors. Like the Charge found in Freemasonry, where the charge is a set of instructions read to a candidate standing in a temple, the Charge of the Goddess was intended to be read immediately before an initiation.

</doc>
<doc id="6849" url="http://en.wikipedia.org/wiki?curid=6849" title="Cy Young">
Cy Young

Denton True "Cy" Young (March 29, 1867 – November 4, 1955) was an American Major League Baseball pitcher. During his 21-year baseball career (1890–1911), he pitched for five different teams. Young established numerous pitching records, some of which have stood for a century. Young compiled 511 wins, which is most in Major League history and 94 ahead of Walter Johnson who is second on the list. Young was elected to the National Baseball Hall of Fame in 1937.
One year after Young's death, the Cy Young Award was created to honor the previous season's best pitcher.
In addition to wins, Young still holds the major league records for most career innings pitched (7,356), most career games started (815), and most complete games (749). He also retired with 316 losses, the most in MLB history. Young's 76 career shutouts are fourth all-time. He also won at least 30 games in a season five times, with ten other seasons of 20 or more wins. In addition, Young pitched three no-hitters, including the third perfect game in baseball history, first in baseball's "modern era".[a] In 1999, 88 years after his final major league appearance and 44 years after his death, editors at "The Sporting News" ranked Cy Young 14th on their list of "Baseball's 100 Greatest Players". That same year, baseball fans named him to the Major League Baseball All-Century Team.
Young's career started in 1890 with the Cleveland Spiders. After eight years with the Spiders, Young was moved to St. Louis in 1899. After two years there, Young jumped to the newly created American League, joining the Boston franchise. He was traded back to Cleveland in 1909, before spending the final two months of his career with the Boston Rustlers. After his retirement, Young went back to his farm in Ohio, where he stayed until his death at age 88 in 1955.
Early life.
Cy Young was the oldest child born to McKinzie Young, Jr. and German American Nancy Mottmiller. The couple had four more children: Jesse Carlton, Alonzo, Ella, and Anthony. When the couple married, McKinzie's father gave him the 54 acre of farm land he owned. Young was born in Gilmore, a tiny farming community located in Washington Township, Tuscarawas County, Ohio. He was christened Denton True Young. Some sources later, and even today, list his middle name erroneously as "Tecumseh", apparently as a result of being nicknamed "The Chief" by teammates.
He was raised on one of the local farms and went by the name Dent Young in his early years. Young was also known as "Farmer Young" and "Farmboy Young". Young stopped his formal education after he completed the sixth grade so he could help out on the family's farm. In 1885, Young moved with his father to Nebraska, and in the summer of 1887, they returned to Gilmore.
Cy Young played for many amateur baseball leagues during his youth, including a "semi-pro" Carrollton team in 1888. Young pitched and played second base. The first box score known containing the name Young came from that season. In that game, Young played first base and had three hits in three at-bats. After the season, Young received an offer to play for the minor league Canton team, which started Young's professional career.
Professional career.
Before Major League Baseball.
Young began his professional career in 1889 with the Canton, Ohio team of the Tri-State League, a professional minor league. During his tryout, Young impressed the scouts, recalling years later, "I almost tore the boards off the grandstand with my fast ball." Cy Young's nickname came from the fences that he had destroyed using his fastball. The fences looked like a cyclone had hit them. Reporters later shortened the name to "Cy", which became the nickname Young used for the rest of his life. During Young's one year with the Canton team, he won 15 games and lost 15.
Franchises in the National League, the major professional baseball league at the time, wanted the best players available to them. Therefore, in 1890, Young signed with the Cleveland Spiders, a team which had moved from the American Association to the National League the previous year.
Cleveland Spiders.
On August 6, 1890, Young's major league debut, he pitched a three-hit shutout. While Young was on the Spiders, Chief Zimmer was his catcher more often than any other player. Bill James, a baseball statistician, estimated that Zimmer caught Young in more games than any other battery in baseball history.
Early on, Young established himself as one of the harder-throwing pitchers in the game. Bill James wrote that Zimmer often put a piece of beefsteak inside his baseball glove to protect his catching hand from Young's fastball. In the absence of radar guns, however, it is impossible to say just how hard Young actually threw. Young continued to perform at a high level during the 1890 season. On the last day of the season, Young won both games of a doubleheader. In the first weeks of Young's career, Cap Anson, the player-manager of the Chicago Colts spotted Young's ability. Anson told Spiders' manager Gus Schmelz, "He's too green to do your club much good, but I believe if I taught him what I know, I might make a pitcher out of him in a couple of years. He's not worth it now, but I'm willing to give you $1,000 ($ today) for him." Schmelz replied, "Cap, you can keep your thousand and we'll keep the rube."
Two years after Young's debut, the National League moved the pitcher's position back by 5 ft. Since 1881, pitchers had pitched within a "box" whose front line was 50 ft from home base, and since 1887 they had been compelled to toe the back line of the box when delivering the ball. The back line was 55 ft away from home. In 1893, 5 ft was added to the back line, yielding the modern pitching distance of 60 ft. In the book "The Neyer/James Guide to Pitchers", sports journalist Rob Neyer wrote that the speed with which pitchers like Cy Young, Amos Rusie, and Jouett Meekin threw was the impetus that caused the move.
The 1892 regular season was a success for Young, who led the National League in wins (36), ERA (1.93), and shutouts (9). Just as many contemporary Minor League Baseball leagues operate today, the National League was using a split season format during the 1892 season. The Boston Beaneaters won the first-half title, and the Spiders won the second-half title, with a best-of-nine series determining the league champion. Despite the Spiders' second half run, the Beaneaters swept the series, five games to none. Young pitched three complete games in the series, but lost two decisions. He also threw a complete game shutout, but the game ended in a 0–0 tie.
The Spiders faced the Baltimore Orioles in the Temple Cup, a precursor to the World Series, in 1895. Young won three games in the series and Cleveland won the Cup, four games to one. It was around this time that Young added what he called a "slow ball" to his pitching repertoire to reduce stress on his arm. The pitch today is called a changeup.
In 1896, Young lost a no-hitter with two outs in the ninth inning when Ed Delahanty of the Philadelphia Phillies hit a single. On September 18, 1897, Young pitched the first no-hitter of his career in a game against the Cincinnati Reds. Although Young did not walk a batter, the Spiders committed four errors while on defense. One of the errors had originally been ruled a hit, but the Cleveland third baseman sent a note to the press box after the eighth inning, saying he had made an error, and the ruling was changed. Young later said, that, despite his teammate's gesture, he considered the game to be a one-hitter.
Shift to St. Louis.
Prior to the 1899 season, Frank Robison, the Spiders owner, bought the St. Louis Browns, thus owning two clubs simultaneously. The Browns were renamed the "Perfectos", and restocked with Cleveland talent. Just weeks before the season opener, most of the better Spiders players were transferred to St. Louis, including fellow pitcher Pete McBride and three future Hall of Famers: Young, Jesse Burkett, and Bobby Wallace. The roster maneuvers failed to create a powerhouse Perfectos team, as St. Louis finished fifth in both 1899 and 1900. Meanwhile, the depleted Spiders lost 134 games, the most in MLB history, before folding. Young spent two years with St. Louis, which is where he found his favorite catcher, Lou Criger. The two men were teammates for a decade.
Move to the American League.
In 1901, the rival American League declared major league status and set about raiding National League rosters. Young left St. Louis and joined the American League's Boston Americans for a $3,500 contract ($ today). Young would remain with the Boston team until 1909. In his first year in the American League, Young was dominant. Pitching to Criger, who had also jumped to Boston, Young led the league in wins, strikeouts, and ERA[b], thus earning the colloquial AL Triple Crown for pitchers. Young won almost 42% of his team's games in 1901, accounting for 33 of his team's 79 wins. In February 1902, before the start of the baseball season, Young served as a pitching coach at Harvard University. The sixth-grade graduate instructing Harvard students delighted Boston newspapers. The following year, Young coached at Mercer University during the spring. The team went on to win the Georgia state championship in 1903, 1904, and 1905.
The Boston Americans played the Pittsburgh Pirates in the first modern World Series in 1903. Young, who started Game One against the visiting Pirates, thus threw the first pitch in modern World Series history. The Pirates scored four runs in that first inning, and Young lost the game. Young performed better in subsequent games, winning his next two starts. He also drove in three runs in Game Five. Young finished the series with a 2–1 record and a 1.85 ERA in four appearances, and Boston defeated Pittsburgh, five games to three games.
After one-hitting Boston on May 2, 1904, Philadelphia Athletics pitcher Rube Waddell taunted Young to face him so that he could repeat his performance against Boston's ace. Three days later, Young pitched a perfect game against Waddell and the Athletics.[a2] It was the first perfect game in American League history.[a3] Waddell was the 27th and last batter, and when he flied out, Young shouted, "How do you like that, you hayseed?" Waddell had picked an inauspicious time to issue his challenge. Young's perfect game was the centerpiece of a pitching streak. Young set major league records for the most consecutive scoreless innings pitched and the most consecutive innings without allowing a hit; the latter record still stands at 25.1 innings, or 76 hitless batters. Even after allowing a hit, Young's scoreless streak reached a then-record 45 shutout innings. Before Young, only two pitchers had thrown perfect games.[a3] This occurred in 1880, when Lee Richmond and John Montgomery Ward pitched perfect games within five days of each other, although under somewhat different rules: the front edge of the pitcher's box was only 45 ft from home base (the modern release point is about 10 ft farther away); walks required eight balls; and pitchers were obliged to throw side-armed. Young's perfect game was the first under the modern rules established in 1893. One year later, on July 4, 1905, Rube Waddell beat Young and the Americans, 4–2, in a 20-inning matchup. Young pitched 13 consecutive scoreless innings before he gave up a pair of unearned runs in the final inning. Young did not walk a batter and was later quoted: "For my part, I think it was the greatest game of ball I ever took part in." In 1907, Young and Waddell faced off in a scoreless 13-inning tie.
In 1908, Young pitched the third no-hitter of his career. Three months past his 41st birthday, Cy Young was the oldest pitcher to record a no-hitter, a record which would stand 82 years until 43-year-old Nolan Ryan surpassed the feat. Only a walk kept Young from his second perfect game. After that runner was caught stealing, no other batter reached base. At this time, Young was the second-oldest player in either league. In another game one month before his no-hitter, he allowed just one single while facing 28 batters. On August 13, 1908, the league celebrated "Cy Young Day." No American League games were played on that day, and a group of All-Stars from the league's other teams gathered in Boston to play against Young and the Red Sox. When the season ended, he posted a 1.26 ERA, which gave him not only the lowest in his career, but also gave him a Major League record of being the oldest pitcher with 150+ innings pitched to post a season ERA under 1.50.
Cleveland Naps and retirement.
Young was traded back to Cleveland, the place where he played over half his career, before the 1909 season, to the Cleveland Naps of the American League. The following season, 1910, he won his 500th career game on July 19 against Washington. He split 1911, his final year, between the Naps and the Boston Rustlers.
On September 22, 1911, Young shut out the Pittsburgh Pirates, 1–0, for his last career victory. In his final start two weeks later, the last eight batters of Young's career combined to hit a triple, four singles, and three doubles.
College coaching career.
In February 1902, before the start of the baseball season, Young served as a pitching coach at Harvard University. The sixth-grade graduate instructing Harvard students delighted Boston newspapers. The following year, Young coached at Mercer University, in Macon, Georgia during the spring. The team went on to win the Georgia state championship in 1903, 1904, and 1905.
After baseball.
Beginning in 1912, Cy Young lived and worked on his farm. In 1913, he served as manager of the Cleveland Green Sox of the Federal League, which was at the time an outlaw minor league. However, he never worked in baseball after that.
Young's wife, Robba, whom he had known since childhood, died in 1933. After she died, Young tried several jobs, and eventually moved in with friends John and Ruth Benedum and did odd jobs for them. Young took part in many baseball events after his retirement. In 1937, 26 years after he retired from baseball, Cy Young was inducted into the Baseball Hall of Fame. He was among the first to donate mementos to the Hall.
By 1940, Young's only source of income was stock dividends of $300 per year ($<br>{Inflation} - Amount must not have "" prefix: 300.   today). On November 4, 1955, Cy Young died on the Benedum's farm at the age of 88. He was buried in Peoli, Ohio.
Baseball legacy.
Young retired with 511 career wins, which remains the record for most career wins by a pitcher. At the time, Pud Galvin had the second most career wins with 364. Walter Johnson, then in his fourth season, finished his career with 417 wins and, as of 2015, remains second on the list. In 1921, Johnson broke Young's career record for strikeouts.
Cy Young's career is seen as a bridge from baseball's earliest days to its modern era; he pitched against stars such as Cap Anson, already an established player when the National League was first formed in 1876, as well as against Eddie Collins, who played until 1930. When Young's career began, pitchers delivered the baseball underhand and fouls were not counted as strikes. The pitcher's mound was not moved back to its present position of 60 ft until Young's fourth season; he did not wear a glove until his sixth season.
Young led his league in wins five times (1892, 1895, and 1901–1903), finishing second twice. His career high was 36 in 1892. He had fifteen seasons with twenty or more wins, two more than the runners-up, Christy Mathewson and Warren Spahn. Young won two ERA titles during his career, in 1892 (1.93) and in 1901 (1.62), and was three times the runner-up. Young's earned run average was below 2.00 six times, but this was not uncommon during the dead-ball era. Although Young threw over 400 innings in each of his first four full seasons, he did not lead his league until 1902. He had 40 or more complete games nine times. Young also led his league in strikeouts twice (with 140 in 1896, and 158 in 1901), and in shutouts seven times. Young led his league in fewest walks per nine innings thirteen times and finished second one season. Only twice in his 22-year career did Young finish lower than 5th in the category. Although the WHIP ratio was not calculated until well after Young's death, Young was the retroactive league leader in this category seven times and was second or third another seven times. Cy Young is tied with Roger Clemens for the most career wins by a Boston Red Sox pitcher. They each won 192 games while with the franchise.
Particularly after his fastball slowed, Young relied upon his control. Young was once quoted as saying, "Some may have thought it was essential to know how to curve a ball before anything else. Experience, to my mind, teaches to the contrary. Any young player who has good control will become a successful curve pitcher long before the pitcher who is endeavoring to master both curves and control at the same time. The curve is merely an accessory to control." In addition to his exceptional control, Young was also a workhorse who avoided injury. For nineteen consecutive years, from 1891 through 1909, Cy Young was in his leagues' top ten for innings pitched; in fourteen of the seasons, he was in the top five. Not until 1900, a decade into his career, did Young pitch two consecutive incomplete games. By habit, Young restricted his practice throws in spring training. "I figured the old arm had just so many throws in it", said Young, "and there wasn't any use wasting them." Young once described his approach before a game:
"I never warmed up ten, fifteen minutes before a game like most pitchers do. I'd loosen up, three, four minutes. Five at the outside. And I never went to the bullpen. Oh, I'd relieve all right, plenty of times, but I went right from the bench to the box, and I'd take a few warm-up pitches and be ready. Then I had good control. I aimed to make the batter hit the ball, and I threw as few pitches as possible. That's why I was able to work every other day."
"Line-Up for Yesterday"
"Y is for Young""The magnificent Cy;""People batted against him,""But I never knew why."
 — "Ogden Nash", "Sport" magazine (January 1949) 
By the time of his retirement, Young's control had faltered. Young had also gained weight. In three of his last four years, he was the oldest player in the league.
In 1956, about one year after Young's death, the Cy Young Award was created. The first award was given to Brooklyn's Don Newcombe. Originally, it was a single award covering the whole of baseball. The honor was divided into two Cy Young Awards in 1967, one for each league.
On September 23, 1993, a statue dedicated to him was unveiled by Northeastern University on the site of the Red Sox's original stadium, the Huntington Avenue Grounds. It was there that Young had pitched the first game of the 1903 World Series, as well as the first perfect game in the modern era of baseball. A home plate-shaped plaque next to the statue reads:
"On October 1, 1903 the first modern World Series between the American League champion Boston Pilgrims (later known as the Red Sox) and the National League champion Pittsburgh Pirates was played on this site. General admission tickets were fifty cents. The Pilgrims, led by twenty-eight game winner Cy Young, trailed the series three games to one but then swept four consecutive victories to win the championship five games to three."

</doc>
<doc id="6851" url="http://en.wikipedia.org/wiki?curid=6851" title="Coronation Street">
Coronation Street

Coronation Street (informally known as Corrie) is a British soap opera created by Granada Television and shown on ITV since 1960. The programme centres on Coronation Street in Weatherfield, a fictional town based on Salford, its terraced houses, café, corner shop, newsagents, textile factory and The Rovers Return pub. The fictional street was built in the early 1900s and named in honour of the coronation of King Edward VII.
The programme was conceived in 1960 by scriptwriter Tony Warren at Granada Television in Manchester. Warren's initial kitchen sink drama proposal was rejected by the station's founder Sidney Bernstein, but he was persuaded by producer Harry Elton to produce the programme for thirteen pilot episodes. Within six months of the show's first broadcast, it had become the most-watched programme on British television, and is now a significant part of British culture. This show has been one of the most financially lucrative programmes on British commercial television, underpinning the success of Granada Television and ITV.
"Coronation Street" is made by Granada Television at MediaCity near Manchester and shown in all ITV regions, as well as internationally. On 17 September 2010, it became the world's longest-running TV soap opera in production. "Coronation Street" is noted for its depiction of a down-to-earth working class community combined with light-hearted humour, and strong characters.
History.
1960s.
The was aired on 9 December 1960, and was not initially a critical success; "Daily Mirror" columnist Ken Irwin claimed the series would only last three weeks. Granada Television initially commissioned only 13 episodes, and some inside the company doubted the show would last beyond its planned production run. Despite the criticism, viewers were immediately drawn into the serial, won over by "Coronation Street"'s 'ordinary' characters. The programme also made use of Northern English language and dialect; affectionate local terms like "eh, chuck?", "nowt" (, meaning "nothing"), and "by 'eck!" became widely heard on British television for the first time.
Early episodes told the story of student Kenneth Barlow (William Roache), who had won a place at university, and thus found his working-class background something of an embarrassment. The character was one of the few to have experienced life 'outside' of "Coronation Street". In some ways this predicts the growth of globalisation, and the decline of similar communities. In an episode from 1961, Barlow declares: "You can't go on just thinking about your own street these days. We're living with people on the other side of the world. There's more to worry about than Elsie Tanner and her boyfriends." Roache is the only remaining member of the original cast, which currently makes him the longest-serving actor in "Coronation Street", as well as in British and global soap history.
At the centre of many early stories, there was Ena Sharples (Violet Carson), caretaker of the Glad Tidings Mission Hall, and her friends: timid Minnie Caldwell (Margot Bryant), and bespectacled Martha Longhurst (Lynne Carol). The trio were likened to the Greek chorus, and the three witches in William Shakespeare's "Macbeth", as they would sit in the snug bar of the Rovers Return, passing judgement over family, neighbours and frequently each other. Headstrong Ena often clashed with Elsie Tanner, whom she believed espoused a dauntlessly loose set of morals. Elsie resented Ena's interference and gossip, which most of the time had little basis in reality.
In April 1961, Jed Stone made his first appearance and returned the following year in 1962. He left in 1963, but returned three years later in 1966. He left again and then returned 42 years later in 2008.
In March 1961, "Coronation Street" reached No.1 in the television ratings and remained there for the rest of the year. Earlier in 1961, a Television Audience Measurement (TAM) showed that 75% of available viewers (15 million) tuned into "Corrie", and by 1964 the programme had over 20 million regular viewers, with ratings peaking on 2 December 1964, at 21.36 million viewers.
Storylines throughout the decade included: a mystery poison-pen letter received by Elsie Tanner, the 1962 marriage of Ken Barlow and Valerie Tatlock, the death of Martha Longhurst in 1964, the birth of the Barlow twins in 1965, Elsie Tanner's wedding to Steve Tanner, as well as a train crashing from the viaduct (both in 1967), the murder of Steve Tanner in 1968, and a coach crash in 1969.
In spite of rising popularity with viewers, "Coronation Street" was criticised by some for its outdated portrayal of the urban working class, and its representation of a community that was a nostalgic fantasy. After the first episode in 1960, the "Daily Mirror" printed: "The programme is doomed from the outset ... For there is little reality in this new serial, which apparently, we have to suffer twice a week." By 1967, critics were suggesting that the programme no longer reflected life in 1960s Britain, but reflected how life was in the "1950s". Granada hurried to update the programme, with the hope of introducing more issue-driven stories, including Lucille Hewitt becoming addicted to drugs, Jerry Booth being in a storyline about homosexuality, Emily Nugent having an out of wedlock child, and introducing a black family, but all of these ideas were dropped for fear of upsetting viewers.
1970s.
The show's production team was tested when many core cast members left the programme in the early 1970s. When Arthur Leslie died suddenly in 1970, his character, Rovers' landlord Jack Walker, died with him. Anne Reid quit as Valerie Barlow, and was killed off in 1971, electrocuting herself with a faulty hairdryer. Ratings reached a low of eight million in February 1973, Pat Phoenix quit as Elsie Tanner, Violet Carson (Ena Sharples) was written out for most of the year due to illness, and Doris Speed (Annie Walker) took two months' leave due to bereavement. ITV's other flagship soap opera "Crossroads" saw a marked increase in viewers at this time, as its established cast, such as Meg Richardson (Noele Gordon), grew in popularity. These sudden departures forced the writing team to quickly develop characters who had previously stood in the background. The roles of Bet Lynch (Julie Goodyear), Ivy Tilsley (Lynne Perrie), Deirdre Hunt (Anne Kirkbride), Rita Littlewood (Barbara Knox), and Mavis Riley (Thelma Barlow) were built up between 1972 and 1973 with characters such as Gail Potter (Helen Worth), Blanche Hunt (Patricia Cutts, Maggie Jones), and Vera Duckworth (Elizabeth Dawn) first appearing in 1974. These characters would remain at the centre of the programme for many years.
Comic storylines had been popular in the series in the 1960s, but had become sparse during the early 1970s. These were re-introduced by new producer Bill Podmore who joined the series in 1976. He had worked on Granada comedy productions prior to his appointment. Stan and Hilda Ogden were often at the centre of overtly funny storylines, with other comic characters including Eddie Yeats (Geoffrey Hughes), Fred Gee (Fred Feast), and Jack Duckworth (William Tarmey) all making their first appearances during the decade.
In 1976, Pat Phoenix returned to her role as Elsie Tanner and, after a spate of ill health, Violet Carson returned on a more regular basis as Ena. "Coronation Street's" stalwart cast slotted back into the programme alongside the newcomers, examining new relationships between characters of different ages and backgrounds: Eddie Yeats became the Ogdens' lodger, Gail Potter and Suzie Birchall moved in with Elsie, Mike Baldwin (Johnny Briggs) arrived in 1976 as the tough factory boss, and Annie Walker reigned at the Rovers with her trio of staff Bet Lynch, Betty Turpin, and Fred Gee.
Storylines throughout the decade included: a warehouse fire in 1975, the birth of Tracy Langton in 1977, the murder of Ernest Bishop in 1978, a lorry crashing into the Rovers Return in 1979, and the marriage of Brian Tilsley and Gail Potter (also in 1979).
For eleven weeks, between August and October 1979, industrial action forced "Coronation Street" and the entire ITV network (apart from the Channel Islands) off the air. When ITV did return, its first evening schedule included a special "catch-up" edition of "Coronation Street". This included storylines which would have taken place during the strike, and they were explained in the form of a narrative chat between Len Fairclough and Bet Lynch. For several weeks the channel had very few fresh episodes to show, and episodes of the game show 3-2-1 were screened in its place. "Coronation Street" returned to ITV screens with a regular scheduled time closer to the end of 1979.
"Coronation Street" had little competition within its prime time slot, and certain critics suggested that the programme had grown complacent, moving away from socially viable storylines and again presenting a dated view of working class life.
1980s.
Between 1980 and 1989, "Coronation Street" underwent some of the biggest changes since its launch. By May 1984, Ken Barlow stood as the only original cast member, after the departures of Ena Sharples (in 1980), Annie Walker (in 1983), Elsie Tanner (in 1984) and Albert Tatlock (also 1984). In 1983, antihero Len Fairclough (Peter Adamson), one of the show's central male characters since 1961, was killed off, and in 1984, Stan Ogden (Bernard Youens) died. While the press predicted the end of "Corrie", H. V. Kershaw declared that "There are no stars in "Coronation Street"." Writers drew on the show's many archetypes, with previously established characters stepping into the roles left by the original cast. Phyllis Pearce (Jill Summers) was hailed as the new Ena Sharples in 1982, the Duckworths moved into No.9 in 1983 and slipped into the role once held by the Ogdens, while Percy Sugden (Bill Waddington) appeared in 1983 and took over the grumpy war veteran role from Albert Tatlock. The question of who would take over the Rovers Return after Annie Walker's 1983 exit was answered in 1985 when Bet Lynch (who also mirrored the vulnerability and strength of Elsie Tanner) was installed as landlady. In 1983, Shirley Armitage became the first major black character in her role as machinist at Baldwin's Casuals.
Ken Barlow married Deirdre Langton on 27 July 1981. The episode was watched by over 24 million viewers – more ITV viewers than the wedding of Prince Charles and Lady Diana two days later. The 1980s also saw the cementing of relationships between established characters: Alf Roberts (Bryan Mosley) married Audrey Potter (Sue Nicholls) in 1985, and Kevin Webster (Michael Le Vell) married Sally Seddon (Sally Dynevor) in 1986. Bet Lynch married Alec Gilroy in 1987, and the marriages of Ivy Tilsley and Don Brennan, as well as Derek Wilton and Mavis Riley took place in 1988.
In 1982, the arrival of Channel 4, and its edgy new soap opera "Brookside", was one of the biggest changes for "Coronation Street". Unlike "Coronation Street", which had a very nostalgic view of working-class life, "Brookside" brought together working and middle-class families in a more contemporary environment. The dialogue often included expletives and the stories were more hard-hitting, and of the current Zeitgeist. Whereas stories at this time in "Coronation Street" were largely about family affairs, "Brookside" concentrated on social affairs such as industrial action, unemployment, and the black market. The BBC also introduced a new prime time soap opera, "EastEnders" in 1985. Like "Brookside", "EastEnders" had a more gritty premise than "Coronation Street", although unlike "Brookside" it tended to steer clear of blue language and politicised stories.
While ratings for "Coronation Street" remained consistent throughout the decade, "EastEnders" regularly obtained higher viewing figures due to its omnibus episodes shown at weekends. With prime time competition, "Corrie" was again seen as being old fashioned, with the introduction of the 'normal' Clayton family in 1985 being a failure with viewers. Between 1988 and 1989, many aspects of the show were modernised by new producer David Liddiment. A new exterior set had been built in 1982, and in 1989 it was redeveloped to include new houses and shops. Production techniques were also changed with a new studio being built, and the inclusion of more location filming, which had moved the show from being shot on film to videotape in 1988. Due to new pressures, an introduction of the third weekly episode aired on 20 October 1989, to broadcast each Friday at 19:30.
The 1980s featured some of the most prominent storylines in the programme's history, such as Deirdre Barlow's affair with Mike Baldwin in 1983, the first soap storyline to receive widespread media attention. The feud between Ken Barlow and Mike Baldwin would continue for many years, with Mike even marrying Ken's daughter, Susan. In 1986, there was a fire at the Rovers Return, which attracted an audience of just under 27 million, and between 1986 and 1989, the story of Rita Fairclough's psychological abuse at the hands of Alan Bradley (Mark Eden), and then his subsequent death under the wheels of a Blackpool tram, was played out. The episode of Alan's death under the tram gave "Coronation Street" its highest ratings ever at 26.9 million, and it's still the 9th most watched UK broadcast of all time. Other stories included: the birth of Nicky Tilsley in 1980, Elsie Tanner's departure and Stan Ogden's funeral in 1984, the birth of Sarah-Louise Tilsley in 1987, and Brian Tilsley's murder in 1989.
New characters were introduced, such as Terry Duckworth (Nigel Pivaro), Curly Watts (Kevin Kennedy), Martin Platt (Sean Wilson), Reg Holdsworth (Ken Morley), and the McDonald family; one of whom, Simon Gregson, started on the show as Steve McDonald a week after his 15th birthday, and has been on the show ever since.
1990s.
In spite of updated sets and production changes, "Coronation Street" still received criticism. In 1992, chairman of the Broadcasting Standards Council, Lord Rees-Mogg, criticised the low-representation of ethnic minorities, and the programme's portrayal of the cosy familiarity of a bygone era. Some newspapers ran headlines such as ""Coronation Street" shuts out blacks" ("The Times"), and "'Put colour in t'Street" ("Daily Mirror"). Patrick Stoddart of "The Times" wrote: "The millions who watch "Coronation Street" – and who will continue to do so despite Lord Rees-Mogg – know real life when they see it ... in the most confident and accomplished soap opera television has ever seen". Black and Asian characters had appeared, but it wasn't until 1999 that the show featured its first regular non-white family, the Desai family.
New characters Des and Steph Barnes moved into one of the new houses in 1990, being dubbed by the media as 'Yuppies'. Raquel Wolstenhulme (Sarah Lancashire) first appeared in 1991 and went on to become one of the most popular characters. The McDonald family were developed and the fiery relationships between Liz, Jim, Steve and Andy interested viewers. Other newcomers were Maud Grimes (Elizabeth Bradley), Roy Cropper (David Neilson), Judy and Gary Mallett, as well as Fred Elliot (John Savident) and Ashley Peacock (Steven Arnold). The amount of slapstick and physical humour in storylines increased during the 1990s, with comical characters such as Reg Holdsworth and his water bed.
In the early 1990s storylines included: the death of newborn Katie McDonald in 1992, Mike Baldwin's wedding to Alma Sedgewick (Amanda Barrie) in 1992, Tommy Duckworth being sold by his father Terry in 1993, Deirdre Barlow's marriage to Moroccan Samir Rachid, and the rise of Tanya Pooley (Eva Pope) between 1993 and 1994.
In 1995, actor Julie Goodyear decided to leave the show forcing the key character Bet Lynch to depart. Bet would subsequently make brief return appearances in 1999, 2002 and 2003.
In 1997, Brian Park took over as producer, with the idea of promoting young characters as opposed to the older cast. On his first day, he cut the characters of Derek Wilton, Don Brennan, Percy Sugden, Bill Webster, Billy Williams, and Maureen Holdsworth. Thelma Barlow, who played Derek's wife Mavis, was angered by the firing of her co-star and resigned. The production team lost some of its key writers when Barry Hill, Adele Rose and Julian Roach all resigned as well.
In line with Park's suggestion, younger characters were introduced: Nick Tilsley was recast, played by Adam Rickitt, single mother Zoe Tattersall (Joanne Froggatt) first appeared, and the Battersbys moved into No.5. Storylines focussed on tackling 'issues', such as drug dealers, eco-warriors, religious cults, and a transsexual woman. Park quit in 1998, after deciding that he had done what he intended to do; he maintained that his biggest achievement was the introduction of Hayley Patterson (Julie Hesmondhalgh), the first transsexual character in a British soap.
Some viewers were alienated by the new "Coronation Street", and sections of the media voiced their disapproval. Having received criticism of being too out of touch, "Corrie" now struggled to emulate the more modern "Brookside" and "EastEnders". In the "Daily Mirror", Victor Lewis-Smith wrote: "Apparently it doesn't matter that this is a first-class soap opera, superbly scripted and flawlessly performed by a seasoned repertory company."
One of "Coronation Street"'s best known storylines took place in March/April 1998, with Deirdre Rachid being wrongfully imprisoned after a relationship with con-man Jon Lindsay. This episode, when Deirdre was sent to prison, was watched by 19 million viewers, and 'Free the Weatherfield One' campaigns sprung up in a media frenzy. The then Prime Minister Tony Blair even passed comment on Deirdre's sentencing in Parliament. Deirdre was freed after three weeks, with Granada stating that they had always intended for her to be released, in spite of the media interest.
2000s.
On 8 December 2000, the show celebrated its fortieth year by broadcasting a live, hour-long . The Prince of Wales appeared as himself in an ITV News bulletin report. Earlier in the year, 13-year-old Sarah-Louise Platt (Tina O'Brien) had become pregnant and given birth to a baby girl, Bethany, on 4 June. The episode where Gail was told of her daughter's pregnancy was watched by 15 million viewers. In September 2000, Mike Baldwin married Linda Sykes but shortly afterwards, his drunken son Mark confessed he and Linda had been having an affair behind his dad's back. The episode attracted an audience of 16.8 million and in the 2001 British Soap Awards won Best Storyline.
From 1999 to 2001, issue-led storylines were introduced such as Toyah Battersby's rape, Roy and Hayley Cropper abducting their foster child, Sarah Platt's Internet chat room abduction and Alma Halliwell's death from cervical cancer. Such storylines were unpopular with viewers and ratings dropped and in October 2001, Macnaught was abruptly moved to another Granada department and Carolyn Reynolds took over. "Corrie" continued to struggle in the ratings, with "EastEnders" introducing some of its strongest stories. In 2002, Kieran Roberts was appointed as producer and aimed to re-introduce "gentle storylines and humour", after deciding that "the Street" should not try to compete with other soaps. In 2002, Gail Platt married Richard Hillman (Brian Capron), a financial advisor, who would go on to leave Duggie Ferguson to die, murder his ex-wife Patricia, attempt to murder his mother-in-law, Audrey Roberts, murder Maxine Peacock and attempt to murder Emily Bishop. After confessing to the murder of Maxine and his ex-wife, Hillman attempted to kill Gail, her children Sarah and David, and her granddaughter Bethany, by driving them into a canal. The storyline received wide press attention, and viewing figures peaked at 19.4 million, with Hillman dubbed a "serial killer" by the media. Todd Grimshaw became "Corrie's" first regular homosexual character. 2003 saw the introduction of another gay male character, Sean Tully played by Antony Cotton. The character of Karen McDonald (Suranne Jones) was developed, with her fiery marriage to Steve and warring with Tracy Barlow. In 2004, "Coronation Street" retconned the Baldwin family when Mike's nephew Danny Baldwin and his wife Frankie moved to the area from Essex, with their two sons Jamie and Warren. Until this time, Mike Baldwin had been portrayed as an only child, with his father appearing in the programme between 1980 and 1982 confirming the fact. The bigamy of Peter Barlow and his addiction to alcohol, later in the decade, Maya Sharma's revenge on former lover Devendra Alahan, Charlie Stubbs's psychological abuse of Shelley Unwin, and the deaths of Mike Baldwin, Vera Duckworth and Fred Elliott. In 2007, Tracy Barlow murdered Charlie Stubbs and claiming it was self-defence, the storyline saw viewing figures peaking at 13.3 million. At the 2007 British Soap Awards, it won Best Storyline, and Kate Ford was voted Best Actress for her portrayal. Other storylines included Leanne Battersby becoming a prostitute and the show's first bi-sexual love triangle (between Michelle Connor, Sonny Dhillon, and Sean Tully). The Connor family were central to many storylines during 2007—the accidental death of a Polish worker at Underworld due to overworking, Michelle's discovery that her brothers Paul and Liam were the cause of her husband's death, Paul's use of an escort service, his kidnapping of Leanne and his subsequent death.
In July 2007, after 34 years in the role of Vera Duckworth, Elizabeth Dawn left the show due to ill health. After conversation between Dawn and producers Kieran Roberts and Steve Frost, the decision was made to kill Vera off. In January 2008, shortly before plans to retire to Blackpool, Vera's husband Jack found that she had died in her armchair.
Tina O'Brien revealed in the British press on 4 April 2007 that she would be leaving "Coronation Street". Sarah-Louise, who was involved in some of the decade's most controversial stories, left in December 2007 with her daughter (who'd been in an ectasy storyline earlier that year), Bethany Platt. In 2008, Michelle learning that Ryan was not her biological son, having been accidentally swapped at birth with Alex Neeson. Carla Connor turned to Liam for comfort and developed feelings for him. In spite of knowing about her feelings, Liam married Maria Sutherland. Maria and Liam's baby son was stillborn in April, and during an estrangement from Maria upon the death of their baby, Liam had a one-night stand with Carla, a story which helped pave the way for his departure. Gail's son David Platt (Jack P. Shepherd) pushed her down the stairs. Enraged that Gail refused to press charges, David vandalised the Street and was sent to a young offenders' facility for several months. In May 2008, Gail finally met Ted Page, the father she'd never known and in 2009, Gail's boyfriend, Joe McIntyre became addicted to pain killers, which came to a head when he broke into the medical centre. In August 2008, Jed Stone returned after 42 years. Liam Connor and his ex-sister-in-law Carla Connor gave into their feelings for each other and began an affair. Carla's fiancee Tony discovered the affair and subsequently had Liam killed in a hit-and-run in October. Carla struggled to come to terms with Liam's death, but decided she still loved Tony and married him on 3 December, in an episode attracting 10.3 million viewers. In April 2009 it was revealed that Eileen Grimshaw's father, Colin had slept with Eileen's old classmate, Paula Carp while she was still at school, and that Paula's daughter, Julie was in fact also Colin's daughter. In May, Norris Cole received a blast from the past with the reappearance of his estranged brother Ramsay Clegg (Andrew Sachs) who wanted a reconciliation. Peter Barlow's battle against alcoholism, Ken Barlow's affair with actress Martha Fraser after his dog Eccles fell in the canal, Maria giving birth to Liam's son and her subsequent relationship with Liam's killer Tony, Steve McDonald's marriage to Becky Granger and Kevin Webster's affair with Molly Dobbs. On Christmas Day 2009, Sally Webster told husband Kevin that she had breast cancer, just as he was about to leave her for lover Molly.
2010s.
Gail Platt married for the fourth time to Joe McIntyre. Molly Dobbs became pregnant, with the father being either Tyrone Dobbs or Kevin Webster; Molly reconciled with Tyrone and nearly lost her life after a crash in a car which had not been properly repaired by Kevin. Tracy Barlow returned on 7 May 2010, plotting to reduce her sentence by convincing cellmate Gail to confess to Joe's murder. The show's first lesbian storyline developed as Sophie Webster realised her true feelings for her best friend Sian Powers. In 2010, Blanche Hunt was written out after the death of actress Maggie Jones on 2 December 2009. ITV honoured Blanche and Maggie with a half-hour special, "Goodbye Blanche", which was aired after the funeral episode. On 31 May 2010, the "Coronation Street" opening sequence was given a revamp. From that date all episodes of the series would be transmitted in High Definition. The change coincided with a week of dramatic storylines known as "Siege Week", being shown on five consecutive nights following "Britain's Got Talent." In the story Tony Gordon escaped from prison to seek vengeance on his enemies, culminating in the dramatic explosion of Underworld. Meanwhile, Gail Platt's murder trial took place, with two different verdicts being shown online. On 17 September 2010 "Coronation Street" entered "Guinness World Records" as the world's longest-running television soap opera after the American soap opera "As the World Turns" concluded. William Roache was listed as the world's longest-running soap actor.
"Coronation Street"'s 50th anniversary week was celebrated with seven episodes, plus a special one-hour live episode, broadcast from 6–10 December. The episodes averaged 14 million viewers, a 52.1% share of the audience. The anniversary was also publicised with ITV specials and news broadcasts. In the storyline, Nick Tilsley and Leanne Battersby's bar—The Joinery—exploded during Peter Barlow's stag party. As a result, the viaduct was destroyed, sending a Metrolink tram careering onto the street, destroying D&S Alahan's Corner Shop and The Kabin. Two characters, Ashley Peacock and Molly Dobbs, along with an unknown taxi driver, were killed as a result of the disaster. Rita Sullivan survived, despite being trapped under the rubble of her destroyed shop. Fiz Stape prematurely gave birth to a baby girl, Hope, after her husband, John, struck his stalker Charlotte Hoyle with a hammer to silence her. He later attempted to turn off Charlotte's life support machine, having gained access to her bedside by allowing her parents to believe he was her fiancé; they later made the decision themselves. Peter Barlow, believing himself on his deathbed, married Leanne Battersby in an impromptu ceremony, shortly before going into cardiac arrest, although he later rallied and made a slow recovery. The episode of "EastEnders" broadcast on the same day as "Coronation Street"‍‍ '​‍s 50th anniversary episode included a tribute, with the character Dot Branning saying that she never missed "Coronation Street".
In January 2011, Dennis Tanner returned after 43 years off screen. Beverley Callard and Vicky Entwistle who play Liz McDonald and Janice Battersby, respectively, quit the street. On 15 October 2011, Betty Driver, who had played Betty Williams since 1969, died of pneumonia, aged 91. In 2011, the major storyline of John Stape and his murder spree came to an end in May after he jumped off a hospital roof but left before he could be arrested. He returned in October before Fiz Stape was imprisoned for the murders. Following a car crash, John revealed the details about the murders and how Fiz was not involved. He later died from his injuries on 28 October.
Frank Foster storyline centered on Maria flirting with Frank. Frank forces himself on a frightened Maria who then confides in Carla Connor. Carla is furious and tries to get Maria to go to the police; however, Maria wants to forget all about it and tells Carla to continue doing business with Frank to save the factory. This leads to a relationship between Frank and Carla. They become engaged but Carla calls it off due to her developing feelings for Peter Barlow. Frank finds out about the affair after hiring a private investigator and he attacks and rapes Carla. Frank was arrested under suspicion of rape but was found not guilty when the case went to court. Frank took over Underworld with the assistance of Sally Webster. One evening Frank gloated about getting away with rape to a terrified Carla whilst the factory was closed, unaware that somebody heard every word. Frank was found dead later that day by the Underworld machinists, he was hit over the head with a bottle of whiskey. A distraught Sally comforted Frank's mother, Anne Foster whilst she was grieving her son's death, but discovered it was Anne herself who murdered her own son. Anne attacked Sally and then attempted to flee the country only to be caught out by Carla and Kevin Webster.
Tyrone Dobbs was also the centre of a controversial domestic abuse storyline when he was introduced to police officer Kirsty Soames whilst on a night out. Tyrone's friends paid Kirsty to flirt with him to cheer him up after his previous wife Molly's death. Tyrone and Kirsty immediately hit it off. Kirsty's temper began to show when she used to become jealous of Tyrone's friends, specifically his friendship with Fiz Stape. Kirsty fell pregnant and her temper soon turned violent as she lashed out at Tyrone on numerous occasions, attacking him with her fists and kitchen appliances. Tyrone hid his injuries from his concerned friends and refused to leave the house. Kirsty gave birth in the Rovers Return after a heated confrontation with Tina, yet Tyrone was absent due to him fixing Fiz's boiler. This made Kirsty secretly leave out Tyrone's name as her newborn daughter Ruby's father on her birth certificate. Kirsty's violent outbursts become more frequent and a terrified Tyrone finally confides in a horrified Fiz about Kirsty's domestic abuse, this leads to Tyrone and Fiz embarking on a secret affair after she confesses her love for him. Fiz and Tyrone conspire a plan together for Tyrone to wed Kirsty then report Kirsty to the police for her domestic abuse, resulting in Kirsty being sent to prison and Tyrone getting full custody of his daughter Ruby. Tyrone proposes to a delighted Kirsty, but Kirsty eventually discovers his and Fiz's affair and their plan. Kirsty eventually reveals Fiz and Tyrone's affair in front of everybody at her and Tyrone's wedding. After the wedding, Kirsty and Tyrone have a blazing row back at their house, resulting in Kirsty falling down the stairs after trying to attack Tyrone. The row is overheard by several neighbours and they think Tyrone has been assaulting Kirsty and call the police. Kirsty lies to the police and makes a false accusation that Tyrone has been beating her for months. This leads to Tyrone's arrest and he is bailed until his appending trial. Whilst on bail, Tyrone flees Weatherfield with Fiz, her daughter Hope and his daughter Ruby causing the police to issue a warrant for his arrest. Tyrone is caught and arrested and is kept in a prison cell until his trial. Kirsty lies again whilst giving evidence at Tyrone's trial by playing the victim to the jury. But whilst struggling being a single mum at home, Kirsty snaps at a screaming Ruby and starts screaming at her child to stop crying. Realising that she doesn't trust herself around her daughter, Kirsty interrupts Tyrone's trial right before the verdict is due and confesses to the judge that it was her who had been assaulting Tyrone and he was innocent. Tyrone gets released and Kirsty gets charged and is sentenced to three years in prison. Tyrone gets full custody of Ruby and starts his new life with his girlfriend Fiz.
On 18 March 2013, Karl Munro starts a fire in the cellar of the Rovers Return killing Sunita Alahan and a female firefighter. This episode attracted over 10 million viewers. Although he gets away with it at first, the truth eventually comes out and Karl is arrested and sentenced to prison later on in the year.
2014 has seen the introduction to new characters such as Kal Nazir's family, the street's first Muslim family. Burglar Michael Rodwell played by comedian Les Dennis, was introduced through a restorative justice storyline with Gail McIntyre. The year has seen the final appearances of three of the show's best-loved characters: Hayley Cropper, Tina McIntyre and Deirdre Barlow. Hayley was diagnosed with a terminal cancer of the pancreas after a simple check up in 2013. Hayley decided to end her life before she succumbs to her illness much to the despair of her husband Roy Cropper. Roy reluctantly agreed to his wife's demands and Hayley drank a concoction of mashed up pills and died in bed of an overdose with her husband laid beside her.
Tina Mcintyre embarked on an affair with Peter Barlow after they shared a passionate kiss during Peter and Carla Connor's wedding reception. Peter lies to Tina, telling her that he'd leave Carla and move away with her to Portsmouth when he was actually committing to Carla after they discovered she was pregnant. The affair causes Peter to relapse and starts drinking again. Carla's brother Rob Donovan discovers the affair and threatens them both that he'll tell Carla if they don't. Tina books a taxi for her and Peter but can't find Peter so she heads to the Rovers Return to find him and Carla celebrating Carla's pregnancy. Feeling crushed, Tina heads back to her flat and is pursued by Peter. They argue and Tina threatens Peter that she's going to reveal all about their affair to Carla, causing Peter to leave her flat and tell a heartbroken Carla about the affair before she does. Peter accidentally leaves Tina's flat door open whilst leaving and Rob notices and enters Tina's flat to confront her about her affair. Rob and Tina argue about her telling Carla about the affair, forcing them both onto her balcony outside. But when Tina threatens Rob about reporting his dodgy dealing at his pawn shop to the police, Rob pushes Tina from the balcony and she plummets to the ground. Rob frantically destroys Tina's flat in an attempt to make it look like a burglary and wipes his fingerprints off of everything he has touched. He heads outside to see a motionless Tina laying on the cobbles, he starts to call for an ambulance when Tina surprisingly is still alive and manages to get back to her feet. Rob tries backtracking his actions and tells Tina that she slipped but Tina goads Rob by saying she'll report him for attempted murder as well as him selling illegal goods. Rob takes desperate measures to shut Tina up so he grabs a lead pipe from a van outside the builder's yard and bludgeons Tina to her death. After Peter's confession, a furious Carla heads on the war path and goes in search of Tina. She notices Tina's door to her flat is open and heads inside to have it out with her. She enters the ransacked flat but doesn't find Tina anywhere, she heads out onto the balcony and looks over to discover Tina's body on the floor below. Tina passes away the week later from her injuries.
Initially, Carla is in the frame for Tina's murder but after evidence and his outburst at Tina's funeral, Peter is arrested and charged with the murder. While in Highfield Prison, desperate for a drink as he is an alcoholic Peter comes across Jim McDonald who supplies with booze while making Peter get son Steve McDonald and Liz to visit. After Peter fails to Jim's command, Jim stops Peter's provision so Peter finds the booze supply and drinks it all down resulting to him being taken to hospital with internal bleeding. Peter later tells Carla he knew she killed Tina even though he didn't do it he'll take her secret to the grave. Carla considers going to the police but Rob is adamant she doesn't. Peter tells Tracy about Jim after he is beaten by Jim's lackeys so she tells Steve stop visiting and Jim turns nasty. A terrified Deirdre Barlow consults Jim's ex-wife Liz McDonald to have words so she does and Jim leaves Peter alone.
In October 2014, Steve thinks he may have melanoma and gets it checked out but is told he may have depression which he disregards. On the 13th Peter's trial begins and a guilty Steve who knew about the affair fears he has ruined Peter's chances of being released but Carla tells the jury on the 15th she believes Peter's innocent making his chances better. While Peter and newfound cell-mate/confidant Eugene Clleland celebrate, murderer Rob yells at Carla making her feel suspicious. At last all is revealed when new evidence emerges and Tracy lures Rob to a derelict factory claiming that they will flee the country together. It is revealed that Tracy lied and had led the police to their location. Rob refuses to admit defeat but finally gives in and is taken away, Tracy breaking down in the process. His name cleared, Peter decides to depart seeing as how there is nothing keeping him in Weatherfield other than son Simon. Peter leaves in November, leaving behind his family, his past, and Carla.
Because of the stress of the trial, Peter's stepmother Deirdre decided to go away to stay with her friend to get away. Unfortunately, Deirdre's portrayer Anne Kirkbride died in January 2015, so the character was written out of the show, although it has been revealed her departure won't be addressed for some time.
It was also revealed this year that Max Turner, son of Kylie, and stepson of David Platt, has a condition known as Attention Deficit Hyperactive Disorder (ADHD) and he needed medication to control his illness. Kylie ends up getting hooked on to these drugs as the stress that Max is placing on her pushes her to the edge. She then rediscovers Max's biological father, Callum Logan, and he begins harassing the Platts, intent on gambling for custody of the boy. Eventually, Kylie begins reverting to her old ways, and Callum even begins supplying her with illegal highs to ensure he would get custody.
By Christmas 2014, much of the street's residents' personal lives were in tatters; the Platts are completely torn apart as Kylie is kicked out following her revelation of taking Max's medication, and Callum, not to mention the disastrous few months at the Windass residence. While attempting to enjoy a Christmas dinner as one happy family, Owen ends up losing his cool and breaks Faye's earphones.
In January 2015, tragedy strikes as, while escorting the men and women of Underworld to an up class hotel, Steve loses control of their minibus due to being distracted by reckless drivers, and ends up crashing, the vehicle hanging across a cliff, and leaving the passengers' lives hanging in the balance as he regains consciousness first and abandons the wreckage. While everyone emerges alive from the crash, young Sinead Tinker is left paralysed, potentially for life, and Steve's depression takes a turn for the worse. The episode where the bus initially crashes was broadcast on the same night actress Anne Kirkbride's death occurred and was announced.
On 26 May, Tracy seeks revenge on Carla which leaves a massive fire in the Victoria Flats. Carla was rescued by Leanne. Leanne went back inside to rescue Amy,followed by Kal. The residents are horrified when they find a gas cannister on fire. Amy and Leanne make it out but Kal is hit by the explosion. We later find out Kal has died.
Characters.
Since 1960, "Coronation Street" has featured many characters whose popularity with viewers and critics has differed greatly. The original cast was created by Tony Warren, with the characters of Ena Sharples (Violet Carson), Elsie Tanner (Patricia Phoenix) and Annie Walker (Doris Speed) as central figures. These three women remained with the show for 20 years or more, and became archetypes of British soap opera, often being emulated by other serials. Ena was the street's busybody, battleaxe and self-proclaimed moral voice. Elsie was the tart with a heart, who was constantly hurt by men in the search for true love. Annie Walker, landlady of the Rovers Return Inn, had delusions of grandeur and saw herself as better than other residents of "Coronation Street".
"Coronation Street" became known for the portrayal of strong female characters, including original cast characters like Sharples, Walker and Tanner, and Hilda Ogden (who first appeared in 1964); who became household names during the 1960s. Warren's programme was largely matriarchal, which some commentators put down to the female-dominant environment in which he grew up. Consequently, the show has a long tradition of psychologically abused husbands, most famously Stan Ogden and Jack Duckworth, husbands of Hilda and Vera, respectively.
Ken Barlow (William Roache) entered the storyline as a young radical, reflecting the youth of 1960s Britain, where figures like The Beatles, The Rolling Stones and the model Twiggy were to reshape the concept of youthful rebellion. Though the rest of the original Barlow family were killed off before the end of the 1970s, Ken, who for 27 years was the only character from the first episode remaining, has remained the constant link throughout the entire series. In 2011, Dennis Tanner (Philip Lowrie), another character from the first episode, returned to "Coronation Street" after a 43-year absence. Since 1984, Ken Barlow had been the show's only remaining original character, although Emily Bishop (Eileen Derbyshire) has remained in the series since first appearing in early 1961, when the show was just weeks old.
Stan Ogden and Hilda Ogden were introduced in 1964, with Hilda (Jean Alexander) becoming one of the most famous British soap characters of all time. In a 1982 poll, she was voted fourth most recognisable woman in Britain, after Queen Elizabeth The Queen Mother, Queen Elizabeth II and Diana, Princess of Wales. Hilda's best-known attributes were her pinny, hair curlers, and the "muriel" in her living room with three "flying" duck ornaments. Hilda Ogden's final episode on Christmas Day 1987, remains the highest-rated episode of "Coronation Street" ever, with nearly 27 million viewers. Stan Ogden had been killed off in 1984 following the death of actor Bernard Youens after a long illness which had restricted his appearances towards the end.
Bet Lynch (Julie Goodyear) first appeared in 1966, before becoming a regular in 1970, and went on to become one of the most famous "Corrie" characters. Bet stood as the central character of the show from 1985 until departing in 1995, often being dubbed as "Queen of the Street" by the media, and indeed herself. The character briefly returned in June 2002.
"Coronation Street" and its characters often rely heavily on archetypes, with the characterisation of some of its current and recent cast based loosely on past characters. Phyllis Pearce, Blanche Hunt (Maggie Jones) and Sylvia Goodwin embodied the role of the acid-tongued busybody originally held by Ena Sharples, Sally Webster (Sally Dynevor) has grown snobbish, like Annie Walker, and a number of the programme's female characters, such as Carla Connor, mirror the vulnerability of Elsie Tanner and Bet Lynch. Other recurring archetypes include the war veteran (Albert Tatlock, Percy Sugden and Gary Windass), the bumbling retail manager (Leonard Swindley, Reg Holdsworth, Norris Cole), quick-tempered toughmen (Len Fairclough, Jim McDonald, Tommy Harris and Owen Armstrong), and the perennial losers (Stan and Hilda Ogden, Jack and Vera Duckworth, Les Battersby-Brown, Beth Tinker and Kirk Sutherland). The show's former archivist and scriptwriter Daran Little disagreed with the characterisation of the show as a collection of stereotypes. "Rather, remember that Elsie, Ena and Co. were the first of their kind ever seen on British television. If later characters are stereotypes, it's because they are from the same original mould. It is the hundreds of programmes that have followed which have copied "Coronation Street"."
Production.
Broadcast format.
Between 9 December 1960 and 3 March 1961, "Coronation Street" was broadcast twice weekly, on Wednesday and Friday. During this period, the Friday episode was broadcast live, with the Wednesday episode being pre-recorded 15 minutes later. When the programme went fully networked on 6 March 1961, broadcast days changed to Monday and Wednesday. The last regular episode to be shown live was broadcast on 3 February 1961.
The series was transmitted in black and white for the majority of the 1960s. Preparations were made to film episode 923, to be transmitted Wednesday 29 October 1969, in colour. This instalment featured the street's residents on a coach trip to the Lake District. In the end, suitable colour film stock for the cameras could not be found and the footage was shot in black and white. The following episode, transmitted Monday 3 November, was videotaped in colour but featured black and white film inserts and title sequence. Like BBC1, the ITV network was officially broadcast in black and white at this point (though programmes were actually broadcast in colour as early as July that year for colour transmission testing and adjustment) so the episode was seen by most in black and white.
Daran Little, for many years the official programme archivist, claims that the first episode to be transmitted in colour was episode 930 shown on 24 November 1969. The ITV network, like BBC1, began full colour transmissions on 15 November 1969 and it is therefore possible that the first transmitted colour episode is number 928 shown on 17 November.
In October 1970 a technicians' dispute turned into a work-to-rule when sound staff were denied a pay rise given to camera staff the year before for working with colour recording equipment. The terms of the work-to-rule were that staff refused to work with the new equipment (though the old black and white equipment had been disposed of by then) and therefore programmes were recorded and transmitted in black and white, including "Coronation Street" The dispute was resolved in early 1971 and the last black and white episode was broadcast on 8 February 1971.
Episode 5191, originally broadcast on 7 January 2002, was the first to be broadcast in widescreen format. "Coronation Street" was the last British soap to make the switch to 16:9 ("Take the High Road" remained in until it finished in 2003).
From 22 March 2010, "Coronation Street" was produced in 1080/50i for transmission on HDTV platforms on ITV HD. The first transmission in this format was episode 7351 on 31 May 2010 with a new set of titles and re-recorded theme tune. On 26 May 2010 ITV previewed the new HD titles on the "Coronation Street" website. Due to copyright reasons only viewers residing in the UK could see them on the ITV site.
Production staff.
"Coronation Street's" creator, Tony Warren, wrote the first 13 episodes of the programme in 1960, and continued to write for the programme intermittently until 1976. He still retains links with "Coronation Street", often advising on storylines.
Harry Kershaw was the script editor for "Coronation Street" when the programme began in 1960, working alongside Tony Warren. Kershaw was also a script writer for the programme and the show's producer between 1962 and 1971. He remains the only person, along with John Finch, to have held the three posts of script editor, writer and producer. Kershaw continued to write for the programme until his retirement in January 1988.
Adele Rose was the longest-serving "Coronation Street" writer, completing 455 scripts between 1961 and 1998. She also created "Byker Grove".
Bill Podmore was the show's longest serving producer. By the time he stepped down in 1988 he had completed 13 years at the production helm. Nicknamed the "godfather" by the tabloid press, he was renowned for his tough, uncompromising style and was feared by both crew and cast alike. He is probably most famous for sacking Peter Adamson, the show's Len Fairclough, in 1983.
Michael Apted, best known for the "Up!" series of documentaries was a director on the programme in the early 1960s. This period of his career marked the first of his many collaborations with writer Jack Rosenthal. Rosenthal, noted for such television plays as "Bar Mitzvah Boy", began his career on the show, writing over 150 episodes between 1961 and 1969. Paul Abbott was a story editor on the programme in the 1980s and began writing episodes in 1989, but left in 1993 to produce "Cracker", for which he later wrote, before creating his own highly acclaimed dramas such as "Touching Evil" and "Shameless". Russell T Davies was briefly a storyliner on the programme in the mid-1990s, also writing the script for the direct-to-video special "" He, too, has become a noted writer of his own high-profile television drama programmes, including "Queer as Folk" and the 2005 revival of "Doctor Who". Jimmy McGovern also wrote some episodes.
The current Executive Producer is Kieran Roberts who was once a Producer of "Emmerdale" and the Producer is ex-Doctor Who producer Phil Collinson, who took over from Kim Crowther in summer 2010.
Theme music.
The show's theme music, a cornet piece, accompanied by a brass band plus clarinet and double bass, reminiscent of northern band music, was written by Eric Spear.
The identity of the trumpeter was not public knowledge until 1994, when jazz musician and journalist Ron Simmonds revealed that it was the Surrey musician Ronnie Hunt. He added, "an attempt was made in later years to re-record that solo, using Stan Roderick, but it sounded too good, and they reverted to the old one." In 2004, the "Manchester Evening News" published a contradictory story that a young musician from Wilmslow called David Browning played the trumpet on both the original recording of the theme in 1960 and a re-recording in 1964, for a one-off payment of £36. In June 2009, the "Mail on Sunday" resolved the matter. Browning conceded that Hunt recorded the original in 1960, but believed that his own re-recording in 1964 or 1972 had been used since that date. ITV then confirmed to the "Mail" that a second version had been recorded in the 1970s, but was only used for a very short while before reverting to Hunt's 1960 recording. In the 1980s the same version was converted to stereo.
Ronnie Hunt said he was paid £6, and found the experience frustrating as Eric Spear insisted on many takes before obtaining the sound that he wanted. After taking a break in a local pub, Hunt achieved the desired mournful sound by playing very close to the microphone.
A new, completely re-recorded version of the theme tune replaced the original when the series started broadcasting in HD on 31 May 2010. It accompanied a new montage-style credits sequence featuring images of Manchester and Weatherfield.
A reggae version of the theme tune was recorded by The I-Royals and released by Media Marvels and WEA in 1983.
Viewing figures.
Most episodes in the 1960s, 70s, and 80s rated with over 20 million viewers and during the 1990s and early 2000s 14–16 million per episode would be typical. Like most terrestrial television in the UK, a decline in viewership has taken place and the show currently posts an average audience of just under 9 million per episode as of 2013 remaining as one of the highest rated programmes in the UK.
The programme currently rates as one of the most watched programmes on UK television for every day it is aired. Viewership peaked on Christmas Day 1987 when an average of 28.5 million viewers tuned in to see Hilda Ogden leave the street to start a new life as a housekeeper for long term friend Dr Lowther (although there is some confusion as to whether or not this was actually the highest rating episode due to a special omnibus repeat of that week's episodes being combined with the original airing). Since EastEnders began in 1985, the two programmes have constantly battled it out for first place in the ratings.
Sets.
The regular exterior buildings shown in "Coronation Street" include a row of terrace houses, several townhouses, and communal areas including a newsagents ("The Kabin"), a cafe ("Roy's Rolls"), a general grocery shop ("D&S Alahan's"), a factory ("Underworld") and "Rovers Return Inn" public house. The Rovers Return Inn is the main meeting place for the show's characters.
Between 1960 and 1968 street scenes were filmed before a set constructed in a studio, with the house fronts reduced in scale to 3/4 and constructed from wood. In 1968 Granada built an outside set not all that different from the interior version previously used, with the wooden façades from the studio simply being erected on the new site. These were replaced with brick façades, and back yards were added in the 1970s.
In 1982 a permanent full-street set was built in the Granada backlot, an area between Quay Street and Liverpool Road in Manchester. The set was constructed from reclaimed Salford brick. The set was updated in 1989 with the construction of a new factory, two shop units and three modern town houses on the south side of the street.
Between 1989 and 1999 the Granada Studios Tour allowed members of the public to visit the set. The exterior set was extended and updated in 1999. This update added to the Rosamund Street and Victoria Street façades, and added a viaduct on Rosamund Street. Most interior scenes are shot in the adjoining purpose-built studio.
In 2008, "Victoria Court", an apartment building full of luxury flats, was started on Victoria Street.
In 2014, production moved to a new site at Trafford Wharf, a former dock area about two miles to the east, part of the MediaCityUK complex. The Trafford Wharf backlot is built upon a former truck stop site next to the Imperial War Museum North. It took two years from start to finish to recreate the iconic Street. The houses were built to almost full scale after previously being three-quarter size.
On 5 April 2014, the staff began to allow booked public visits to the old Quay Street set. An advert, with a voiceover from Victoria Wood, appeared on TV to advertise the tour.
Broadcast.
United Kingdom.
For 54 years, "Coronation Street" has remained at the centre of ITV's prime time schedule. The programme is currently shown in the UK in five episodes, over three evenings a week on ITV. From Friday 9 December 1960 until Friday 3 March 1961, the programme was shown in two episodes broadcast Wednesday and Friday at 19:00. Schedules were changed and from Monday 6 March 1961 until Wednesday 11 October 1989, the programme was shown in two episodes broadcast Monday and Wednesday at 19:30. The third weekly episode was introduced on Friday 20 October 1989, broadcast at 19:30. From 1996, an extra episode was broadcast at 19:30 on Sunday nights. Aside from Granada, the programme originally appeared on the following stations of the ITV network: Anglia Television, Associated-Rediffusion, Television Wales and the West, Scottish Television, Southern Television and Ulster Television. From episode 14 on Wednesday 25 January 1961, Tyne Tees Television broadcast the programme. That left ATV in the Midlands as the only ITV station not carrying the show. When they decided to broadcast the programme, national transmission was changed from Wednesday and Friday at 19:00 to Monday and Wednesday at 19:30 and the programme became fully networked under this new arrangement from episode 25 on Monday 6 March 1961.
As the ITV network grew over the next few years, the programme was transmitted by these new stations on these dates onward: Westward Television from episode 40 on 29 April 1961, Border Television from episode 76 on 1 September 1961, Grampian Television from episode 84 on 30 September 1961, Channel Television from episode 180 on 1 September 1962 and Teledu Cymru (north and west Wales) from episode 184 on 14 September 1962. At this point, the ITV network became complete and the programme was broadcast almost continuously across the country at 19:30 on Monday and Wednesday for the next twenty-seven years.
From episode 2981 on Friday 20 October 1989 at 19:30, a third weekly episode was introduced and this increased to four episodes a week from episode 4096 on Sunday 24 November 1996, again at 19:30. The second Monday episode was introduced in 2002 and was broadcast at 20:30 to usher in the return of Bet Lynch. The Monday 20:30 episode was used intermittently during the popular Richard Hillman story line but has become fully scheduled since episode 5568 on Monday 25 August 2003. Additional episodes have been broadcast during the weekly schedule of ITV at certain times, notably in 2004 when, between 22 and 26 November, eight episodes were shown.
Older episodes had been broadcast by satellite and cable channel Granada Plus from launch in 1996. The first episodes shown were from episode 1588 (originally transmitted on Monday 5 April 1976) onwards. Originally listed and promoted as "Classic Coronation Street", the "classic" was dropped in early 2002, at which stage the episodes were from late 1989. By the time of the channel's closure in 2004, the repeats had reached January 1994. In addition to this, "specials" were broadcast on Saturday afternoons in the early years of the channel with several episodes based on a particular theme or character(s) were shown. The latest episode shown in these specials was from 1991. In addition, on 27 and 28 December 2003, several Christmas Day editions of the show were broadcast.
From 23 July 2009 "Coronation Street" has been broadcast in five weekly instalments, at 19:30 and 20:30 on Mondays and Fridays, and at 20:30 on Thursday. The Thursday episode replaces the former Wednesday show. Occasional late night episodes of "Coronation Street" begin at 22:00, due to the watershed. Repeat episodes, omnibus broadcasts and specials have been shown on ITV and ITV2. In January 2008 the omnibus returned to the main ITV channel where it was aired on Saturday mornings/afternoons depending on the schedule and times. In May 2008 it moved to Sunday mornings until August 2008 when it returned to Saturdays. In January 2009 it moved back to Sunday mornings usually broadcasting at around 09:25 until December 2010. In January 2011 the omnibus moved to Saturday mornings on ITV at 09:25. During the Rugby World Cup, which took place in New Zealand, matches had to be broadcast on a Saturday morning, so the omnibus moved to Saturday lunchtimes/afternoons during September and October 2011. However as of 22 October 2011 the omnibus moved back to Saturday mornings at 09:25 on ITV. From January 2012 the omnibus was no longer broadcast on ITV after four years, however it remains on ITV2.
On 30 June 2011 it was confirmed that Coronation Street would return to its traditional 19:30 timeslot on a Wednesday evening in September 2012.
International.
"Coronation Street" is also shown in various countries worldwide.
The programme was first aired in Australia in 1963 on TCN-9 Sydney, GTV-9 Melbourne and NWS-9 Adelaide, and by 1966 "Coronation Street" was more popular in Australia than in the UK. The show eventually left free-to-air television in Australia in the 1970s. It briefly returned to the Nine Network in a daytime slot during 1994–95. In 2005 STW-9 Perth began to show episodes before the 18:00 news to improve the lead in to Nine News Perth, but this did not work and the show was cancelled a few months later. In 1996 Pay-TV began and Arena began screening the series in one-hour instalments on Saturdays and Sundays at 18:30 EST. The series was later moved to Pay-TV channel UKTV where it is still shown. Currently as at September 2014 "Coronation Street" is shown on weeknights at 18:50 EST. Episodes on UKTV are now about two weeks behind the UK. Seven broadcast old episodes daily on 7Two until September 2014.
In Canada, "Coronation Street" is broadcast on CBC Television. Until 2011, episodes were shown in Canada approximately 10 months after they aired in Britain; however, beginning in the fall of 2011, the CBC began showing two episodes every weekday, in order to catch up with the ITV showings, at 18:30 and 19:00 local time Monday-Friday, with an omnibus on Sundays at 07:30. By May 2014, the CBC was only two weeks behind Britain, so the show was reduced to a single showing weeknights at 18:30 local time. The show debuted on Toronto's CBLT in July 1966. The 2002 edition of the "Guinness Book of Records" recognises the 1,144 episodes sold to the now-defunct CBC-owned Saskatoon, Saskatchewan, TV station CBKST by Granada TV on 31 May 1971 to be the largest number of TV shows ever purchased in one transaction. The show traditionally aired on weekday afternoons in Canada, with a Sunday morning omnibus. In 2004, CBC moved the weekday airings from their daytime slot to prime time. In light of recent austerity measures imposed on the CBC in 2012, which includes further cutbacks on non-Canadian programming, one of the foreign shows to remain on the CBC schedule is "Coronation Street", according to the CBC's director of content planning Christine Wilson, who commented: "Unofficially I can tell you "Coronation Street" is coming back. If it didn't come back, something would happen on Parliament Hill." Kirstine Stewart, the head of the CBC's English-language division, once remarked: "Coronation Street fans are the most loyal, except maybe for curling viewers, of all CBC viewers," In late September 2014, CBC aired extra episodes to become only one week behind the UK in airing of new episodes.
In the Republic of Ireland, "Coronation Street" is simulcast on UTV Ireland. The show is UTV Ireland's most watched programme with an average of 365,000 people watching each night. An omnibus is also shown on weekends. For a number of months in 2009 TV3 provided repeats of the night's episode on sister channel 3e at 21:00 Monday, Wednesday and Friday nights, this has since stopped. The show was first aired in 1978, beginning with episodes from 1976. Ireland eventually caught up with the current UK episodes in 1983. Until 1992 it was broadcast on RTÉ2 and from 1992 to 2001 it was broadcast on RTÉ One. In 2001 Granada TV bought 45 percent of TV3, which resulted in TV3 broadcasting series from 2001 to 2014. In 2006 ITV sold its share of the channel but TV3. TV3 continue to buy the soap until the end of 2014 when it moved to UTV Ireland. Coronation Street has broadcast on each of the main Irish networks, except for the Irish Language Network TG4.
In New Zealand, "Coronation Street" has been shown locally since 1964, first on NZBC television until 1975, and then on TV One, which broadcasts it in a 4-episode/2-hour block on Fridays from 19:30. Since September 2014, TVOne has added a 2-episode/1-hour block on Saturday from 20:30. Because TVOne has never upgraded to showing the equivalent of five episodes per week, New Zealand continues to fall further and further behind with episodes, and is 23 months behind Britain (as of 28 March 2014). During the weekday nights of the week ending 11 April 2014 and previous weeks, Coronation Street was the least watched programme on TV One in the 19:30 slot by a considerable margin in comparison to other weeknights, The serial aired on Tuesdays and Thursdays at 19:30 until October 2011, when the show moved to a 17:30 half-hour slot every weekday. The move proved highly unpopular with fans, and the series was quickly moved into its present prime-time slot within weeks. Episodes 7883, 7884, 7885 and 7886 were screened on 16 May 2014. These were originally aired in the UK between 4 and 11 June 2012.
In the United States, "Coronation Street" is available by broadcast or cable only in northern markets where CBC coverage from Canada overlaps the border or is available on local cable systems. It was broadcast on CBC's US cable channel, Trio until the CBC sold its stake in the channel to Universal, before it was shut down in 2006. Since 2009, episodes have been available in the United States through Amazon.com's on-demand service. Episodes are one month behind their original UK airdates. The final series of shows available from Amazon appears to be from November 2012, as no new episodes have been uploaded. On 15 January 2013, online distributor Hulu began airing episodes of the show, posting a new episode daily, two weeks after their original airdates.
Coronation Street was also shown on USA Network for an unknown period starting in 1982.
HM Forces and their families stationed overseas can watch "Coronation Street" on ITV, carried by the British Forces Broadcasting Service, which is also available to civilians in the Falkland Islands. It used to be shown on BFBS1.
Satellite channel ITV Choice shows the programme in Asia, Middle East, Cyprus, and Malta. In the United Arab Emirates, episodes of "Coronation Street" are broadcast one month after their UK showing.
Merchandise.
Several classic episodes were released on VHS video in the 1980s and 1990s in different sets, while a number of specially recorded feature-length episodes were released exclusively to video (see Coronation Street VHS and DVD releases).
"The Street", a magazine dedicated to the show, was launched in 1989. Edited by Bill Hill, the magazine contained a summary of recent storylines, interviews, articles about classic episodes, and stories that occurred from before 1960. The format was initially A5 size, expanding to A4 from the seventh issue. The magazine folded after issue 23 in 1993 when the publisher's contract with Granada Studios Tour expired and Granada wanted to produce their own magazine.
During the time when the studio tour was operating, a huge amount of branded merchandise was available from an on-site shop—everything from soap, to tea-towels, to model houses. These items gradually became scarce as the tours complex was wound down. Although there were large numbers produced, these items are becoming collectable by fans.
On 25 June 2010 a video game of the show was released on Nintendo DS. Players take the role of a detective as they work through a brand new storyline and befriend the various characters from the street, including Ken, Norris, Maria and Blanche.
Discography.
In 1995, to commemorate the programme's 35th anniversary, a CD called "The Coronation Street Album" was released, featuring cover versions of modern songs and standards by contemporary cast members.
An album featuring songs sung by some of the cast was released for the show's 50th anniversary. The singers include William Roache, Betty Driver, Kevin Kennedy and Katherine Kelly. The album is titled "Rogues, Angels, Heroes & Fools".
Spin-offs and specials.
Granada launched one spin-off in 1965, "Pardon the Expression", following the story of clothing store manager Leonard Swindley (Arthur Lowe) after he left Weatherfield. Swindley's management experience was tested when he was appointed assistant manager at a fictional department store, Dobson and Hawks. Granada produced two series of the spin-off, which ended in 1966.
In 1967, Arthur Lowe returned as Leonard Swindley in "Turn Out the Lights", a short-lived sequel to "Pardon the Expression". It ran for just one series of six episodes before it was cancelled.
The German TV series "Lindenstraße" took "Coronation Street" as the model. "Lindenstraße" started in 1985.
In 1985, a sister series, "Albion Market" was launched. It ran for one year, with 100 episodes produced.
On 8 December 2000 and 9 December 2010, live episodes were aired to mark the 40th and 50th anniversaries of the show. The first was mainly based around Vera Duckworth in hospital and the campaign to save the cobbles. The second was based around events following the tram crash.
"Coronation Street: Family Album" was several documentaries about various families living on the street.
"Farewell ..." was several documentaries featuring the best moments of a single character who had recently left the series—most notably, Farewell Blanche (Hunt), Farewell Jack (Duckworth), Farewell Mike (Baldwin), Farewell Vera (Duckworth), Farewell Janice (Battersby), Farewell Liz (McDonald) and Farewell Becky (McDonald). Most of these were broadcast on the same day as the character's final scenes in the series.
On 21 December 2008, a web-based miniseries ran on ITV.com; called "Corrie Confidential"; the first episode featured the characters Rosie and Sophie Webster in "Underworld".
"Stars on the Street" was aired around Christmas 2009. It featured actors from the soap talking about the famous guest stars who had appeared in the series including people who were in it before they were famous.
In 2010, several actors from the show appeared on "The Jeremy Kyle Show" as their soap characters: David Platt (Jack P. Shepherd), Nick Tilsley (Ben Price) and Tina McIntyre (Michelle Keegan). In the fictional, semi-improvised scenario, David accused Nick (his brother) and Tina (his ex-girlfriend) of sleeping together.
On 21 December 2012, Coronation Street produced a Text Santa special entitled "A Christmas Corrie" which featured Norris Cole in the style of Scrooge, being visited by the ghosts of dead characters. The ghosts were Mike Baldwin, Maxine Peacock, Derek Wilton and Vera Duckworth. Other special guests include Torvill and Dean, Lorraine Kelly and Sheila Reid. The episode concluded with Norris learning the error of his ways and dancing on the cobbles. The original plan for this feature was to have included Jack Duckworth, along with Vera, but actor Bill Tarmey died before filming commenced. In the end a recording of his voice was played.
On 3 February 2014, another web-based miniseries ran on ITV.com; called "Streetcar Stories". It showed what Steve and Lloyd get up to during the late nights in their Streetcar cab office. The first episode shows Steve and Lloyd making a cup of tea with The Stripper playing in the background, highly referencing Morcambe and Wise's Breakfast Sketch. The second episode involves the pair having a biscuit dunking competition.
"Corrie Extra!".
ITV.com launched a small spin-off drama series called 'Gary's Army Diaries' which revolves around Gary's experiences in Afghanistan and the loss of his best friend, Quinny. Due to their popularity, the three five-minute episodes were recut into a single 30-minute episode, which was broadcast on ITV2.
William Roache and Anne Kirkbride starred as Ken and Deirdre in a series of ten three-minute internet 'webisodes'. The first episode of the series titled, "Ken and Deirdre's Bedtime Stories" was activated on Valentine's Day 2011.
In 2011, an internet based spin-off starring Helen Flanagan as Rosie Webster followed her on her quest to be a supermodel.
Stage.
In August 2010, many "Coronation Street" characters were brought to the stage in Jonathan Harvey's comedy play "Corrie!". The play was commissioned to celebrate the 50th Anniversary of the TV series and was presented at The Lowry in Salford, England by ITV Studios and Phil McIntyre Entertainments. Featuring a cast of six actors who alternate roles of favourite characters including Ena Sharples, Hilda Ogden, Hayley and Roy, Richard Hillman, Jack Duckworth, Bet Lynch, Steve, Karen and Becky, the play weaves together some of the most memorable moments from the TV show. It toured UK theatres between February 2011 and July 2011 with guest star narrators including Roy Barraclough, Ken Morley and Gaynor Faye.
Films.
Over the years "Coronation Street" has released several straight-to-video films. Unlike other soaps which often used straight-to-video films to cover more contentious plot lines that may not be allowed by the broadcaster, "Coronation Street" has largely used these films to reset their characters in other locations.
In 1995, "Coronation Street: The Cruise" also known as "Coronation Street: The Feature Length Special" was released on VHS to celebrate the 35th anniversary of the show. ITV heavily promoted the programme as a direct-to-video exclusive but broadcast a brief version of it on 24 March 1996. The Independent Television Commission investigated the broadcast, as viewers complained that ITV misled them.
In 1997, following the controversial cruise spin-off, "" was released on VHS, featuring Jack Duckworth, Vera Duckworth, Fiona Middleton and Maxine Peacock on a trip to Las Vegas.
In 1999, six special episodes of "Coronation Street" were produced, following the story of Steve McDonald, Vicky McDonald, Vikram Desai, Bet Gilroy and Reg Holdsworth in Brighton. This video was titled "Coronation Street: Open All Hours" and released on VHS.
In 2008, ITV announced filming was to get underway for a new special DVD episode, ", following the Battersby-Brown family, which saw the temporary return of Cilla Battersby-Brown.
In 2009, another DVD special, ", was released. The feature-length comedy drama followed Roy, Hayley and Becky as they travelled to Romania for the wedding of a face from their past.
On 1 November 2010, "Coronation Street: A Knight's Tale" was released. Reg Holdsworth and Curly Watts returned in the film. Mary tries to take Norris to an apparently haunted castle where she hoped to seduce him. Rosie gets a job there and she takes Jason with her. Brian Capron also guest starred as an assumed relative of Richard Hillman. He rises out of a lake as a comedic "wink to the audience" after Hillman drowned in 2003.
50th Anniversary (2010).
The BBC commissioned a one-off drama called "The Road to Coronation Street", about how the series first came into being. Jessie Wallace plays Pat Phoenix (Elsie Tanner) with Lynda Baron as Violet Carson (Ena Sharples), Celia Imrie as Doris Speed (Annie Walker) and James Roache as his own father William Roache (Ken Barlow). It was broadcast on 16 September 2010 on BBC Four.
In December 2010, ITV made a few special programmes to mark the 50th anniversary. "Coronation Street Uncovered: Live", hosted by Stephen Mulhern was shown after the episode with the tram crash was aired on ITV 2. On 7 and 9 December a countdown on the greatest Corrie moments, "Coronation Street: 50 Years, 50 Moments", the viewers voted "The Barlows at Alcoholics Anonymous" as the greatest moment. On 10 December Paul O'Grady hosted a quiz show, "Coronation Street: The Big 50" with three teams from the soap and a celebrity team answering questions about Coronation Street and other soaps. Also, "Come Dine with Me" and "Celebrity Juice" aired Coronation Street specials in the anniversary week.
Crossovers.
"Coronation Street" and rival soap opera "EastEnders" had a crossover for "Children in Need" in November 2010 called "East Street". "EastEnders" stars that visited Weatherfield include Laurie Brett as Jane Beale, Charlie G. Hawkins as Darren Miller, Kylie Babbington as Jodie Gold, Nina Wadia as Zainab Masood and John Partridge as Christian Clarke.
"Coronation Street: Viva Las Vegas!" also included some characters from "Emmerdale".
Corrie in popular culture.
The British rock band Queen produced a single "I Want to Break Free" in 1984 which reached number 3 position in UK charts and which is largely known for its music video for which all the band members dressed in women's clothes, which parodied the characters and is considered as a homage to the show. The video depicts Mercury as a housewife, loosely based on Bet Lynch, who wants to "break free" from his life. Although Lynch was a blonde in the soap opera, Mercury thought he would look too silly as a blonde and chose a dark wig. May plays another, more relaxed housewife based on Hilda Ogden.
Sponsorship.
Cadbury was the first sponsor of "Coronation Street" beginning in July 1996. The original sponsorship had a chocolate-like version of the street (which can be seen in place at the Cadbury World museum in Bournville, Birmingham) with chocolate characters resembling some of the actual "Coronation Street" characters. In the summer of 2006, Cadbury Trebor Bassetts had to recall over one million chocolate bars, due to suspected salmonella contamination, and "Coronation Street" stopped the sponsorship for several months. In late 2006, Cadbury did not renew their contract, but agreed to sponsor the show until "Coronation Street" found a new sponsor.
In July 2007, an ITV press release announced that Harveys was the new sponsor of "Coronation Street" on the ITV Network. Harveys' sponsorship began on 30 September 2007. In the "Coronation Street: Romanian Holiday" film, Roy and Hayley Cropper are filmed in front of a Harveys store. In "Coronation Street: A Knights Tale", a Harveys truck can be seen driving past Mary Taylor's motor-home to further promote the brand. On 11 April 2012, it was announced that Harveys had decided not to renew their contract and ceased sponsorship in December 2012. Compare The Market were named as the new sponsor.
In November 2011 a Nationwide Building Society ATM in Dev and Sunita Alahan's shop became the first use of paid-for product placement in a UK primetime show.
Awards and nominations.
"Coronation Street" is the second most award-winning British soap opera in the UK, behind rival soap "EastEnders".
Producers.
The first producer was Stuart Latham, from December 1960 to July 1961. In the 1960s and 1970s, most producers did stints of about one year. Longer-running producers included Eric Prytherch (May 1972 – April 1974); Bill Podmore (September 1977 – July 1982); Carolyn Reynolds (1991–1993); and Sue Pritchard (1993–1996). From 2008 until Summer 2010 the soap was produced by Kim Crowther, who was replaced by Phil Collinson (producer of Doctor Who from 2005 to 2009). From April 2013, to January 2015 former Emmerdale producer Stuart Blackburn took over production on Coronation Street.
See also.
Listen to this article ()
This audio file was created from a revision of the "Coronation Street" article dated 2007-03-11, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="6852" url="http://en.wikipedia.org/wiki?curid=6852" title="Caligula">
Caligula

Caligula () was the popular nickname of Gaius Julius Caesar Augustus Germanicus (31 August AD 12 – 24 January AD 41), Roman emperor (AD 37–41). Caligula was a member of the house of rulers conventionally known as the Julio-Claudian dynasty. Caligula's father Germanicus, the nephew and adopted son of Emperor Tiberius, was a very successful general and one of Rome's most beloved public figures. The young Gaius earned the nickname "Caligula" (meaning "little soldier's boot"), the diminutive form of "caliga", hob-nailed military boot) from his father's soldiers while accompanying him during his campaigns in Germania.
When Germanicus died at Antioch in AD 19, his wife Agrippina the Elder returned to Rome with her six children where she became entangled in a bitter feud with Tiberius. The conflict eventually led to the destruction of her family, with Caligula as the sole male survivor. Untouched by the deadly intrigues, Caligula accepted the invitation to join the Emperor on the island of Capri in AD 31, to where Tiberius, himself, had withdrawn five years earlier. With the death of Tiberius in AD 37, Caligula succeeded his grand uncle and adoptive grandfather as emperor.
There are few surviving sources about the reign of Emperor Caligula, although he is described as a noble and moderate ruler during the first six months of his reign. After this, the sources focus upon his cruelty, sadism, extravagance, and sexual perversity, presenting him as an insane tyrant. While the reliability of these sources is questionable, it is known that during his brief reign, Caligula worked to increase the unconstrained personal power of the emperor, as opposed to countervailing powers within the principate. He directed much of his attention to ambitious construction projects and luxurious dwellings for himself, and initiated the construction of two aqueducts in Rome: the Aqua Claudia and the Anio Novus. During his reign, the empire annexed the Kingdom of Mauretania as a province.
In early AD 41, Caligula was assassinated as a result of a conspiracy by officers of the Praetorian Guard, senators, and courtiers. The conspirators' attempt to use the opportunity to restore the Roman Republic was thwarted: on the day of the assassination of Caligula, the Praetorian Guard declared Caligula's uncle, Claudius, the next Roman emperor.
Early life.
Family.
Caligula was born in Antium (modern Anzio and Nettuno) on 31 August 12 AD, the third of six surviving children born to Germanicus and Germanicus' second cousin Agrippina the Elder. Gaius's brothers were Nero and Drusus. His sisters were Agrippina the Younger, Julia Drusilla, and Julia Livilla. Gaius was nephew to Claudius (the future emperor).
Agrippina the Elder was the daughter of Marcus Vipsanius Agrippa and Julia the Elder. She was a granddaughter of Augustus and Scribonia.
Youth and early career.
As a boy of just two or three, Gaius accompanied his father, Germanicus, on campaigns in the north of Germania. The soldiers were amused that Gaius was dressed in a miniature soldier's uniform, including boots and armour. He was soon given his nickname "Caligula", meaning "little (soldier's) boot" in Latin, after the small boots he wore as part of his uniform. Gaius, though, reportedly grew to dislike this nickname.
Suetonius claims that Germanicus was poisoned in Syria by an agent of Tiberius, who viewed Germanicus as a political rival.
After the death of his father, Caligula lived with his mother until her relations with Tiberius deteriorated. Tiberius would not allow Agrippina to remarry for fear her husband would be a rival. Agrippina and Caligula's brother, Nero, were banished in 29 AD on charges of treason.
The adolescent Caligula was then sent to live first with his great-grandmother (and Tiberius's mother) Livia. Following Livia's death, he was sent to live with his grandmother Antonia. In 30 AD, his brother, Drusus Caesar, was imprisoned on charges of treason and his brother Nero died in exile from either starvation or suicide. Suetonius writes that after the banishment of his mother and brothers, Caligula and his sisters were nothing more than prisoners of Tiberius under the close watch of soldiers.
In 31 AD, Caligula was remanded to the personal care of Tiberius on Capri, where he lived for six years. To the surprise of many, Caligula was spared by Tiberius. According to historians, Caligula was an excellent natural actor and, recognizing danger, hid all his resentment towards Tiberius. An observer said of Caligula, "Never was there a better servant or a worse master!"
Caligula claimed to have planned to kill Tiberius with a dagger in order to avenge his mother and brother: however, having brought the weapon into Tiberius's bedroom he did not kill the Emperor but instead threw the dagger down on the floor. Supposedly Tiberius knew of this but never dared to do anything about it. Suetonius claims that Caligula was already cruel and vicious: he writes that, when Tiberius brought Caligula to Capri, his purpose was to allow Caligula to live in order that he "... prove the ruin of himself and of all men, and that he was rearing a viper for the Roman people and a Phaethon for the world."
In 33 AD, Tiberius gave Caligula an honorary quaestorship, a position he held until his rise to emperor. Meanwhile, both Caligula's mother and his brother Drusus died in prison. Caligula was briefly married to Junia Claudilla, in 33, though she died during childbirth the following year. Caligula spent time befriending the Praetorian prefect, Naevius Sutorius Macro, an important ally. Macro spoke well of Caligula to Tiberius, attempting to quell any ill will or suspicion the Emperor felt towards Caligula.
In 35 AD, Caligula was named joint heir to Tiberius's estate along with Tiberius Gemellus.
Emperor.
Early reign.
When Tiberius died on 16 March 37 AD, his estate and the titles of the principate were left to Caligula and Tiberius's own grandson, Gemellus, who were to serve as joint heirs. Although Tiberius was 78 and on his death bed, some ancient historians still conjecture that he was murdered. Tacitus writes that the Praetorian Prefect, Macro, smothered Tiberius with a pillow to hasten Caligula's accession, much to the joy of the Roman people, while Suetonius writes that Caligula may have carried out the killing, though this is not recorded by any other ancient historian. Seneca the elder and Philo, who both wrote during Tiberius's reign, as well as Josephus record Tiberius as dying a natural death. Backed by Macro, Caligula had Tiberius' will nullified with regards to Gemellus on grounds of insanity, but otherwise carried out Tiberius' wishes.
Caligula accepted the powers of the principate as conferred by the senate and entered Rome on 28 March amid a crowd that hailed him as "our baby" and "our star," among other nicknames. Caligula is described as the first emperor who was admired by everyone in "all the world, from the rising to the setting sun." Caligula was loved by many for being the beloved son of the popular Germanicus, and because he was not Tiberius. It was said by Suetonius that over 160,000 animals were sacrificed during three months of public rejoicing to usher in the new reign. Philo describes the first seven months of Caligula's reign as completely blissful.
Caligula's first acts were said to be generous in spirit, though many were political in nature. To gain support, he granted bonuses to those in the military including the Praetorian Guard, city troops and the army outside Italy. He destroyed Tiberius's treason papers, declared that treason trials were a thing of the past, and recalled those who had been sent into exile. He helped those who had been harmed by the imperial tax system, banished certain sexual deviants, and put on lavish spectacles for the public, including gladiatorial games. Caligula collected and brought back the bones of his mother and of his brothers and deposited their remains in the tomb of Augustus.
In October 37 AD, Caligula fell seriously ill or perhaps was poisoned. He recovered from his illness soon thereafter, but many believed that the illness turned the young emperor toward the diabolical as he started to kill off or exile those who were close to him or whom he saw as a serious threat. Perhaps his illness reminded him of his mortality and of the desire of others to advance into his place. He had his cousin and adopted son Tiberius Gemellus executed – an act that outraged Caligula's and Gemellus's mutual grandmother Antonia Minor. She is said to have committed suicide, although Suetonius hints that Caligula actually poisoned her. He had his father-in-law Marcus Junius Silanus and his brother-in-law Marcus Lepidus executed as well. His uncle Claudius was spared only because Caligula preferred to keep him as a laughing stock. His favorite sister Julia Drusilla died in 38 AD of a fever: his other two sisters, Livilla and Agrippina the Younger, were exiled. He hated being the grandson of Agrippa and slandered Augustus by repeating a falsehood that his mother was actually conceived as the result of an incestuous relationship between Augustus and his daughter Julia the Elder.
Public reform.
In AD 38, Caligula focused his attention on political and public reform. He published the accounts of public funds, which had not been made public during the reign of Tiberius. He aided those who lost property in fires, abolished certain taxes, and gave out prizes to the public at gymnastic events. He allowed new members into the equestrian and senatorial orders.
Perhaps most significantly, he restored the practice of democratic elections. Cassius Dio said that this act "though delighting the rabble, grieved the sensible, who stopped to reflect, that if the offices should fall once more into the hands of the many ... many disasters would result".
During the same year, though, Caligula was criticized for executing people without full trials and for forcing his supporter Macro to commit suicide.
Financial crisis and famine.
According to Cassius Dio, a financial crisis emerged in AD 39. Suetonius places the beginning of this crisis in 38. Caligula's political payments for support, generosity and extravagance had exhausted the state's treasury. Ancient historians state that Caligula began falsely accusing, fining and even killing individuals for the purpose of seizing their estates.
Historians describe a number of Caligula's other desperate measures. In order to gain funds, Caligula asked the public to lend the state money. He levied taxes on lawsuits, weddings and prostitution. Caligula began auctioning the lives of the gladiators at shows. Wills that left items to Tiberius were reinterpreted to leave the items instead to Caligula. Centurions who had acquired property during plundering were forced to turn over spoils to the state.
The current and past highway commissioners were accused of incompetence and embezzlement and forced to repay money. According to Suetonius, in the first year of Caligula's reign he squandered 2,7 billion sesterces that Tiberius had amassed. His nephew Nero Caesar both envied and admired the fact that Gaius had run through the vast wealth Tiberius had left him in so short a time.
A brief famine of unknown extent occurred, perhaps caused by this financial crisis, but Suetonius claims it resulted from Caligula's seizure of public carriages; according to Seneca, grain imports were disturbed because Caligula repurposed grain boats for a pontoon bridge.
Construction.
Despite financial difficulties, Caligula embarked on a number of construction projects during his reign. Some were for the public good, though others were for himself.
Josephus describes Caligula's improvements to the harbours at Rhegium and Sicily, allowing increased grain imports from Egypt, as his greatest contributions. These improvements may have been in response to the famine.
Caligula completed the temple of Augustus and the theatre of Pompey and began an amphitheatre beside the Saepta. He expanded the imperial palace. He began the aqueducts Aqua Claudia and Anio Novus, which Pliny the Elder considered engineering marvels. He built a large racetrack known as the "circus of Gaius and Nero" and had an Egyptian obelisk (now known as the "Vatican Obelisk") transported by sea and erected in the middle of Rome.
At Syracuse, he repaired the city walls and the temples of the gods. He had new roads built and pushed to keep roads in good condition. He had planned to rebuild the palace of Polycrates at Samos, to finish the temple of Didymaean Apollo at Ephesus and to found a city high up in the Alps. He planned to dig a canal through the isthmus in Greece and sent a chief centurion to survey the work.
In 39, Caligula performed a spectacular stunt by ordering a temporary floating bridge to be built using ships as pontoons, stretching for over two miles from the resort of Baiae to the neighboring port of Puteoli. It was said that the bridge was to rival that of the Persian king, Xerxes', crossing of the Hellespont. Caligula, who could not swim, then proceeded to ride his favorite horse, Incitatus, across, wearing the breastplate of Alexander the Great. This act was in defiance of a prediction by Tiberius's soothsayer Thrasyllus of Mendes that Caligula had "no more chance of becoming emperor than of riding a horse across the Bay of Baiae".
Caligula had two large ships constructed for himself, which were recovered from the bottom of Lake Nemi during the dictatorship of Benito Mussolini. The ships are among the largest vessels in the ancient world. The smaller ship was designed as a temple dedicated to Diana. The larger ship was essentially an elaborate floating palace that counted marble floors and plumbing among its amenities. Thirteen years after being raised, the ships were burned during an attack in the Second World War, and almost nothing remains of their hulls, though many archeological treasures remain intact in the museum at Lake Nemi and in the Museo Nazionale Romano (Palazzo Massimo) at Rome.
Feud with the senate.
In AD 39, relations between Caligula and the Roman Senate deteriorated. The subject of their disagreement is unknown. A number of factors, though, aggravated this feud. The Senate had become accustomed to ruling without an emperor between the departure of Tiberius for Capri in AD 26 and Caligula's accession. Additionally, Tiberius's treason trials had eliminated a number of pro-Julian senators such as Asinius Gallus.
Caligula reviewed Tiberius's records of treason trials and decided that numerous senators, based on their actions during these trials, were not trustworthy. He ordered a new set of investigations and trials. He replaced the consul and had several senators put to death. Suetonius reports that other senators were degraded by being forced to wait on him and run beside his chariot.
Soon after his break with the Senate, Caligula faced a number of additional conspiracies against him. A conspiracy involving his brother-in-law was foiled in late 39. Soon afterwards, the Governor of Germany, Gnaeus Cornelius Lentulus Gaetulicus, was executed for connections to a conspiracy.
Western expansion.
In AD 40, Caligula expanded the Roman Empire into Mauretania and made a significant attempt at expanding into Britannia – even challenging Neptune in his campaign. The conquest of Britannia was fully realized by his successors.
Mauretania.
Mauretania was a client kingdom of Rome ruled by Ptolemy of Mauretania. Caligula invited Ptolemy to Rome and then had him suddenly executed. Mauretania was annexed by Caligula and subsequently divided into two provinces, Mauretania Tingitana and Mauretania Caesariensis, separated by the river Malua. Pliny claims that division was the work of Caligula, but Dio states that in 42 AD an uprising took place, which was subdued by Gaius Suetonius Paulinus and Gnaeus Hosidius Geta, only after which the division took place. This confusion might mean that Caligula originally made the decision to divide the province, but the implementation was postponed because of the rebellion. The first known equestrian governor of the two provinces was one Marcus Fadius Celer Flavianus, in office in 44 AD.
Details on the Mauretanian events of 39–44 are unclear. Cassius Dio wrote an entire chapter on the annexation of Mauretania by Caligula, but it is now lost. Caligula's
move seemingly had a strictly personal political motive – that is, fear and jealousy of his cousin Ptolemy – and thus the expansion may not have been prompted by pressing military or economic needs. However, the rebellion of Tacfarinas had shown how exposed Africa Proconsularis was to its west and how the Mauretanian client kings were unable to provide protection to the province, and it is thus possible that Caligula's expansion was a prudent response to potential future threats.
Britannia.
There seemed to be a northern campaign to Britannia that was aborted. This campaign is derided by ancient historians with accounts of Gauls dressed up as Germanic tribesmen at his triumph and Roman troops ordered to collect seashells as "spoils of the sea". The few primary sources disagree on what precisely occurred. Modern historians have put forward numerous theories in an attempt to explain these actions. This trip to the English Channel could have merely been a training and scouting mission. The mission may have been to accept the surrender of the British chieftain Adminius. "Seashells", or "conchae" in Latin, may be a metaphor for something else such as female genitalia (perhaps the troops visited brothels) or boats (perhaps they captured several small British boats).
Claims of divinity.
When several kings came to Rome to pay their respects to him and argued about their nobility of descent, he cried out "Let there be one lord, one king." In AD 40, Caligula began implementing very controversial policies that introduced religion into his political role. Caligula began appearing in public dressed as various gods and demigods such as Hercules, Mercury, Venus and Apollo. Reportedly, he began referring to himself as a god when meeting with politicians and he was referred to as "Jupiter" on occasion in public documents.
A sacred precinct was set apart for his worship at Miletus in the province of Asia and two temples were erected for worship of him in Rome. The Temple of Castor and Pollux on the forum was linked directly to the imperial residence on the Palatine and dedicated to Caligula. He would appear here on occasion and present himself as a god to the public. Caligula had the heads removed from various statues of gods and replaced with his own in some temples. It is said that he wished to be worshipped as "Neos Helios," the "New Sun." Indeed, he was represented as a sun god on Egyptian coins.
Caligula's religious policy was a departure from that of his predecessors. According to Cassius Dio, living emperors could be worshipped as divine in the east and dead emperors could be worshipped as divine in Rome. Augustus had the public worship his spirit on occasion, but Dio describes this as an extreme act that emperors generally shied away from. Caligula took things a step further and had those in Rome, including senators, worship him as a tangible, living god.
Eastern policy.
Caligula needed to quell several riots and conspiracies in the eastern territories during his reign. Aiding him in his actions was his good friend, Herod Agrippa, who became governor of the territories of Batanaea and Trachonitis after Caligula became emperor in AD 37.
The cause of tensions in the east was complicated, involving the spread of Greek culture, Roman Law and the rights of Jews in the empire.
Caligula did not trust the prefect of Egypt, Aulus Avilius Flaccus. Flaccus had been loyal to Tiberius, had conspired against Caligula's mother and had connections with Egyptian separatists. In AD 38, Caligula sent Agrippa to Alexandria unannounced to check on Flaccus. According to Philo, the visit was met with jeers from the Greek population who saw Agrippa as the king of the Jews. Flaccus tried to placate both the Greek population and Caligula by having statues of the emperor placed in Jewish synagogues. As a result, riots broke out in the city. Caligula responded by removing Flaccus from his position and executing him.
In AD 39, Agrippa accused Herod Antipas, the tetrarch of Galilee and Perea, of planning a rebellion against Roman rule with the help of Parthia. Herod Antipas confessed and Caligula exiled him. Agrippa was rewarded with his territories.
Riots again erupted in Alexandria in AD 40 between Jews and Greeks. Jews were accused of not honoring the emperor. Disputes occurred in the city of Jamnia. Jews were angered by the erection of a clay altar and destroyed it. In response, Caligula ordered the erection of a statue of himself in the Jewish Temple of Jerusalem, a demand in conflict with Jewish monotheism. In this context, Philo wrote that Caligula "regarded the Jews with most especial suspicion, as if they were the only persons who cherished wishes opposed to his".
The Governor of Syria, Publius Petronius, fearing civil war if the order were carried out, delayed implementing it for nearly a year. Agrippa finally convinced Caligula to reverse the order.
Scandals.
Philo of Alexandria and Seneca the Younger describe Caligula as an insane emperor who was self-absorbed, angry, killed on a whim, and indulged in too much spending and sex. He is accused of sleeping with other men's wives and bragging about it, killing for mere amusement, deliberately wasting money on his bridge, causing starvation, and wanting a statue of himself erected in the Temple of Jerusalem for his worship. Once, at some games at which he was presiding, he ordered his guards to throw an entire section of the crowd into the arena during intermission to be eaten by animals because there were no criminals to be prosecuted and he was bored.
While repeating the earlier stories, the later sources of Suetonius and Cassius Dio provide additional tales of insanity. They accuse Caligula of incest with his sisters, Agrippina the Younger, Drusilla, and Livilla, and say he prostituted them to other men. They state he sent troops on illogical military exercises, turned the palace into a brothel, and, most famously, planned or promised to make his horse, Incitatus, a consul,
and actually appointed him a priest.
The validity of these accounts is debatable. In Roman political culture, insanity and sexual perversity were often presented hand-in-hand with poor government.
Assassination and aftermath.
Caligula's actions as emperor were described as being especially harsh to the senate, the nobility and the equestrian order. According to Josephus, these actions led to several failed conspiracies against Caligula. Eventually, a successful murder was planned by officers within the Praetorian Guard led by Cassius Chaerea. The plot is described as having been planned by three men, but many in the senate, army and equestrian order were said to have been informed of it and involved in it.
The situation escalated when, in 40 AD, Caligula announced to the senate that he would be leaving Rome permanently and moving to Alexandria, in Egypt, where he hoped to be worshiped as a living god. The prospect of Rome losing its emperor and thus its political power was the final straw for many. Such a move would have left both the senate and the Praetorian Guard powerless to stop Caligula's repression and debauchery. With this in mind Chaerea convinced his fellow conspirators to quickly put their plot into action.
According to Josephus, Chaerea had political motivations for the assassination. Suetonius sees the motive in Caligula calling Chaerea derogatory names. Caligula considered Chaerea effeminate because of a weak voice and for not being firm with tax collection. Caligula would mock Chaerea with names like "Priapus" and "Venus".
On 22 January 41, although Suetonius dates it as 24, Cassius Chaerea and other guardsmen accosted Caligula while he was addressing an acting troupe of young men during a series of games and dramatics held for the Divine Augustus. Details on the events vary somewhat from source to source, but they agree that Chaerea was first to stab Caligula, followed by a number of conspirators. Suetonius records that Caligula's death was similar to that of Julius Caesar's. He states that both the elder Gaius Julius Caesar (Julius Caesar) and the younger Gaius Julius Caesar (Caligula) were stabbed 30 times by conspirators led by a man named Cassius (Cassius Longinus and Cassius Chaerea).
The "cryptoporticus" (underground corridor) where this event took place was discovered beneath the imperial palaces on the Palatine Hill. By the time Caligula's loyal Germanic guard responded, the Emperor was already dead. The Germanic guard, stricken with grief and rage, responded with a rampaging attack on the assassins, conspirators, innocent senators and bystanders alike.
The senate attempted to use Caligula's death as an opportunity to restore the republic. Chaerea attempted to persuade the military to support the senate. The military, though, remained loyal to the office of the emperor. The grieving Roman people assembled and demanded that Caligula's murderers be brought to justice. Uncomfortable with lingering imperial support, the assassins sought out and stabbed Caligula's wife, Caesonia, and killed their young daughter, Julia Drusilla, by smashing her head against a wall. They were unable to reach Caligula's uncle, Claudius, who was spirited out of the city, after being found by a soldier hiding behind a palace curtain, to the nearby Praetorian camp.
Claudius became emperor after procuring the support of the Praetorian Guard and ordered the execution of Chaerea and any other known conspirators involved in the death of Caligula. According to Suetonius, Caligula's body was placed under turf until it was burned and entombed by his sisters. He was buried within the Mausoleum of Augustus; in 410, during the Sack of Rome the tomb's ashes were scattered.
Legacy.
Historiography.
The history of Caligula's reign is extremely problematic as only two sources contemporary with Caligula have survived — the works of Philo and Seneca. Philo's works, "On the Embassy to Gaius" and "Flaccus", give some details on Caligula's early reign, but mostly focus on events surrounding the Jewish population in Judea and Egypt with whom he sympathizes. Seneca's various works give mostly scattered anecdotes on Caligula's personality. Seneca was almost put to death by Caligula in AD 39 likely due to his associations with conspirators.
At one time, there were detailed contemporaneous histories on Caligula, but they are now lost. Additionally, the historians who wrote them are described as biased, either overly critical or praising of Caligula. Nonetheless, these lost primary sources, along with the works of Seneca and Philo, were the basis of surviving secondary and tertiary histories on Caligula written by the next generations of historians. A few of the contemporaneous historians are known by name. Fabius Rusticus and Cluvius Rufus both wrote condemning histories on Caligula that are now lost. Fabius Rusticus was a friend of Seneca who was known for historical embellishment and misrepresentation. Cluvius Rufus was a senator involved in the assassination of Caligula.
Caligula's sister, Agrippina the Younger, wrote an autobiography that certainly included a detailed explanation of Caligula's reign, but it too is lost. Agrippina was banished by Caligula for her connection to Marcus Lepidus, who conspired against Caligula. The inheritance of Nero, Agrippina's son and the future emperor, was seized by Caligula. Gaetulicus, a poet, produced a number of flattering writings about Caligula, but they too are lost.
The bulk of what is known of Caligula comes from Suetonius and Cassius Dio. Suetonius wrote his history on Caligula 80 years after his death, while Cassius Dio wrote his history over 180 years after Caligula's death. Cassius Dio's work is invaluable because it alone gives a loose chronology of Caligula's reign.
A handful of other sources add a limited perspective on Caligula. Josephus gives a detailed description of Caligula's assassination. Tacitus provides some information on Caligula's life under Tiberius. In a now lost portion of his "Annals", Tacitus gave a detailed history of Caligula. Pliny the Elder's "Natural History" has a few brief references to Caligula.
There are few surviving sources on Caligula and no surviving source paints Caligula in a favorable light. The paucity of sources has resulted in significant gaps in modern knowledge of the reign of Caligula. Little is written on the first two years of Caligula's reign. Additionally, there are only limited details on later significant events, such as the annexation of Mauretania, Caligula's military actions in Britannia, and his feud with the Roman Senate.
Health.
All surviving sources, except Pliny the Elder, characterize Caligula as insane. However, it is not known whether they are speaking figuratively or literally. Additionally, given Caligula's unpopularity among the surviving sources, it is difficult to separate fact from fiction. Recent sources are divided in attempting to ascribe a medical reason for his behavior, citing as possibilities encephalitis, epilepsy or meningitis. The question of whether or not Caligula was insane (especially after his illness early in his reign) remains unanswered.
Philo of Alexandria, Josephus and Seneca state that Caligula was insane, but describe this madness as a personality trait that came through experience. Seneca states that Caligula became arrogant, angry and insulting once becoming emperor and uses his personality flaws as examples his readers can learn from. According to Josephus, power made Caligula incredibly conceited and led him to think he was a god. Philo of Alexandria reports that Caligula became ruthless after nearly dying of an illness in the eighth month of his reign in AD 37. Juvenal reports he was given a magic potion that drove him insane.
Suetonius said that Caligula suffered from "falling sickness", or epilepsy, when he was young. Modern historians have theorized that Caligula lived with a daily fear of seizures. Despite swimming being a part of imperial education, Caligula could not swim. Epileptics are discouraged from swimming in open waters because unexpected fits in such difficult rescue circumstances can be fatal. Additionally, Caligula reportedly talked to the full moon. Epilepsy was long associated with the moon.
Some modern historians think that Caligula suffered from hyperthyroidism. This diagnosis is mainly attributed to Caligula's irritability and his "stare" as described by Pliny the Elder.
Possible rediscovery of burial site.
On 17 January 2011, police in Nemi, Italy, announced that they believed they had discovered the site of Caligula's burial, after arresting a thief caught smuggling a statue which they believed to be of the emperor. The claim has been met with scepticism by Cambridge historian Mary Beard.
Ancestry.
Ancestors of Caligula
In popular culture.
In film.
Emlyn Williams was cast as Caligula in the never-completed 1937 film "I, Claudius".
American actor Jay Robinson famously portrayed a sinister and scene-stealing Caligula in two epic films of the 1950s, "The Robe" (1953) and its sequel "Demetrius and the Gladiators" (1954).
A feature-length historical film "Caligula" was completed in 1979, in which Malcolm McDowell played the lead role. The film alienated audiences with extremely explicit sex and violence and received extremely negative reviews.
David Brandon portrayed Caligula in the 1982 Italian exploitation film "Emperor Caligula, the Untold Story" which was directed by Joe D'Amato. 
Courtney Love appeared as Caligula in a fake trailer for "Gore Vidal's Caligula", ostensibly a remake of the 1979 film, but actually a parodic short film by conceptual artist Francesco Vezzoli.
Szabolcs Hajdu portrayed Caligula in the 1996 film "Caligula".
In games and video games.
Adult Swim created a game called "Viva Caligula" for their official website. This game follows the scandalous rumors about Caligula. The user controls Caligula and has a mission to destroy anything in sight. Each letter on the keyboard is a different means of killing the various enemies he encounters.
In literature and theatre.
"Caligula", by French author Albert Camus, is a play in which Caligula returns after deserting the palace for three days and three nights following the death of his beloved sister, Drusilla. The young emperor then uses his unfettered power to "bring the impossible into the realm of the likely".
In the 1934 novel "I, Claudius" by English writer Robert Graves, Caligula is presented as being a murderous sociopath from his childhood, who became clinically insane early in his reign. At the age of only seven, he drove his father Germanicus to despair and death by secretly terrorising him. Graves's Caligula commits incest with all three of his sisters and is implied to have murdered Drusilla.
In the BBC series based on Graves' novel (where the role is played by John Hurt), Caligula, although unhinged since early childhood, becomes dangerously psychotic after an apparent epileptic seizure and awakens believing that he has metamorphosed into the god Zeus. He kills Drusilla while trying to reenact the birth of Athena by cutting his child from her womb.
In 1941, Edgar Rice Burroughs wrote I Am a Barbarian. The story is pitched as a free translation of the memoirs of Britannicus (a fictional character created by Burroughs) who was the slave of Caligula from early childhood till Caligula's death.
The character Ellsworth Toohey in Ayn Rand's 1943 novel The Fountainhead references Caligula in his climactic speech to Peter Keating stating, "Remember the Roman Emperor who said he wished humanity had a single neck so he could cut it? People have laughed at him for centuries. But we'll have the last laugh. We've accomplished what he couldn't accomplish. We've taught men to unite. This makes one neck ready for one leash."
The play "The Reckoning of Kit and Little Boots", by Nat Cassidy, examines the lives of the Elizabethan playwright Christopher Marlowe and Caligula, with the fictional conceit that Marlowe was working on a play about Caligula around the time of his own murder. It emphasizes the similarities between the two characters—both stabbed to death at 29, both in part as a result of their controversial religious perspectives. The play focuses on Caligula's love for his sister Drusilla and his deep-rooted loathing for Tiberius. It received its world premiere in New York City in June 2008.
In music.
Canadian death metal band Ex Deo released an album called "Caligula", styled as "Caligvla". The band's video, "I Caligula", features Caligula and other members of his court that were important in his rule.
The Smiths song "Heaven Knows I'm Miserable Now" refers to Caligula: "What she asked of me at the end of the day/Caligula would have blushed".
The Dickies' 1989 album "Second Coming" includes the song "Caligula," which relates his origins and reign of terror.
Post-hardcore band Glassjaw's song "Convectuoso" refers to Caligula in the opening lines: "I am Caligula, glutton of gluttons, man over woman, man of all women, the whore of all man."
In television.
Caligula has been played by Ralph Bates in the 1968 ITV television series "The Caesars"; John Hurt in the 1976 BBC television series "I, Claudius"; John McEnery in the 1985 miniseries "A.D."; Tony Hawks in the "Red Dwarf" episode "Meltdown" (1991); Simon Farnaby in "Horrible Histories"; Justin Timberlake in 2013 on Saturday Night Live and John Simm in the 2004 miniseries "Imperium Nerone".

</doc>
<doc id="6854" url="http://en.wikipedia.org/wiki?curid=6854" title="Church–Turing thesis">
Church–Turing thesis

In computability theory, the Church–Turing thesis (also known as the Turing–Church thesis, the Church–Turing conjecture, Church's thesis, Church's conjecture, and Turing's thesis) is a hypothesis ("thesis") about the nature of computable functions. In simple terms, the Church–Turing thesis states that a function on the natural numbers is computable in an informal sense (i.e., computable by a human being using a pencil-and-paper method, ignoring resource limitations) if and only if it is computable by a Turing machine. The thesis is named after American mathematician Alonzo Church and his Ph.D. student, the British mathematician Alan Turing.
Before the precise definition of computable function, mathematicians often used the informal term effectively calculable to describe functions that are computable by paper-and-pencil methods. In the 1930s, several independent attempts were made to formalize the notion of computability:
Church and Turing proved that these three formally defined classes of computable functions coincide: a function is λ-computable if and only if it is Turing computable if and only if it is "general recursive". This has led mathematicians and computer scientists to believe that the concept of computability is accurately characterized by these three equivalent processes.
On the other hand, the Church–Turing thesis states that the above three formally defined classes of computable functions coincide with the "informal" notion of an effectively calculable function. Since, as an informal notion, the concept of effective calculability does not have a formal definition, the thesis, although it has near-universal acceptance, cannot be formally proven.
Statement in Church's and Turing's words.
J. B. Rosser (1939) addresses the notion of "effective computability" as follows: "Clearly the existence of CC and RC (Church's and Rosser's proofs) presupposes a precise definition of 'effective'. 'Effective method' is here used in the rather special sense of a method each step of which is precisely predetermined and which is certain to produce the answer in a finite number of steps". Thus the adverb-adjective "effective" is used in a sense of "1a: producing a decided, decisive, or desired effect", and "capable of producing a result".
In the following, the words "effectively calculable" will mean "produced by any intuitively 'effective' means whatsoever" and "effectively computable" will mean "produced by a Turing-machine or equivalent mechanical device". Turing's "definitions" given in a footnote in his 1939 Ph.D. thesis "Systems of Logic Based on Ordinals", supervised by Church, are virtually the same:
The thesis can be stated as follows:
Turing stated it this way:
History.
One of the important problems for logicians in the 1930s was David Hilbert's Entscheidungsproblem, which asked whether there was a mechanical procedure for separating mathematical truths from mathematical falsehoods. This quest required that the notion of "algorithm" or "effective calculability" be pinned down, at least well enough for the quest to begin. But from the very outset Alonzo Church's attempts began with a debate that continues to this day. Was the notion of "effective calculability" to be (i) an "axiom or axioms" in an axiomatic system, or (ii) merely a "definition" that "identified" two or more propositions, or (iii) an "empirical hypothesis" to be verified by observation of natural events, or (iv) or just "a proposal" for the sake of argument (i.e. a "thesis").
Circa 1930–1952.
In the course of studying the problem, Church and his student Stephen Kleene introduced the notion of λ-definable functions, and they were able to prove that several large classes of functions frequently encountered in number theory were λ-definable. The debate began when Church proposed to Gödel that one should define the "effectively computable" functions as the λ-definable functions. Gödel, however, was not convinced and called the proposal "thoroughly unsatisfactory". Rather, in correspondence with Church (ca 1934–5), Gödel proposed "axiomatizing" the notion of "effective calculability"; indeed, in a 1935 letter to Kleene, Church reported that:
But Gödel offered no further guidance. Eventually, he would suggest his (primitive) recursion, modified by Herbrand's suggestion, that Gödel had detailed in his 1934 lectures in Princeton NJ (Kleene and Rosser transcribed the notes). But "he did not think that the two ideas could be satisfactorily identified "except heuristically".
Next, it was necessary to identify and prove the equivalence of two notions of effective calculability. Equipped with the λ-calculus and "general" recursion, Stephen Kleene with help of Church and J. B. Rosser produced proofs (1933, 1935) to show that the two calculi are equivalent. Church subsequently modified his methods to include use of Herbrand–Gödel recursion and then proved (1936) that the Entscheidungsproblem is unsolvable: There is no generalized "effective calculation" (method, algorithm) that can determine whether or not a formula in either the recursive- or λ-calculus is "valid" (more precisely: no method to show that a well formed formula has a "normal form").
Many years later in a letter to Davis (ca 1965), Gödel would confess that "he was, at the time of these [1934] lectures, not at all convinced that his concept of recursion comprised all possible recursions". By 1963–4 Gödel would disavow Herbrand–Gödel recursion and the λ-calculus in favor of the Turing machine as the definition of "algorithm" or "mechanical procedure" or "formal system".
A hypothesis leading to a natural law?: In late 1936 Alan Turing's paper (also proving that the Entscheidungsproblem is unsolvable) was delivered orally, but had not yet appeared in print. On the other hand, Emil Post's 1936 paper had appeared and was certified independent of Turing's work. Post strongly disagreed with Church's "identification" of effective computability with the λ-calculus and recursion, stating:
Rather, he regarded the notion of "effective calculability" as merely a "working hypothesis" that might lead by inductive reasoning to a "natural law" rather than by "a definition or an axiom". This idea was "sharply" criticized by Church.
Thus Post in his 1936 paper was also discounting Kurt Gödel's suggestion to Church in 1934–5 that the thesis might be expressed as an axiom or set of axioms.
Turing adds another definition, Rosser equates all three: Within just a short time, Turing's 1936–37 paper "On Computable Numbers, with an Application to the Entscheidungsproblem" appeared. In it he stated another notion of "effective computability" with the introduction of his a-machines (now known as the Turing machine abstract computational model). And in a proof-sketch added as an "Appendix" to his 1936–37 paper, Turing showed that the classes of functions defined by λ-calculus and Turing machines coincided. Church was quick to recognise how compelling Turing's analysis was. In his review of Turing's paper he made clear that Turing's notion made "the identification with effectiveness in the ordinary (not explicitly defined) sense evident immediately".
In a few years (1939) Turing would propose, like Church and Kleene before him, that "his" formal definition of mechanical computing agent was the correct one. Thus, by 1939, both Church (1934) and Turing (1939) had individually proposed that their "formal systems" should be "definitions" of "effective calculability"; neither framed their statements as "theses".
Rosser (1939) formally identified the three notions-as-definitions:
Kleene proposes "Church's Thesis": This left the overt expression of a "thesis" to Kleene. In his 1943 paper "Recursive Predicates and Quantifiers" Kleene proposed his "THESIS I":
Kleene goes on to note that:
Kleene's Church–Turing Thesis: A few years later (1952) Kleene would overtly name, defend, and express the two "theses" and then "identify" them (show equivalence) by use of his Theorem XXX:
Later developments.
An attempt to understand the notion of "effective computability" better led Robin Gandy (Turing's student and friend) in 1980 to analyze "machine" computation (as opposed to human-computation acted out by a Turing machine). Gandy's curiosity about, and analysis of, "cellular automata", "Conway's game of life", "parallelism" and "crystalline automata" led him to propose four "principles (or constraints) ... which it is argued, any machine must satisfy." His most-important fourth, "the principle of causality" is based on the "finite velocity of propagation of effects and signals; contemporary physics rejects the possibility of instantaneous action at a distance." From these principles and some additional constraints—(1a) a lower bound on the linear dimensions of any of the parts, (1b) an upper bound on speed of propagation (the velocity of light), (2) discrete progress of the machine, and (3) deterministic behavior—he produces a theorem that "What can be calculated by a device satisfying principles I–IV is computable. ".
In the late 1990s Wilfried Sieg analyzed Turing's and Gandy's notions of "effective calculability" with the intent of "sharpening the informal notion, formulating its general features axiomatically, and investigating the axiomatic framework". In his 1997 and 2002 Sieg presents a series of constraints on the behavior of a "computor"—"a human computing agent who proceeds mechanically"; these constraints reduce to:
The matter remains in active discussion within the academic community.
The thesis as a definition.
The thesis can be viewed as nothing but an ordinary mathematical definition. Comments by Gödel on the subject suggest this view, e.g. "the correct definition of mechanical computability was established beyond any doubt by Turing". The case for viewing the thesis as nothing more than a definition is made explicitly by Robert I. Soare in where it is also argued that Turing's definition of computability is no less likely to be correct than the epsilon-delta definition of a continuous function.
Success of the thesis.
Other formalisms (besides recursion, the λ-calculus, and the Turing machine) have been proposed for describing effective calculability/computability. Stephen Kleene (1952) adds to the list the functions ""reckonable" in the system S1" of Kurt Gödel 1936, and Emil Post's (1943, 1946) ""canonical" [also called "normal"] "systems"". In the 1950s Hao Wang and Martin Davis greatly simplified the one-tape Turing-machine model (see Post–Turing machine). Marvin Minsky expanded the model to two or more tapes and greatly simplified the tapes into "up-down counters", which Melzak and Lambek further evolved into what is now known as the counter machine model. In the late 1960s and early 1970s researchers expanded the counter machine model into the register machine, a close cousin to the modern notion of the computer. Other models include combinatory logic and Markov algorithms. Gurevich adds the pointer machine model of Kolmogorov and Uspensky (1953, 1958): "... they just wanted to ... convince themselves that there is no way to extend the notion of computable function."
All these contributions involve proofs that the models are computationally equivalent to the Turing machine; such models are said to be Turing complete. Because all these different attempts at formalizing the concept of "effective calculability/computability" have yielded equivalent results, it is now generally assumed that the Church–Turing thesis is correct. In fact, Gödel (1936) proposed something stronger than this; he observed that there was something "absolute" about the concept of "reckonable in S1":
Informal usage in proofs.
Proofs in computability theory often invoke the Church–Turing thesis in an informal way to establish the computability of functions while avoiding the (often very long) details which would be involved in a rigorous, formal proof. To establish that a function is computable by Turing machine, it is usually considered sufficient to give an informal English description of how the function can be effectively computed, and then conclude "By the Church–Turing thesis" that the function is Turing computable (equivalently partial recursive).
Dirk van Dalen (in Gabbay 2001:284) gives the following example for the sake of illustrating this informal use of the Church–Turing thesis:
(Emphasis added). In order to make the above example completely rigorous, one would have to carefully construct a Turing Machine, or λ-function, or carefully invoke recursion axioms, or at best, cleverly invoke various theorems of computability theory. But because the computability theorist believes that Turing computability correctly captures what can be computed effectively, and because an effective procedure is spelled out in English for deciding the set B, the computability theorist accepts this as proof that the set is indeed recursive.
As a rule of thumb, the Church–Turing thesis should only be invoked to simplify proofs in cases where the writer would be capable of, and expects the readers also to be capable of, easily (but not necessarily without tedium) producing a rigorous proof if one were demanded.
Variations.
The success of the Church–Turing thesis prompted variations of the thesis to be proposed. For example, the Physical Church–Turing thesis (PCTT) states:
The Church–Turing thesis says nothing about the efficiency with which one model of computation can simulate another. It has been proved for instance that a (multi-tape) universal Turing machine only suffers a logarithmic slowdown factor in simulating any Turing machine. No such result has been proved in general for an arbitrary but "reasonable" model of computation. A variation of the Church–Turing thesis that addresses this issue is the Feasibility Thesis or (Classical) Complexity-Theoretic Church–Turing Thesis (SCTT), which is not due to Church or Turing, but rather was realized gradually in the development of complexity theory. It states:
The word 'efficiently' here means up to polynomial-time reductions. This thesis was originally called "Computational Complexity-Theoretic Church–Turing Thesis" by Ethan Bernstein and Umesh Vazirani (1997). The Complexity-Theoretic Church–Turing Thesis, then, posits that all 'reasonable' models of computation yield the same class of problems that can be computed in polynomial time. Assuming the conjecture that probabilistic polynomial time (BPP) equals deterministic polynomial time (P), the word 'probabilistic' is optional in the Complexity-Theoretic Church–Turing Thesis. A similar thesis, called the "Invariance Thesis", was introduced by Cees F. Slot and Peter van Emde Boas. It states: ""Reasonable" machines can simulate each other within a polynomially bounded overhead in time and a constant-factor overhead in space". The thesis originally appeared in a paper at STOC'84, which was the first paper to show that polynomial-time overhead and constant-space overhead could be "simultaneously" achieved for a simulation of a Random Access Machine on a Turing machine.
If BQP is shown to be a strict superset of BPP, it would invalidate the Complexity-Theoretic Church–Turing Thesis. In other words, there would be efficient quantum algorithms that perform tasks that do not have efficient probabilistic algorithms. This would not however invalidate the original Church–Turing thesis, since a quantum computer can always be simulated by a Turing machine, but it would invalidate the classical Complexity-Theoretic Church–Turing thesis for efficiency reasons. Consequently, the Quantum Complexity-Theoretic Church–Turing thesis states:
Eugene Eberbach and Peter Wegner claim that the Church–Turing thesis is sometimes interpreted too broadly,
stating "the broader assertion that algorithms precisely capture
what can be computed is invalid". They claim that forms of computation not captured by the thesis are relevant today,
terms which they call super-Turing computation.
Philosophical implications.
Philosophers have interpreted the Church–Turing thesis as having implications for the philosophy of mind; however, many of the philosophical interpretations of the Thesis involve basic misunderstandings of the thesis statement. B. Jack Copeland states that it's an open empirical question whether there are actual deterministic physical processes that, in the long run, elude simulation by a Turing machine; furthermore, he states that it is an open empirical question whether any such processes are involved in the working of the human brain. There are also some important open questions which cover the relationship between the Church–Turing thesis and physics, and the possibility of hypercomputation. When applied to physics, the thesis has several possible meanings:
There are many other technical possibilities which fall outside or between these three categories, but these serve to illustrate the range of the concept.
Non-computable functions.
One can formally define functions that are not computable. A well-known example of such a function is the Busy Beaver function. This function takes an input "n" and returns the largest number of symbols that a Turing machine with "n" states can print before halting, when run with no input. Finding an upper bound on the busy beaver function is equivalent to solving the halting problem, a problem known to be unsolvable by Turing machines. Since the busy beaver function cannot be computed by Turing machines, the Church–Turing thesis states that this function cannot be effectively computed by any method.
Several computational models allow for the computation of (Church-Turing) non-computable functions. These are known as
hypercomputers.
Mark Burgin argues that super-recursive algorithms such as inductive Turing machines disprove the Church–Turing thesis. His argument relies on a definition of algorithm broader than the ordinary one, so that non-computable functions obtained from some inductive Turing machines are called computable. This interpretation of the Church–Turing thesis differs from the interpretation commonly accepted in computability theory, discussed above. The argument that super-recursive algorithms are indeed algorithms in the sense of the Church–Turing thesis has not found broad acceptance within the computability research community.

</doc>
<doc id="6856" url="http://en.wikipedia.org/wiki?curid=6856" title="Chomsky (surname)">
Chomsky (surname)

Chomsky (Belarusian: Хомскі, Russian: Хомский, Ukrainian: Хомський, Hebrew: חומסקי‎, "from (Vyoska) Khomsk (nearby Brest, now Belarus)") is a Belarus'-origin surname. Notable people with the surname include:
Elsie, William, Avram Noam, Carol, Marvin, and Aviva are all closely related. William and Elsie were husband and wife. Avram Noam, generally referred by his given name Noam, is their son. Carol and Noam were married until Carol's death in 2008; Aviva is their daughter. Marvin is Noam's cousin. Also, Judith is Noam's sister in-law.

</doc>
<doc id="6857" url="http://en.wikipedia.org/wiki?curid=6857" title="Computer multitasking">
Computer multitasking

In computing, multitasking is a concept of performing multiple tasks (also known as processes) over a certain period of time by executing them concurrently. New tasks start and interrupt already started ones before they have reached completion, instead of executing the tasks sequentially so each started task needs to reach its end before a new one is started. As a result, a computer executes segments of multiple tasks in an interleaved manner, while the tasks share common processing resources such as central processing units (CPUs) and main memory.
Multitasking does "not" necessarily mean that multiple tasks are executing at exactly the same time. In other words, multitasking does not imply parallel execution, but it does mean that more than one task can be part-way through execution at the same time, and that more than one task is advancing over a given period of time. Even on multiprocessor or multicore computers, which have multiple CPUs/cores so more than one task can be executed at once (physically, one per CPU or core), multitasking allows many more tasks to be run than there are CPUs.
In the case of a computer with a single CPU, only one task is said to be running at any point in time, meaning that the CPU is actively executing instructions for that task. Multitasking solves the problem by scheduling which task may be the one running at any given time, and when another waiting task gets a turn. The act of reassigning a CPU from one task to another one is called a context switch; the illusion of parallelism is achieved when context switches occur frequently enough. Operating systems may adopt one of many different scheduling strategies, which generally fall into the following categories:
The term "multitasking" has become an international term, as the same word is used in many other languages such as German, Italian, Dutch, Danish and Norwegian.
Multiprogramming.
In the early days of computing, CPU time was expensive, and peripherals were very slow. When the computer ran a program that needed access to a peripheral, the central processing unit (CPU) would have to stop executing program instructions while the peripheral processed the data. This was deemed very inefficient.
The first computer using a multiprogramming system was the British "Leo III" owned by J. Lyons and Co. Several different programs in batch were loaded in the computer memory, and the first one began to run. When the first program reached an instruction waiting for a peripheral, the context of this program was stored away, and the second program in memory was given a chance to run. The process continued until all programs finished running.
The use of multiprogramming was enhanced by the arrival of virtual memory and virtual machine technology, which enabled individual programs to make use of memory and operating system resources as if other concurrently running programs were, for all practical purposes, non-existent and invisible to them.
Multiprogramming doesn't give any guarantee that a program will run in a timely manner. Indeed, the very first program may very well run for hours without needing access to a peripheral. As there were no users waiting at an interactive terminal, this was no problem: users handed in a deck of punched cards to an operator, and came back a few hours later for printed results. Multiprogramming greatly reduced wait times when multiple batches were being processed.
Cooperative multitasking.
The expression "time sharing" usually designated computers shared by interactive users at terminals, such as IBM's TSO, and VM/CMS. The term "time-sharing" is no longer commonly used, having been replaced by "multitasking", following the advent of personal computers and workstations rather than shared interactive systems.
Early multitasking systems used applications that voluntarily ceded time to one another. This approach, which was eventually supported by many computer operating systems, is known today as cooperative multitasking. Although it is now rarely used in larger systems except for specific applications such as CICS or the JES2 subsystem, cooperative multitasking was once the scheduling scheme employed by Microsoft Windows (prior to Windows 95 and Windows NT) and Mac OS (prior to OS X) in order to enable multiple applications to be run simultaneously. Windows 9x also used cooperative multitasking, but only for 16-bit legacy applications, much the same way as pre-Leopard PowerPC versions of Mac OS X used it for Classic applications. The network operating system NetWare used cooperative multitasking up to NetWare 6.5. Cooperative multitasking is still used today on RISC OS systems.
As a cooperatively multitasked system relies on each process regularly giving up time to other processes on the system, one poorly designed program can consume all of the CPU time for itself, either by performing extensive calculations or by busy waiting; both would cause the whole system to hang. In a server environment, this is a hazard that makes the entire environment unacceptably fragile.
Preemptive multitasking.
Preemptive multitasking allows the computer system to guarantee more reliably each process a regular "slice" of operating time. It also allows the system to deal rapidly with important external events like incoming data, which might require the immediate attention of one or another process. Operating systems were developed to take advantage of these hardware capabilities and run multiple processes preemptively. Preemptive multitasking was supported on DEC's PDP-8 computers, and implemented in OS/360 MFT in 1967, in MULTICS (1964), and Unix (1969); it is a core feature of all Unix-like operating systems, such as Linux, Solaris and BSD with its derivatives.
At any specific time, processes can be grouped into two categories: those that are waiting for input or output (called "I/O bound"), and those that are fully utilizing the CPU ("CPU bound"). In primitive systems, the software would often "poll", or "busywait" while waiting for requested input (such as disk, keyboard or network input). During this time, the system was not performing useful work. With the advent of interrupts and preemptive multitasking, I/O bound processes could be "blocked", or put on hold, pending the arrival of the necessary data, allowing other processes to utilize the CPU. As the arrival of the requested data would generate an interrupt, blocked processes could be guaranteed a timely return to execution.
The earliest preemptive multitasking OS available to home users was Sinclair QDOS on the Sinclair QL, released in 1984, but very few people bought the machine. Commodore's powerful Amiga, released the following year, was the first commercially successful home computer to use the technology, and its multimedia abilities make it a clear ancestor of contemporary multitasking personal computers. Microsoft made preemptive multitasking a core feature of their flagship operating system in the early 1990s when developing Windows NT 3.1 and then Windows 95. It was later adopted on the Apple Macintosh by Mac OS X that, as a Unix-like operating system, uses preemptive multitasking for all native applications.
A similar model is used in Windows 9x and the Windows NT family, where native 32-bit applications are multitasked preemptively, and legacy 16-bit Windows 3.x programs are multitasked cooperatively within a single process, although in the NT family it is possible to force a 16-bit application to run as a separate preemptively multitasked process. 64-bit editions of Windows, both for the x86-64 and Itanium architectures, no longer provide support for legacy 16-bit applications, and thus provide preemptive multitasking for all supported applications.
Real time.
Another reason for multitasking was in the design of real-time computing systems, where there are a number of possibly unrelated external activities needed to be controlled by a single processor system. In such systems a hierarchical interrupt system is coupled with process prioritization to ensure that key activities were given a greater share of available process time.
Multithreading.
As multitasking greatly improved the throughput of computers, programmers started to implement applications as sets of cooperating processes (e. g., one process gathering input data, one process processing input data, one process writing out results on disk). This, however, required some tools to allow processes to efficiently exchange data.
Threads were born from the idea that the most efficient way for cooperating processes to exchange data would be to share their entire memory space. Thus, threads are effectively processes that run in the same memory context and share other resources with their parent processes, such as open files. Threads are described as "lightweight processes" because switching between threads does not involve changing the memory context.
While threads are scheduled preemptively, some operating systems provide a variant to threads, named "fibers", that are scheduled cooperatively. On operating systems that do not provide fibers, an application may implement its own fibers using repeated calls to worker functions. Fibers are even more lightweight than threads, and somewhat easier to program with, although they tend to lose some or all of the benefits of threads on machines with multiple processors.
Some systems directly support multithreading in hardware.
Memory protection.
Essential to any multitasking system is to safely and effectively share access to system resources. Access to memory must be strictly managed to ensure that no process can inadvertently or deliberately read or write to memory locations outside of the process's address space. This is done for the purpose of general system stability and data integrity, as well as data security.
In general, memory access management is the operating system kernel's responsibility, in combination with hardware mechanisms (such as the memory management unit (MMU)) that provide supporting functionalities. If a process attempts to access a memory location outside of its memory space, the MMU denies the request and signals the kernel to take appropriate actions; this usually results in forcibly terminating the offending process. Depending on the software and kernel design and the specific error in question, the user may receive an access violation error message such as "segmentation fault".
In a well designed and correctly implemented multitasking system, a given process can never directly access memory that belongs to another process. An exception to this rule is in the case of shared memory; for example, in the System V inter-process communication mechanism the kernel allocates memory to be mutually shared by multiple processes. Such features are often used by database management software such as PostgreSQL.
Inadequate memory protection mechanisms, either due to flaws in their design or poor implementations, allow for security vulnerabilities that may be potentially exploited by malicious software.
Memory swapping.
Use of a swap file or swap partition is a way for the operating system to provide more memory than is physically available by keeping portions of the primary memory in secondary storage. While multitasking and memory swapping are two completely unrelated techniques, they are very often used together, as swapping memory allows more tasks to be loaded at the same time. Typically, a multitasking system allows another process to run when the running process hits a point where it has to wait for some portion of memory to be reloaded from secondary storage.
Programming.
Processes that are entirely independent are not much trouble to program in a multitasking environment. Most of the complexity in multitasking systems comes from the need to share computer resources between tasks and to synchronize the operation of co-operating tasks.
Various concurrent computing techniques are used to avoid potential problems caused by multiple tasks attempting to access the same resource.
Bigger systems were sometimes built with a central processor(s) and some number of I/O processors, a kind of asymmetric multiprocessing.
Over the years, multitasking systems have been refined. Modern operating systems generally include detailed mechanisms for prioritizing processes, while symmetric multiprocessing has introduced new complexities and capabilities.

</doc>
<doc id="6859" url="http://en.wikipedia.org/wiki?curid=6859" title="Chiang Kai-shek">
Chiang Kai-shek

Chiang Kai-shek (October 31, 1887 – April 5, 1975) was a Chinese political and military leader who served as the leader of the Republic of China between 1928 and 1975. He is known as Chiang Chung-cheng (蔣中正) or Chiang Chieh-shih (蔣介石) in Standard Chinese. Chiang was an influential member of the Kuomintang (KMT), the Chinese Nationalist Party, and was a close ally of Sun Yat-sen. He became the Commandant of the Kuomintang's Whampoa Military Academy and took Sun's place as leader of the KMT when Sun died in 1925. In 1926, Chiang led the Northern Expedition to unify the country, becoming China's nominal leader. He served as Chairman of the National Military Council of the Nationalist government of the Republic of China (ROC) from 1928 to 1948. Chiang led China in the Second Sino-Japanese War (the Chinese theater of World War II), consolidating power from the party's former regional warlords. Unlike Sun Yat-sen, Chiang Kai-shek was socially conservative, promoting traditional Chinese culture in the New Life Movement and rejecting western democracy and the nationalist democratic socialism that Sun embraced in favour of an authoritarian government.
Chiang's predecessor, Sun Yat-sen, was well-liked and respected by the Communists, but after Sun's death Chiang was not able to maintain good relations with the Chinese Communist Party (CCP). A major split between the Nationalists and Communists occurred in 1927; and, under Chiang's leadership, the Nationalists fought a nationwide civil war against the Communists. After Japan invaded China in 1937, Chiang agreed to a temporary truce with the CCP. Despite some early cooperative military successes against Japan, by the time that the Japanese surrendered in 1945 neither the CCP nor the KMT trusted each other nor were actively cooperating.
After American-sponsored attempts to negotiate a coalition government failed in 1946, the Chinese Civil War resumed. The CCP defeated the Nationalists in 1949. Westad says the Communists won the Civil War because they made fewer military mistakes than Chiang Kai-Shek, and because in his search for a powerful centralized government, Chiang antagonized too many interest groups in China. Furthermore, his party was weakened in the war against Japan. Meanwhile the Communists told different groups, such as peasants, exactly what they wanted to hear, and cloaked themselves in the cover of Chinese Nationalism.
Chiang's government and army retreated to Taiwan, where Chiang imposed martial law and persecuted people critical of his rule in a period known as the "White Terror". After evacuating to Taiwan, Chiang's government continued to declare its intention to retake mainland China. Chiang ruled the island securely as President of the Republic of China and General of the Kuomintang until his . He ruled mainland China for 22 years, and Taiwan for 30 years.
Early life.
Childhood.
Chiang was born in Xikou, a town approximately 30 km southwest of downtown Ningbo, in Fenghua, Zhejiang. However, his ancestral home, a concept important in Chinese society, was the town of Heqiao (和橋鎮) in Yixing, Jiangsu, approximately 38 km southwest of downtown Wuxi, and 10 km from the shores of Lake Tai. Chiang's father, Jiang Zhaocong (蔣肇聰), and mother, Wang Caiyu (王采玉), were members of an upper-middle to upper-class family of salt merchants. Chiang's father died when he was only eight years of age, and he wrote of his mother as the "embodiment of Confucian virtues".
In Japan: 1905–1911.
Chiang grew up in a time period in which military defeats and civil wars among warlords had left China destabilized and in debt. Several demands of Western Powers and Japan since the Opium Wars had left China owing millions of Taels of silver. He decided to pursue a military career. He began his military education at the Baoding Military Academy, in 1906 coincidentally the same year that Japan devalued its paper currency (promissory notes), de-pairing it with gold and silver. He then left for the "Tokyo Shinbu Gakko" (東京振武學校), an Imperial Japanese Army Academy Preparatory School for Chinese students, in 1907. There he was influenced by his compatriots to support the revolutionary movement to overthrow the Qing Dynasty and to set up a Chinese republic. He befriended fellow Zhejiang native Chen Qimei, and, in 1908, Chen brought Chiang into the Tongmenghui, a precursor of the Kuomintang (KMT) organization. Chiang served in the Imperial Japanese Army from 1909 to 1911.
Return to China.
Chiang returned to China in 1911 after learning of the outbreak of the Wuchang Uprising, intending to fight as an artillery officer. He served in the revolutionary forces, where he led a regiment in Shanghai under his friend and mentor Chen Qimei, as one of Chen's chief lieutenants. In early 1912, a dispute arose between Chen and Tao Chen-Chang, who was an influential member of the Revolutionary Alliance and who opposed both Sun Yat-sen and Chen. Tao sought to avoid escalating the quarrel by hiding in a hospital but was discovered there by Chiang. Chen dispatched assassins. Chiang may not have taken part in the act but would later assume responsibility to help Chen avoid trouble. Chen valued Chiang despite Chiang's by now legendary quick temper, believing that such bellicosity was useful in a military leader. Alternatively, Professor Pichon Loh reports that Chiang may have killed Tao in the hospital with a pistol.
Chiang's friendship with Chen Qimei signaled an association with Shanghai's criminal syndicate (the Green Gang headed by Du Yuesheng and Huang Jinrong). During Chiang's time in Shanghai, he was watched by British-administered Shanghai International Settlement police, who charged him with various felonies. These charges never resulted in a trial and Chiang was never jailed.
Chiang became a founding member of the KMT after the success of the 1911 Revolution. After the takeover of the Republican government by Yuan Shikai and the failed Second Revolution in 1913, Chiang, like his KMT comrades, divided his time between exile in Japan and the havens of the Shanghai International Settlement. In Shanghai, Chiang cultivated ties with the city's underworld gangs, which were dominated by the notorious Green Gang and its leader Du Yuesheng. On May 18, 1916, agents of Yuan Shikai assassinated Chen Qimei. Chiang then succeeded Chen as leader of the Chinese Revolutionary Party in Shanghai. Sun Yat-sen's political career was at its lowest point during this time when most of his old Revolutionary Alliance comrades refused to join him in the exiled Chinese Revolutionary Party.
Establishment of the Kuomintang in Guangzhou.
In 1917, Sun Yat-sen moved his base of operations to Guangzhou, and Chiang joined him in 1918. At this time Sun remained largely sidelined; and, without arms or money, was soon expelled from Guangzhou and exiled again to Shanghai. He was restored to Guangzhou with mercenary help in 1920. After returning to Guangzhou, a rift developed between Sun, who sought to militarily unify China under the KMT, and Guangdong Governor Chen Jiongming, who wanted to implement a federalist system with Guangdong as a model province. On June 16, 1923, Chen attempted to assassinate Sun and had his residence shelled. During a prolonged skirmish between the troops of these opposing forces, Sun and his wife Soong Ching-ling narrowly evaded heavy machine gun fire and were rescued by gunboats under Chiang's direction. The incident earned Chiang the trust of Sun Yat-sen.
Sun regained essential control of Guangzhou in early 1924, again with the help of mercenaries from Yunnan, and accepted aid from the Comintern. Undertaking a reform of the KMT, he established a revolutionary government aimed at unifying China under the KMT. That same year, Sun sent Chiang to spend three months in Moscow studying the Soviet political and military system. During his trip in Russia, Chiang met Leon Trotsky and other Soviet leaders, but quickly came to the conclusion that the Russian model of government was not suitable for China. Chiang later sent his eldest son, Ching-kuo, to study in Russia. After his father's split from the First United Front in 1927, Ching-kuo was forced to stay there, as a hostage, until 1937. Chiang wrote in his diary, "It is not worth it to sacrifice the interest of the country for the sake of my son." Chiang even refused to negotiate a prisoner swap for his son in exchange for the Chinese Communist Party leader. His attitude remained consistent, and he continued to maintain, by 1937, that "I would rather have no offspring than sacrifice our nation's interests." Chiang had absolutely no intention of ceasing the war against the Communists.
Chiang Kai-shek returned to Guangzhou and in 1924 was appointed Commandant of the Whampoa Military Academy by Sun. Chiang resigned from the office for one month in disagreement with Sun's extremely close cooperation with the Comintern, but returned at Sun's demand. The early years at Whampoa allowed Chiang to cultivate a cadre of young officers loyal to both the KMT and himself.
Throughout his rise to power, Chiang also benefited from membership within the nationalist Tiandihui fraternity, to which Sun Yat-sen also belonged, and which remained a source of support during his leadership of the Kuomintang.
Succession of Sun Yat-sen.
Competition with Wang Jingwei.
Sun Yat-sen died on March 12, 1925, creating a power vacuum in the Kuomintang. A contest ensued between Chiang, who stood at the right wing of the KMT, and Sun Yat-sen's close comrade-in-arms Wang Jingwei, who leaned towards the left. Although Wang succeeded Sun as Chairman of the National Government, Chiang's relatively low position in the party's internal hierarchy was bolstered by his military backing and adept political maneuvering following the Zhongshan Warship Incident. On June 5, 1926, Chiang became Commander-in-Chief of the National Revolutionary Army (NRA), and on July 27 he launched a military campaign known as the Northern Expedition in order to defeat the warlords controlling northern China and to unify the country under the KMT.
The NRA branched into three divisions: to the west was Wang Jingwei, who led a column to take Wuhan; Bai Chongxi's column went east to take Shanghai; Chiang himself led in the middle route, planning to take Nanjing before pressing ahead to capture Beijing. However, in January 1927, Wang Jingwei and his KMT leftist allies took the city of Wuhan amid much popular mobilization and fanfare. Allied with a number of Chinese Communists and advised by Soviet agent Mikhail Borodin, Wang declared the National Government as having moved to Wuhan. Having taken Nanjing in March (and briefly visited Shanghai, now under the control of his close ally Bai Chongxi), Chiang halted his campaign and prepared a violent break with Wang's leftist elements, which he believed threatened his control of the KMT.
Now with an established national government in Nanjing, and supported by conservative allies including Hu Hanmin, Chiang's expulsion of the Communists and their Soviet advisers led to the beginning of the Chinese Civil War. Wang Jingwei's National Government was weak militarily, and was soon ended by Chiang with the support of a local warlord (Li Zongren of Guangxi). Eventually, Wang and his leftist party surrendered to Chiang and joined him in Nanjing. In the Central Plains War, Beijing was taken on June, 1928, from an alliance of the warlords Feng Yuxiang and Yan Xishan. In December, the Manchurian warlord Zhang Xueliang pledged allegiance to Chiang's government, completing Chiang's nominal unification of China and ending the Warlord Era.
In 1927, when he was setting up the Nationalist government in Nanjing, he was preoccupied with "the elevation of our leader Dr. Sun Yat-sen to the rank of 'Father of our Chinese Republic'. Dr. Sun worked for 40 years to lead our people in the Nationalist cause, and we cannot allow any other personality to usurp this honored position". He asked Chen Guofu to purchase a photograph that had been taken in Japan around 1895 or 1898. It showed members of the Revive China Society with Yeung Kui-wan (楊衢雲 or 杨衢云, pinyin Yáng Qúyún) as President, in the place of honour, and Sun, as secretary, on the back row, along with members of the Japanese Chapter of the Revive China Society. When told that it was not for sale, Chiang offered a million dollars to recover the photo and its negative. "The party must have this picture and the negative at any price. They must be destroyed as soon as possible. It would be embarrassing to have our Father of the Chinese Republic shown in a subordinate position". Chiang never obtained either the photo or its negative.
Chiang made great efforts to gain recognition as the official successor of Sun Yat-sen. In a pairing of great political significance, Chiang was Sun's brother-in-law: he had married Soong May-ling, the younger sister of Soong Ching-ling, Sun's widow, on December 1, 1927. Originally rebuffed in the early 1920s, Chiang managed to ingratiate himself to some degree with Soong May-ling's mother by first divorcing his wife and concubines and promising to sincerely study the precepts of Christianity. He read the copy of the Bible that May-ling had given him twice before making up his mind to become a Christian, and three years after his marriage he was baptized in the Soong's Methodist church. Although some observers felt that he adopted Christianity as a political move, studies of his recently opened diaries suggest that his faith was strong and sincere and that he felt that Christianity reinforced Confucian moral teachings.
Upon reaching Beijing, Chiang paid homage to Sun Yat-sen and had his body moved to the new capital of Nanjing to be enshrined in a grand mausoleum.
Rising power.
In the West and in the Soviet Union, Chiang Kai-shek was known as the "Red General". Movie theaters in the Soviet Union showed newsreels and clips of Chiang. At Moscow, Sun Yat-sen University portraits of Chiang were hung on the walls; and, in the Soviet May Day Parades that year, Chiang's portrait was to be carried along with the portraits of Karl Marx, Vladimir Lenin, Joseph Stalin, and other socialist leaders. The United States consulate and other Westerners in Shanghai were concerned about the approach of "Red General" Chiang as his army was seizing control of large areas of the country in the Northern Expedition. The Western powers backed the Zhili Clique, and were concerned about either the Soviet-backed Kuomintang or the Japanese-backed Fengtian Clique seizing control of China. The Japanese were also concerned that Chiang might defeat the Fengtian Clique.
On April 12, Chiang carried out a purge of thousands of suspected Communists and dissidents in Shanghai, and began large-scale massacres across the country collectively known as the "White Terror". Throughout April 1927, more than people were killed in Shanghai. The killings drove most Communists from urban cities and into the rural countryside, where the KMT was less powerful. In the year after April 1927, over 300,000 people died across China in anti-Communist suppression campaigns executed by the KMT. More liberal estimates regarding the White Terror in China estimate it took millions of lives, most of them in the rural areas. Chiang allowed for the "escape" of Soviet agent and advisor Mikhail Borodin and Soviet military officer Vasily Blücher (Galens) to safety after the purge.
Tutelage of China.
Carrying out Sun Yat-sen's will.
Having gained control of China, Chiang's party remained surrounded by "surrendered" warlords who remained relatively autonomous within their own regions. On October 10, 1928, Chiang was named director of the State Council, the equivalent to President of the country, in addition to his other titles. As with his predecessor Sun Yat-sen, the Western media dubbed him "Generalissimo".
According to Sun Yat-sen's plans, the Kuomintang (KMT) was to rebuild China in three steps: military rule, political tutelage, and constitutional rule. The ultimate goal of the KMT revolution was democracy, which was not considered to be feasible in China's fragmented state. Since the KMT had completed the first step of revolution through seizure of power in 1928, Chiang's rule thus began a period of what his party considered to be "political tutelage" in Sun Yat-sen's name. During this so-called Republican Era, many features of a modern, functional Chinese state emerged and developed.
The decade of 1928 to 1937 saw some aspects of foreign imperialism, concessions and privileges in China, moderated through diplomacy. The government acted to modernize the legal and penal systems, attempted to stabilize prices, amortize debts, reform the banking and currency systems, build railroads and highways, improve public health facilities, legislate against traffic in narcotics, and augment industrial and agricultural production. Not all of these projects were successfully completed. Efforts were made towards improving education standards; and, in an effort to unify Chinese society, the New Life Movement was launched to encourage Confucian moral values and personal discipline. "Guoyu" ("National language"), was promoted as a standard tongue, and the establishment of communications facilities (including radio) were used to encourage a sense of Chinese nationalism in a way that was not possible when the nation lacked an effective central government.
Challenges and limitations.
Any successes that the Nationalists did make, however, were met with constant political and military upheavals. While much of the urban areas were now under the control of the KMT, much of the countryside remained under the influence of weakened yet undefeated warlords and Communists. Chiang often resolved issues of warlord obstinacy through military action, but such action was costly in terms of men and material. The 1930 Central Plains War alone nearly bankrupted the Nationalist government and caused almost casualties on both sides. In 1931, Hu Hanmin, Chiang's old supporter, publicly voiced a popular concern that Chiang's position as both premier and president flew in the face of the democratic ideals of the Nationalist government. Chiang had Hu put under house arrest, but he was released after national condemnation after which he left Nanjing and supported a rival government in Guangzhou. The split resulted in a military conflict between Hu's Guangzhou government and Chiang's Nationalist government. Chiang only won the campaign against Hu after a shift in allegiance by the warlord Zhang Xueliang, who had previously supported Hu Hanmin.
Throughout his rule, complete eradication of the Communists remained Chiang's dream. After assembling his forces in Jiangxi, Chiang led his armies against the newly established Chinese Soviet Republic. With help from foreign military advisers, Chiang's Fifth Campaign finally surrounded the Chinese Red Army in 1934. The Communists, tipped off that a Nationalist offensive was imminent, retreated in the Long March, during which Mao Zedong rose from a mere military official to the most influential leader of the Communist Party of China.
Ideology: nationalism, anti-capitalism, and anti-communism.
Chiang, as a nationalist and a Confucianist, was against the iconoclasm of the May Fourth Movement. Motivated by his sense of nationalism, he viewed some Western ideas as foreign, and he believed that the great introduction of Western ideas and literature that the May Fourth Movement promoted was not beneficial to China. He and Dr. Sun criticized the May Fourth intellectuals as corrupting the morals of China's youth.
Contrary to Communist propaganda that Chiang was pro-capitalism, Chiang Kai-shek antagonized the capitalists of Shanghai, often attacking them and confisticating their capital and assets for the use of the government. Chiang confiscated the wealth of capitalists even while he denounced and fought against communists. Chiang crushed pro-communist worker and peasant organizations and rich Shanghai capitalists at the same time. Chiang continued Dr. Sun Yat-sen's anti capitalist ideology, directing Kuomintang media to openly attack capitalists and capitalism, demanding government controlled industry instead.
Chiang has often been interpreted as being pro-capitalist, but this conclusion may be problematic. Shanghai capitalists did briefly support him out of fear of communism in 1927, but this support eroded in 1928 when Chiang turned his tactics of intimidation on them. The relationship between Chiang Kai-shek and Chinese capitalists remained poor throughout the period of his administration. Chiang blocked Chinese capitalists from gaining any political power or voice within his regime. Once Chiang Kai-shek was done with his White Terror on pro-communist laborers, he proceeded to turn on the capitalists. Gangster connections allowed Chiang to attack them in the International Settlement, successfully forcing capitalists to back him up with their assets for his military expeditions.
Views on Imperialism.
Chiang viewed Japan, America, the Soviet Union, France, and Britain as all being imperialists with nobody else's interests in mind but their own, seeing them as hypocritical to condemn each other for imperialism which they all practiced. He manipulated America, Nazi Germany, and the Soviet Union to regain lost territories for China as he viewed all the powers as imperialists trying to curtail and suppress China's power and national resurrection.
Wartime leader of China.
Chinese Civil War: 1927–1937.
In Nanjing, on April 1931, Chiang Kai-shek attended a national leadership conference with Zhang Xueliang and Muslim General Ma Fuxiang, in which Chiang and Zhang dauntlessly upheld that Manchuria was part of China in the face of the Japanese invasion. After the Japanese invasion of Manchuria in 1931, Chiang resigned as Chairman of the National Government. He returned shortly afterwards, adopting the slogan "first internal pacification, then external resistance". However, this policy of avoiding a frontal war against the Japanese was widely unpopular. In 1932, while Chiang was seeking first to defeat the Communists, Japan launched an advance on Shanghai and bombarded Nanjing. This disrupted Chiang's offensives against the Communists for a time, although it was the northern factions of Hu Hanmin's Guangzhou (Canton) government (notably the 19th Route Army) that primarily led the offensive against the Japanese during this skirmish. Brought into the Nationalist army immediately after the battle, the 19th Route Army's career under Chiang would be cut short after it was disbanded for demonstrating socialist tendencies.
In December 1936, Chiang flew to Xi'an to coordinate a major assault on the Red Army and the Communist Republic that had retreated into Yan'an. However, Chiang's allied commander Zhang Xueliang, whose forces were used in his attack and whose homeland of Manchuria had been recently invaded by the Japanese, did not support the attack on the Communists. On December 12, Zhang and several other Nationalist generals kidnapped Chiang for two weeks in what is known as the Xi'an Incident. They forced Chiang into making a "Second United Front" with the Communists against Japan. After releasing Chiang and returning to Nanjing with him, Zhang was placed under house arrest and the generals who had assisted him were executed. Chiang's commitment to the Second United Front was nominal at best, and it was all but broken up in 1941.
Second Sino-Japanese War: 1937–1945.
The Second Sino-Japanese War broke out in July 1937, and in August of that year Chiang sent of his best-trained and equipped soldiers to defend Shanghai. With over 200,000 Chinese casualties, Chiang lost the political cream of his Whampoa-trained officers. Though Chiang lost militarily, the battle dispelled Japanese claims that it could conquer China in three months and demonstrated to the Western powers that the Chinese would continue the fight. By December, the capital city of Nanjing had fallen to the Japanese resulting in the Rape of Nanking. Chiang moved the government inland, first to Wuhan and later to Chongqing.
Having lost most of China's economic and industrial centers, Chiang withdrew into the hinterlands, stretching the Japanese supply lines and bogging down Japanese soldiers in the vast Chinese interior. As part of a policy of protracted resistance, Chiang authorized the use of scorched earth tactics, resulting in many civilian deaths. During the Nationalists' retreat from Nanjing, the dams around the city were deliberately destroyed by the Nationalist army in order to delay the Japanese advance, killing 500,000 people in the subsequent 1938 Yellow River flood.
After heavy fighting, the Japanese occupied Wuhan in the fall of 1938 and the Nationalists retreated farther inland, to Chongqing. While en route to Chongqing, the Nationalist army intentionally started the "fire of Changsha", as a part of the scorched earth policy. The fire destroyed much of the city, killed twenty thousand civilians, and left hundreds of thousands of people homeless. Due to an organizational error (it was claimed), the fire was begun without any warning to the residents of the city. The Nationalists eventually blamed three local commanders for the fire and executed them. Newspapers across China blamed the fire on (non-KMT) arsonists, but the blaze contributed to a nationwide loss of support for the KMT.
In 1939 Muslim leaders Isa Yusuf Alptekin and Ma Fuliang were sent by Chiang to several Middle eastern countries, including Egypt, Turkey, and Syria, to gain support for the Chinese War against Japan, and to express his support for Muslims.
The Japanese, controlling the puppet-state of Manchukuo and much of China's eastern seaboard, appointed Wang Jingwei as a Quisling-ruler of the occupied Chinese territories around Nanjing. Wang named himself President of the Executive Yuan and Chairman of the National Government (not the same 'National Government' as Chiang's), and led a surprisingly large minority of anti-Chiang/anti-Communist Chinese against his old comrades. He died in 1944, within a year of the end of World War II.
In 1942 Generalissimo Chiang Kai-shek personally went on tour in northwestern China in Xinjiang, Gansu, Ningxia, Shaanxi, and Qinghai, where he met both Muslim Generals Ma Buqing and Ma Bufang. He also met the Muslim Generals Ma Hongbin and Ma Hongkui separately.
A border crisis erupted with Tibet in 1942. Under orders from Chiang Kai-shek, Ma Bufang repaired Yushu airport to prevent Tibetan separatists from seeking independence. Chiang also ordered Ma Bufang to put his Muslim soldiers on alert for an invasion of Tibet in 1942. Ma Bufang complied and moved several thousand troops to the border with Tibet. Chiang also threatened the Tibetans with aerial bombardment if they worked with the Japanese. Ma Bufang attacked the Tibetan Buddhist Tsang monastery in 1941. He also constantly attacked the Labrang monastery.
With the attack on Pearl Harbor and the opening of the Pacific War, China became one of the Allied Powers. During and after World War II, Chiang and his American-educated wife Soong May-ling, known in the United States as "Madame Chiang", held the support of the United States' China Lobby, which saw in them the hope of a Christian and democratic China. Chiang was even named the Supreme Commander of Allied forces in the China war zone. He was created a Knight Grand Cross of the Order of the Bath by King George VI of the United Kingdom in 1942.
General Joseph Stilwell, an American military adviser to Chiang during World War II, strongly criticized Chiang and his generals for what he saw as their incompetence and corruption. In 1944, the United States Army Air Corps commenced Operation Matterhorn in order to bomb Japan's steel industry from bases to be constructed in mainland China. This was meant to fulfill President Roosevelt's promise to Chiang Kai-shek to begin bombing operations against Japan by November 1944. However, Chiang Kai-shek's subordinates refused to take airbase construction seriously until enough capital had been delivered to permit embezzlement on a massive scale. Stilwell estimated that at least half of the $100 million spent on construction of airbases was embezzled by Nationalist party officials.
Relationship with the United States and Soviet Union.
Chiang played off the Soviets and Americans against each other during the war. He first told the Americans that they would be welcome in talks between the Soviet Union and China then secretly told the Soviets that the Americans were unimportant and that their opinions would not be considered. Chiang also used American support and military power in China against the ambitions of the Soviet Union to dominate the talks, stopping the Soviets from taking full advantage of the situation in China with the threat of American military action against the Soviets.
Refusal of French Indochina.
Franklin D. Roosevelt, through General Stilwell, privately made it clear that they preferred that the French not reacquire French Indochina (modern day Vietnam, Cambodia, and Laos) after the war was over. Roosevelt offered Chiang control of all of Indochina. It was said that Chiang replied: "Under no circumstances!"
After the war, 200,000 Chinese troops under General Lu Han were sent by Chiang Kai-shek to northern Indochina (north of the 16th parallel) to accept the surrender of Japanese occupying forces there, and remained in Indochina until 1946, when the French returned. The Chinese used the VNQDD, the Vietnamese branch of the Chinese Kuomintang, to increase their influence in Indochina and to put pressure on their opponents. Chiang Kai-shek threatened the French with war in response to maneuvering by the French and Ho Chi Minh's forces against each other, forcing them to come to a peace agreement. In February 1946 he also forced the French to surrender all of their concessions in China and to renounce their extraterritorial privileges in exchange for the Chinese withdrawing from northern Indochina and allowing French troops to reoccupy the region. Following France's agreement to these demands, the withdrawal of Chinese troops began in March 1946.
Ryukyus.
During the Cairo Conference in 1943, Roosevelt asked Chiang whether China would like to claim the Ryukyu Islands from Japan in addition to retaking Taiwan, the Pescadores and Manchuria. Chiang said he was in favor of an international presence on the islands. However, the U.S. became the sole protector of the Ryukyus in 1945 and reverted control to the Japanese in 1972.
Losing Mainland China.
Treatment and use of Japanese soldiers.
In 1945, when Japan surrendered, Chiang's Chongqing government was ill-equipped and ill-prepared to reassert its authority in formerly Japanese-occupied China, and asked the Japanese to postpone their surrender until Kuomintang (KMT) authority could arrive to take over. This was an unpopular move among a population that, for many, had spent more than a decade under often brutal foreign occupation. American troops and weapons soon bolstered KMT forces, allowing them to reclaim cities. The countryside, however, remained largely under Communist control.
For over a year after the Japanese surrender, rumours circulated throughout China that the Japanese had entered into a secret agreement with Chiang, in which the Japanese would assist the Nationalists in fighting the Communists in exchange for the protection of Japanese persons and property there. Many top nationalist generals, including Chiang, had studied and trained in Japan before the Nationalists had returned to the mainland in the 1920s, and maintained close personal friendships with top Japanese officers. The Japanese general in charge of all forces in China, General Okamura, had personally trained officers who later became generals in Chiang's staff. Reportedly, General Okamura, before surrendering command of all Japanese military forces in Nanjing, offered Chiang control of all 1.5 million Japanese military and civilian support staff then present in China. Reportedly, Chiang seriously considered accepting this offer, but declined only in the knowledge that the United States would certainly be outraged by the gesture. Even so, armed Japanese troops remained in China well into 1947, with some noncommissioned officers finding their way into the Nationalist officer corps. That the Japanese in China came to regard Chiang as a magnanimous figure to whom many Japanese owed their lives and livelihoods was a fact attested by both Nationalist and Communist sources.
Conditions during the Chinese Civil War.
Following the war, the United States encouraged peace talks between Chiang and Communist leader Mao Zedong in Chongqing. Due to concerns about widespread and well-documented corruption in Chiang's government throughout his rule, the U.S. government limited aid to Chiang for much of the period of 1946 to 1948, in the midst of fighting against the People's Liberation Army led by Mao Zedong. Alleged infiltration of the U.S. government by Chinese Communist agents may have also played a role in the suspension of American aid.
Chiang's right-hand man, the secret police Chief Dai Li, was both anti-American and anti-Communist. Dai ordered Kuomintang agents to spy on American officers. Earlier, Dai had been involved with the Blue Shirts Society, a fascist-inspired paramilitary group within the Kuomintang, which wanted to expel Western and Japanese imperialists, crush the Communists, and eliminate feudalism. Dai Li died in a plane crash, which was suspected to be an assassination orchestrated by Chiang.
Though Chiang had achieved status abroad as a world leader, his government deteriorated as the result of corruption and inflation. In his diary on June 1948, Chiang wrote that the KMT had failed, not because of external enemies but because of rot from within. The war had severely weakened the Nationalists, while the Communists were strengthened by their popular land-reform policies, and by a rural population that supported and trusted them. The Nationalists initially had superiority in arms and men, but their lack of popularity, infiltration by Communist agents, low morale, and disorganization soon allowed the Communists to gain the upper hand in the civil war.
Competition with Li Zongren.
A new Constitution was promulgated in 1947, and Chiang was formally elected by the National Assembly as the first term President of the Republic of China on May 20, 1948. This marked the beginning of what was termed the "democratic constitutional government" period by the KMT political orthodoxy, but the Communists refused to recognize the new Constitution, and its government, as legitimate. Chiang resigned as President on January 21, 1949, as KMT forces suffered bitter losses and defections to the Communists. After Chiang's resignation the vice-president of the ROC, Li Zongren, became China's president.
Shortly after Chiang's resignation the Communists halted their advances and attempted to negotiate the virtual surrender of the ROC. Li attempted to negotiate milder terms that would have ended the civil war, but without success. When it became clear that Li was unlikely to accept Mao's terms, the Communists issued an ultimatum in April 1949, warning that they would resume their attacks if Li did not agree within five days. Li refused.
Li's attempts to carry out his policies faced varying degrees of opposition from Chiang's supporters, and were generally unsuccessful. Chiang especially antagonized Li by taking possession of (and moving to Taiwan) US$200 million of gold and US dollars belonging to the central government that Li desperately needed to cover the government's soaring expenses. When the Communists captured the Nationalist capital of Nanjing in April 1949, Li refused to accompany the central government as it fled to Guangdong, instead expressing his dissatisfaction with Chiang by retiring to Guangxi.
The former warlord Yan Xishan, who had fled to Nanjing only one month before, quickly insinuated himself within the Li-Chiang rivalry, attempting to have Li and Chiang reconcile their differences in the effort to resist the Communists. At Chiang's request Yan visited Li in order to convince Li not to withdraw from public life. Yan broke down in tears while talking of the loss of his home province of Shanxi to the Communists, and warned Li that the Nationalist cause was doomed unless Li went to Guangzhou. Li agreed to return under the condition that Chiang surrender most of the gold and US dollars in his possession that belonged to the central government, and that Chiang stop overriding Li's authority. After Yan communicated these demands and Chiang agreed to comply with them, Li departed for Guangdong.
In Guangdong, Li attempted to create a new government composed of both Chiang supporters and those opposed to Chiang. Li's first choice of premier was Chu Cheng, a veteran member of the Kuomintang who had been virtually driven into exile due to his strong opposition to Chiang. After the Legislative Yuan rejected Chu, Li was obliged to choose Yan Xishan instead. By this time Yan was well known for his adaptability, and Chiang welcomed his appointment.
Conflict between Chiang and Li persisted. Although he had agreed to do so as a prerequisite of Li's return, Chiang refused to surrender more than a fraction of the wealth that he had sent to Taiwan. Without being backed by gold or foreign currency, the money issued by Li and Yan quickly declined in value until it became virtually worthless.
Although he did not hold a formal executive position in the government, Chiang continued to issue orders to the army, and many officers continued to obey Chiang rather than Li. The inability of Li to coordinate KMT military forces led him to put into effect a plan of defense that he had contemplated in 1948. Instead of attempting to defend all of southern China, Li ordered what remained of the Nationalist armies to withdraw to Guangxi and Guangdong, hoping that he could concentrate all available defenses on this smaller, and more easily defensible, area. The object of Li's strategy was to maintain a foothold on the Chinese mainland in the hope that the United States would eventually be compelled to enter the war in China on the Nationalist side.
Final Communist advance.
Chiang opposed Li's plan of defense because it would have placed most of the troops still loyal to Chiang under the control of Li and Chiang's other opponents in the central government. To overcome Chiang's intransigence Li began ousting Chiang's supporters within the central government. Yan Xishan continued in his attempts to work with both sides, creating the impression among Li's supporters that he was a "stooge" of Chiang, while those who supported Chiang began to bitterly resent Yan for his willingness to work with Li. Because of the rivalry between Chiang and Li, Chiang refused to allow Nationalist troops loyal to him to aid in the defense of Guangxi and Guangdong, with the result that Communist forces occupied Guangdong in October 1949.
After Guangdong fell to the Communists, Chiang relocated the government to Chongqing, while Li effectively surrendered his powers and flew to New York for treatment of his chronic duodenum illness at the Hospital of Columbia University. Li visited the President of the United States, Harry S. Truman, and denounced Chiang as a dictator and an usurper. Li vowed that he would "return to crush" Chiang once he returned to China. Li remained in exile, and did not return to Taiwan.
In the early morning of December 10, 1949, Communist troops laid siege to Chengdu, the last KMT-controlled city in mainland China, where Chiang Kai-shek and his son Chiang Ching-kuo directed the defense at the Chengdu Central Military Academy. Chiang Kai-shek, father and son, sang the Republic of China National Anthem while leaving the Academy all the way to the airfield. The aircraft "May-ling" evacuated them to Taiwan on the same day. Chiang Kai-shek would never return to the mainland.
Chiang did not formally re-assume the presidency until March 1, 1950. On January 1952, Chiang commanded the Control Yuan, now in Taiwan, to impeach Li in the "Case of Li Zongren's Failure to carry out Duties due to Illegal Conduct" (李宗仁違法失職案). Chiang officially relieved Li of the position as vice-president in the National Assembly on March 1954.
Presidency in Taiwan: 1950–1975.
Preparations to retake the mainland.
Chiang moved the government to Taipei, Taiwan, where he formally resumed duties as President of the Republic of China on March 1, 1950. Chiang was reelected by the National Assembly to be the President of the Republic of China (ROC) on May 20, 1954, and again in 1960, 1966, and 1972. He continued to claim sovereignty over all of China, including the territories held by his government and the People's Republic, as well as territory the latter ceded to foreign governments, such as Tuva and Outer Mongolia. In the context of the Cold War, most of the Western world recognized this position and the ROC represented China in the United Nations and other international organizations until the 1970s.
During his presidency in Taiwan, Chiang continued to prepare to take back mainland China. He developed the ROC army to prepare for an invasion of the mainland, and to defend Taiwan in case of an attack by the Communist forces. He also financed armed groups in mainland China, such as Muslim soldiers of the ROC Army left in Yunnan under Li Mi, to continue to fight. It was only in the 1980s that these troops were airlifted to Taiwan. He promoted the Uyghur Yulbars Khan to Governor during the Kuomintang Islamic Insurgency in China (1950–1958) for resisting the Communists, even though the government had already evacuated to Taiwan. He planned an invasion of the mainland in 1962. In the 1950s Chiang's airplanes dropped supplies to Kuomintang Muslim insurgents in Amdo.
Dictatorships.
Despite the democratic constitution, the government under Chiang was a one-party state, consisting almost completely of mainlanders; the "Temporary Provisions Effective During the Period of Communist Rebellion" greatly enhanced executive powers, and the goal of retaking mainland China allowed the KMT to maintain a monopoly on power and the prohibition of opposition parties. The government's official line for these martial law provisions stemmed from the claim that emergency provisions were necessary, since the Communists and Kuomintang (KMT) were still in a state of war. Seeking to promote Chinese nationalism, Chiang's government actively ignored and suppressed local cultural expression, even forbidding the use of local languages in mass media broadcasts or during class sessions.
The first decades after the Nationalists moved the seat of government to the province of Taiwan are associated with the organized effort to resist Communism known as the "White Terror", during which about 140,000 Taiwanese were imprisoned for their real or perceived opposition to the Kuomintang. Most of those prosecuted were labeled by the Kuomintang as "bandit spies" (匪諜), meaning spies for Chinese Communists, and punished as such.
Under Chiang, the government recognized limited civil and economic freedoms, property rights (personal and intellectual) and other liberties. Despite these restrictions, free debate within the confines of the legislature was permitted. Under the pretext that new elections could not be held in Communist-occupied constituencies, the National Assembly, Legislative Yuan, and Control Yuan members held their posts indefinitely. The Temporary Provisions also allowed Chiang to remain as president beyond the two-term limit in the Constitution. He was reelected by the National Assembly as president four times—doing so in 1954, 1960, 1966, and 1972.
Believing that corruption and a lack of morals were key reasons that the KMT lost mainland China to the Communists, Chiang attempted to purge corruption by dismissing members of the KMT accused of graft. Some major figures in the previous mainland China government, such as H. H. Kung and T. V. Soong, exiled themselves to the United States. Though politically authoritarian and, to some extent, dominated by government-owned industries, Chiang's new Taiwanese state also encouraged economic development, especially in the export sector. A popular sweeping Land Reform Act, as well as American foreign aid during the 1950s, laid the foundation for Taiwan's economic success, becoming one of the Four Asian Tigers.
After Chiang's death, the next president, Chiang's son, Chiang Ching-kuo, and Chiang Ching-kuo's successor, Lee Teng-hui a native Taiwanese, would, in the 1980s and 1990s, increase native Taiwanese representation in the government and loosen the many authoritarian controls of the early era of ROC control in Taiwan.
Relationships with foreign governments.
Japan.
In 1971, shortly after he had switched his country's diplomatic recognition from the Republic of China to the People's Republic of China, the Australian Prime Minister, Gough Whitlam, visited Japan. After meeting with the Japanese Prime Minister, Eisaku Sato, Whitlam observed that the reason Japan at that time was hesitant to withdraw recognition from the Nationalist government was "the presence of a treaty between the Japanese government and that of Chiang Kai-shek". Sato explained that the continued recognition of Japan towards the Nationalist government was due largely to the personal relationship that various members of the Japanese government felt towards Chiang. This relationship was rooted largely in the generous and lenient treatment of Japanese prisoners-of-war by the Nationalist government in the years immediately following the Japanese surrender in 1945, and was felt especially strongly as a bond of personal obligation by the most senior members then in power.
Although Japan eventually recognized the People's Republic in 1972, shortly after Kakuei Tanaka succeeded Sato as Prime Minister of Japan, the memory of this relationship was strong enough to be reported by "The New York Times" (April 15, 1978) as a significant factor inhibiting trade between Japan and the mainland. There is speculation that a clash between Communist forces and a Japanese warship in 1978 was caused by Chinese anger after Prime Minister Takeo Fukuda attended Chiang's funeral. Historically, Japanese attempts to normalize their relationship with the People's Republic were met with accusations of ingratitude in Taiwan.
United States.
Chiang was suspicious that covert operatives of the United States plotted a coup against him. In 1950, Chiang Ching-kuo became director of the secret police (Bureau of Investigation and Statistics), which he remained until 1965. Chiang was suspicious of politicians who were overly friendly to the United States, and considered them his enemies. In 1953, seven days after surviving an assassination attempt, Wu Kuo-chen lost his position as governor of Taiwan Province to Chiang Ching-kuo. After fleeing to United States the same year, he became a vocal critic of Chiang's family and government.
Chiang Ching-kuo, educated in the Soviet Union, initiated Soviet-style military organization in the Republic of China Military. He reorganized and Sovietized the political officer corps, and propagated Kuomintang ideology throughout the military. Sun Li-jen, who was educated at the American Virginia Military Institute, was opposed to this.
Chiang Ching-kuo orchestrated the controversial court-martial and arrest of General Sun Li-jen in August 1955, for plotting a coup d'état with the American CIA against his father Chiang Kai-shek and the Kuomintang. The CIA allegedly wanted to help Sun take control of Taiwan and declare its independence.
Death.
In 1975, 26 years after Chiang came to Taiwan, he died in Taipei at the age of 87. He had suffered a heart attack and pneumonia in the foregoing months and died from renal failure aggravated with advanced cardiac malfunction on April 5.
A month of mourning was declared. Chinese music composer Hwang Yau-tai wrote the Chiang Kai-shek Memorial Song. In mainland China, however, Chiang's death was met with little apparent mourning and Communist state-run newspapers gave the brief headline "Chiang Kai-shek Has Died." Chiang's body was put in a copper coffin and temporarily interred at his favorite residence in Cihu, Daxi, Taoyuan. When his son Chiang Ching-kuo died in 1988, he was entombed in a separate mausoleum in nearby Touliao (頭寮). The hope was to have both buried at their birthplace in Fenghua if and when it was possible. In 2004, Chiang Fang-liang, the widow of Chiang Ching-kuo, asked that both father and son be buried at Wuzhi Mountain Military Cemetery in Xizhi, Taipei County (now New Taipei City). Chiang's ultimate funeral ceremony became a political battle between the wishes of the state and the wishes of his family.
Chiang was succeeded as President by Vice President Yen Chia-kan and as Kuomintang party leader by his son Chiang Ching-kuo, who retired Chiang Kai-shek's title of Director-General and instead assumed the position of Chairman. Yen's presidency was interim; Chiang Ching-kuo, who was the Premier, became President after Yen's term ended three years later.
Cult of personality.
Chiang's portrait hung over the gate of the Forbidden City before Mao's portrait was set up in its place. People also put portraits of Chiang in their homes and in public on the streets. Until recently, it was a widespread practice for Taiwanese people to hang portraits of Chiang in their homes.
Chiang was popular among many people and dressed in plain, simple clothes, unlike contemporary Chinese warlords who dressed extravagantly.
Quotes from the Quran and Hadith were used by Muslims in the Kuomintang-controlled Muslim publication, the "Yuehua", to justify Chiang Kai-shek's rule over China.
When the Muslim General and Warlord Ma Lin was interviewed, Ma Lin was described as having "high admiration and unwavering loyalty to Chiang Kai-shek".
In the Philippines, a school was erected in 1939 in his honour. Today, Chiang Kai Shek College is the largest educational institution for the Chinoy community in the country.
Philosophy.
The Kuomintang used traditional Chinese religious ceremonies, and promoted Martyrdom in Chinese culture. Kuomintang ideology promoted the view that the souls of Party martyrs who died fighting for the Kuomintang, the revolution, and the party founder Dr. Sun Yatsen were sent to heaven. Chiang Kai-shek believed that these martyrs witnessed events on earth from heaven.
When the Northern Expedition was complete, Kuomintang Generals led by Chiang Kai-shek paid tribute to Dr. Sun's soul in heaven with a sacrificial ceremony at the Xiangshan Temple in Beijing in July 1928. Among the Kuomintang Generals present were the Muslim Generals Bai Chongxi and Ma Fuxiang.
Chiang Kai-shek considered both the Han Chinese and all the minority peoples of China, the Five Races Under One Union, as descendants of Huangdi, the Yellow Emperor and semi mythical founder of the Chinese nation, and belonging to the Chinese Nation Zhonghua Minzu and he introduced this into Kuomintang ideology, which was propagated into the educational system of the Republic of China.
Contemporary public perception.
Chiang's legacy has been the target of heated debates because of the different views held about him. For some, Chiang was a national hero who led the victorious Northern Expedition against the Beiyang Warlords in 1927, achieving Chinese unification, and who subsequently led China to ultimate victory against Japan in 1945. Some blamed him for not doing enough against the Japanese forces in the lead-up to, and during, the Second Sino-Japanese War, preferring to withhold his armies for the fight against the Communists, or merely waiting and hoping that the United States would get involved. Some also see him as a champion of anti-Communism, being a key figure during the formative years of the World Anti-Communist League. During the Cold War, he was also seen as the leader who led Free China and the bulwark against a possible Communist invasion. However, Chiang presided over purges, political authoritarianism, and graft during his tenure in mainland China, and ruled throughout a period of imposed martial law. His governments were accused of being corrupt even before he even took power in 1928. He also allied with known criminals like Du Yuesheng for political and financial gains. Some opponents charge that Chiang's efforts in developing Taiwan were mostly to make the island a strong base from which to one day return to mainland China, and that Chiang had little regard for the long-term prosperity and well-being of the Taiwanese people.
Today, Chiang's popularity in Taiwan is divided along political lines, enjoying greater support among Kuomintang supporters. He is generally unpopular among Democratic Progressive Party (DPP) voters and supporters. In sharp contrast to his son, Chiang Ching-kuo, and to Sun Yat-sen, his memory is rarely invoked by current political parties, including the Kuomintang.
In the United States and Europe, Chiang was often perceived negatively as the one who lost China to the Communists. His constant demands for Western support and funding also earned him the nickname of "General Cash-My-Check". In the West he has been criticized for his poor military skills. He had a record of issuing unrealistic orders and persistently attempting to fight unwinnable battles, leading to the loss of his best troops.
In recent years, there has been an attempt to find a more moderate interpretation of Chiang. Chiang is now increasingly perceived as a man simply overwhelmed by the events in China, having to fight simultaneously Communists, Japanese and provincial warlords while having to reconstruct and unify the country. His sincere, albeit often unsuccessful attempts to build a more powerful nation have been noted by scholars such as Jonathan Fenby and Rana Mitter. Mitter has observed that, ironically, today's China is closer to Chiang's vision than to Mao Zedong's. He argues that the Communists, since the 1980s, have essentially created the state envisioned by Chiang in the 1930s. Mitter concludes by writing that "one can imagine Chiang Kai-shek's ghost wandering round China today nodding in approval, while Mao's ghost follows behind him, moaning at the destruction of his vision". Liang Shuming opined that Chiang Kai-shek's "greatest contribution was to make the CCP successful. If he had been a bit more trustworthy, if his character was somewhat better, the CCP would have been unable to beat him".
Formosa Betrayed, one of the few American movies concerning the process of democratization in Taiwan, depicts Chiang Kai-shek as a brutal dictator, responsible for the execution of thousands native Taiwanese during the days following the 228 Incident.
Names.
Various traditional names of Chiang Kai-shek 
Like many other Chinese historical figures, Chiang used several names throughout his life. That inscribed in the genealogical records of his family is Jiang Zhoutai (). This so-called "register name" (譜名) is the one under which his extended relatives knew him, and the one he used in formal occasions, such as when he got married. In deference to tradition, family members did not use the register name in conversation with people outside of the family. In fact, the concept of real or original name is not as clear-cut in China as it is in the Western world.
In honor of tradition, Chinese families waited a number of years before officially naming their offspring. In the meantime, they used a "milk name" (乳名), given to the infant shortly after his birth and known only to the close family. Thus, the actual name that Chiang received at birth was Jiang Ruiyuan ().
In 1903, the 16-year-old Chiang went to Ningbo to be a student, and he chose a "school name" (學名). This was actually the formal name of a person, used by older people to address him, and the one he would use the most in the first decades of his life (as the person grew older, younger generations would have to use one of the courtesy names instead). (Colloquially, the school name is called "big name" (大名), whereas the "milk name" is known as the "small name" (小名).) The school name that Chiang chose for himself was Zhiqing (; means "purity of intentions"). For the next fifteen years or so, Chiang was known as Jiang Zhiqing (Wade-Giles: Chiang Chi-ching). This is the name under which Sun Yat-sen knew him when Chiang joined the republicans in Guangzhou in the 1910s.
In 1912, when Jiang Zhiqing was in Japan, he started to use the name Chiang Kai-shek (Chinese: 蔣介石; Pinyin:   ; Wade-Giles: Chiang Chieh-shih) as a pen name for the articles that he published in a Chinese magazine he founded ("Voice of the Army" (Chinese: 軍聲). ("Jieshi" is the Pinyin romanization of the name, based on Mandarin, but the common romanized rendering is "Kai-shek" which is in Cantonese romanization. As the republicans were based in Canton (a Cantonese speaking area, now commonly known as Guangzhou), Chiang became known by Westerners under the Cantonese romanization of his courtesy name, while the family name as known in English seems to be the Mandarin pronunciation of his Chinese family name, transliterated in Wade-Giles)
"Kai-shek" soon became his courtesy name (字). Some think the name was chosen from the classic Chinese book the "I Ching"; others note that the first character of his courtesy name is also the first character of the courtesy name of his brother and other male relatives on the same generation line, while the second character of his courtesy name "shi" (石—meaning "stone") suggests the second character of his "register name" "tai" (泰—the famous Mount Tai of China). Courtesy names in China often bore a connection with the personal name of the person. As the courtesy name is the name used by people of the same generation to address the person, Chiang soon became known under this new name.
Sometime in 1917 or 1918, as Chiang became close to Sun Yat-sen, he changed his name from Jiang Zhiqing to Jiang Zhongzheng (). By adopting the name Chung-cheng ("central uprightness"), he was choosing a name very similar to the name of Sun Yat-sen, who was (and still is) known among Chinese as Zhongshan (中山—meaning "central mountain"), thus establishing a link between the two. The meaning of uprightness, rectitude, or orthodoxy, implied by his name, also positioned him as the legitimate heir of Sun Yat-sen and his ideas. Not surprisingly, the Chinese Communists always rejected the use of this name and it is not well known in mainland China. However, it was readily accepted by members of the Chinese Nationalist Party and is the name under which Chiang Kai-shek is still commonly known in Taiwan. Often the name is shortened to "Chung-cheng" only ("Zhongzheng" in Pinyin). For many years passengers arriving at the Chiang Kai-shek International Airport were greeted by signs in Chinese welcoming them to the "Chung Cheng International Airport". Similarly, the monument erected to Chiang's memory in Taipei, known in English as Chiang Kai-shek Memorial Hall, was literally named "Chung Cheng Memorial Hall" in Chinese. In Singapore, Chung Cheng High School was named after him.
His name is also written in Taiwan as "The Late President Lord Chiang" (先總統　蔣公), where the one-character-wide space known as nuo tai shows respect; this practice has lost some popularity. However, he is still known as "Lord Chiang" (蔣公) (without the title or space), along with the name "Chiang Chung-cheng", in Taiwan.
Wives.
In an arranged marriage, Chiang was married to a fellow villager named Mao Fumei. While married to Mao, Chiang adopted two concubines (concubinage was still a common practice for well-to-do, non-Christian males in China): he married Yao Yecheng (姚冶誠, 1889–1972) in 1912 and Chen Jieru (陳潔如, 1906–1971) in December 1921. While he was still living in Shanghai, Chiang and Yao adopted a son, Wei-kuo. Chen adopted a daughter in 1924, named Yaoguang (瑤光), who later adopted her mother's surname. Chen's autobiography refuted the idea that she was a concubine. Chen claiming that, by the time she married Chiang, he had already divorced Mao, and that Chen was therefore his wife. Chiang and Mao had a son, Ching-kuo.
According to the memoirs of Chen Jieru, Chiang's second wife, she contracted gonorrhea from Chiang soon after their marriage. He told her that he acquired this disease after separating from his first wife and living with his concubine Yao Yecheng, as well as with many other women he consorted with. His doctor explained to her that Chiang had sex with her before completing his treatment for the disease. As a result, both Chiang and Ch'en Chieh-ju believed they had become sterile, which would explain why he had only one child, by his first wife; however, a purported miscarriage by Soong May-ling in August 1928 would, if it actually occurred, cast serious doubt on whether this was true.
Relationships with ethnic minorities, religions and their leaders.
Chiang personally dealt extensively with religions and power figures in China during his regime.
Relationship with Muslims.
Chiang Kai-shek developed relationships with other Generals. Chiang became a sworn brother of the Muslim General Ma Fuxiang and appointed him to high ranking positions. Chiang addressed Ma Fuxiang's son Ma Hongkui as Shao Yun Shixiong Ma Fuxiang attended national leadership conferences with Chiang during Battles against Japan. Ma Hongkui was eventually scapegoated for the failure of the Ningxia Campaign against the Communists, so he moved to the US instead of remaining in Taiwan with Chiang.
When Chiang became President of China after the Northern Expedition, he carved out Ningxia and Qinghai out of Gansu province, and appointed Muslim Generals as Military Governors of all three provinces: Ma Hongkui, Ma Hongbin, and Ma Qi. The three Muslim governors, known as Xibei San Ma (lit. "the three Mas of the Northwest"), controlled armies composed entirely of Muslims. Chiang called on the three and their suboordinates to wage war against the Soviet peoples, Tibetans, Communists, and the Japanese. Chiang continued to appoint Muslims as Governors of the three provinces, including Ma Lin and Ma Fushou. Chiang's appointments, the first time that Muslims had been appointed as governors of Gansu, increased the prestige of Muslim officials in northwestern China. The armies raised by this "Ma Clique", most notably their Muslim cavalry, were incorporated into the KMT army. Chiang appointed a Muslim General, Bai Chongxi, as the Minister of National Defence of the Republic of China, which controlled the ROC military.
Chiang also supported the Muslim General Ma Zhongying, whom he had trained at Whampoa Military Academy during the Kumul Rebellion, in a Jihad against Jin Shuren, Sheng Shicai, and the Soviet Union during the Soviet Invasion of Xinjiang. Chiang designated Ma's Muslim army as the 36th Division (National Revolutionary Army) and gave his troops Kuomintang flags and uniforms. Chiang then supported Muslim General Ma Hushan against Sheng Shicai and the Soviet Union in the Xinjiang War (1937). All Muslim Generals commissioned by Chiang in the National Revolutionary Army paid alleigance to him. Several, like Ma Shaowu and Ma Hushan were loyal to Chiang and Kuomintang hardliners.
The Ili Rebellion and Pei-ta-shan Incident plagued relations with the Soviet Union during Chiang's rule and caused trouble with the Uyghurs. During the Ili Rebellion and Peitashan incident, Chiang deployed Hui troops against Uyghur mobs in Turfan, and against Soviet Russian and Mongols at Peitashan.
During Chiang's rule, attacks on foreigners by Kuomintang forces flared up in several incidents. One of these was the Battle of Kashgar (1934) where a Muslim army loyal to the Kuomintang massacred 4,500 Uyghurs, and killed several British at the British consulate in Kashgar. The British were unable to retaliate.
Hu Songshan, a Muslim Imam, backed Chiang Kai-shek's regime and gave prayers for his government. ROC flags were saluted by Muslims in Ningxia during prayer along with exhortations to nationalism during Chiang's rule. Chiang sent Muslim students abroad to study at places like Al Azhar and Muslim schools throughout China taught loyalty to his regime.
The Yuehua, a Chinese Muslim publication, quoted the Quran and Hadith to justify submitting to Chiang Kai-shek as the leader of China, and as justification for Jihad in the war against Japan.
The Yihewani (Ikhwan al Muslimun a.k.a. Muslim brotherhood) was the predominant Muslim sect backed by the Chiang government during Chiang's regime. Other Muslim sects, like the Xidaotang and Sufi brotherhoods like Jahriyya and Khuffiya were also supported by his regime. The Chinese Muslim Association, a pro-Kuomintang and anti-Communist organization, was set up by Muslims working in his regime. Salafism attempted to gain a foothold in China during his regime, but the Yihewani and Hanafi Sunni Gedimu denounced the Salafis as radicals, engaged in fights against them, and declared them heretics, forcing the Salafis to form a separate sect. Ma Ching-chiang, a Muslim General, served as an advisor to Chiang Kai-shek. Ma Buqing was another Muslim General who fled to Taiwan along with Chiang. His government donated money to build the Taipei Grand Mosque on Taiwan.
Relationship with Buddhists and Christians.
Chiang had uneasy relations with the Tibetans. He fought against them in the Sino-Tibetan War, and he supported the Muslim General Ma Bufang in his war against Tibetan rebels in Qinghai. Chiang ordered Ma Bufang to prepare his Islamic army to invade Tibet several times, to deter Tibetan independence, and threatened them with aerial bombardment. After the war, Chiang appointed Ma Bufang as ambassador to Saudi Arabia.
Chiang incorporated Methodist values into the New Life Movement under the influence of his wife. Dancing and Western music were discouraged. In one incident, several youths splashed acid on people wearing Western clothing, although Chiang was not directly responsible for these incidents. Despite being a Methodist, he made reference to the Buddha in his diary, and encouraged the establishment of a Buddhist political party under Master Taixu.
Further reading.
</dl>
External links.
class="wikitable succession-box" style="margin:0.5em auto; font-size:95%;clear:both;"

</doc>
